focus_area,provider,url,title,source,published,relevance,date_added
dummy1,openai,https://dummy1.com,dummy1,OpenAI Blog,2025-12-01,dummy example 1,2025-12-20
dummy2,anthropic,https://dummy2.com,dummy2,arXiv,2025-12-15,dummy example 2,2025-12-21
reasoning_agent,openai,https://openai.com/index/gpt-5-2-codex,Introducing GPT-5.2-Codex,OpenAI blog,2025-12-18,"Official release of an agentic coding model emphasizing long-horizon workflows, reliable tool use, and token-efficient reasoning (practical test-time compute).",2025-12-24
reasoning_agent,openai,https://openai.com/index/introducing-gpt-5-2/,Introducing GPT-5.2,OpenAI blog,2025-12-11,Details GPT-5.2 “Thinking/Pro” modes and API reasoning controls—directly relevant to test-time compute allocation for reasoning.,2025-12-24
reasoning_agent,openai,https://blog.google/products/gemini/gemini-3/,Introducing Gemini 3: our most intelligent model that helps you bring any idea to life,Google Blog (Gemini/DeepMind),2025-11-18,"Announces Gemini 3 with “thinking”/Deep Think reasoning modes and agentic capabilities, relevant to inference-time reasoning/compute.",2025-12-24
reasoning_agent,openai,https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills,Equipping agents for the real world with Agent Skills,Anthropic Engineering,2025-10-16 (updated 2025-12-18),"Introduces (and updates) a modular skills system for tool-using agents—key infrastructure for agent planning, portability, and reliable execution.",2025-12-24
reasoning_agent,openai,https://github.com/anthropics/skills,anthropics/skills — Agent Skills repository (spec + examples),GitHub,recent,"Official repository containing the Agent Skills spec and example skills (instructions/scripts/resources) for composable, discoverable agent capabilities.",2025-12-24
reasoning_agent,openai,https://simonwillison.net/2025/Dec/19/agent-skills/,Agent Skills,Simon Willison’s blog,2025-12-19,Technical commentary highlighting implementation-relevant details and gaps in the Agent Skills open specification.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2512.02008,The Art of Scaling Test-Time Compute for Large Language Models,arXiv,2025-12-01,"Large-scale empirical comparison of test-time scaling strategies across models/datasets, offering guidance for practical TTS selection.",2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2512.02304,When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers,arXiv,2025-12-02,Systematic solver–verifier study (including cross-family verification) and introduction of “verifier gain” for test-time scaling.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2512.10313,EpiPlanAgent: Agentic Automated Epidemic Response Planning,arXiv,2025-12-11 (v2 2025-12-12),"Multi-agent LLM workflow for real planning/validation (decomposition, grounding, simulation), illustrating agentic planning in a high-stakes domain.",2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2512.17912,Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning,arXiv,2025-11-26,Agentic GraphRAG approach combining MCTS + RL for interactive stepwise reasoning over graphs—planning/search applied to reasoning tasks.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2511.17006,Budget-Aware Tool-Use Enables Effective Agent Scaling,arXiv,2025-11-21,Shows that tool-call budgets require explicit budget awareness; proposes budget-aware planning/verification (BATS) for scalable tool-using agents.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2511.17330,Agentic Program Verification,arXiv,2025-11-21,LLM agent that iteratively refines proofs via interaction with the Rocq/Coq theorem prover—tight tool-feedback loops for structured reasoning.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2511.02424,ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning,arXiv,2025-11-04,Hierarchical agent-tree decomposition and control flow for long-horizon planning beyond monolithic trajectories (ReAct-style baselines).,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2511.11373,MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism,arXiv,2025-11-14,RL framework that trains multi-agent solver/verifier/corrector pipelines efficiently—relevant to building strong multi-agent reasoning systems.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2511.00086,Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph,arXiv,2025-10-29,Formalizes compute-optimal test-time scaling as a graph optimization problem over roles/models/topologies in multi-LLM collaboration.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2510.24803,MASPRM: Multi-Agent System Process Reward Model,arXiv,2025-10-28,Introduces a process reward model for multi-agent transcripts to steer inference-time beam search/MCTS and spend compute more effectively.,2025-12-24
reasoning_agent,openai,https://github.com/milad1378yz/MASPRM,milad1378yz/MASPRM,GitHub,recent,Code to reproduce MASPRM and integrate process-reward-guided search into multi-agent reasoning pipelines.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2510.09988,Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey,arXiv,2025-10-11,Survey/taxonomy linking tree search mechanisms and reward formulation across test-time scaling and self-improving reasoning agents.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2510.20272,Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs,arXiv,2025-10-23,Empirical evidence that PRM-guided tree search can fail to beat best-of-N due to reward model depth/generalization issues—important for TTS design.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2510.18982,"Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality",arXiv,2025-10-21,"Theory for verifier-based test-time scaling analyzing generator coverage, verifier ROC, and sampling sub-optimality tradeoffs.",2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2510.14913,Budget-aware Test-time Scaling via Discriminative Verification,arXiv,2025-10-16,"Proposes discriminative verification as a compute-efficient alternative to generative verifiers, improving performance under fixed inference budgets.",2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2510.02611,On the Role of Temperature Sampling in Test-Time Scaling,arXiv,2025-10-02,Shows single-temperature best-of-K saturates and proposes multi-temperature scaling to enlarge the set of problems solved at test time.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2510.05746,ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems,arXiv,2025-10-07,Automatically discovers modular reasoning steps via tree search over code—treating CoT as a unit to optimize for generalizable multi-agent reasoning.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2510.09599,Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation,arXiv,2025-10-10,Uses test-time scaling prompting to synthesize diverse reasoning trajectories for finetuning—bridges inference-time scaling and training improvements.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2506.07976,Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction,arXiv,2025-06-09,"Argues agents can scale capability by extending interaction horizon (exploration/backtracking/replanning), complementing token-based test-time compute.",2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2503.24235,"What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models",arXiv,2025-03-31,"Comprehensive taxonomy/survey of test-time scaling techniques, applications (including agentic tasks), and evaluation dimensions.",2025-12-24
reasoning_agent,openai,https://github.com/testtimescaling/testtimescaling.github.io,testtimescaling/testtimescaling.github.io (companion repo to TTS survey),GitHub,2025-04-09,Companion repository with a structured map of TTS methods plus figures/tables and ongoing updates for tracking the field.,2025-12-24
reasoning_agent,openai,https://arxiv.org/abs/2501.19393,s1: Simple test-time scaling,arXiv,2025-01-31,Influential open baseline for test-time compute control (“budget forcing”) and small curated reasoning data to elicit stronger reasoning.,2025-12-24
reasoning_agent,openai,https://github.com/simplescaling/s1,simplescaling/s1,GitHub,recent,Open-source implementation and data for s1 and budget forcing—useful for reproducing and extending test-time scaling experiments.,2025-12-24
reasoning_agent,openai,https://github.com/ThreeSR/Awesome-Inference-Time-Scaling,ThreeSR/Awesome-Inference-Time-Scaling,GitHub,recent,"Curated, frequently updated reading list of inference/test-time scaling papers (verifiers, search, MCTS, PRMs), helpful for ongoing monitoring.",2025-12-24
reasoning_agent,anthropic,https://arxiv.org/abs/2501.12948,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,arXiv,"January 22, 2025","Landmark paper showing pure RL can elicit reasoning capabilities without SFT, achieving o1-level performance on math/code benchmarks.",2025-12-24
reasoning_agent,anthropic,https://github.com/deepseek-ai/DeepSeek-R1,DeepSeek-R1 GitHub Repository,GitHub,January 2025,Open-source release of DeepSeek's reasoning models including distilled versions from 1.5B to 70B parameters under MIT license.,2025-12-24
reasoning_agent,anthropic,https://openai.com/index/learning-to-reason-with-llms/,Learning to reason with LLMs,OpenAI,September 2024,OpenAI's foundational post on o1 reasoning model explaining chain-of-thought via reinforcement learning and test-time compute scaling.,2025-12-24
reasoning_agent,anthropic,https://openai.com/index/introducing-o3-and-o4-mini/,Introducing OpenAI o3 and o4-mini,OpenAI,April 2025,"Latest o-series reasoning models with agentic tool use, representing state-of-the-art in combined reasoning and tool capabilities.",2025-12-24
reasoning_agent,anthropic,https://arxiv.org/abs/2408.03314,Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters,arXiv,August 2024,Key Berkeley research showing test-time compute scaling can outperform 14x larger models on certain problems via compute-optimal strategies.,2025-12-24
reasoning_agent,anthropic,https://testtimescaling.github.io/,"What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models",Academic Survey,2025,"Comprehensive survey organizing TTS research across what to scale, how to scale, where to apply, and evaluation metrics.",2025-12-24
reasoning_agent,anthropic,https://www.anthropic.com/news/visible-extended-thinking,Claude's extended thinking,Anthropic,February 2025,Anthropic's approach to hybrid reasoning with configurable thinking budgets and visible thought process for Claude models.,2025-12-24
reasoning_agent,anthropic,https://www.anthropic.com/engineering/claude-think-tool,The 'think' tool: Enabling Claude to stop and think,Anthropic Engineering,2025,"New technique allowing Claude to pause during tool calling chains for deliberate reasoning, showing 54% improvement on agentic tasks.",2025-12-24
reasoning_agent,anthropic,https://developer.nvidia.com/blog/an-easy-introduction-to-llm-reasoning-ai-agents-and-test-time-scaling/,"An Easy Introduction to LLM Reasoning, AI Agents, and Test Time Scaling",NVIDIA Technical Blog,June 2025,"Comprehensive NVIDIA tutorial covering LLM agents, reasoning patterns, and test-time compute techniques like chain-of-thought and tree-of-thought.",2025-12-24
reasoning_agent,anthropic,https://huggingface.co/blog/Kseniase/testtimecompute,What is test-time compute and how to scale it?,Hugging Face Blog,recent,Practical guide covering 5 methods for scaling test-time compute including DeepSeek-R1's approach and reinforcement learning techniques.,2025-12-24
reasoning_agent,anthropic,https://blog.langchain.com/langchain-langgraph-1dot0/,LangChain and LangGraph Agent Frameworks Reach v1.0 Milestones,LangChain Blog,November 2025,"Major stable release of production-ready agent frameworks with durable execution, human-in-the-loop, and comprehensive memory.",2025-12-24
reasoning_agent,anthropic,https://blog.langchain.com/planning-agents/,Plan-and-Execute Agents,LangChain Blog,February 2024,LangGraph architectures for plan-and-execute style agents that separate planning from execution for improved multi-step reasoning.,2025-12-24
reasoning_agent,anthropic,https://ai.google.dev/gemini-api/docs/thinking,Gemini thinking | Gemini API,Google AI,December 2024,Official Gemini API documentation on thinking modes with configurable thinking budgets for Gemini 2.5/3 series models.,2025-12-24
reasoning_agent,anthropic,https://huggingface.co/Qwen/QwQ-32B,QwQ-32B: Embracing the Power of Reinforcement Learning,Hugging Face / Qwen,March 2025,Open-source 32B reasoning model achieving DeepSeek-R1-level performance with smaller compute requirements under Apache 2.0 license.,2025-12-24
reasoning_agent,anthropic,https://arxiv.org/abs/2305.10601,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,arXiv / NeurIPS 2023,December 2023,"Foundational paper on ToT prompting enabling LLMs to explore multiple reasoning paths with backtracking, achieving 74% on Game of 24 vs 4% for CoT.",2025-12-24
reasoning_agent,anthropic,https://arxiv.org/abs/2504.19678,From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review,arXiv,April 2025,"Comprehensive survey covering ~60 benchmarks, agent frameworks, and collaboration protocols (ACP, MCP, A2A) from 2019-2025.",2025-12-24
reasoning_agent,anthropic,https://arxiv.org/html/2504.09037v1,"A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems",arXiv,April 2025,"Survey organizing reasoning research across inference scaling, learning-to-reason, and single/multi-agent systems with unified framework.",2025-12-24
reasoning_agent,anthropic,https://arxiv.org/html/2502.04644v2,Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools,arXiv,February 2025,"Framework integrating Mind-Map memory, web search agents, and computational tools into reasoning chains for deep research tasks.",2025-12-24
reasoning_agent,anthropic,https://techcrunch.com/2024/12/20/openai-announces-new-o3-model/,OpenAI announces new o3 models,TechCrunch,"December 20, 2024","Coverage of o3 announcement with record benchmark scores: 87.5% on ARC-AGI, 96.7% on AIME, 71.7% on SWE-Bench Verified.",2025-12-24
reasoning_agent,anthropic,https://simonwillison.net/2024/Dec/19/gemini-thinking-mode/,Gemini 2.0 Flash 'Thinking mode',Simon Willison's Blog,"December 19, 2024",Technical deep-dive on Google's first reasoning model with practical examples and llm-gemini plugin implementation.,2025-12-24
reasoning_agent,anthropic,https://blog.langchain.com/top-5-langgraph-agents-in-production-2024/,Top 5 LangGraph Agents in Production 2024,LangChain Blog,January 2025,"Real-world production case studies from LinkedIn, Elastic, Replit showing controllable agent architectures with LangGraph.",2025-12-24
reasoning_agent,anthropic,https://techxplore.com/news/2025-12-ai-agents-debate-mathematical.html,AI agents debate their way to improved mathematical reasoning,Tech Xplore,December 2024,Research on Adaptive Heterogeneous Multi-Agent Debate framework achieving 4-6% higher accuracy and 30% fewer factual errors.,2025-12-24
reasoning_agent,anthropic,https://aclanthology.org/2024.acl-long.331/,Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?,ACL Anthology,August 2024,ACL 2024 paper finding single-agent LLMs with strong prompts can match multi-agent discussion performance on reasoning tasks.,2025-12-24
reasoning_agent,anthropic,https://www.datacamp.com/blog/gemini-2-0-flash-experimental,Gemini 2.0 Flash Thinking Experimental: A Guide With Examples,DataCamp,February 2025,Practical guide to Google's reasoning model with benchmarks showing 73.3% on AIME2024 vs 35.5% for base Flash model.,2025-12-24
reasoning_agent,anthropic,https://arxiv.org/abs/2502.05171,Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach,arXiv,February 2025,Novel architecture for test-time compute scaling using recurrent blocks for implicit reasoning in latent space.,2025-12-24
reasoning_agent,anthropic,https://blogs.nvidia.com/blog/ai-scaling-laws/,"How Scaling Laws Drive Smarter, More Powerful AI",NVIDIA Blog,May 2025,NVIDIA's perspective on test-time scaling enabling AI reasoning models that can require 100x compute for complex queries.,2025-12-24
reasoning_agent,anthropic,https://www.nature.com/articles/s41586-025-09422-z,DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning,Nature,September 2025,Peer-reviewed Nature publication validating DeepSeek-R1's RL-based reasoning approach and emergence of self-reflection capabilities.,2025-12-24
reasoning_and_planning,openai,https://www.anthropic.com/news/claude-opus-4-5,Introducing Claude Opus 4.5,Anthropic,2025-11-24,Frontier-model announcement emphasizing stronger coding/agent performance and improved reasoning—useful for tracking state-of-the-art reasoning model capabilities.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2512.00466,SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling,arXiv,2025-11-29,Selective compute allocation across subproblems to avoid uniform test-time scaling bottlenecks; directly targets diminishing returns in long-CoT reasoning.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2511.15738,"Extending Test-Time Scaling: A 3D Perspective with Context, Batch, and Turn",arXiv,2025-11-18,"Unifies context-length, parallel sampling, and iterative self-refinement as distinct scaling axes—highly relevant for planning/agent workflows.",2025-12-24
reasoning_and_planning,openai,https://www.microsoft.com/en-us/research/publication/towards-thinking-optimal-scaling-of-test-time-compute-for-llm-reasoning/,Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning,Microsoft Research,2025-02,Finds that excessively long CoTs can hurt performance and proposes “thinking-optimal” length distributions—useful for controlling inference-time reasoning effort.,2025-12-24
reasoning_and_planning,openai,https://www.microsoft.com/en-us/research/publication/putting-the-value-back-in-rl-better-test-time-scaling-by-unifying-llm-reasoners-with-verifiers/,Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers,Microsoft Research,2025-07,RL^V jointly trains a reasoner and generative verifier to enable more efficient verification-driven test-time scaling (parallel + sequential).,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2502.20379,Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers,arXiv,2025-02-27,Proposes scaling the number of verifiers as a new test-time compute dimension; useful for robust reasoning/planning pipelines beyond self-verification.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2510.06135,Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification,arXiv,2025-10-07,"Studies sequential+parallel scaling for deep search agents leveraging “asymmetric verification,” reporting large gains on agent benchmarks (e.g., BrowseComp/GAIA).",2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2504.16828,Process Reward Models That Think,arXiv,2025-04-23,ThinkPRM: generative CoT-based process reward models with far fewer process labels; key for step-level verification and safer inference-time scaling.,2025-12-24
reasoning_and_planning,openai,https://github.com/mukhal/thinkprm,mukhal/thinkprm,GitHub,recent,Reference implementation/models for ThinkPRM—useful for reproducing generative PRM verification and building new process-supervision pipelines.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2504.00891,GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning,arXiv,2025-04-01,Generative PRM that uses explicit CoT + code verification and supports test-time scaling of verification compute; relevant to PRM robustness and reward hacking.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2512.03244,SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning,arXiv,2025-12-02,"Reference-free pipeline that synthesizes step-level rewards via parallel/sequential verification and trains generative PRMs, with anti–reward-hacking constraints.",2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2501.03124,PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models,arXiv,2025-01-06,Benchmark for fine-grained PRM step-error detection beyond simple step correctness; foundational for evaluating verifiers used in test-time scaling.,2025-12-24
reasoning_and_planning,openai,https://github.com/ssmisya/PRMBench,ssmisya/PRMBench,GitHub,recent,Official PRMBench code + PRM evaluation toolkit (mr_eval/mr_annotate) to benchmark process-level reward models and LLM critics.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2512.09897,SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments,arXiv,2025-12-10,Hierarchical planning approach that uses LLM-generated subgoals only once at initialization to pretrain a lightweight planner—cuts repeated LLM queries at inference.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2512.09629,An End-to-end Planning Framework with Agentic LLMs and PDDL,arXiv,2025-12-10,Agentic orchestration from natural language specs to PDDL with iterative refinement and external planners/validators—strong planning+verification integration.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2511.18165,Towards a General Framework for HTN Modeling with LLMs,arXiv,2025-11-22,Targets hierarchical task network (HTN) model generation with LLMs and evaluates limitations—relevant to structured planning representations and reliability.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2502.04728,Generating Symbolic World Models via Test-time Scaling of Large Language Models,arXiv,2025-02-07,"Uses test-time scaling to generate PDDL symbolic world models enabling classical search (A*/planners), bridging LLM reasoning and formal planning.",2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2502.13092,Text2World: Benchmarking Large Language Models for Symbolic World Model Generation,arXiv,2025-02-18,PDDL-based benchmark for symbolic world model generation with execution-based metrics; includes analysis of test-time scaling and agent-training strategies.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2506.12928,Scaling Test-time Compute for LLM Agents,arXiv,2025-06-15,"Systematic exploration of test-time scaling for language agents (parallel sampling, sequential revision, verifiers, rollout diversity), including “when to reflect” findings.",2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2505.13672,A*-Decoding: Token-Efficient Inference Scaling,arXiv,2025-05-19,Search-based decoding that prioritizes high-quality reasoning paths to achieve strong test-time scaling with fewer tokens/PRM passes—important for efficient deployment.,2025-12-24
reasoning_and_planning,openai,https://aclanthology.org/2025.findings-emnlp.880/,Simple Factuality Probes Detect Hallucinations in Long-Form Natural Language Generation,ACL Anthology (EMNLP 2025 Findings),2025-11,Demonstrates single-pass hidden-state probes can flag hallucinations with orders-of-magnitude less compute than multi-sample approaches—key for factuality at scale.,2025-12-24
reasoning_and_planning,openai,https://aclanthology.org/2025.findings-emnlp.672/,MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search,ACL Anthology (EMNLP 2025 Findings),2025-11,"Integrates adaptive retrieval with MCTS to refine reasoning paths, improving factual accuracy and reducing hallucinations—grounding + inference-time search.",2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2502.11169,Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long Chain-of-Thought for Mathematical Reasoning,arXiv,2025-02-16,Constrained-action MCTS with PRM guidance for generating reliable long CoTs; ties MCTS directly to reasoning-trace quality and efficiency.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2510.08992,Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search,arXiv,2025-10-10,Represents steps as intent/constraint pairs and uses MCTS to prune infeasible branches—improves constraint-aligned planning and reduces hallucinated actions.,2025-12-24
reasoning_and_planning,openai,https://arxiv.org/abs/2509.17116,MCTS-EP: Empowering Embodied Planning with Online Preference Optimization,arXiv,2025-09-21,Combines LLMs with MCTS-guided exploration and online preference optimization for embodied agents; shows strong gains on ALFWorld/WebShop planning tasks.,2025-12-24
reasoning_and_planning,anthropic,https://magazine.sebastianraschka.com/p/understanding-reasoning-llms,Understanding Reasoning LLMs,Sebastian Raschka's Ahead of AI (Substack),"February 5, 2025","Comprehensive guide to reasoning LLMs covering DeepSeek-R1, model distillation, and four main approaches to building reasoning models.",2025-12-24
reasoning_and_planning,anthropic,https://arxiv.org/abs/2502.12521,Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights,arXiv,"February 18, 2025","Introduces Sys2Bench benchmark evaluating inference-time techniques across 11 diverse tasks, revealing limitations of current methods.",2025-12-24
reasoning_and_planning,anthropic,https://sebastianraschka.com/blog/2025/state-of-llm-reasoning-and-inference-scaling.html,Inference-Time Compute Scaling Methods to Improve Reasoning Models,Sebastian Raschka's Blog,"March 8, 2025",Deep dive into inference-time scaling methods from simple CoT to sophisticated search strategies for reasoning.,2025-12-24
reasoning_and_planning,anthropic,https://deepmind.google/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/,FACTS Grounding: A new benchmark for evaluating the factuality of LLMs,Google DeepMind Blog,December 2024,"New benchmark and leaderboard for evaluating LLM grounding and factuality, addressing hallucination problem.",2025-12-24
reasoning_and_planning,anthropic,https://arxiv.org/abs/2311.05232,"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",arXiv,November 2024 (updated),"Comprehensive survey on LLM hallucinations covering taxonomy, detection methods, benchmarks, and mitigation strategies.",2025-12-24
reasoning_and_planning,anthropic,https://proceedings.neurips.cc/paper_files/paper/2024/hash/3c1e1fdf305195cd620c118aaa9717ad-Abstract-Conference.html,LLM-Check: Investigating Detection of Hallucinations in Large Language Models,NeurIPS 2024,"December 16, 2024",NeurIPS 2024 paper introducing efficient hallucination detection using internal LLM representations with up to 450x speedup.,2025-12-24
reasoning_and_planning,anthropic,https://www.nature.com/articles/s41586-024-07421-0,Detecting hallucinations in large language models using semantic entropy,Nature,June 2024,Influential Nature paper introducing semantic entropy for hallucination detection measuring uncertainty at meaning level.,2025-12-24
reasoning_and_planning,anthropic,https://arxiv.org/abs/2405.06682,Self-Reflection in LLM Agents: Effects on Problem-Solving Performance,arXiv,October 2024,Empirical study showing LLM agents significantly improve problem-solving through self-reflection across multiple models.,2025-12-24
reasoning_and_planning,anthropic,https://arxiv.org/abs/2402.02716,Understanding the planning of LLM agents: A survey,arXiv,February 2024,"First systematic survey of LLM-based agent planning covering task decomposition, plan selection, reflection, and memory.",2025-12-24
reasoning_and_planning,anthropic,https://arxiv.org/abs/2405.00451,Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning,arXiv,June 2024,"Demonstrates using MCTS to iteratively collect preference data, breaking down instance-level rewards into step-level signals.",2025-12-24
reasoning_and_planning,anthropic,https://arxiv.org/abs/2501.08603,Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design,arXiv,January 2025,"Proposes MCTS-AHD for LLM-based heuristic evolution, organizing generated heuristics in tree structure for better exploration.",2025-12-24
reasoning_and_planning,anthropic,https://arxiv.org/html/2505.14656v1,Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning (CATS),arXiv,May 2025,"Introduces cost-aware MCTS for LLM planning, achieving better task success rates than raw LLMs under budget constraints.",2025-12-24
reasoning_and_planning,anthropic,https://arxiv.org/abs/2410.08146,Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning,arXiv,October 2024,Key paper on process reward models showing how to design effective process rewards measuring progress toward solutions.,2025-12-24
reasoning_and_planning,anthropic,https://arxiv.org/html/2502.10325v1,Process Reward Models for LLM Agents: Practical Framework and Directions (AgentPRM),arXiv,February 2025,"Introduces AgentPRM framework for training LLM agents with process rewards, showing 3B models outperform GPT-4o baselines.",2025-12-24
reasoning_and_planning,anthropic,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5285532,The Decreasing Value of Chain of Thought in Prompting,SSRN / Wharton,June 2025,"Research showing CoT prompting effectiveness varies by task and model, with implications for reasoning strategies.",2025-12-24
reasoning_and_planning,anthropic,https://en.wikipedia.org/wiki/OpenAI_o3,OpenAI o3,Wikipedia,recent,Overview of OpenAI's o3 reasoning model achieving 87.5% on ARC-AGI and state-of-the-art on multiple reasoning benchmarks.,2025-12-24
reasoning_and_planning,anthropic,https://huggingface.co/deepseek-ai/DeepSeek-R1,DeepSeek-R1 Model Card,Hugging Face,January 2025,Official model page for DeepSeek-R1 with distilled checkpoints from 1.5B to 70B enabling open-source reasoning research.,2025-12-24
reasoning_and_planning,anthropic,https://huggingface.co/learn/cookbook/en/search_and_learn,Scaling Test-Time Compute for Longer Thinking in LLMs,Hugging Face Cookbook,recent,Practical guide for implementing test-time compute scaling with open models using search strategies and reward models.,2025-12-24
reasoning_and_planning,anthropic,https://pmc.ncbi.nlm.nih.gov/articles/PMC12546433/,Toward large reasoning models: A survey of reinforced reasoning with large language models,PMC / NIH,recent,Comprehensive survey reviewing LLM reasoning progress from CoT to RL-based approaches with scaling analysis.,2025-12-24
reasoning_and_planning,anthropic,https://magazine.sebastianraschka.com/p/llm-research-papers-2025-list-one,LLM Research Papers: The 2025 List (January to June),Sebastian Raschka's Ahead of AI,July 2025,Topic-organized collection of 200+ LLM research papers from 2025 including reasoning and inference scaling papers.,2025-12-24
reasoning_and_planning,anthropic,https://research.google/blog/effective-large-language-model-adaptation-for-improved-grounding/,Effective large language model adaptation for improved grounding (AGREE),Google Research Blog,recent,Introduces AGREE framework for adapting LLMs to self-ground responses with test-time adaptation for improved factuality.,2025-12-24
reasoning_and_planning,anthropic,https://arxiv.org/abs/2410.02725,"Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation",arXiv,October 2024,"Novel approach for LLMs to self-evaluate and adaptively allocate inference compute, achieving 74% improvement efficiency.",2025-12-24
reasoning_and_planning,anthropic,https://aclanthology.org/2023.findings-emnlp.123/,Towards Mitigating LLM Hallucination via Self Reflection,ACL Anthology / EMNLP 2023,December 2023,Introduces self-reflection methodology for hallucination mitigation in medical QA with knowledge acquisition loop.,2025-12-24
agents_and_finance,openai,https://daloopa.com/blog/press-release/daloopa-expands-financial-data-model-context-protocol,Daloopa expands financial data Model Context Protocol (MCP) through a new connector with OpenAI,Daloopa Blog (press release),2025-12-09,Announces an OpenAI MCP connector that brings Daloopa’s source-linked fundamentals/KPIs into ChatGPT for agentic financial analysis workflows.,2025-12-24
agents_and_finance,openai,https://pitchbook.com/media/press-releases/pitchbook-launches-new-generative-ai-experiences-with-the-introduction-of-pitchbook-navigator-and-upcoming-integration-with-openai,PitchBook Launches New Generative AI Experiences with the Introduction of PitchBook Navigator and Upcoming Integration with OpenAI,PitchBook (press release),2025-11-10,Introduces PitchBook Navigator and announces an upcoming OpenAI MCP connector so PitchBook subscribers can query private market data within ChatGPT.,2025-12-24
agents_and_finance,openai,https://pitchbook.com/media/press-releases/pitchbooks-industry-leading-private-capital-market-intelligence-is-now-accessible-in-claude,PitchBook’s Industry Leading Private Capital Market Intelligence is Now Accessible in Claude,PitchBook (press release),2025-10-28,General availability of PitchBook private markets data inside Claude (Anthropic) for financial research and decision-making.,2025-12-24
agents_and_finance,openai,https://pitchbook.com/media/press-releases/pitchbook-announces-llm-partnerships-with-anthropic-perplexity-rogo-and-hebbia-to-expand-access-to-private-capital-market-data,"PitchBook Announces LLM Partnerships with Anthropic, Perplexity, Rogo and Hebbia to Expand Access to Private Capital Market Data",PitchBook (press release),2025-07-22,"Key rollup of PitchBook’s strategy to embed private-market intelligence into multiple agentic/LLM financial workflows (memos, decks, research).",2025-12-24
agents_and_finance,openai,https://rogo.ai/news/announcing-rogo-s-new-partnership-with-pitchbook,Announcing Rogo’s New Partnership with PitchBook,Rogo,2025-07-25,"Details how Rogo grounds banking/investing workflows in PitchBook datasets for screening, benchmarking, and research automation.",2025-12-24
agents_and_finance,openai,https://www.lseg.com/en/media-centre/press-releases/2025/lseg-and-rogo-announce-strategic-partnership,LSEG and Rogo Announce Strategic Partnership,LSEG (press release),2025-08-20,"Integrates LSEG Workspace content (fundamentals, estimates, M&A database) into Rogo’s AI agents for IB/PE workflows.",2025-12-24
agents_and_finance,openai,https://www.lseg.com/en/media-centre/press-releases/2025/lseg-announces-collaboration-with-anthropic,LSEG and Anthropic collaborate to make more financial data accessible to Claude for Enterprise customers,LSEG (press release),2025-10-27,"Brings licensed LSEG data into Claude for Financial Services via MCP to enable agentic workflows (earnings, diligence, market signals).",2025-12-24
agents_and_finance,openai,https://www.hebbia.com/newsroom/hebbia-partners-with-pitchbook,Hebbia Partners With PitchBook,Hebbia (newsroom),2025-06-17,"Adds a PitchBook data source inside Hebbia Matrix for citation-backed private market comps, rounds, and deal data.",2025-12-24
agents_and_finance,openai,https://www.businesswire.com/news/home/20250908104474/en/Hebbia-Partners-With-FactSet-to-Power-AI-Driven-Financial-Research,Hebbia Partners With FactSet to Power AI-Driven Financial Research,Business Wire,2025-09-08,"FactSet structured data (prices, estimates, financials) becomes available within Hebbia’s finance-focused agentic document workflows.",2025-12-24
agents_and_finance,openai,https://www.hebbia.com/blog/hebbia-integrates-with-microsoft-azure-ai-foundry-to-elevate-financial,Hebbia Integrates with Microsoft Azure AI Foundry to Elevate Financial Analysis,Hebbia (newsroom/blog),2025-08-27,"Announces GPT-5 availability via Azure AI Foundry in Hebbia’s Matrix platform for diligence, deal sourcing, and finance research at scale.",2025-12-24
agents_and_finance,openai,https://finster.ai/news/finster-partners-with-pitchbook-to-enhance-private-market-data,Finster partners with PitchBook to enhance private market data,Finster,2025-08-19,"Embeds PitchBook company/deal data into an AI-native research platform for private-company screening, modeling, and writeups.",2025-12-24
agents_and_finance,openai,https://finster.ai/resources/news/finster-ai-partners-with-preqin-to-expand-alternative-assets-coverage,Finster AI partners with Preqin to expand alternative assets coverage,Finster,2025-10-29,"Brings Preqin private markets datasets (fundraising, fund performance benchmarks/indices) into Finster’s agentic research workflows.",2025-12-24
agents_and_finance,openai,https://www.modelml.com/articles/model-ml-and-pitchbook,Turning Private Market Data into Action: Model ML Announces Partnership with Pitchbook,Model ML,2025-09-30,Integrates PitchBook private markets data into Model ML’s platform for automating finance deliverables (memos/decks/research).,2025-12-24
agents_and_finance,openai,https://www.farsight-ai.com/news/farsight-integrates-s-p-capital-iq-pro-data,Farsight Integrates with S&P Capital IQ Pro,Farsight,2025-07-01,"Example of Capital IQ Pro integration powering financial AI agents for CIMs, pitch decks, benchmarking, and transcript-driven analysis.",2025-12-24
agents_and_finance,openai,https://www.alpha-sense.com/press/alphasense-launches-autonomous-ai-agent-interviewer-debuts-channel-checks-to-deliver-real-time-market-signals-across-all-sectors-of-the-economy/,"AlphaSense Launches Autonomous AI Agent Interviewer, Debuts Channel Checks to Deliver Real-Time Market Signals Across All Sectors of the Economy",AlphaSense (press),2025-08-19,Launches an autonomous “AI agent interviewer” to scale expert insights (Tegus) for investment research and market monitoring.,2025-12-24
agents_and_finance,openai,https://www.alpha-sense.com/press/alphasense-acquires-carousel/,AlphaSense Acquires Carousel to Power AI-Driven Excel Modeling,AlphaSense (press),2025-10-07,Adds AI-assisted Excel modeling to AlphaSense’s agentic research stack (documents → insights → dynamic models).,2025-12-24
agents_and_finance,openai,https://press.spglobal.com/2025-10-22-S-P-Global-Redefines-Financial-Insights-with-New-AI-Powered-Multi-Document-Research-and-Analysis-Tool-in-Capital-IQ-Pro-ChatIQ,S&P Global Redefines Financial Insights with New AI-Powered Multi-Document Research and Analysis Tool in Capital IQ Pro ChatIQ,S&P Global (press),2025-10-22,"Capital IQ Pro adds multi-document GenAI analysis with citations + workflow enhancements (screening/alerts), supporting agentic research.",2025-12-24
agents_and_finance,openai,https://www.marketplace.spglobal.com/en/solutions/document-intelligence-for-salesforce-agentforce-%283eba81d8-91b9-42e3-b2ef-e7887dbe8781%29,Document Intelligence for Salesforce Agentforce Solution,S&P Global Marketplace,2025-10-14,Capital IQ Pro Document Intelligence packaged as an Agentforce/AgentExchange solution—useful signal for CRM-integrated agentic finance workflows.,2025-12-24
agents_and_finance,openai,https://investor.spglobal.com/news-releases/news-details/2025/SP-Global-Agrees-to-Acquire-With-Intelligence-from-Motive-Partners-for-1-8-Billion-Establishing-Its-Leadership-in-Private-Markets-Intelligence/default.aspx,"S&P Global Agrees to Acquire With Intelligence from Motive Partners for $1.8 Billion, Establishing Its Leadership in Private Markets Intelligence",S&P Global (investor relations),2025-10-15,Major private-markets data acquisition expanding coverage across PE/credit/alternatives—relevant data layer for agentic diligence and portfolio monitoring.,2025-12-24
agents_and_finance,openai,https://www.datasite.com/en/company/news/datasite-acquires-leading-private-market-intelligence-company-grata-with--500-million-investment-commitment,Datasite Acquires Leading Private Market Intelligence Company Grata with $500 Million Investment Commitment,Datasite (press release),2025-06-03,Combines M&A workflow tooling with AI-native private market intelligence for deal sourcing and due diligence automation.,2025-12-24
agents_and_finance,openai,https://www.datasite.com/en/company/news/datasite-acquires-leading-agentic-ai-company-blueflame,Datasite Acquires Leading Agentic AI Company Blueflame,Datasite (press release),2025-07-23,"Acquisition of an agentic AI platform purpose-built for investment workflows (search, doc processing, automation) and data room integrations.",2025-12-24
agents_and_finance,openai,https://www.prnewswire.com/news-releases/chronograph-and-anthropic-announce-integration-of-private-capital-data-into-claude-302595846.html,Chronograph and Anthropic Announce Integration of Private Capital Data into Claude,PR Newswire,2025-10-28,Private capital portfolio monitoring/valuation analytics made accessible inside Claude via MCP for PE/VC investor workflows.,2025-12-24
agents_and_finance,openai,https://www.thirdbridge.com/en-us/about-us/media/news/third-bridge-integrates-with-claude-for-financial-services,Third Bridge integrates expert insights with Anthropic’s Claude for Financial Services,Third Bridge,2025-10-28,Brings proprietary expert interview transcript libraries into Claude for agentic diligence and investment research (via MCP).,2025-12-24
agents_and_finance,openai,https://aiera.com/newsroom/aiera-and-anthropic-integration/,Aiera and Anthropic Announce Integration of Aiera’s Trusted Market Intelligence into Claude for Financial Services,Aiera,2025-10-27,Aiera MCP server exposes investor events and real-time market intelligence directly inside Claude to speed research and decision-making.,2025-12-24
agents_and_finance,openai,https://arxiv.org/abs/2512.14744,VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation,arXiv,2025-12-12,Proposes a verified agentic framework (GAAP/SEC/math policies) to reduce calculation/compliance errors in financial AI systems.,2025-12-24
agents_and_finance,openai,https://arxiv.org/abs/2510.20099,AI PB: A Grounded Generative Agent for Personalized Investment Insights,arXiv,2025-10-23,"Describes a production-scale grounded investment-insights agent with orchestration, hybrid retrieval, and compliance constraints.",2025-12-24
agents_and_finance,openai,https://arxiv.org/abs/2510.15949,ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination,arXiv,2025-10-10,Multi-agent trading framework with dynamic prompt optimization and order-aware action space (bridging model output to executable orders).,2025-12-24
agents_and_finance,openai,https://arxiv.org/abs/2508.16571,LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence,arXiv,2025-08-22,Agentic due-diligence benchmark + system (biotech VC memos); reports major analyst time savings and compares against Deep Research/Perplexity.,2025-12-24
agents_and_finance,openai,https://arxiv.org/abs/2505.10278,MASS: Multi-Agent Simulation Scaling for Portfolio Construction,arXiv,2025-05-15,Portfolio construction via scaling multi-agent simulations; paper includes an open-source implementation link for reproducibility/extension.,2025-12-24
agents_and_finance,openai,https://github.com/gta0804/MASS,gta0804/MASS — Official implementation of MASS: Multi-Agent Simulation Scaling for Portfolio Construction,GitHub,recent,Open-source codebase for running MASS multi-agent portfolio construction simulations and agent distribution optimization.,2025-12-24
agents_and_finance,anthropic,https://arxiv.org/abs/2412.20138,TradingAgents: Multi-Agents LLM Financial Trading Framework,arXiv,"December 28, 2024","Proposes a novel multi-agent trading framework with specialized roles (fundamental, sentiment, technical analysts) showing superior returns over baselines.",2025-12-24
agents_and_finance,anthropic,https://arxiv.org/html/2411.04788v1,Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research,arXiv,November 2024,Academic paper from ICAIF '24 demonstrating multi-agent collaboration outperforms single-agent models in investment research.,2025-12-24
agents_and_finance,anthropic,https://aws.amazon.com/blogs/industries/agentic-ai-in-financial-services-choosing-the-right-pattern-for-multi-agent-systems/,Agentic AI in Financial Services: Choosing the Right Pattern for Multi-Agent Systems,AWS Blog,December 2024,"AWS details multi-agent patterns (workflow, swarm, graph) for financial services with Amazon Bedrock AgentCore.",2025-12-24
agents_and_finance,anthropic,https://www.weforum.org/stories/2024/12/agentic-ai-financial-services-autonomy-efficiency-and-inclusion/,How Agentic AI will transform financial services,World Economic Forum,December 2024,WEF analysis on agentic AI transforming financial services with autonomous decision-making capabilities.,2025-12-24
agents_and_finance,anthropic,https://www.deloitte.com/cz-sk/en/services/consulting/blogs/where-is-the-value-of-AI-in-MA-why-multi-agent-systems-needs-modern-data-architecture.html,Where is the value of AI in M&A: why multi-agent systems needs modern data architecture,Deloitte,recent,Deloitte analysis on multi-agent AI systems for M&A and due diligence with Gartner projections.,2025-12-24
agents_and_finance,anthropic,https://www.alpha-sense.com/press/alphasense-launches-financial-data/,AlphaSense Launches Financial Data with Deal Intelligence Agent,AlphaSense,October 2025,"AlphaSense launches agentic workflows for deal sourcing, sector analysis, and investment decisions with 950K+ M&A deals.",2025-12-24
agents_and_finance,anthropic,https://www.prnewswire.com/news-releases/alphasense-launches-deep-research-automating-in-depth-analysis-with-agentic-ai-302476710.html,AlphaSense Launches Deep Research - Automating In-Depth Analysis with Agentic AI,PR Newswire,June 2025,AlphaSense's Deep Research AI agent automates complex research tasks for financial analysts.,2025-12-24
agents_and_finance,anthropic,https://www.bloomberg.com/company/stories/closing-the-agentic-ai-productionization-gap-bloomberg-embraces-mcp/,Closing the Agentic AI productionization gap: Bloomberg embraces MCP,Bloomberg,July 2025,Bloomberg adopts Model Context Protocol (MCP) for agentic AI development in finance applications.,2025-12-24
agents_and_finance,anthropic,https://www.bloomberg.com/company/stories/bloombergs-ai-engineers-introduce-an-improved-agent-tool-calling-methodology-acl-2025/,Bloomberg's AI engineers introduce an improved agent tool-calling methodology at ACL 2025,Bloomberg,July 2025,Bloomberg researchers present new Cost-Aware Pass Rate metric for LLM agent efficiency in finance.,2025-12-24
agents_and_finance,anthropic,https://github.com/TauricResearch/TradingAgents,TradingAgents: Multi-Agents LLM Financial Trading Framework,GitHub,recent,"Open-source multi-agent trading framework built with LangGraph featuring analyst, researcher, and risk management agents.",2025-12-24
agents_and_finance,anthropic,https://github.com/AI4Finance-Foundation/FinRobot,FinRobot: An Open-Source AI Agent Platform for Financial Analysis using LLMs,GitHub,recent,"Open-source platform with market forecasting, document analysis, and trading strategy agents using FinGPT.",2025-12-24
agents_and_finance,anthropic,https://github.com/AI4Finance-Foundation/FinGPT,FinGPT: Open-Source Financial Large Language Models,GitHub,recent,"Open-source financial LLM framework for sentiment analysis and trading, democratizing FinLLM access.",2025-12-24
agents_and_finance,anthropic,https://blog.langchain.com/captide/,How Captide agents compress investment research from days to seconds,LangChain Blog,June 2025,Case study of Captide using LangGraph Platform for investment research and equity modeling agents.,2025-12-24
agents_and_finance,anthropic,https://blog.langchain.com/customers-harmonic/,How Harmonic built an investment agent with LangGraph and LangSmith,LangChain Blog,April 2025,"Harmonic uses LangGraph for VC deal sourcing and startup research agents, achieving 30% better search outcomes.",2025-12-24
agents_and_finance,anthropic,https://www.salesforce.com/news/stories/agentforce-for-financial-services-announcement/,Salesforce Introduces Agentforce for Financial Services,Salesforce,May 2025,"Pre-built AI agent templates for financial advisors, bankers, and insurance with compliance controls.",2025-12-24
agents_and_finance,anthropic,https://www.mongodb.com/company/blog/innovation/reimagining-investment-portfolio-management-with-agentic-ai,Reimagining Investment Portfolio Management with Agentic AI,MongoDB Blog,June 2025,MongoDB solution architecture for multi-agent portfolio management with market analysis and sentiment agents.,2025-12-24
agents_and_finance,anthropic,https://digiqt.com/blog/ai-agents-for-private-equity/,AI Agents in Private Equity: Proven Wins and Pitfalls,Digiqt Blog,September 2025,Real-world PE use cases showing 35% IR cycle time reduction with LP relations automation agents.,2025-12-24
agents_and_finance,anthropic,https://www.v7labs.com/blog/ai-for-private-equity-venture-capital,5 Applications of AI in Venture Capital and Private Equity,V7 Labs,recent,"Survey shows 82% of PE/VC firms using AI in Q4 2024, up from 47% the prior year for due diligence.",2025-12-24
agents_and_finance,anthropic,https://www.affinity.co/guides/vc-ai-tools,10 AI Tools for Venture Capital Firms in 2025,Affinity,2025,"Guide to AI-powered deal sourcing, relationship intelligence, and portfolio management tools for VCs.",2025-12-24
agents_and_finance,anthropic,https://govclab.com/2025/04/12/ai-for-vc/,AI for Venture Capital (VC) - VC Lab,VC Lab,April 2025,Comprehensive overview of AI adoption in VC with up to 40% time savings on routine tasks.,2025-12-24
agents_and_finance,anthropic,https://www.calcalistech.com/ctechnews/article/bktsdpzole,How AI became the ultimate partner for venture capitalists,Calcalist Tech,August 2025,Survey of Israeli VCs showing AI agents delivering 3-4x more qualified leads per week without headcount increase.,2025-12-24
agents_and_finance,anthropic,https://www.pwc.com/us/en/services/audit-assurance/library/ai-agents-for-finance-and-reporting.html,How AI agents are transforming finance and reporting,PwC,recent,PwC's perspective on AI agents in financial reporting including Next Gen Audit vision.,2025-12-24
agents_and_finance,anthropic,https://www.highradius.com/resources/Blog/agentic-ai-in-financial-reporting/,How Agentic AI Enhances Financial Reporting: 5 Key Benefits,HighRadius,April 2025,"Practical guide on AI agents for journal entry automation, compliance, and narrative reporting.",2025-12-24
agents_and_finance,anthropic,https://cacm.acm.org/blogcacm/leveraging-ai-multi-agent-systems-in-financial-analysis/,Leveraging AI Multi-Agent Systems in Financial Analysis,Communications of the ACM,July 2025,Academic perspective on multi-agent systems for fundamental analysis and real-time market monitoring.,2025-12-24
agents_and_finance,anthropic,https://arxiv.org/html/2408.06361v1,Large Language Model Agent in Financial Trading: A Survey,arXiv,August 2024,"Comprehensive survey of LLM agents in trading covering architectures, memory, and reasoning mechanisms.",2025-12-24
agents_and_finance,anthropic,https://arxiv.org/html/2504.10789v1,Can Large Language Models Trade? Testing Financial Theories with LLM Agents,arXiv,April 2025,Research on LLM trading agents showing they can execute strategies and produce realistic market dynamics.,2025-12-24
agents_and_finance,anthropic,https://github.com/pipiku915/FinMem-LLM-StockTrading,FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory,GitHub,recent,Open-source LLM agent with cognitive memory architecture for financial decision-making.,2025-12-24
agents_and_finance,anthropic,https://www.llamaindex.ai/industry/finance,Build document agents for finance - LlamaIndex,LlamaIndex,recent,LlamaIndex platform for building financial document agents for compliance and due diligence.,2025-12-24
agents_and_finance,anthropic,https://learn.deeplearning.ai/courses/multi-ai-agent-systems-with-crewai/lesson/ixy19/mutli-agent-collaboration-for-financial-analysis-(code),Multi AI Agent Systems with CrewAI - Financial Analysis,DeepLearning.AI,May 2024,Educational course on building multi-agent financial analysis crews with stock research capabilities.,2025-12-24
agents_and_finance,anthropic,https://www.researchgate.net/publication/396731115_StockVision_A_Multi-Agent_AI_framework_for_Intelligent_Stock_Investment_and_Analysis_using_CrewAI,StockVision: A Multi-Agent AI framework for Stock Investment using CrewAI,ResearchGate,September 2025,Academic paper on Django-based multi-agent platform for automated stock analysis and recommendations.,2025-12-24
agent_infrastructure,openai,https://openai.com/index/agentic-ai-foundation,OpenAI co-founds the Agentic AI Foundation under the Linux Foundation,OpenAI (Company blog),"Dec 9, 2025","Announces AAIF and the donation of AGENTS.md, signaling accelerating standardization around agent interoperability and project-level agent instructions. citeturn23search0",2025-12-24
agent_infrastructure,openai,https://openai.com/index/new-tools-and-features-in-the-responses-api/,New tools and features in the Responses API,OpenAI (Product blog),"May 21, 2025",Adds built-in tool upgrades including support for remote MCP servers in the Responses API—key infrastructure for tool-use and agent connectivity. citeturn22search2,2025-12-24
agent_infrastructure,openai,https://openai.github.io/openai-agents-python/,OpenAI Agents SDK (Docs),OpenAI (Documentation),recent,"Canonical docs for agent primitives (agents, handoffs, guardrails, sessions) plus tracing and structured outputs patterns. citeturn22search0",2025-12-24
agent_infrastructure,openai,https://github.com/openai/openai-agents-python,openai/openai-agents-python — OpenAI Agents SDK (Python),GitHub,recent,"Reference implementation + examples for building multi-agent workflows (handoffs, guardrails, sessions, tracing) in Python. citeturn22search1",2025-12-24
agent_infrastructure,openai,https://github.com/openai/openai-agents-js,openai/openai-agents-js — OpenAI Agents SDK (JavaScript/TypeScript),GitHub,recent,"JS/TS Agents SDK with multi-agent workflows, structured outputs, tracing, and local MCP server support (plus voice/realtime agent primitives). citeturn25view0",2025-12-24
agent_infrastructure,openai,https://modelcontextprotocol.io/development/roadmap,Roadmap — Model Context Protocol (MCP),Model Context Protocol (Docs),"Last updated Oct 31, 2025",Tracks upcoming MCP spec priorities and timeline—useful for client/server implementors planning compatibility work. citeturn0search4,2025-12-24
agent_infrastructure,openai,https://modelcontextprotocol.io/docs/tools/inspector,MCP Inspector,Model Context Protocol (Docs),recent,"Official debugging UI/CLI guide for inspecting MCP servers (tools/resources/prompts), critical for MCP server development workflows. citeturn4search0",2025-12-24
agent_infrastructure,openai,https://github.com/modelcontextprotocol/python-sdk,modelcontextprotocol/python-sdk — Official Python SDK for MCP servers and clients,GitHub,recent,"Core Python SDK for implementing MCP servers/clients (incl. FastMCP patterns, structured outputs, elicitation/OAuth examples). citeturn3search2",2025-12-24
agent_infrastructure,openai,https://github.com/modelcontextprotocol/typescript-sdk,modelcontextprotocol/typescript-sdk — Official TypeScript SDK for MCP servers and clients,GitHub,recent,Core TS SDK for building MCP servers/clients across stdio + Streamable HTTP transports; foundation for JS MCP tooling ecosystems. citeturn3search1,2025-12-24
agent_infrastructure,openai,https://github.com/modelcontextprotocol/go-sdk,modelcontextprotocol/go-sdk — Official Go SDK for MCP servers and clients,GitHub,recent,Go SDK (maintained in collaboration with Google) enabling MCP servers/clients and OAuth primitives for Go infra stacks. citeturn3search0,2025-12-24
agent_infrastructure,openai,https://github.com/github/github-mcp-server/releases,Releases — GitHub MCP Server,GitHub,"Nov 25, 2025 (v0.23.0)","Changelog for GitHub’s MCP server; tracks real-world MCP feature evolution (e.g., tool-specific config and SDK migrations). citeturn1search1",2025-12-24
agent_infrastructure,openai,https://aws.amazon.com/about-aws/whats-new/2025/11/the-aws-api-mcp-server-aws-marketplace,The AWS API MCP Server is now available on AWS Marketplace,AWS (What’s New),"Nov 26, 2025",Shows enterprise-grade packaging/deployment of an MCP server (managed deployment + auth/session isolation via Bedrock AgentCore). citeturn4search1,2025-12-24
agent_infrastructure,openai,https://www.microsoft.com/en-us/dynamics-365/blog/it-professional/2025/11/11/dynamics-365-erp-model-context-protocol/,Dynamics 365 ERP MCP Server: Adaptive & Analytics-Ready,Microsoft Dynamics 365 Blog,"Nov 11, 2025",Enterprise adoption signal: Microsoft outlines MCP-first strategy for ERP agents plus public preview of a dynamic ERP MCP server. citeturn0search0,2025-12-24
agent_infrastructure,openai,https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk,Building agents with the Claude Agent SDK,Anthropic Engineering,"Sep 29, 2025",Official guide + best practices for building agent loops on Claude via the Claude Agent SDK (formerly Claude Code SDK). citeturn2search1,2025-12-24
agent_infrastructure,openai,https://pypi.org/project/claude-agent-sdk/,claude-agent-sdk (PyPI),PyPI,"Dec 18, 2025 (0.1.18 upload)",Package distribution page for the Claude Agent SDK—useful for tracking released versions and install targets across platforms. citeturn2search0,2025-12-24
agent_infrastructure,openai,https://github.com/anthropics/claude-agent-sdk-typescript,anthropics/claude-agent-sdk-typescript,GitHub,recent,TypeScript SDK for building autonomous agents that can edit files/run commands and orchestrate workflows on Claude. citeturn2search6,2025-12-24
agent_infrastructure,openai,https://www.anthropic.com/engineering/advanced-tool-use,Introducing advanced tool use on the Claude Developer Platform,Anthropic Engineering,recent,"Details beta capabilities for higher-reliability tool orchestration (tool discovery/search, programmatic tool calling, defer loading, access controls). citeturn2search4",2025-12-24
agent_infrastructure,openai,https://developers.googleblog.com/introducing-agent-development-kit-for-typescript-build-ai-agents-with-the-power-of-a-code-first-approach/,Introducing Agent Development Kit for TypeScript: Build AI Agents with the Power of a Code-First Approach,Google Developers Blog,"Dec 17, 2025",Announces open-source ADK for TypeScript/JavaScript for building agents and multi-agent systems with code-first orchestration. citeturn2search3,2025-12-24
agent_infrastructure,openai,https://developers.googleblog.com/announcing-the-agent-development-kit-for-go-build-powerful-ai-agents-with-your-favorite-languages/,Announcing the Agent Development Kit for Go: Build Powerful AI Agents with Your Favorite Languages,Google Developers Blog,"Nov 7, 2025","Extends Google’s ADK ecosystem to Go, relevant for teams building agent infra in Go services. citeturn2search2",2025-12-24
agent_infrastructure,openai,https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/,LangChain & LangGraph 1.0 alpha releases,LangChain Blog,"Sep 2, 2025","Explains v1 alpha changes (create_agent, standard content blocks, durable runtime) that affect tool-calling, structured outputs, and agent orchestration. citeturn21view0",2025-12-24
agent_infrastructure,openai,https://changelog.langchain.com/announcements/langchain-1-0-now-generally-available,LangChain 1.0 now generally available,LangChain Changelog,"Oct 22, 2025","Release notes highlighting create_agent, middleware, and improved structured output generation integrated into the main agent loop. citeturn19search0",2025-12-24
agent_infrastructure,openai,https://changelog.langchain.com/announcements/langgraph-1-0-is-now-generally-available,LangGraph 1.0 is now generally available,LangChain Changelog,"Oct 22, 2025","Marks LangGraph’s stable durable-agent runtime (persistence, HITL, graph execution) for production agent infrastructure. citeturn19search1",2025-12-24
agent_infrastructure,openai,https://github.com/advisories/GHSA-wwqv-p2pp-99h5,"LangGraph Checkpoint affected by RCE in ""json"" mode of JsonPlusSerializer (GHSA-wwqv-p2pp-99h5)",GitHub Advisory Database,"Nov 5, 2025 (updated Nov 7, 2025)",Security-critical advisory for durable agent state/checkpointing; patched in langgraph-checkpoint 3.0.0. citeturn17search8,2025-12-24
agent_infrastructure,openai,https://www.llamaindex.ai/blog/llamaagents-build-serve-and-deploy-document-agents,"Announcing LlamaAgents Open Preview: Build, Serve & Deploy Document Agents",LlamaIndex Blog,"Nov 18, 2025",Agent deployment tooling (llamactl + templates + cloud deployment) for productionizing multi-step agent workflows over documents. citeturn9view0,2025-12-24
agent_infrastructure,openai,https://www.llamaindex.ai/blog/making-coding-agents-safe-using-llamaindex,Making Coding Agents Safe: Using LlamaIndex to Secure Filesystem Access,LlamaIndex Blog,"Dec 15, 2025",Practical agent-safety/infrastructure pattern: sandboxed filesystem virtualization (AgentFS) + MCP tools + hooks for coding agents. citeturn12view0,2025-12-24
agent_infrastructure,openai,https://arxiv.org/abs/2509.25292,A Measurement Study of Model Context Protocol,arXiv,"Sep 29, 2025","Large-scale empirical analysis of MCP markets/servers/clients (validity, security/maintenance risks), useful for MCP infra strategy. citeturn0academia17",2025-12-24
agent_infrastructure,openai,https://www.itpro.com/software/development/openais-skills-in-codex-service-aims-to-supercharge-agent-efficiency-for-developers,OpenAI's 'Skills in Codex' service aims to supercharge agent efficiency for developers,ITPro,"Dec 23, 2025",Recent reporting on modular “skills” packages for Codex and cross-platform Agent Skills standardization—important for agent workflow reuse. citeturn2news13,2025-12-24
agent_infrastructure,openai,https://www.techradar.com/pro/anthropic-takes-the-fight-to-openai-with-enterprise-ai-tools-and-theyre-going-open-source-too,Anthropic takes the fight to OpenAI with enterprise AI tools - and they're going open source too,TechRadar,"Dec 22, 2025",Recent coverage of Anthropic open-sourcing Agent Skills and positioning them alongside MCP for enterprise agent infrastructure. citeturn2news12,2025-12-24
agent_infrastructure,anthropic,https://techcrunch.com/2025/12/10/google-is-going-all-in-on-mcp-servers-agent-ready-by-design/,Google launches managed MCP servers that let AI agents simply plug into its tools,TechCrunch,"December 10, 2025","Major announcement of Google's fully managed remote MCP servers for Maps, BigQuery, and more with enterprise security features.",2025-12-24
agent_infrastructure,anthropic,https://www.linuxfoundation.org/press/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation,Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF),Linux Foundation,"December 9, 2025","Foundational news: Anthropic, Block, and OpenAI co-founded AAIF, donating MCP, goose, and AGENTS.md to neutral governance.",2025-12-24
agent_infrastructure,anthropic,https://www.infoq.com/news/2025/12/agentic-ai-foundation/,OpenAI and Anthropic Donate AGENTS.md and Model Context Protocol to New Agentic AI Foundation,InfoQ,"December 23, 2025",Breaking coverage of AAIF launch with analysis of project contributions and industry implications.,2025-12-24
agent_infrastructure,anthropic,https://openai.com/index/agentic-ai-foundation/,OpenAI co-founds the Agentic AI Foundation under the Linux Foundation,OpenAI Blog,December 2025,"Official OpenAI announcement of AAIF co-founding and contribution of AGENTS.md standard adopted by 60,000+ projects.",2025-12-24
agent_infrastructure,anthropic,https://openai.com/index/introducing-agentkit/,Introducing AgentKit,OpenAI Blog,2025,"OpenAI's complete set of tools for building, deploying, and optimizing agents including visual Agent Builder.",2025-12-24
agent_infrastructure,anthropic,https://openai.github.io/openai-agents-js/,OpenAI Agents SDK TypeScript,OpenAI,recent,"Official TypeScript version of Agents SDK with Zod validation, guardrails, and full MCP primitives support.",2025-12-24
agent_infrastructure,anthropic,https://temporal.io/blog/announcing-openai-agents-sdk-integration,Production-ready agents with the OpenAI Agents SDK + Temporal,Temporal Blog,2025,Critical integration: Temporal adds durable execution to OpenAI Agents SDK for production-grade reliability.,2025-12-24
agent_infrastructure,anthropic,https://www.anthropic.com/engineering/code-execution-with-mcp,Code execution with MCP: building more efficient AI agents,Anthropic Engineering Blog,recent,Anthropic's approach to reducing token usage by 98.7% using code execution with MCP servers.,2025-12-24
agent_infrastructure,anthropic,https://github.blog/open-source/maintainers/mcp-joins-the-linux-foundation-what-this-means-for-developers-building-the-next-era-of-ai-tools-and-agents/,MCP joins the Linux Foundation: What this means for developers,GitHub Blog,December 2025,"Deep technical dive on MCP's journey, OAuth support for remote servers, and enterprise adoption patterns.",2025-12-24
agent_infrastructure,anthropic,https://google.github.io/adk-docs/,Agent Development Kit (ADK) Documentation,Google,recent,"Google's model-agnostic, open-source framework for building multi-agent systems with workflow agents and MCP integration.",2025-12-24
agent_infrastructure,anthropic,https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/,Agent Development Kit: Making it easy to build multi-agent applications,Google Developers Blog,April 2025,"Official ADK launch announcement with multi-agent design, rich tool ecosystem, and Vertex AI integration.",2025-12-24
agent_infrastructure,anthropic,https://github.com/google/adk-python,Google ADK Python - Open-source AI agent toolkit,GitHub,recent,"ADK v0.3.0 with code execution sandbox, tool confirmation HITL, and A2A protocol integration.",2025-12-24
agent_infrastructure,anthropic,https://blog.langchain.com/standard-message-content/,Standard message content,LangChain Blog,September 2025,"New standardized content blocks for reasoning, citations, web searches across all LLM providers.",2025-12-24
agent_infrastructure,anthropic,https://github.com/langchain-ai/langchain/releases,LangChain Releases,GitHub,recent,"Latest releases including langchain-core 1.2.4, GPT-5 support, and enhanced init_chat_model validation.",2025-12-24
agent_infrastructure,anthropic,https://www.llamaindex.ai/blog/announcing-workflows-1-0-a-lightweight-framework-for-agentic-systems,Announcing Workflows 1.0: A Lightweight Framework for Agentic systems,LlamaIndex Blog,recent,LlamaIndex Workflows 1.0 standalone release with event-driven orchestration for multi-agent systems.,2025-12-24
agent_infrastructure,anthropic,https://www.llamaindex.ai/blog/introducing-agentworkflow-a-powerful-system-for-building-ai-agent-systems,Introducing AgentWorkflow: A Powerful System for Building AI Agent Systems,LlamaIndex Blog,recent,New AgentWorkflow abstraction for orchestrating multi-agent teams with state management and tool coordination.,2025-12-24
agent_infrastructure,anthropic,https://github.com/pydantic/pydantic-ai,PydanticAI - GenAI Agent Framework,GitHub,recent,"Type-safe agent framework with MCP, A2A, durable execution, and graph support from the Pydantic team.",2025-12-24
agent_infrastructure,anthropic,https://ai.pydantic.dev/,Pydantic AI Documentation,Pydantic,recent,"Comprehensive docs for the FastAPI-inspired agent framework with human-in-the-loop, streaming outputs, and model-agnostic design.",2025-12-24
agent_infrastructure,anthropic,https://github.com/crewAIInc/crewAI,CrewAI - Multi-Agent Orchestration Framework,GitHub,recent,"Standalone high-performance framework with Crews and Flows architecture, 5.76x faster than LangGraph in benchmarks.",2025-12-24
agent_infrastructure,anthropic,https://github.com/block/goose,goose - Open source AI agent framework by Block,GitHub,recent,"Block's extensible AI agent with MCP integration, donated to AAIF - works with any LLM and runs locally.",2025-12-24
agent_infrastructure,anthropic,https://block.xyz/inside/block-open-source-introduces-codename-goose,Block Open Source Introduces 'codename goose',Block,January 2025,Original launch announcement of goose with MCP collaboration with Anthropic for real-world agent actions.,2025-12-24
agent_infrastructure,anthropic,https://github.com/lastmile-ai/mcp-agent,mcp-agent: Build effective agents using MCP,GitHub,recent,Framework implementing all Anthropic Building Effective Agents patterns with Temporal backend for durable workflows.,2025-12-24
agent_infrastructure,anthropic,https://github.com/punkpeye/awesome-mcp-servers,Awesome MCP Servers,GitHub,recent,Comprehensive curated list of MCP server implementations across platforms - essential resource for tool discovery.,2025-12-24
agent_infrastructure,anthropic,https://clickhouse.com/blog/how-to-build-ai-agents-mcp-12-frameworks,How to build AI agents with MCP: 12 framework comparison,ClickHouse Blog,October 2025,"Detailed comparison of 12 agent frameworks with MCP including code examples for Claude SDK, OpenAI, LangChain.",2025-12-24
agent_infrastructure,anthropic,https://arxiv.org/abs/2502.12110,A-MEM: Agentic Memory for LLM Agents,arXiv,February 2025,Novel agentic memory system using Zettelkasten principles for dynamic knowledge organization in agents.,2025-12-24
agent_infrastructure,anthropic,https://github.com/agiresearch/A-mem,A-MEM: Agentic Memory Implementation,GitHub,recent,Open-source implementation of agentic memory with ChromaDB and intelligent indexing for LLM agents.,2025-12-24
agent_infrastructure,anthropic,https://simonw.substack.com/p/i-think-agent-may-finally-have-a,I think 'agent' may finally have a widely enough agreed upon definition to be useful jargon now,Simon Willison Newsletter,September 2025,Influential analysis crystallizing agent definition: 'An LLM agent runs tools in a loop to achieve a goal.',2025-12-24
agent_infrastructure,anthropic,https://simonw.substack.com/p/the-lethal-trifecta-for-ai-agents,The lethal trifecta for AI agents,Simon Willison Newsletter,June 2025,"Critical security analysis of prompt injection risks when agents combine private data, untrusted content, and exfiltration.",2025-12-24
retrieval_and_embeddings,openai,https://milvus.io/docs/zh/v2.5.x/release_notes.md,"Milvus v2.5.x Release Notes (v2.5.24 released Dec 23, 2025)",Milvus documentation,2025-12-23,Fresh vector database release notes (incl. security/stability work and BM25/hybrid-search-related improvements) for teams running Milvus in production.,2025-12-24
retrieval_and_embeddings,openai,https://github.com/milvus-io/milvus/releases,Releases · milvus-io/milvus,GitHub,2025-12-04 (latest shown: v2.6.7),"Canonical tags/releases for Milvus (useful for tracking new vector-search features, perf fixes, and upgrade cadence).",2025-12-24
retrieval_and_embeddings,openai,https://americas.kioxia.com/en-us/business/news/2025/ssd-20251216-1.html,KIOXIA AiSAQ Technology Integrated into Milvus Vector Database,KIOXIA (Press release),2025-12-16,ANN/search-acceleration software integration into Milvus (impacting vector index scalability and DRAM footprint).,2025-12-24
retrieval_and_embeddings,openai,https://qdrant.tech/blog/2025-recap/,Qdrant 2025 Recap: Powering the Agentic Era,Qdrant blog,2025-12-17,"End-of-year roundup of Qdrant’s retrieval features (hybrid, reranking, quantization, multitenancy) and direction for agentic workloads.",2025-12-24
retrieval_and_embeddings,openai,https://qdrant.tech/blog/qdrant-1.16.x/,Qdrant 1.16 - Tiered Multitenancy & Disk-Efficient Vector Search,Qdrant blog,2025-11-19,"Major Qdrant release writeup covering filtered-search quality (ACORN), disk-efficient HNSW, and operational changes relevant to RAG infra.",2025-12-24
retrieval_and_embeddings,openai,https://github.com/qdrant/qdrant/releases/tag/v1.16.0,Release v1.16.0 · qdrant/qdrant,GitHub,2025-11-17,"Detailed changelog for Qdrant 1.16 (ACORN-1, inline storage, RRF tuning, quantization/SIMD improvements) useful for implementers.",2025-12-24
retrieval_and_embeddings,openai,https://weaviate.io/blog/weaviate-1-34-release,Weaviate 1.34 Release,Weaviate blog,2025-11-11,Weaviate release with flat index + rotational quantization preview and new production-focused retrieval/ops improvements.,2025-12-24
retrieval_and_embeddings,openai,https://weaviate.io/blog/weaviate-1-33-release,Weaviate 1.33 Release,Weaviate blog,2025-10-02,Compression-by-default and new quantization options; important for cost/latency tradeoffs in vector + hybrid retrieval.,2025-12-24
retrieval_and_embeddings,openai,https://weaviate.io/blog/weaviate-1-32-release,Weaviate 1.32 Release,Weaviate blog,2025-07-22,Introduces rotational quantization and HNSW memory optimizations—core for scaling embeddings and hybrid search economically.,2025-12-24
retrieval_and_embeddings,openai,https://weaviate.io/blog/muvera,More efficient multi-vector embeddings with MUVERA,Weaviate blog,2025-06-05,Deep dive on MUVERA encoding to compress ColBERT/ColPali-style multi-vector embeddings into fixed-size vectors for cheaper retrieval.,2025-12-24
retrieval_and_embeddings,openai,https://weaviate.io/blog/weaviate-1-31-release,Weaviate 1.31 Release,Weaviate blog,2025-06-03,"Release post covering MUVERA support plus model integrations (VoyageAI, Cohere, model2vec) relevant to embedding pipelines.",2025-12-24
retrieval_and_embeddings,openai,https://www.pinecone.io/community/events/getting-started-2025-12/,Getting started with Pinecone (December 2025),Pinecone community/events,2025-12-18,Recent Pinecone session explicitly covering hybrid search + rerankers and RAG build patterns (useful for up-to-date product capabilities).,2025-12-24
retrieval_and_embeddings,openai,https://www.mongodb.com/company/blog/product-release-announcements/rerank-2-5-and-rerank-2-5-lite-instruction-following-rerankers,rerank-2.5 and rerank-2.5-lite: Instruction-Following Rerankers,MongoDB blog,2025-09-09 (updated),"Practical reranker release info (instruction-following reranking, longer context, benchmark deltas) for RAG relevance pipelines.",2025-12-24
retrieval_and_embeddings,openai,https://www.mongodb.com/products/updates/public-preview-mongodb-community-edition-now-offers-native-full-text-and-vector-search/,Public Preview: MongoDB Community Edition Now Offers Native Full-Text and Vector Search,MongoDB product updates,2025-09-17,Native hybrid stack (FTS + vectors) in a mainstream DB—important for teams consolidating retrieval infra.,2025-12-24
retrieval_and_embeddings,openai,https://opensearch.org/announcements/opensearch-3-0-enhances-vector-database-performance/,"OpenSearch 3.0 Enhances Vector Database Performance, Search Infrastructure and Scalability",OpenSearch announcement,2025-05-06 (updated 2025-08-25),Vector+hybrid search infra update with performance claims and release details relevant to large-scale retrieval deployments.,2025-12-24
retrieval_and_embeddings,openai,https://www.oracle.com/database/ai-native-database-26ai/,"Oracle introduces its AI-native database, Oracle AI Database 26ai",Oracle product page,2025-10 (release update referenced),Enterprise database release emphasizing native vector search + hybrid AI features; relevant for embedding storage/search in legacy stacks.,2025-12-24
retrieval_and_embeddings,openai,https://blog.voyageai.com/2025/05/20/voyage-3-5/,voyage-3.5 and voyage-3.5-lite: improved quality for a new retrieval frontier,Voyage AI blog,2025-05-20,Embedding model release focused on retrieval quality vs cost (Matryoshka dims + quantization options) for vector DB efficiency.,2025-12-24
retrieval_and_embeddings,openai,https://docs.cohere.com/changelog/embed-multimodal-v4,Announcing Embed Multimodal v4,Cohere documentation (changelog),2025-04-15,"Cohere’s multimodal embedding model update (Matryoshka dims, long context) for text+image retrieval and RAG over PDFs.",2025-12-24
retrieval_and_embeddings,openai,https://docs.contextual.ai/release-notes/latest,2025 Release Updates (Contextual AI),Contextual AI docs,recent,"Tracks latest reranker/agent updates (e.g., filtering chunks by reranker score) that directly affect retrieval quality controls.",2025-12-24
retrieval_and_embeddings,openai,https://www.contextual.ai/blog/rerank-v2/,Open-Sourcing Reranker v2,Contextual AI blog,2025-08-27,Open-source instruction-following reranker family + eval datasets—high-signal update for reranking and RAG relevance.,2025-12-24
retrieval_and_embeddings,openai,https://www.contextual.ai/blog/introducing-instruction-following-reranker/,Introducing the world’s first instruction-following reranker,Contextual AI blog,2025-03-11,"Core concept post on instruction-steerable reranking, useful for hybrid retrieval policies and domain-specific ranking rules.",2025-12-24
retrieval_and_embeddings,openai,https://huggingface.co/vec-ai/lychee-rerank,Lychee Rerank,Hugging Face,recent,New open reranker (Qwen2.5-based) advertising long context (32k) and multilingual reranking—useful for modern RAG stacks.,2025-12-24
retrieval_and_embeddings,openai,https://huggingface.co/abdoelsayed/dear-8b-reranker-ce-v1,DeAR-8B-Reranker-CE-v1,Hugging Face,recent,Recent 8B pointwise reranker (Llama-3.1-based) with distillation—useful as an OSS reranker baseline.,2025-12-24
retrieval_and_embeddings,openai,https://huggingface.co/teslov/reranker,jina-reranker-v2-base-multilingual,Hugging Face,recent,Multilingual cross-encoder reranker model card + evaluation notes; relevant for cross-lingual retrieval/reranking.,2025-12-24
retrieval_and_embeddings,openai,https://arxiv.org/abs/2512.15922,Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems,arXiv,2025-12-17,Very recent GraphRAG-style retrieval method using spreading activation over auto-constructed graphs to improve multi-hop evidence retrieval.,2025-12-24
retrieval_and_embeddings,openai,https://arxiv.org/abs/2512.09487,RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning,arXiv,2025-12-10,December 2025 hybrid text+graph RAG framework using RL to adaptively decide when/what to retrieve—relevant to agentic retrieval.,2025-12-24
retrieval_and_embeddings,openai,https://arxiv.org/abs/2512.12694,Hybrid Retrieval-Augmented Generation for Robust Multilingual Document Question Answering,arXiv,2025-12-14,Recent hybrid retrieval pipeline with query expansion + RRF fusion aimed at robustness on noisy/OCR’d multilingual corpora.,2025-12-24
retrieval_and_embeddings,openai,https://arxiv.org/abs/2511.19987,R²R: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers,arXiv,2025-11-25,New reranker post-training framework for multi-domain specialization (routing + abstraction) directly relevant to RAG reranking.,2025-12-24
retrieval_and_embeddings,openai,https://arxiv.org/abs/2511.16681,Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search,arXiv,2025-11-12,Targets vector DB retrieval efficiency for RAG via multi-resolution indexing and query-adaptive control; includes FAISS/Qdrant integration claim.,2025-12-24
retrieval_and_embeddings,openai,https://arxiv.org/abs/2511.07025,Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks,arXiv,2025-11-10,Open-weights embedding model focused on multilingual/cross-lingual retrieval with instruction-awareness—strong candidate embedder for RAG.,2025-12-24
retrieval_and_embeddings,anthropic,https://arxiv.org/abs/2412.15605,Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks,arXiv,"December 20, 2024",Proposes Cache-Augmented Generation (CAG) as an alternative to RAG that eliminates retrieval latency by preloading documents into extended context windows.,2025-12-24
retrieval_and_embeddings,anthropic,https://www.microsoft.com/en-us/research/blog/moving-to-graphrag-1-0-streamlining-ergonomics-for-developers-and-users/,GraphRAG 1.0 includes major ergonomic and structural updates,Microsoft Research,"December 16, 2024","Microsoft officially releases GraphRAG 1.0 with simplified configuration, API layer improvements, and 80% disk space reduction for output files.",2025-12-24
retrieval_and_embeddings,anthropic,https://ragflow.io/blog/the-rise-and-evolution-of-rag-in-2024-a-year-in-review,The Rise and Evolution of RAG in 2024: A Year in Review,RAGFlow Blog,"December 24, 2024","Comprehensive 2024 retrospective covering GraphRAG, multimodal RAG, agentic RAG integration, and VLM advances.",2025-12-24
retrieval_and_embeddings,anthropic,https://www.llamaindex.ai/blog/rag-is-dead-long-live-agentic-retrieval,"RAG is dead, long live agentic retrieval",LlamaIndex Blog,recent,"LlamaIndex showcases advanced agentic retrieval techniques including composite retrievers, query routing, and multi-index orchestration.",2025-12-24
retrieval_and_embeddings,anthropic,https://venturebeat.com/data-infrastructure/aws-claims-90-vector-cost-savings-with-s3-vectors-ga-calls-it-complementary,AWS claims 90% vector cost savings with S3 Vectors GA,VentureBeat,December 2024,"AWS launches Amazon S3 Vectors in GA, enabling native vector storage and search in S3 with claimed 90% cost reduction versus dedicated vector DBs.",2025-12-24
retrieval_and_embeddings,anthropic,https://huggingface.co/blog/train-reranker,Training and Finetuning Reranker Models with Sentence Transformers v4,Hugging Face Blog,recent,Major Sentence Transformers v4 release enables training cross-encoder rerankers with new loss functions and evaluation methods.,2025-12-24
retrieval_and_embeddings,anthropic,https://jina.ai/news/jina-reranker-v2-for-agentic-rag-ultra-fast-multilingual-function-calling-and-code-search/,"Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling & Code Search",Jina AI,July 2024,Jina Reranker v2 achieves 6x throughput improvement with 100+ language support and function-calling capabilities for agentic RAG.,2025-12-24
retrieval_and_embeddings,anthropic,https://arxiv.org/abs/2408.16672,Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever,arXiv,September 2024,"New multilingual ColBERT variant with 89 language support, Matryoshka embeddings, and 8192 token context for late interaction retrieval.",2025-12-24
retrieval_and_embeddings,anthropic,https://weaviate.io/blog/late-interaction-overview,"An Overview of Late Interaction Retrieval Models: ColBERT, ColPali, and ColQwen",Weaviate Blog,April 2025,Comprehensive overview of late interaction models including ColPali and ColQwen for multimodal document retrieval without OCR.,2025-12-24
retrieval_and_embeddings,anthropic,https://www.businesswire.com/news/home/20251119343840/en/Qdrant-Introduces-Tiered-Multitenancy-to-Eliminate-Noisy-Neighbor-Problems-in-Vector-Search,Qdrant Introduces Tiered Multitenancy in v1.16,Business Wire,November 2024,"Qdrant v1.16 adds tiered multitenancy, ACORN search algorithm for filtered search, and HNSW index storage improvements.",2025-12-24
retrieval_and_embeddings,anthropic,https://thenewstack.io/vector-search-is-reaching-its-limit-heres-what-comes-next/,Vector Search Is Reaching Its Limit. Here's What Comes Next,The New Stack,August 2025,Analysis of vector database limitations and emerging tensor-based retrieval approaches for next-generation RAG.,2025-12-24
retrieval_and_embeddings,anthropic,https://stackoverflow.blog/2024/12/27/breaking-up-is-hard-to-do-chunking-in-rag-applications/,Breaking up is hard to do: Chunking in RAG applications,Stack Overflow Blog,December 2024,"Practical guide to chunking strategies including adaptive chunking, sliding windows, and metadata-based filtering.",2025-12-24
retrieval_and_embeddings,anthropic,https://arxiv.org/abs/2402.03216,"BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings",arXiv,February 2024,"BAAI releases BGE-M3 supporting 100+ languages, dense/sparse/multi-vector retrieval, and 8192 token inputs in single unified model.",2025-12-24
retrieval_and_embeddings,anthropic,https://github.com/FlagOpen/FlagEmbedding,FlagEmbedding: Retrieval and Retrieval-augmented LLMs,GitHub,recent,"Active repository with BGE-VL multimodal embeddings, bge-reranker-v2.5-gemma2-lightweight, and multilingual gemma2 embeddings.",2025-12-24
retrieval_and_embeddings,anthropic,https://modal.com/blog/mteb-leaderboard-article,Top embedding models on the MTEB leaderboard,Modal Blog,2025,Analysis of top MTEB performers including Qwen3-Embedding-8B and NVIDIA NV-Retriever-v2 with practical deployment guidance.,2025-12-24
retrieval_and_embeddings,anthropic,https://www.zenml.io/blog/best-embedding-models-for-rag,9 Best Embedding Models for RAG to Try This Year,ZenML Blog,recent,"Comparison of Voyage-3.5, ModernBERT-Embed, Gemini Embedding, and OpenAI text-embedding-3-large for production RAG.",2025-12-24
retrieval_and_embeddings,anthropic,https://github.com/microsoft/graphrag/releases,GraphRAG Releases - v2.3.0,GitHub,recent,"Latest GraphRAG releases with dynamic community selection, drift search improvements, and updated fnllm integration.",2025-12-24
retrieval_and_embeddings,anthropic,https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-11-19,LlamaIndex Newsletter 2024-11-19,LlamaIndex Blog,November 2024,Features dynamic section retrieval technique and ColPali integration for multimodal RAG with Cohere embeddings.,2025-12-24
retrieval_and_embeddings,anthropic,https://www.elastic.co/what-is/hybrid-search,A Comprehensive Hybrid Search Guide,Elastic,recent,Detailed guide on RRF and linear combination methods for hybrid lexical-semantic search implementation.,2025-12-24
retrieval_and_embeddings,anthropic,https://supabase.com/docs/guides/ai/hybrid-search,Hybrid search with pgvector and tsvector,Supabase Docs,December 2024,Tutorial implementing hybrid search in Postgres using pgvector for semantic and tsvector for keyword search with RRF.,2025-12-24
retrieval_and_embeddings,anthropic,https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chunking-strategies-for-rag-applications/ba-p/113089,The Ultimate Guide to Chunking Strategies for RAG Applications with Databricks,Databricks Community,April 2025,"Comprehensive chunking guide covering fixed-size, semantic, recursive, and AI-driven chunking with Databricks examples.",2025-12-24
retrieval_and_embeddings,anthropic,https://www.analyticsvidhya.com/blog/2024/10/chunking-techniques-to-build-exceptional-rag-systems/,15 Chunking Techniques to Build Exceptional RAGs Systems,Analytics Vidhya,October 2024,"Covers hierarchical, sliding window, modality-specific chunking and table-aware strategies for complex documents.",2025-12-24
retrieval_and_embeddings,anthropic,https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview,Retrieval Augmented Generation (RAG) in Azure AI Search,Microsoft Learn,recent,Microsoft introduces agentic retrieval in Azure AI Search with LLM-assisted query planning and multi-source access.,2025-12-24
retrieval_and_embeddings,anthropic,https://venturebeat.com/ai/beyond-rag-how-cache-augmented-generation-reduces-latency-complexity-for-smaller-workloads,"Beyond RAG: How cache-augmented generation reduces latency, complexity for smaller workloads",VentureBeat,August 2025,Detailed analysis showing CAG outperforms RAG on SQuAD and HotPotQA when knowledge base fits in context window.,2025-12-24
retrieval_and_embeddings,anthropic,https://pinecone.io/learn/series/rag/rerankers/,Rerankers and Two-Stage Retrieval,Pinecone Learn,recent,"Explains cross-encoder rerankers, latency tradeoffs, and two-stage retrieval implementation with code examples.",2025-12-24
retrieval_and_embeddings,anthropic,https://www.analyticsvidhya.com/blog/2025/06/top-rerankers-for-rag/,Top 7 Rerankers for RAG,Analytics Vidhya,June 2025,"Comparison of Cohere, Voyage, Jina, and BGE rerankers with performance benchmarks and cost analysis.",2025-12-24
retrieval_and_embeddings,anthropic,https://www.elastic.co/search-labs/blog/elastic-semantic-reranker-part-3,Semantic Reranker: Selecting optimal reranking depth for models,Elastic Search Labs,September 2025,Quantitative analysis of reranking depth impact on BEIR benchmarks with Elastic Rerank model.,2025-12-24
retrieval_and_embeddings,anthropic,https://www.llamaindex.ai/blog/the-year-in-llamaindex-2024,The Year in LlamaIndex: 2024,LlamaIndex Blog,December 2024,"LlamaIndex 2024 retrospective covering LlamaCloud, LlamaParse, Property Graph Index, and create-llama milestones.",2025-12-24
retrieval_and_embeddings,anthropic,https://signalhub.substack.com/p/cursors-new-acquisition-means-a-lot,"Cursor's new Acquisition Means a lot, a New Vector Database Joins the 10×ARR Club",SignalHub Substack,December 2024,Reports on emerging vector database startup reaching tens of millions in ARR with customers including Anthropic and Cursor.,2025-12-24
multimodal_and_generation,openai,https://openai.com/index/new-chatgpt-images-is-here/,The new ChatGPT Images is here (GPT Image 1.5),OpenAI blog,2025-12-16,New flagship image generation/editing model (GPT Image 1.5) with faster generation and improved instruction-following—useful for multimodal content generation workflows.,2025-12-24
multimodal_and_generation,openai,https://blog.google/products/gemini/gemini-3-flash/,Gemini 3 Flash: frontier intelligence built for speed,Google Blog,2025-12-17,"Release details for Gemini 3 Flash (benchmarks, pricing, availability) emphasizing fast multimodal reasoning for production apps and agentic workflows.",2025-12-24
multimodal_and_generation,openai,https://blog.google/products/search/google-ai-mode-update-gemini-3-flash,Gemini 3 Flash is rolling out globally in Google Search,Google Blog (Search),2025-12-17,Describes Gemini 3 Flash rollout in Search AI Mode and expanded access to Gemini 3 Pro and Nano Banana Pro image generation/editing—relevant for multimodal UX patterns.,2025-12-24
multimodal_and_generation,openai,https://deepmind.google/models/model-cards/,"Model cards (Gemini family, incl. Gemini 3 Pro/Flash)",Google DeepMind,2025-12-17 (updated),Central index of DeepMind Gemini model cards (incl. Gemini 3 Flash/Pro/Pro Image) for evaluation details and safety/usage notes.,2025-12-24
multimodal_and_generation,openai,https://docs.mistral.ai/models/mistral-large-3-25-12,Mistral Large 3 (v25.12),Mistral Docs,2025-12-02,Official specs for Mistral’s open-weight multimodal MoE model with long context and features like OCR and structured outputs—key for document QA and multimodal RAG.,2025-12-24
multimodal_and_generation,openai,https://docs.mistral.ai/models/ocr-3-25-12,OCR 3 (v25.12),Mistral Docs,2025-12-15,"Mistral’s OCR service powering Document AI stacks (interleaved text+images, annotations, bbox extraction) for robust PDF/document parsing pipelines.",2025-12-24
multimodal_and_generation,openai,https://docs.mistral.ai/models/ministral-3-8b-25-12,Ministral 3 8B (v25.12),Mistral Docs,2025-12-02,Edge-friendly multimodal model (vision + long context) positioned for local deployments that still need document/image understanding and structured extraction.,2025-12-24
multimodal_and_generation,openai,https://docs.anthropic.com/pt/docs/build-with-claude/pdf-support,PDF support,Anthropic Docs,recent,"Official guide to Claude PDF input (limits, supported models, URL/base64/file_id options) enabling visual+text understanding of PDFs including charts/tables.",2025-12-24
multimodal_and_generation,openai,https://arxiv.org/abs/2512.17221,DAVE: A VLM Vision Encoder for Document Understanding and Web Agents,arXiv,2025-12-19,New vision encoder tailored for document/web UI structure and spatial features—important for doc understanding and web-agent perception stacks.,2025-12-24
multimodal_and_generation,openai,https://arxiv.org/abs/2511.11552,DocLens: A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding,arXiv,2025-11-14,Multi-agent “zoom-in” approach for evidence localization in long documents; targets hallucination reduction and cross-page reasoning.,2025-12-24
multimodal_and_generation,openai,https://arxiv.org/abs/2511.11313,DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding,arXiv,2025-11-14,Memory/latency-efficient VLM for long documents using hierarchical compression and streaming abstention—relevant for deployment-constrained doc AI.,2025-12-24
multimodal_and_generation,openai,https://arxiv.org/abs/2511.12003,Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning,arXiv,2025-11-15,Introduces Chain-of-Evidence reasoning with page+box citations trained via RL—directly applicable to verifiable multimodal RAG over PDFs.,2025-12-24
multimodal_and_generation,openai,https://arxiv.org/abs/2512.00305,ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning,arXiv,2025-11-29,Chart reasoning method + dataset (ChartPoint-SFT) using bbox grounding/reflection to reduce numerical hallucinations in chart QA/summarization.,2025-12-24
multimodal_and_generation,openai,https://arxiv.org/abs/2503.11576,SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion,arXiv,2025-03-14,Compact VLM that outputs structured DocTags for full-page document conversion (tables/charts/equations) enabling cleaner downstream RAG ingestion.,2025-12-24
multimodal_and_generation,openai,https://huggingface.co/ds4sd/SmolDocling-256M-preview,SmolDocling-256M-preview,Hugging Face,recent,Practical release of SmolDocling weights and datasets; a ready-to-run VLM for document conversion with layout and bbox-aware outputs.,2025-12-24
multimodal_and_generation,openai,https://www.ibm.com/new/announcements/granite-docling-end-to-end-document-conversion,IBM Granite-Docling: End-to-end document conversion,IBM announcements,recent,"Product-ready successor to SmolDocling focused on stable, faithful structured document conversion for downstream RAG and enterprise workflows.",2025-12-24
multimodal_and_generation,openai,https://research.ibm.com/publications/docling-an-efficient-open-source-toolkit-for-ai-driven-document-conversion,Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion (AAAI 2025),IBM Research,2025-02-25,Canonical reference for Docling’s modular document conversion pipeline and structured representations; useful baseline for PDF parsing stacks.,2025-12-24
multimodal_and_generation,openai,https://github.com/IBM/docling-graph,IBM/docling-graph,GitHub,recent,Converts documents into validated Pydantic objects and directed knowledge graphs—useful for structured extraction beyond vector RAG.,2025-12-24
multimodal_and_generation,openai,https://github.com/opendatalab/MinerU,opendatalab/MinerU,GitHub,2025-08-01 (v2.1.10 noted),Open-source pipeline turning complex PDFs into LLM-ready Markdown/JSON with bbox fields and improved table parsing—strong option for doc ingestion.,2025-12-24
multimodal_and_generation,openai,https://github.com/sylphxltd/pdf-reader-mcp,SylphxAI/pdf-reader-mcp,GitHub,recent,"Production MCP server for PDF processing (parallel extraction, agent-friendly) to plug into tool-using LLM systems for document workflows.",2025-12-24
multimodal_and_generation,openai,https://github.com/Bessouat40/RAGLight,Bessouat40/RAGLight,GitHub,2025-10-13 (latest release shown),Modular RAG framework with MCP integration and flexible document ingestion (incl. PDFs); useful scaffolding for multimodal RAG prototypes.,2025-12-24
multimodal_and_generation,openai,https://nbrosse.github.io/posts/pdf-parsing/pdf-parsing.html,PDF Parsing for LLM Input,Nicolas’ Notebook (blog),2025-02-18,"Hands-on overview of PDF-to-LLM preprocessing tradeoffs (layout, OCR, segmentation) and references to key tools/papers for document pipelines.",2025-12-24
multimodal_and_generation,openai,https://arxiv.org/abs/2505.17471,FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain,arXiv,2025-05-23,Benchmark for visual RAG over finance PDFs with visual citations and evaluation methods—targets grounding and traceability.,2025-12-24
multimodal_and_generation,openai,https://huggingface.co/datasets/zhaosuifeng/FinRAGBench-V,FinRAGBench-V dataset,Hugging Face Datasets,recent,Downloadable corpus + QA for building/benchmarking citation-grounded multimodal RAG over document page images (finance domain).,2025-12-24
multimodal_and_generation,openai,https://t2ragbench.demo.hcds.uni-hamburg.de/,T²-RAGBench,University of Hamburg (benchmark site),2025-05 (released),"Realistic benchmark for RAG over financial reports combining text and tables, emphasizing numerical reasoning and retrieval robustness.",2025-12-24
multimodal_and_generation,openai,https://www.microsoft.com/en-us/research/publication/mmed-rag-versatile-multimodal-rag-system-for-medical-vision-language-models/?lang=fr-ca%2Fbibtex%2F,MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models,Microsoft Research,2024-10,Multimodal RAG system for medical VLMs aimed at reducing hallucinations in VQA/report generation; includes linked code/data.,2025-12-24
multimodal_and_generation,openai,https://arxiv.org/abs/2508.00579,MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval,arXiv,2025-08-01,Document-QA-focused multimodal RAG method with hierarchical indexing and cross-page retrieval—directly relevant to long PDF QA.,2025-12-24
multimodal_and_generation,openai,https://arxiv.org/abs/2407.01523,MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations,arXiv,2024-07-01,Long-context multimodal document benchmark built from lengthy PDFs with cross-page and unanswerable questions; useful for evaluating VLM document QA.,2025-12-24
multimodal_and_generation,anthropic,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,Google introduces Gemini 2.0: A new AI model for the agentic era,Google Blog,"December 11, 2024",Major release of Google's flagship multimodal model with native image/audio output and advanced agentic capabilities.,2025-12-24
multimodal_and_generation,anthropic,https://simonwillison.net/2024/Dec/11/gemini-2/,Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode,Simon Willison's Blog,"December 11, 2024",Technical deep-dive into Gemini 2.0 Flash capabilities with hands-on exploration of multimodal features.,2025-12-24
multimodal_and_generation,anthropic,https://huggingface.co/blog/vlms-2025,"Vision Language Models (Better, faster, stronger)",Hugging Face Blog,recent,Comprehensive overview of VLM advances in 2024-2025 including MoE architectures and multimodal RAG trends.,2025-12-24
multimodal_and_generation,anthropic,https://github.com/docling-project/docling,Docling: Get your documents ready for gen AI,GitHub,recent,"IBM's open-source document conversion toolkit with advanced PDF understanding, table structure detection, and RAG integrations.",2025-12-24
multimodal_and_generation,anthropic,https://arxiv.org/abs/2407.01449,ColPali: Efficient Document Retrieval with Vision Language Models,arXiv,June 2024 (updated Feb 2025),"Groundbreaking approach to document retrieval by directly embedding page images, outperforming traditional pipelines.",2025-12-24
multimodal_and_generation,anthropic,https://huggingface.co/learn/cookbook/en/multimodal_rag_using_document_retrieval_and_vlms,Multimodal RAG with ColPali and Vision Language Models,Hugging Face Cookbook,recent,Practical tutorial on building multimodal RAG systems combining ColPali retriever with Qwen2-VL.,2025-12-24
multimodal_and_generation,anthropic,https://arxiv.org/abs/2409.12191,Qwen2-VL: Enhancing Vision-Language Model's Perception at Any Resolution,arXiv,September 2024,State-of-the-art open VLM with dynamic resolution processing and multimodal position embeddings.,2025-12-24
multimodal_and_generation,anthropic,https://internvl.github.io/blog/2024-12-05-InternVL-2.5/,InternVL 2.5: Expanding Performance Boundaries of Open-Source MLLMs,InternVL Blog,December 2024,"First open-source MLLM to achieve 70% on MMMU benchmark, rivaling GPT-4o performance.",2025-12-24
multimodal_and_generation,anthropic,https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/,An Easy Introduction to Multimodal RAG,NVIDIA Technical Blog,December 2024,"NVIDIA's guide to building RAG systems that handle tables, charts, and mixed modalities.",2025-12-24
multimodal_and_generation,anthropic,https://github.com/HKUDS/RAG-Anything,RAG-Anything: All-in-One Multimodal RAG Framework,GitHub,recent,"End-to-end multimodal RAG framework handling text, images, tables, equations with knowledge graph integration.",2025-12-24
multimodal_and_generation,anthropic,https://arxiv.org/html/2506.16035v1,Vision-Guided Chunking for RAG with Multimodal Document Understanding,arXiv,June 2025,Novel approach using VLMs to improve document chunking quality for RAG systems.,2025-12-24
multimodal_and_generation,anthropic,https://www.vectara.com/blog/open-rag-benchmark-a-new-frontier-for-multimodal-pdf-understanding-in-rag,Open RAG Benchmark: Multimodal PDF Understanding,Vectara Blog,recent,"New benchmark for evaluating RAG systems on text, tables, and images from arXiv PDFs.",2025-12-24
multimodal_and_generation,anthropic,https://www.llamaindex.ai/blog/pdf-parsing-llamaparse,Parsing PDFs with LlamaParse: A How-To Guide,LlamaIndex Blog,recent,GenAI-native PDF parsing platform for extracting complex document structures into LLM-ready formats.,2025-12-24
multimodal_and_generation,anthropic,https://arxiv.org/abs/2311.16483,ChartLlama: A Multimodal LLM for Chart Understanding and Generation,arXiv,November 2023,Specialized MLLM for chart comprehension achieving SOTA on ChartQA and chart extraction benchmarks.,2025-12-24
multimodal_and_generation,anthropic,https://arxiv.org/html/2402.05121v2,Large Language Model for Table Processing: A Survey,arXiv,July 2024,"Comprehensive survey on LLM and VLM techniques for table QA, extraction, and data analysis.",2025-12-24
multimodal_and_generation,anthropic,https://nanonets.com/blog/table-extraction-using-llms-unlocking-structured-data-from-documents/,Table Extraction using LLMs: Unlocking Structured Data,Nanonets Blog,September 2025,"Practical guide comparing GPT-4o, Gemini, and other models for table extraction with prompt engineering tips.",2025-12-24
multimodal_and_generation,anthropic,https://www.llamaindex.ai/blog/building-blocks-of-llm-report-generation-beyond-basic-rag,Building Blocks of LLM Report Generation: Beyond Basic RAG,LlamaIndex Blog,November 2024,"Framework for generating multimodal reports using structured outputs, multi-agent workflows, and template processing.",2025-12-24
multimodal_and_generation,anthropic,https://github.com/iamarunbrahma/vision-parse,Vision-Parse: Parse PDFs into Markdown using Vision LLMs,GitHub,recent,"Open-source tool using GPT-4o, Gemini, and Llama vision models to convert PDFs to markdown.",2025-12-24
multimodal_and_generation,anthropic,https://arxiv.org/abs/2411.06284,A Comprehensive Survey of MLLMs in Vision-Language Tasks,arXiv,December 2024,"Latest survey covering MLLM architectures, training methods, and applications in visual understanding.",2025-12-24
multimodal_and_generation,anthropic,https://www.nature.com/articles/s41467-025-61040-5,MiniCPM-V: Efficient GPT-4V Level Multimodal LLM for Edge Devices,Nature Communications,July 2025,8B model outperforming GPT-4V on benchmarks while running efficiently on mobile phones.,2025-12-24
multimodal_and_generation,anthropic,https://www.ibm.com/think/tutorials/build-multimodal-rag-langchain-with-docling-granite,Build Multimodal RAG with Docling and Granite,IBM,November 2025,Tutorial on building AI-powered multimodal RAG pipeline using IBM's Docling and Granite vision models.,2025-12-24
multimodal_and_generation,anthropic,https://www.sciencedirect.com/science/article/abs/pii/S156625352500867X,Small Vision-Language Models Survey: Optimization Strategies,ScienceDirect,October 2025,"Survey on SVLMs covering quantization, MoE architectures, and edge deployment strategies.",2025-12-24
multimodal_and_generation,anthropic,https://arxiv.org/html/2501.02189v6,"Survey of SOTA Large Vision Language Models: Alignment, Benchmarks, Evaluations",arXiv,April 2025,Comprehensive survey of VLMs from 2019-2024 covering training objectives and multimodal integration.,2025-12-24
multimodal_and_generation,anthropic,https://github.com/Filimoa/open-parse,Open-Parse: Improved File Parsing for LLMs,GitHub,recent,Open-source visually-driven document parser with semantic chunking and table extraction.,2025-12-24
multimodal_and_generation,anthropic,https://medium.com/kx-systems/rag-llamaparse-advanced-pdf-parsing-for-retrieval-c393ab29891b,RAG + LlamaParse: Advanced PDF Parsing for Retrieval,Medium (KX Systems),June 2025,Technical guide on using LlamaParse for extracting complex documents for RAG pipelines.,2025-12-24
multimodal_and_generation,anthropic,https://towardsdatascience.com/mulitmodal-llms-interpreting-charts-b212f5c0aa1f/,I Tested Frontline M-LLMs on Chart Interpretation Skills,Towards Data Science,January 2025,"Benchmark comparison of Claude 3.5 Sonnet, GPT-4o, and Llama 3.2 on chart understanding tasks.",2025-12-24
arxiv,arxiv,https://arxiv.org/abs/2512.20618v1,LongVideoAgent: Multi-Agent Reasoning with Long Videos,arXiv,2025-12-23,"Summary: Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20612v1,Making Large Language Models Efficient Dense Retrievers,arXiv,2025-12-23,"Summary: Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20610v1,FedPOD: the deployable units of training for federated learning,arXiv,2025-12-23,"Summary: This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20607v1,Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures,arXiv,2025-12-23,"Summary: Neural networks trained with gradient descent often learn solutions of increasing complexity over time, a phenomenon known as simplicity bias. Despite being widely observed across architectures, existing theoretical treatments lack a unifying framework. We present a theoretical framework that explains a simplicity bias arising from saddle-to-saddle learning dynamics for a general class of neural networks, incorporating fully-connected, convolutional, and attention-based architectures. Here, simple means expressible with few hidden units, i.e., hidden neurons, convolutional kernels, or attention heads. Specifically, we show that linear networks learn solutions of increasing rank, ReLU networks learn solutions with an increasing number of kinks, convolutional networks learn solutions with an increasing number of convolutional kernels, and self-attention models learn solutions with an increasing number of attention heads. By analyzing fixed points, invariant manifolds, and dynamics of gradient descent learning, we show that saddle-to-saddle dynamics operates by iteratively evolving near an invariant manifold, approaching a saddle, and switching to another invariant manifold. Our analysis also illuminates the effects of data distribution and weight initialization on the duration and number of plateaus in learning, dissociating previously confounding factors. Overall, our theory offers a framework for understanding when and why gradient descent progressively learns increasingly complex solutions.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20605v1,Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning,arXiv,2025-12-23,"Summary: Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term ""internal RL"", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20604v1,MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts,arXiv,2025-12-23,"Summary: We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20600v1,Modeling Economic Systems as Multiport Networks,arXiv,2025-12-23,"Summary: In this paper, we demonstrate how multiport network theory can be used as a powerful modeling tool in economics. The critical insight is using the port concept to pair the flow of goods (the electrical current) with the agent's incentive (the voltage) in an economic interaction. By building networks of agents interacting through ports, we create models with multiple levels of abstraction, from the macro level down to the micro level. We are thereby able to model complex macroeconomic systems whose dynamical behavior is emergent from the micro level. Using the LTSpice circuit simulator, we then design and analyze a series of example systems that range in complexity from the textbook Robinson Crusoe economy to a model of an entire economy.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20595v1,Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs,arXiv,2025-12-23,"Summary: We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20589v1,Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information,arXiv,2025-12-23,"Summary: As systems engineering (SE) objectives evolve from design and operation of monolithic systems to complex System of Systems (SoS), the discipline of Mission Engineering (ME) has emerged which is increasingly being accepted as a new line of thinking for the SE community. Moreover, mission environments are uncertain, dynamic, and mission outcomes are a direct function of how the mission assets will interact with this environment. This proves static architectures brittle and calls for analytically rigorous approaches for ME. To that end, this paper proposes an intelligent mission coordination methodology that integrates digital mission models with Reinforcement Learning (RL), that specifically addresses the need for adaptive task allocation and reconfiguration. More specifically, we are leveraging a Digital Engineering (DE) based infrastructure that is composed of a high-fidelity digital mission model and agent-based simulation; and then we formulate the mission tactics management problem as a Markov Decision Process (MDP), and employ an RL agent trained via Proximal Policy Optimization. By leveraging the simulation as a sandbox, we map the system states to actions, refining the policy based on realized mission outcomes. The utility of the RL-based intelligent mission coordinator is demonstrated through an aerial firefighting case study. Our findings indicate that the RL-based intelligent mission coordinator not only surpasses baseline performance but also significantly reduces the variability in mission performance. Thus, this study serves as a proof of concept demonstrating that DE-enabled mission simulations combined with advanced analytical tools offer a mission-agnostic framework for improving ME practice; which can be extended to more complicated fleet design and selection problems in the future from a mission-first perspective.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20586v1,Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent,arXiv,2025-12-23,"Summary: Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20584v1,A human-centered approach to reframing job satisfaction in the BIM-enabled construction industry,arXiv,2025-12-23,"Summary: As the construction industry undergoes rapid digital transformation, ensuring that new technologies enhance rather than hinder human experience has become essential. The inclusion of Building Information Modeling (BIM) plays a central role in this shift, yet its influence on job satisfaction remains underexplored. In response, this study developed a human-centered measurement model for evaluating job satisfaction in BIM work environments by adapting Hackman and Oldham's Job Characteristics Model for the architecture, engineering, and construction (AEC) industry to create a survey that captured industry perspectives on BIM use and job satisfaction. The model uses Partial Least Squares Structural Equation Modeling to analyze the survey results and identify what dimensions of BIM-related work affect job satisfaction. While it was hypothesized that BIM use increases job satisfaction, the results show that only some dimensions of BIM use positively impact BIM job satisfaction; the use of BIM does not guarantee an increase in overall job satisfaction. Additionally, more frequent BIM use was not associated with higher satisfaction levels. These findings suggest that in the AEC industry, sustainable job satisfaction depends less on technological autonomy and more on human-centric factors, particularly collaboration and meaningful engagement within digital workflows.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20582v1,Relu and softplus neural nets as zero-sum turn-based games,arXiv,2025-12-23,"Summary: We show that the output of a ReLU neural network can be interpreted as the value of a zero-sum, turn-based, stopping game, which we call the ReLU net game. The game runs in the direction opposite to that of the network, and the input of the network serves as the terminal reward of the game. In fact, evaluating the network is the same as running the Shapley-Bellman backward recursion for the value of the game. Using the expression of the value of the game as an expected total payoff with respect to the path measure induced by the transition probabilities and a pair of optimal policies, we derive a discrete Feynman-Kac-type path-integral formula for the network output. This game-theoretic representation can be used to derive bounds on the output from bounds on the input, leveraging the monotonicity of Shapley operators, and to verify robustness properties using policies as certificates. Moreover, training the neural network becomes an inverse game problem: given pairs of terminal rewards and corresponding values, one seeks transition probabilities and rewards of a game that reproduces them. Finally, we show that a similar approach applies to neural networks with Softplus activation functions, where the ReLU net game is replaced by its entropic regularization.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20578v1,Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits,arXiv,2025-12-23,"Summary: Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20577v1,Improving ML Training Data with Gold-Standard Quality Metrics,arXiv,2025-12-23,"Summary: Hand-tagged training data is essential to many machine learning tasks. However, training data quality control has received little attention in the literature, despite data quality varying considerably with the tagging exercise. We propose methods to evaluate and enhance the quality of hand-tagged training data using statistical approaches to measure tagging consistency and agreement. We show that agreement metrics give more reliable results if recorded over multiple iterations of tagging, where declining variance in such recordings is an indicator of increasing data quality. We also show one way a tagging project can collect high-quality training data without requiring multiple tags for every work item, and that a tagger burn-in period may not be sufficient for minimizing tagger errors.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20576v1,Performative Policy Gradient: Optimality in Performative Reinforcement Learning,arXiv,2025-12-23,"Summary: Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20573v1,"Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",arXiv,2025-12-23,"Summary: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It ""fails fast"" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and ""wins big"" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.4$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20569v1,Distilling to Hybrid Attention Models via KL-Guided Layer Selection,arXiv,2025-12-23,"Summary: Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20563v1,LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving,arXiv,2025-12-23,"Summary: Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20562v1,Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention,arXiv,2025-12-23,"Summary: We study the problem of learning a low-degree spherical polynomial of degree $\ell_0 = Θ(1) \ge 1$ defined on the unit sphere in $\RR^d$ by training an over-parameterized two-layer neural network (NN) with channel attention in this paper. Our main result is the significantly improved sample complexity for learning such low-degree polynomials. We show that, for any regression risk $\eps \in (0,1)$, a carefully designed two-layer NN with channel attention and finite width of $m \ge Θ({n^4 \log (2n/δ)}/{d^{2\ell_0}})$ trained by the vanilla gradient descent (GD) requires the lowest sample complexity of $n \asymp Θ(d^{\ell_0}/\eps)$ with probability $1-δ$ for every $δ\in (0,1)$, in contrast with the representative sample complexity $Θ\pth{d^{\ell_0} \max\set{\eps^{-2},\log d}}$, where $n$ is the training daata size. Moreover, such sample complexity is not improvable since the trained network renders a sharp rate of the nonparametric regression risk of the order $Θ(d^{\ell_0}/{n})$ with probability at least $1-δ$. On the other hand, the minimax optimal rate for the regression risk with a kernel of rank $Θ(d^{\ell_0})$ is $Θ(d^{\ell_0}/{n})$, so that the rate of the nonparametric regression risk of the network trained by GD is minimax optimal. The training of the two-layer NN with channel attention consists of two stages. In Stage 1, a provable learnable channel selection algorithm identifies the ground-truth channel number $\ell_0$ from the initial $L \ge \ell_0$ channels in the first-layer activation, with high probability. This learnable selection is achieved by an efficient one-step GD update on both layers, enabling feature learning for low-degree polynomial targets. In Stage 2, the second layer is trained by standard GD using the activation function with the selected channels.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20552v1,Information-theoretic signatures of causality in Bayesian networks and hypergraphs,arXiv,2025-12-23,"Summary: Analyzing causality in multivariate systems involves establishing how information is generated, distributed and combined, and thus requires tools that capture interactions beyond pairwise relations. Higher-order information theory provides such tools. In particular, Partial Information Decomposition (PID) allows the decomposition of the information that a set of sources provides about a target into redundant, unique, and synergistic components. Yet the mathematical connection between such higher-order information-theoretic measures and causal structure remains undeveloped. Here we establish the first theoretical correspondence between PID components and causal structure in both Bayesian networks and hypergraphs. We first show that in Bayesian networks unique information precisely characterizes direct causal neighbors, while synergy identifies collider relationships. This establishes a localist causal discovery paradigm in which the structure surrounding each variable can be recovered from its immediate informational footprint, eliminating the need for global search over graph space. Extending these results to higher-order systems, we prove that PID signatures in Bayesian hypergraphs differentiate parents, children, co-heads, and co-tails, revealing a higher-order collider effect unique to multi-tail hyperedges. We also present procedures by which our results can be used to characterize systematically the causal structure of Bayesian networks and hypergraphs. Our results position PID as a rigorous, model-agnostic foundation for inferring both pairwise and higher-order causal structure, and introduce a fundamentally local information-theoretic viewpoint on causal discovery.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20548v1,Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model,arXiv,2025-12-23,"Summary: Teachers' emotional states are critical in educational scenarios, profoundly impacting teaching efficacy, student engagement, and learning achievements. However, existing studies often fail to accurately capture teachers' emotions due to the performative nature and overlook the critical impact of instructional information on emotional expression.In this paper, we systematically investigate teacher sentiment analysis by building both the dataset and the model accordingly. We construct the first large-scale teacher multimodal sentiment analysis dataset, T-MED.To ensure labeling accuracy and efficiency, we employ a human-machine collaborative labeling process.The T-MED dataset includes 14,938 instances of teacher emotional data from 250 real classrooms across 11 subjects ranging from K-12 to higher education, integrating multimodal text, audio, video, and instructional information.Furthermore, we propose a novel asymmetric attention-based multimodal teacher sentiment analysis model, AAM-TSA.AAM-TSA introduces an asymmetric attention mechanism and hierarchical gating unit to enable differentiated cross-modal feature fusion and precise emotional classification. Experimental results demonstrate that AAM-TSA significantly outperforms existing state-of-the-art methods in terms of accuracy and interpretability on the T-MED dataset.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20533v1,Over-the-Air Goal-Oriented Communications,arXiv,2025-12-23,"Summary: Goal-oriented communications offer an attractive alternative to the Shannon-based communication paradigm, where the data is never reconstructed at the Receiver (RX) side. Rather, focusing on the case of edge inference, the Transmitter (TX) and the RX cooperate to exchange features of the input data that will be used to predict an unseen attribute of them, leveraging information from collected data sets. This chapter demonstrates that the wireless channel can be used to perform computations over the data, when equipped with programmable metasurfaces. The end-to-end system of the TX, RX, and MS-based channel is treated as a single deep neural network which is trained through backpropagation to perform inference on unseen data. Using Stacked Intelligent Metasurfaces (SIM), it is shown that this Metasurfaces-Integrated Neural Network (MINN) can achieve performance comparable to fully digital neural networks under various system parameters and data sets. By offloading computations onto the channel itself, important benefits may be achieved in terms of energy consumption, arising from reduced computations at the transceivers and smaller transmission power required for successful inference.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20523v1,ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification,arXiv,2025-12-23,"Summary: This study proposes Riesz representer estimation methods based on score matching. The Riesz representer is a key component in debiased machine learning for constructing $\sqrt{n}$-consistent and efficient estimators in causal inference and structural parameter estimation. To estimate the Riesz representer, direct approaches have garnered attention, such as Riesz regression and the covariate balancing propensity score. These approaches can also be interpreted as variants of direct density ratio estimation (DRE) in several applications such as average treatment effect estimation. In DRE, it is well known that flexible models can easily overfit the observed data due to the estimand and the form of the loss function. To address this issue, recent work has proposed modeling the density ratio as a product of multiple intermediate density ratios and estimating it using score-matching techniques, which are often used in the diffusion model literature. We extend score-matching-based DRE methods to Riesz representer estimation. Our proposed method not only mitigates overfitting but also provides insights for causal inference by bridging marginal effects and average policy effects through time score functions.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20520v1,Benchmarking LLMs for Predictive Applications in the Intensive Care Units,arXiv,2025-12-23,"Summary: With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20518v1,"Expected Revenue, Risk, and Grid Impact of Bitcoin Mining: A Decision-Theoretic Perspective",arXiv,2025-12-23,"Summary: Most current assessments use ex post proxies that miss uncertainty and fail to consistently capture the rapid change in bitcoin mining. We introduce a unified, ex ante statistical model that derives expected return, downside risk, and upside potential profit from the first principles of mining: Each hash is a Bernoulli trial with a Bitcoin block difficulty-based success probability. The model yields closed-form expected revenue per hash-rate unit, risk metrics in different scenarios, and upside-profit probabilities for different fleet sizes. Empirical calibration closely matches previously reported observations, yielding a unified, faithful quantification across hardware, pools, and operating conditions. This foundation enables more reliable analysis of mining impacts and behavior.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20515v1,Modeling Bank Systemic Risk of Emerging Markets under Geopolitical Shocks: Empirical Evidence from BRICS Countries,arXiv,2025-12-23,"Summary: The growing economic influence of the BRICS nations requires risk models that capture complex, long-term dynamics. This paper introduces the Bank Risk Interlinkage with Dynamic Graph and Event Simulations (BRIDGES) framework, which analyzes systemic risk based on the level of information complexity (zero-order, first-order, and second-order). BRIDGES utilizes the Dynamic Time Warping (DTW) distance to construct a dynamic network for 551 BRICS banks based on their strategic similarity, using zero-order information such as annual balance sheet data from 2008 to 2024. It then employs first-order information, including trends in risk ratios, to detect shifts in banks' behavior. A Temporal Graph Neural Network (TGNN), as the core of BRIDGES, is deployed to learn network evolutions and detect second-order information, such as anomalous changes in the structural relationships of the bank network. To measure the impact of anomalous changes on network stability, BRIDGES performs Agent-Based Model (ABM) simulations to assess the banking system's resilience to internal financial failure and external geopolitical shocks at the individual country level and across BRICS nations. Simulation results show that the failure of the largest institutions causes more systemic damage than the failure of the financially vulnerable or dynamically anomalous ones, driven by powerful panic effects. Compared to this ""too big to fail"" scenario, a geopolitical shock with correlated country-wide propagation causes more destructive systemic damage, leading to a near-total systemic collapse. It suggests that the primary threats to BRICS financial stability are second-order panic and large-scale geopolitical shocks, which traditional risk analysis models might not detect.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20514v1,Explainable time-series forecasting with sampling-free SHAP for Transformers,arXiv,2025-12-23,"Summary: Time-series forecasts are essential for planning and decision-making in many domains. Explainability is key to building user trust and meeting transparency requirements. Shapley Additive Explanations (SHAP) is a popular explainable AI framework, but it lacks efficient implementations for time series and often assumes feature independence when sampling counterfactuals. We introduce SHAPformer, an accurate, fast and sampling-free explainable time-series forecasting model based on the Transformer architecture. It leverages attention manipulation to make predictions based on feature subsets. SHAPformer generates explanations in under one second, several orders of magnitude faster than the SHAP Permutation Explainer. On synthetic data with ground truth explanations, SHAPformer provides explanations that are true to the data. Applied to real-world electrical load data, it achieves competitive predictive performance and delivers meaningful local and global insights, such as identifying the past load as the key predictor and revealing a distinct model behavior during the Christmas period.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20513v1,Recurrent Off-Policy Deep Reinforcement Learning Doesn't Have to be Slow,arXiv,2025-12-23,"Summary: Recurrent off-policy deep reinforcement learning models achieve state-of-the-art performance but are often sidelined due to their high computational demands. In response, we introduce RISE (Recurrent Integration via Simplified Encodings), a novel approach that can leverage recurrent networks in any image-based off-policy RL setting without significant computational overheads via using both learnable and non-learnable encoder layers. When integrating RISE into leading non-recurrent off-policy RL algorithms, we observe a 35.6% human-normalized interquartile mean (IQM) performance improvement across the Atari benchmark. We analyze various implementation strategies to highlight the versatility and potential of our proposed framework.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20503v1,$L^2-$posterior contraction rates for Gaussian process and random series priors in Bayesian nonparametric regression models,arXiv,2025-12-23,"Summary: The nonparametric regression model with normal errors has been extensively studied, both from the frequentist and Bayesian viewpoint. A central result in Bayesian nonparametrics is that under assumptions on the prior, the data-generating distribution (assuming a true frequentist model) and a semi-metric $ρ(.,.)$ on the space of regression functions that satisfy the so called testing condition, the posterior contracts around the true distribution with respect to $ρ(.,.)$, and the rate of contraction can be estimated. In the regression setting, the semi-metric $ρ(.,.)$ is often taken to be the Hellinger distance or the empirical $L^2$ norm (i.e., the $L^2$ norm with respect to the empirical distribution of the design) in the present regression context. However, extending contraction rates to the ``integrated"" $L^2$ norm usually requires more work, and has previously been done for instance under sufficient smoothness or boundedness assumptions, which may not necessarily hold. In this work we show that, for classes of priors based on random basis expansions or Gaussian processes with RKHS of Sobolev type and in the random design setting, such $L^2$ posterior contraction rates can be obtained under substantially weaker assumptions than those currently used in the literature. Importantly we do not require a known a priori upper bound on its supremum norm or that its smoothness is larger than $d/2$, where $d$ is the dimension of the covariates. Our proof crucially relies on an application of the matrix Bernstein concentration inequality to empirical inner product matrices, which require explicit upper bounds on the basis functions at hand that we prove in several cases of interest. In particular we obtain upper bounds on the supremum norm of Mercer eigenfunctions of several reproducing kernels (including several Matérn kernels) which are of independent interest.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20491v1,Step-DeepResearch Technical Report,arXiv,2025-12-23,"Summary: As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20485v1,WOC: Dual-Path Weighted Object Consensus Made Efficient,arXiv,2025-12-23,"Summary: Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but serializes all operations through a global leader, limiting parallelism. EPaxos enables parallel execution for independent operations but treats all nodes uniformly, ignoring performance differences. To tackle this problem, we present WOC, a dual-path consensus protocol that dynamically routes operations into two paths based on their access patterns. Independent operations execute through a fast path that uses object-specific weighted quorums and completes in one network round-trip. Conflicting or shared objects route through a leader-coordinated slow path employing node-weighted consensus. Our evaluation demonstrates that WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20482v1,"SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization",arXiv,2025-12-23,"Summary: Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20481v1,Coherence in the brain unfolds across separable temporal regimes,arXiv,2025-12-23,"Summary: Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20477v1,Switching between states and the COVID-19 turbulence,arXiv,2025-12-23,"Summary: In Aarab (2020), I examine U.S. stock return predictability across economic regimes and document evidence of time-varying expected returns across market states in the long run. The analysis introduces a state-switching specification in which the market state is proxied by the slope of the yield curve, and proposes an Aligned Economic Index built from the popular predictors of Welch and Goyal (2008) (augmented with bond and equity premium measures). The Aligned Economic Index under the state-switching model exhibits statistically and economically meaningful in-sample ($R^2 = 5.9\%$) and out-of-sample ($R^2_{\text{oos}} = 4.12\%$) predictive power across both recessions and expansions, while outperforming a range of widely used predictors. In this work, I examine the added value for professional practitioners by computing the economic gains for a mean-variance investor and find substantial added benefit of using the new index under the state switching model across all market states. The Aligned Economic Index can thus be implemented on a consistent real-time basis. These findings are crucial for both academics and practitioners as expansions are much longer-lived than recessions. Finally, I extend the empirical exercises by incorporating data through September 2020 and document sizable gains from using the Aligned Economic Index, relative to more traditional approaches, during the COVID-19 market turbulence.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20469v1,Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale,arXiv,2025-12-23,"Summary: AI agents are emerging as a practical way to run multi-step scientific workflows that interleave reasoning with tool use and verification, pointing to a shift from isolated AI-assisted steps toward \emph{agentic science at scale}. This shift is increasingly feasible, as scientific tools and models can be invoked through stable interfaces and verified with recorded execution traces, and increasingly necessary, as AI accelerates scientific output and stresses the peer-review and publication pipeline, raising the bar for traceability and credible evaluation.
  However, scaling agentic science remains difficult: workflows are hard to observe and reproduce; many tools and laboratory systems are not agent-ready; execution is hard to trace and govern; and prototype AI Scientist systems are often bespoke, limiting reuse and systematic improvement from real workflow signals.
  We argue that scaling agentic science requires an infrastructure-and-ecosystem approach, instantiated in Bohrium+SciMaster. Bohrium acts as a managed, traceable hub for AI4S assets -- akin to a HuggingFace of AI for Science -- that turns diverse scientific data, software, compute, and laboratory systems into agent-ready capabilities. SciMaster orchestrates these capabilities into long-horizon scientific workflows, on which scientific agents can be composed and executed. Between infrastructure and orchestration, a \emph{scientific intelligence substrate} organizes reusable models, knowledge, and components into executable building blocks for workflow reasoning and action, enabling composition, auditability, and improvement through use.
  We demonstrate this stack with eleven representative master agents in real workflows, achieving orders-of-magnitude reductions in end-to-end scientific cycle time and generating execution-grounded signals from real workloads at multi-million scale.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20464v1,Snapshot 3D image projection using a diffractive decoder,arXiv,2025-12-23,"Summary: 3D image display is essential for next-generation volumetric imaging; however, dense depth multiplexing for 3D image projection remains challenging because diffraction-induced cross-talk rapidly increases as the axial image planes get closer. Here, we introduce a 3D display system comprising a digital encoder and a diffractive optical decoder, which simultaneously projects different images onto multiple target axial planes with high axial resolution. By leveraging multi-layer diffractive wavefront decoding and deep learning-based end-to-end optimization, the system achieves high-fidelity depth-resolved 3D image projection in a snapshot, enabling axial plane separations on the order of a wavelength. The digital encoder leverages a Fourier encoder network to capture multi-scale spatial and frequency-domain features from input images, integrates axial position encoding, and generates a unified phase representation that simultaneously encodes all images to be axially projected in a single snapshot through a jointly-optimized diffractive decoder. We characterized the impact of diffractive decoder depth, output diffraction efficiency, spatial light modulator resolution, and axial encoding density, revealing trade-offs that govern axial separation and 3D image projection quality. We further demonstrated the capability to display volumetric images containing 28 axial slices, as well as the ability to dynamically reconfigure the axial locations of the image planes, performed on demand. Finally, we experimentally validated the presented approach, demonstrating close agreement between the measured results and the target images. These results establish the diffractive 3D display system as a compact and scalable framework for depth-resolved snapshot 3D image projection, with potential applications in holographic displays, AR/VR interfaces, and volumetric optical computing.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20460v1,The Aligned Economic Index & The State Switching Model,arXiv,2025-12-23,"Summary: A growing empirical literature suggests that equity-premium predictability is state dependent, with much of the forecasting power concentrated around recessionary periods \parencite{Henkel2011,DanglHalling2012,Devpura2018}. I study U.S. stock return predictability across economic regimes and document strong evidence of time-varying expected returns across both expansionary and contractionary states. I contribute in two ways. First, I introduce a state-switching predictive regression in which the market state is defined in real time using the slope of the yield curve. Relative to the standard one-state predictive regression, the state-switching specification increases both in-sample and out-of-sample performance for the set of popular predictors considered by \textcite{WelchGoyal2008}, improving the out-of-sample performance of most predictors in economically meaningful ways. Second, I propose a new aggregate predictor, the Aligned Economic Index, constructed via partial least squares (PLS). Under the state-switching model, the Aligned Economic Index exhibits statistically and economically significant predictive power in sample and out of sample, and it outperforms widely used benchmark predictors and alternative predictor-combination methods.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20458v1,Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register,arXiv,2025-12-23,"Summary: Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20457v1,When Natural Strategies Meet Fuzziness and Resource-Bounded Actions (Extended Version),arXiv,2025-12-23,"Summary: In formal strategic reasoning for Multi-Agent Systems (MAS), agents are typically assumed to (i) employ arbitrarily complex strategies, (ii) execute each move at zero cost, and (iii) operate over fully crisp game structures. These idealized assumptions stand in stark contrast with human decision making in real world environments. The natural strategies framework along with some of its recent variants, partially addresses this gap by restricting strategies to concise rules guarded by regular expressions. Yet, it still overlook both the cost of each action and the uncertainty that often characterizes human perception of facts over the time. In this work, we introduce HumanATLF, a logic that builds upon natural strategies employing both fuzzy semantics and resource bound actions: each action carries a real valued cost drawn from a non refillable budget, and atomic conditions and goals have degrees in [0,1]. We give a formal syntax and semantics, and prove that model checking is in P when both the strategy complexity k and resource budget b are fixed, NP complete if just one strategic operator over Boolean objectives is allowed, and Delta^P_2 complete when k and b vary. Moreover, we show that recall based strategies can be decided in PSPACE. We implement our algorithms in VITAMIN, an open source model checking tool for MAS and validate them on an adversarial resource aware drone rescue scenario.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20438v1,Machine Learning to Predict Digital Frustration from Clickstream Data,arXiv,2025-12-23,"Summary: Many businesses depend on their mobile apps and websites, so user frustration while trying to complete a task on these channels can cause lost sales and complaints. In this research, I use clickstream data from a real e-commerce site to predict whether a session is frustrated or not. Frustration is defined using certain rules based on rage bursts, back and forth navigation (U turns), cart churn, search struggle, and long wandering sessions, and applies these rules to 5.4 million raw clickstream events (304,881 sessions). From each session, I build tabular features and train standard classifier models. I also use the full event sequence to train a discriminative LSTM classifier. XGBoost reaches about 90% accuracy, ROC AUC of 0.9579, while the LSTM performs best with about 91% accuracy and a ROC AUC of 0.9705. Finally, the research shows that with only the first 20 to 30 interactions, the LSTM already predicts frustration reliably.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20436v1,Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI,arXiv,2025-12-23,"Summary: Accurate segmentation of ischemic stroke lesions from diffusion magnetic resonance imaging (MRI) is essential for clinical decision-making and outcome assessment. Diffusion-Weighted Imaging (DWI) and Apparent Diffusion Coefficient (ADC) scans provide complementary information on acute and sub-acute ischemic changes; however, automated lesion delineation remains challenging due to variability in lesion appearance.
  In this work, we study ischemic stroke lesion segmentation using multimodal diffusion MRI from the ISLES 2022 dataset. Several state-of-the-art convolutional and transformer-based architectures, including U-Net variants, Swin-UNet, and TransUNet, are benchmarked. Based on performance, a dual-encoder TransUNet architecture is proposed to learn modality-specific representations from DWI and ADC inputs. To incorporate spatial context, adjacent slice information is integrated using a three-slice input configuration.
  All models are trained under a unified framework and evaluated using the Dice Similarity Coefficient (DSC). Results show that transformer-based models outperform convolutional baselines, and the proposed dual-encoder TransUNet achieves the best performance, reaching a Dice score of 85.4% on the test set. The proposed framework offers a robust solution for automated ischemic stroke lesion segmentation from diffusion MRI.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20432v1,High Dimensional Data Decomposition for Anomaly Detection of Textured Images,arXiv,2025-12-23,"Summary: In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20423v1,Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit,arXiv,2025-12-23,"Summary: The purpose of this project is to assess how well defenders can detect DNS-over-HTTPS (DoH) file exfiltration, and which evasion strategies can be used by attackers. While providing a reproducible toolkit to generate, intercept and analyze DoH exfiltration, and comparing Machine Learning vs threshold-based detection under adversarial scenarios. The originality of this project is the introduction of an end-to-end, containerized pipeline that generates configurable file exfiltration over DoH using several parameters (e.g., chunking, encoding, padding, resolver rotation). It allows for file reconstruction at the resolver side, while extracting flow-level features using a fork of DoHLyzer. The pipeline contains a prediction side, which allows the training of machine learning models based on public labelled datasets and then evaluates them side-by-side with threshold-based detection methods against malicious and evasive DNS-Over-HTTPS traffic. We train Random Forest, Gradient Boosting and Logistic Regression classifiers on a public DoH dataset and benchmark them against evasive DoH exfiltration scenarios. The toolkit orchestrates traffic generation, file capture, feature extraction, model training and analysis. The toolkit is then encapsulated into several Docker containers for easy setup and full reproducibility regardless of the platform it is run on. Future research regarding this project is directed at validating the results on mixed enterprise traffic, extending the protocol coverage to HTTP/3/QUIC request, adding a benign traffic generation, and working on real-time traffic evaluation. A key objective is to quantify when stealth constraints make DoH exfiltration uneconomical and unworthy for the attacker.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20420v1,Simplifying Multi-Task Architectures Through Task-Specific Normalization,arXiv,2025-12-23,"Summary: Multi-task learning (MTL) aims to leverage shared knowledge across tasks to improve generalization and parameter efficiency, yet balancing resources and mitigating interference remain open challenges. Architectural solutions often introduce elaborate task-specific modules or routing schemes, increasing complexity and overhead. In this work, we show that normalization layers alone are sufficient to address many of these challenges. Simply replacing shared normalization with task-specific variants already yields competitive performance, questioning the need for complex designs. Building on this insight, we propose Task-Specific Sigmoid Batch Normalization (TS$σ$BN), a lightweight mechanism that enables tasks to softly allocate network capacity while fully sharing feature extractors. TS$σ$BN improves stability across CNNs and Transformers, matching or exceeding performance on NYUv2, Cityscapes, CelebA, and PascalContext, while remaining highly parameter-efficient. Moreover, its learned gates provide a natural framework for analyzing MTL dynamics, offering interpretable insights into capacity allocation, filter specialization, and task relationships. Our findings suggest that complex MTL architectures may be unnecessary and that task-specific normalization offers a simple, interpretable, and efficient alternative.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20417v1,Chain-of-Anomaly Thoughts with Large Vision-Language Models,arXiv,2025-12-23,"Summary: Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20409v1,DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning,arXiv,2025-12-23,"Summary: Aligning egocentric video with wearable sensors have shown promise for human action recognition, but face practical limitations in user discomfort, privacy concerns, and scalability. We explore exocentric video with ambient sensors as a non-intrusive, scalable alternative. While prior egocentric-wearable works predominantly adopt Global Alignment by encoding entire sequences into unified representations, this approach fails in exocentric-ambient settings due to two problems: (P1) inability to capture local details such as subtle motions, and (P2) over-reliance on modality-invariant temporal patterns, causing misalignment between actions sharing similar temporal patterns with different spatio-semantic contexts. To resolve these problems, we propose DETACH, a decomposed spatio-temporal framework. This explicit decomposition preserves local details, while our novel sensor-spatial features discovered via online clustering provide semantic grounding for context-aware alignment. To align the decomposed features, our two-stage approach establishes spatial correspondence through mutual supervision, then performs temporal alignment via a spatial-temporal weighted contrastive loss that adaptively handles easy negatives, hard negatives, and false negatives. Comprehensive experiments with downstream tasks on Opportunity++ and HWU-USP datasets demonstrate substantial improvements over adapted egocentric-wearable baselines.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20407v1,AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition,arXiv,2025-12-23,"Summary: Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework's potential for deployment in security and surveillance applications where visual or radar sensing may be limited.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20404v1,Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining,arXiv,2025-12-23,"Summary: With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20403v1,BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples,arXiv,2025-12-23,"Summary: Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20399v1,GeoTransolver: Learning Physics on Irregumar Domains Using Multi-scale Geometry Aware Physics Attention Transformer,arXiv,2025-12-23,"Summary: We present GeoTransolver, a Multiscale Geometry-Aware Physics Attention Transformer for CAE that replaces standard attention with GALE, coupling physics-aware self-attention on learned state slices with cross-attention to a shared geometry/global/boundary-condition context computed from multi-scale ball queries (inspired by DoMINO) and reused in every block. Implemented and released in NVIDIA PhysicsNeMo, GeoTransolver persistently projects geometry, global and boundary condition parameters into physical state spaces to anchor latent computations to domain structure and operating regimes. We benchmark GeoTransolver on DrivAerML, Luminary SHIFT-SUV, and Luminary SHIFT-Wing, comparing against Domino, Transolver (as released in PhysicsNeMo), and literature-reported AB-UPT, and evaluate drag/lift R2 and Relative L1 errors for field variables. GeoTransolver delivers better accuracy, improved robustness to geometry/regime shifts, and favorable data efficiency; we include ablations on DrivAerML and qualitative results such as contour plots and design trends for the best GeoTransolver models. By unifying multiscale geometry-aware context with physics-based attention in a scalable transformer, GeoTransolver advances operator learning for high-fidelity surrogate modeling across complex, irregular domains and non-linear physical regimes.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20394v1,Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults,arXiv,2025-12-23,"Summary: As Network-on-Chip (NoC) and Wireless Sensor Network architectures continue to scale, the topology of the underlying network becomes a critical factor in performance. Gaussian Interconnected Networks based on the arithmetic of Gaussian integers, offer attractive properties regarding diameter and symmetry. Despite their attractive theoretical properties, adaptive routing techniques in these networks are vulnerable to node and link faults, leading to rapid degradation in communication reliability. Node failures (particularly those following Gaussian distributions, such as thermal hotspots or physical damage clusters) pose severe challenges to traditional deterministic routing. This paper proposes a fault-aware Reinforcement Learning (RL) routing scheme tailored for Gaussian Interconnected Networks. By utilizing a PPO (Proximal Policy Optimization) agent with a specific reward structure designed to penalize fault proximity, the system dynamically learns to bypass faulty regions. We compare our proposed RL-based routing protocol against a greedy adaptive shortest-path routing algorithm. Experimental results demonstrate that the RL agent significantly outperforms the adaptive routing sustaining a Packet Delivery Ratio (PDR) of 0.95 at 40% fault density compared to 0.66 for the greedy. Furthermore, the RL approach exhibits effective delivery rates compared to the greedy adaptive routing, particularly under low network load of 20% at 0.57 vs. 0.43, showing greater proficiency in managing congestion, validating its efficacy in stochastic, fault-prone topologies",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20387v1,Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems,arXiv,2025-12-23,"Summary: We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20381v1,Identifying Appropriately-Sized Services with Deep Reinforcement Learning,arXiv,2025-12-23,"Summary: Service-based architecture (SBA) has gained attention in industry and academia as a means to modernize legacy systems. It refers to a design style that enables systems to be developed as suites of small, loosely coupled, and autonomous components (services) that encapsulate functionality and communicate via language-agnostic APIs. However, defining appropriately sized services that capture cohesive subsets of system functionality remains challenging. Existing work often relies on the availability of documentation, access to project personnel, or a priori knowledge of the target number of services, assumptions that do not hold in many real-world scenarios. Our work addresses these limitations using a deep reinforcement learning-based approach to identify appropriately sized services directly from implementation artifacts. We present Rake, a reinforcement learning-based technique that leverages available system documentation and source code to guide service decomposition at the level of implementation methods. Rake does not require specific documentation or access to project personnel and is language-agnostic. It also supports a customizable objective function that balances modularization quality and business capability alignment, i.e., the degree to which a service covers the targeted business capability. We applied Rake to four open-source legacy projects and compared it with two state-of-the-art techniques. On average, Rake achieved 7-14 percent higher modularization quality and 18-22 percent stronger business capability alignment. Our results further show that optimizing solely for business context can degrade decomposition quality in tightly coupled systems, highlighting the need for balanced objectives.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20370v1,Cross-Population White Matter Atlas Creation for Concurrent Mapping of Brain Connections in Neonates and Adults with Diffusion MRI Tractography,arXiv,2025-12-23,"Summary: Comparing white matter (WM) connections between adults and neonates using diffusion MRI (dMRI) can advance our understanding of typical brain development and potential biomarkers for neurological disorders. However, existing WM atlases are population-specific (adult or neonatal) and reside in separate spaces, preventing direct cross-population comparisons. A unified WM atlas spanning both neonates and adults is still lacking. In this study, we propose a neonatal/adult brain atlas (NABA), a WM tractography atlas built from dMRI data of both neonates and adults. NABA is constructed using a robust, data-driven fiber clustering pipeline, enabling group-wise WM atlasing across populations despite substantial anatomical variability. The atlas provides a standardized template for WM parcellation, allowing direct comparison of WM tracts between neonates and adults. Using NABA, we conduct four analyses: (1) evaluating the feasibility of joint WM mapping across populations, (2) characterizing WM development across neonatal ages relative to adults, (3) assessing sex-related differences in neonatal WM development, and (4) examining the effects of preterm birth. Our results show that NABA robustly identifies WM tracts in both populations. We observe rapid fractional anisotropy (FA) development in long-range association tracts, including the arcuate fasciculus and superior longitudinal fasciculus II, whereas intra-cerebellar tracts develop more slowly. Neonatal females exhibit faster overall FA development than males. Although preterm neonates show lower overall FA development rates, they demonstrate relatively higher FA growth in specific tracts, including the corticospinal tract, corona radiata-pontine pathway, and intracerebellar tracts. These findings demonstrate that NABA is a useful tool for investigating WM development across neonates and adults.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20368v1,Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability,arXiv,2025-12-23,"Summary: Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\sqrt{d \log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.
  A key structural property that circumvents this limitation is the \emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \emph{without} incurring the $\sqrt{d \log T}$ price of adaptivity.
  In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20363v1,Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning,arXiv,2025-12-23,"Summary: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $α$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20353v1,"Allocating Students to Schools: Theory, Methods, and Empirical Insights",arXiv,2025-12-23,"Summary: This chapter surveys the application of matching theory to school choice, motivated by the shift from neighborhood assignment systems to choice-based models. Since educational choice is not mediated by price, the design of allocation mechanisms is critical. The chapter first reviews theoretical contributions, exploring the fundamental trade-offs between efficiency, stability, and strategy-proofness, and covers design challenges such as tie-breaking, cardinal welfare, and affirmative action. It then transitions to the empirical landscape, focusing on the central challenge of inferring student preferences from application data, especially under strategic mechanisms. We review various estimation approaches and discuss key insights on parental preferences, market design trade-offs, and the effectiveness of school choice policies?",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20352v1,Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation,arXiv,2025-12-23,"Summary: Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20350v1,Field-Space Attention for Structure-Preserving Earth System Transformers,arXiv,2025-12-23,"Summary: Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20348v1,Physics-guided Neural Network-based Shaft Power Prediction for Vessels,arXiv,2025-12-23,"Summary: Optimizing maritime operations, particularly fuel consumption for vessels, is crucial, considering its significant share in global trade. As fuel consumption is closely related to the shaft power of a vessel, predicting shaft power accurately is a crucial problem that requires careful consideration to minimize costs and emissions. Traditional approaches, which incorporate empirical formulas, often struggle to model dynamic conditions, such as sea conditions or fouling on vessels. In this paper, we present a hybrid, physics-guided neural network-based approach that utilizes empirical formulas within the network to combine the advantages of both neural networks and traditional techniques. We evaluate the presented method using data obtained from four similar-sized cargo vessels and compare the results with those of a baseline neural network and a traditional approach that employs empirical formulas. The experimental results demonstrate that the physics-guided neural network approach achieves lower mean absolute error, root mean square error, and mean absolute percentage error for all tested vessels compared to both the empirical formula-based method and the base neural network.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20346v1,Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation,arXiv,2025-12-23,"Summary: Physics-based machine learning blends traditional science with modern data-driven techniques. Rather than relying exclusively on empirical data or predefined equations, this methodology embeds domain knowledge directly into the learning process, resulting in models that are both more accurate and robust. We leverage this paradigm to accelerate simulations of the Zero Degree Calorimeter (ZDC) of the ALICE experiment at CERN. Our method introduces a novel loss function and an output variability-based scaling mechanism, which enhance the model's capability to accurately represent the spatial distribution and morphology of particle showers in detector outputs while mitigating the influence of rare artefacts on the training. Leveraging Normalizing Flows (NFs) in a teacher-student generative framework, we demonstrate that our approach not only outperforms classic data-driven model assimilation but also yields models that are 421 times faster than existing NF implementations in ZDC simulation literature.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20344v1,A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice,arXiv,2025-12-23,"Summary: A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20333v1,SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization,arXiv,2025-12-23,"Summary: Generative artificial intelligence has revolutionized the exploration of chemical space, yet a critical bottleneck remains that a substantial fraction of generated molecules is synthetically inaccessible. Current solutions, such as post-hoc filtering or projection-based methods, often compromise structural novelty or disrupt key pharmacophores by forcing molecules into pre-defined synthetic templates. Herein, we introduce SynCraft, a reasoning-based framework that reframes synthesizability optimization not as a sequence translation task, but as a precise structural editing problem. Leveraging the emergent reasoning capabilities of Large Language Models, SynCraft navigates the ""synthesis cliff"" where minimal structural modifications yield significant gains in synthetic feasibility. By predicting executable sequences of atom-level edits rather than generating SMILES strings directly, SynCraft circumvents the syntactic fragility of LLMs while harnessing their chemical intuition. Extensive benchmarks demonstrate that SynCraft outperforms state-of-the-art baselines in generating synthesizable analogs with high structural fidelity. Furthermore, through interaction-aware prompting, SynCraft successfully replicates expert medicinal chemistry intuition in editing PLK1 inhibitors and rescuing high-scoring but previously discarded RIPK1 candidates in previous molecular generation literatures.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20329v1,FedDPC : Handling Data Heterogeneity and Partial Client Participation in Federated Learning,arXiv,2025-12-23,"Summary: Data heterogeneity is a significant challenge in modern federated learning (FL) as it creates variance in local model updates, causing the aggregated global model to shift away from the true global optimum. Partial client participation in FL further exacerbates this issue by skewing the aggregation of local models towards the data distribution of participating clients. This creates additional variance in the global model updates, causing the global model to converge away from the optima of the global objective. These variances lead to instability in FL training, which degrades global model performance and slows down FL training. While existing literature primarily focuses on addressing data heterogeneity, the impact of partial client participation has received less attention. In this paper, we propose FedDPC, a novel FL method, designed to improve FL training and global model performance by mitigating both data heterogeneity and partial client participation. FedDPC addresses these issues by projecting each local update onto the previous global update, thereby controlling variance in both local and global updates. To further accelerate FL training, FedDPC employs adaptive scaling for each local update before aggregation. Extensive experiments on image classification tasks with multiple heterogeneously partitioned datasets validate the effectiveness of FedDPC. The results demonstrate that FedDPC outperforms state-of-the-art FL algorithms by achieving faster reduction in training loss and improved test accuracy across communication rounds.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20328v1,Toward Explaining Large Language Models in Software Engineering Tasks,arXiv,2025-12-23,"Summary: Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20325v1,"Top-K Exterior Power Persistent Homology: Algorithm, Structure, and Stability",arXiv,2025-12-23,"Summary: Exterior powers play important roles in persistent homology in computational geometry. In the present paper we study the problem of extracting the $K$ longest intervals of the exterior-power layers of a tame persistence module. We prove a structural decomposition theorem that organizes the exterior-power layers into monotone per-anchor streams with explicit multiplicities, enabling a best-first algorithm. We also show that the Top-$K$ length vector is $2$-Lipschitz under bottleneck perturbations of the input barcode, and prove a comparison-model lower bound. Our experiments confirm the theory, showing speedups over full enumeration in high overlap cases. By enabling efficient extraction of the most prominent features, our approach makes higher-order persistence feasible for large datasets and thus broadly applicable to machine learning, data science, and scientific computing.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20324v1,Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles,arXiv,2025-12-23,"Summary: Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20319v1,Deep Learning Classification of EEG Responses to Multi-Dimensional Transcranial Electrical Stimulation,arXiv,2025-12-23,"Summary: A major shortcoming of medical practice is the lack of an objective measure of conscious level. Impairment of consciousness is common, e.g. following brain injury and seizures, which can also interfere with sensory processing and volitional responses. This is also an important pitfall in neurophysiological methods that infer awareness via command following, e.g. using functional MRI or electroencephalography (EEG).
  Transcranial electrical stimulation (TES) can be employed to non-invasively stimulate the brain, bypassing sensory inputs, and has already showed promising results in providing reliable indicators of brain state. However, current non-invasive solutions have been limited to magnetic stimulation, which is not easily translatable to clinical settings. Our long-term vision is to develop an objective measure of brain state that can be used at the bedside, without requiring patients to understand commands or initiate motor responses.
  In this study, we demonstrated the feasibility of a framework using Deep Learning algorithms to classify EEG brain responses evoked by a defined multi-dimensional pattern of TES. We collected EEG-TES data from 11 participants and found that delivering transcranial direct current stimulation (tDCS) to posterior cortical areas targeting the angular gyrus elicited an exceptionally reliable brain response. For this paradigm, our best Convolutional Neural Network model reached a 92% classification F1-score on Holdout data from participants never seen during training, significantly surpassing human-level performance at 60-70% accuracy.
  These findings establish a framework for robust consciousness measurement for clinical use. In this spirit, we documented and open-sourced our datasets and codebase in full, to be used freely by the neuroscience and AI research communities, who may replicate our results with free tools like GitHub, Kaggle, and Colab.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20312v1,TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning,arXiv,2025-12-23,"Summary: Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20311v1,Algorithm for Interpretable Graph Features via Motivic Persistent Cohomology,arXiv,2025-12-23,"Summary: We present the Chromatic Persistence Algorithm (CPA), an event-driven method for computing persistent cohomological features of weighted graphs via graphic arrangements, a classical object in computational geometry. We establish rigorous complexity results: CPA is exponential in the worst case, fixed-parameter tractable in treewidth, and nearly linear for common graph families such as trees, cycles, and series-parallel graphs. Finally, we demonstrate its practical applicability through a controlled experiment on molecular-like graph structures.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20308v1,SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision,arXiv,2025-12-23,"Summary: The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20306v1,Structured Visualization Design Knowledge for Grounding Generative Reasoning and Situated Feedback,arXiv,2025-12-23,"Summary: Automated visualization design navigates a tension between symbolic systems and generative models. Constraint solvers enforce structural and perceptual validity, but the rules they require are difficult to author and too rigid to capture situated design knowledge. Large language models require no formal rules and can reason about contextual nuance, but they prioritize popular conventions over empirically grounded best practices. We address this tension by proposing a cataloging scheme that structures visualization design knowledge as natural-language guidelines with semantically typed metadata. This allows experts to author knowledge that machines can query. An expert study ($N=18$) indicates that practitioners routinely adapt heuristics to situational factors such as audience and communicative intent. To capture this reasoning, guideline sections specify not only advice but also the contexts where it applies, exceptions that invalidate it, and the sources from which it derives. We demonstrate the scheme's expressiveness by cataloging 744 guidelines drawn from cognitive science, accessibility standards, data journalism, and research on rhetorical aspects of visual communication. We embed guideline sections in a vector space, opening the knowledge itself to structural analysis. This reveals conflicting advice across sources and transferable principles between domains. Rather than replacing constraint-based tools, our scheme provides what they lack: situated guidance that generative systems can retrieve to ground their reasoning, users can verify against cited sources, and experts can author as knowledge evolves.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20305v1,KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis,arXiv,2025-12-23,"Summary: Survival analysis relies fundamentally on the semi-parametric Cox Proportional Hazards (CoxPH) model and the parametric Accelerated Failure Time (AFT) model. CoxPH assumes constant hazard ratios, often failing to capture real-world dynamics, while traditional AFT models are limited by rigid distributional assumptions. Although deep learning models like DeepAFT address these constraints by improving predictive accuracy and handling censoring, they inherit the significant challenge of black-box interpretability. The recent introduction of CoxKAN demonstrated the successful integration of Kolmogorov-Arnold Networks (KANs), a novel architecture that yields highly accurate and interpretable symbolic representations, within the CoxPH framework. Motivated by the interpretability gains of CoxKAN, we introduce KAN-AFT (Kolmogorov Arnold Network-based AFT), the first framework to apply KANs to the AFT model. KAN-AFT effectively models complex nonlinear relationships within the AFT framework. Our primary contributions include: (i) a principled AFT-KAN formulation, (ii) robust optimization strategies for right-censored observations (e.g., Buckley-James and IPCW), and (iii) an interpretability pipeline that converts the learned spline functions into closed-form symbolic equations for survival time. Empirical results on multiple datasets confirm that KAN-AFT achieves performance comparable to or better than DeepAFT, while uniquely providing transparent, symbolic models of the survival process.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20299v1,KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System,arXiv,2025-12-23,"Summary: Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20298v1,Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives,arXiv,2025-12-23,"Summary: Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term ""narcissism."" Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20296v1,TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation,arXiv,2025-12-23,"Summary: The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20293v1,AprielGuard,arXiv,2025-12-23,"Summary: Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20292v1,SlideTailor: Personalized Presentation Slide Generation for Scientific Papers,arXiv,2025-12-23,"Summary: Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20291v1,Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity,arXiv,2025-12-23,"Summary: Mixture-of-Experts (MoE) architectures achieve parameter efficiency through conditional computation, yet contemporary designs suffer from two fundamental limitations: structural parameter isolation that causes catastrophic forgetting, and instruction-overfitting that degrades performance in instruction-free scenarios. We propose CDSP-MoE (Conflict-Driven Subspace Pruning MoE), a framework that addresses these issues through a paradigm shift from isolated expert containers to dynamic expert instantiation within a shared physical subspace. Grounded in the Universal Weight Subspace Hypothesis, CDSP-MoE maintains a super-complete parameter backbone where logical experts are carved out via learnable topology masks. Unlike prior work that uses gradient conflict for token reassignment or optimization surgery, we leverage it as a structural supervisory signal: a Lagged Gradient Game penalizes interfering connections in the shared manifold, enabling the topology to spontaneously prune conflicting pathways and evolve interpretable modular structures. Experimental results demonstrate that CDSP-MoE achieves robust content-driven routing without human-defined task labels, maintaining semantic specialization even under strict blind inference protocols where explicit instructions are absent. Code is available at: https://github.com/konodiodaaaaa1/Conflict-Driven-Subspace-Pruning-Mixture-of-Experts",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20288v1,UbiQVision: Quantifying Uncertainty in XAI for Image Recognition,arXiv,2025-12-23,"Summary: Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20286v1,"Replacing Gas with Low-cost, Abundant Long-duration Pumped Hydro in Electricity Systems",arXiv,2025-12-23,"Summary: Fossil gas is sometimes presented as an enabler of variable solar and wind generation beyond 2050, despite being a primary source of greenhouse gas emissions from methane leakage and combustion. We find that balancing solar and wind generation with pumped hydro energy storage eliminates the need for fossil gas without incurring a cost penalty. However, many existing long-term electricity system plans are biased to rely on fossil gas due to using temporal aggregation methods that either heavily constrain storage cycling behaviour or lose track of the state-of-charge, failing to consider the potential of low-cost long-duration off-river pumped hydro, and ignoring the broad suite of near-optimal energy transition pathways. We show that a temporal aggregation method based on 'segmentation' (fitted chronology) closely resembles the full-series optimisation, captures long-duration storage behaviour (48- and 160-hour durations), and finds a near-optimal 100% renewable electricity solution. We develop a new electricity system model to rapidly evaluate millions of other near-optimal solutions, stressing the importance of modelling pumped hydro sites with a low energy volume cost (<US$50 per kilowatt-hour), long economic lifetime (~75 years), and low real discount rate akin to other natural monopolies (<=3%). Almost every region of the world has access to sufficient 50 - 5000 gigawatt-hour off-river pumped hydro options that enable them to entirely decarbonise their future electricity systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20279v1,Auditing Reproducibility in Non-Targeted Analysis: 103 LC/GC--HRMS Tools Reveal Temporal Divergence Between Openness and Operability,arXiv,2025-12-23,"Summary: In 2008, melamine in infant formula forced laboratories across three continents to verify a compound they had never monitored. Non-targeted analysis using LC/GC-HRMS handles these cases. But when findings trigger regulatory action, reproducibility becomes operational: can an independent laboratory repeat the analysis and reach the same conclusion?
  We assessed 103 tools (2004-2025) against six pillars drawn from FAIR and BP4NTA principles: laboratory validation (C1), data availability (C2), code availability (C3), standardised formats (C4), knowledge integration (C5), and portable implementation (C6). Health contributed 51 tools, Pharma 31, and Chemistry 21.
  Nine in ten tools shared data (C2, 90/103, 87%). Fewer than four in ten supported portable implementations (C6, 40/103, 39%). Validation and portability rarely appeared together (C1+C6, 18/103, 17%). Over twenty-one years, openness climbed from 56% to 86% while operability dropped from 55% to 43%. No tool addressed food safety.
  Journal data-sharing policies increased what authors share but not what reviewers can run. Tools became easier to find but harder to execute. Strengthening C1, C4, and C6 would turn documented artifacts into workflows that external laboratories can replay.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20278v1,Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation,arXiv,2025-12-23,"Summary: While CodeMem establishes executable code as the optimal representation for agentic procedural memory, the mechanism for autonomously synthesizing this memory from a blank slate remains underexplored. This paper operationalizes the transition of Large Language Models from passive tool-users to active workflow architects. Through a high-fidelity case study of a cross-service orchestration task involving Outlook and OneDrive, we identify and address four structural bottlenecks in automated skill generation: the Discovery Gap involving navigation of large tool registries, the Verification Gap regarding grounding tool response structures, the Decomposition Gap which replaces inefficient search with Linear State Anchoring, and the Scaling Gap focused on concurrency and persistence. We demonstrate that by enforcing a scientific methodology of hypothesize, probe, and code, agents can autonomously write robust, production-grade code skills.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20276v1,ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge,arXiv,2025-12-23,"Summary: Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20275v1,Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks,arXiv,2025-12-23,"Summary: As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20272v1,HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training,arXiv,2025-12-23,"Summary: Neural Stochastic Differential Equations (Neural SDEs) provide a principled framework for modeling continuous-time stochastic processes and have been widely adopted in fields ranging from physics to finance. Recent advances suggest that Generative Adversarial Networks (GANs) offer a promising solution to learning the complex path distributions induced by SDEs. However, a critical bottleneck lies in designing a discriminator that faithfully captures temporal dependencies while remaining computationally efficient. Prior works have explored Neural Controlled Differential Equations (CDEs) as discriminators due to their ability to model continuous-time dynamics, but such architectures suffer from high computational costs and exacerbate the instability of adversarial training. To address these limitations, we introduce HGAN-SDEs, a novel GAN-based framework that leverages Neural Hermite functions to construct a structured and efficient discriminator. Hermite functions provide an expressive yet lightweight basis for approximating path-level dynamics, enabling both reduced runtime complexity and improved training stability. We establish the universal approximation property of our framework for a broad class of SDE-driven distributions and theoretically characterize its convergence behavior. Extensive empirical evaluations on synthetic and real-world systems demonstrate that HGAN-SDEs achieve superior sample quality and learning efficiency compared to existing generative models for SDEs",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20271v1,Automated Training of Learned Database Components with Generative AI,arXiv,2025-12-23,"Summary: The use of deep learning for database optimization has gained significant traction, offering improvements in indexing, cardinality estimation, and query optimization. However, acquiring high-quality training data remains a significant challenge. This paper explores the possibility of using generative models, such as GPT, to synthesize training data for learned database components. We present an initial feasibility study investigating their ability to produce realistic query distributions and execution plans for database workloads. Additionally, we discuss key challenges, such as data scalability and labeling, along with potential solutions. The initial results suggest that generative models can effectively augment training datasets, improving the adaptability of learned database techniques.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20270v1,Optimality-Informed Neural Networks for Solving Parametric Optimization Problems,arXiv,2025-12-23,"Summary: Many engineering tasks require solving families of nonlinear constrained optimization problems, parametrized in setting-specific variables. This is computationally demanding, particularly, if solutions have to be computed across strongly varying parameter values, e.g., in real-time control or for model-based design. Thus, we propose to learn the mapping from parameters to the primal optimal solutions and to their corresponding duals using neural networks, giving a dense estimation in contrast to gridded approaches. Our approach, Optimality-informed Neural Networks (OptINNs), combines (i) a KKT-residual loss that penalizes violations of the first-order optimality conditions under standard constraint qualifications assumptions, and (ii) problem-specific output activations that enforce simple inequality constraints (e.g., box-type/positivity) by construction. This design reduces data requirements, allows the prediction of dual variables, and improves feasibility and closeness to optimality compared to penalty-only training. Taking quadratic penalties as a baseline, since this approach has been previously proposed for the considered problem class in literature, our method simplifies hyperparameter tuning and attains tighter adherence to optimality conditions. We evaluate OptINNs on different nonlinear optimization problems ranging from low to high dimensions. On small problems, OptINNs match a quadratic-penalty baseline in primal accuracy while additionally predicting dual variables with low error. On larger problems, OptINNs achieve lower constraint violations and lower primal error compared to neural networks based on the quadratic-penalty method. These results suggest that embedding feasibility and optimality into the network architecture and loss can make learning-based surrogates more accurate, feasible, and data-efficient for parametric optimization.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20268v1,DeepONet-accelerated Bayesian inversion for moving boundary problems,arXiv,2025-12-23,"Summary: This work demonstrates that neural operator learning provides a powerful and flexible framework for building fast, accurate emulators of moving boundary systems, enabling their integration into digital twin platforms. To this end, a Deep Operator Network (DeepONet) architecture is employed to construct an efficient surrogate model for moving boundary problems in single-phase Darcy flow through porous media. The surrogate enables rapid and accurate approximation of complex flow dynamics and is coupled with an Ensemble Kalman Inversion (EKI) algorithm to solve Bayesian inverse problems.
  The proposed inversion framework is demonstrated by estimating the permeability and porosity of fibre reinforcements for composite materials manufactured via the Resin Transfer Moulding (RTM) process. Using both synthetic and experimental in-process data, the DeepONet surrogate accelerates inversion by several orders of magnitude compared with full-model EKI. This computational efficiency enables real-time, accurate, high-resolution estimation of local variations in permeability, porosity, and other parameters, thereby supporting effective monitoring and control of RTM processes, as well as other applications involving moving boundary flows. Unlike prior approaches for RTM inversion that learn mesh-dependent mappings, the proposed neural operator generalises across spatial and temporal domains, enabling evaluation at arbitrary sensor configurations without retraining, and represents a significant step toward practical industrial deployment of digital twins.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20261v1,Maximizing the Egalitarian Welfare in Friends and Enemies Games,arXiv,2025-12-23,"Summary: We consider the complexity of maximizing egalitarian welfare in Friends and Enemies Games -- a subclass of hedonic games in which every agent partitions other agents into friends and enemies. We investigate two classic scenarios proposed in the literature, namely, Friends Appreciation ($\mathsf{FA}$) and Enemies Aversion ($\mathsf{EA}$): in the former, each agent primarily cares about the number of friends in her coalition, breaking ties based on the number of enemies, while in the latter, the opposite is true. For $\mathsf{EA}$, we show that our objective is hard to approximate within $O(n^{1-ε})$, for any fixed $ε>0$, and provide a polynomial-time $(n-1)$-approximation. For $\mathsf{FA}$, we obtain an NP-hardness result and a polynomial-time approximation algorithm. Our algorithm achieves a ratio of $2-Θ(\frac{1}{n})$ when every agent has at least two friends; however, if some agent has at most one friend, its approximation ratio deteriorates to $n/2$. We recover the $2-Θ(\frac{1}{n})$ approximation ratio for two important variants: when randomization is allowed and when the friendship relationship is symmetric. Additionally, for both $\mathsf{EA}$ and $\mathsf{FA}$ we identify special cases where the optimal egalitarian partition can be computed in polynomial time.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20260v1,${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations,arXiv,2025-12-23,"Summary: Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20249v1,Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion,arXiv,2025-12-23,"Summary: Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20245v1,Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds,arXiv,2025-12-23,"Summary: The memory of contemporary Large Language Models is bound by a physical paradox: as they learn, they fill up. The linear accumulation (O(N)) of Key-Value states treats context as a warehouse of static artifacts, eventually forcing a destructive choice between amnesia and latency. We challenge this discrete orthodoxy, proposing that long-term memory is not the storage of items, but the persistence of a trajectory. We introduce Phonetic Trajectory Memory (PTM), a neuro-symbolic architecture that encodes language not as a sequence of tensors, but as a continuous path on an ergodic manifold governed by irrational rotation matrices. By decoupling the navigation (an invariant O(1) geometric signal) from the reconstruction (a probabilistic generative act), PTM achieves a compression magnitude of greater than 3,000x relative to dense caches. We demonstrate that retrieval becomes a process of resonance: the phonetic trace stabilizes the model against hallucination via ""Signal Consensus"" mechanism, securing up to approximately 92% factual accuracy. While this aggressive abstraction alters generative texture, it unlocks immediate access latency (approximately 34ms) independent of depth. Our results suggest that infinite context does not require infinite silicon; it requires treating memory not as data to be stored, but as a reconstructive process acting on a conserved, undying physical signal.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20237v1,MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents,arXiv,2025-12-23,"Summary: Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20233v1,How I Met Your Bias: Investigating Bias Amplification in Diffusion Models,arXiv,2025-12-23,"Summary: Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20232v1,Adaptive Multi-task Learning for Probabilistic Load Forecasting,arXiv,2025-12-23,"Summary: Simultaneous load forecasting across multiple entities (e.g., regions, buildings) is crucial for the efficient, reliable, and cost-effective operation of power systems. Accurate load forecasting is a challenging problem due to the inherent uncertainties in load demand, dynamic changes in consumption patterns, and correlations among entities. Multi-task learning has emerged as a powerful machine learning approach that enables the simultaneous learning across multiple related problems. However, its application to load forecasting remains underexplored and is limited to offline learning-based methods, which cannot capture changes in consumption patterns. This paper presents an adaptive multi-task learning method for probabilistic load forecasting. The proposed method can dynamically adapt to changes in consumption patterns and correlations among entities. In addition, the techniques presented provide reliable probabilistic predictions for loads of multiples entities and assess load uncertainties. Specifically, the method is based on vectorvalued hidden Markov models and uses a recursive process to update the model parameters and provide predictions with the most recent parameters. The performance of the proposed method is evaluated using datasets that contain the load demand of multiple entities and exhibit diverse and dynamic consumption patterns. The experimental results show that the presented techniques outperform existing methods both in terms of forecasting performance and uncertainty assessment.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20221v1,The Effect of Empathic Expression Levels in Virtual Human Interaction: A Controlled Experiment,arXiv,2025-12-23,"Summary: As artificial intelligence (AI) systems become increasingly embedded in everyday life, the ability of interactive agents to express empathy has become critical for effective human-AI interaction, particularly in emotionally sensitive contexts. Rather than treating empathy as a binary capability, this study examines how different levels of empathic expression in virtual human interaction influence user experience. We conducted a between-subject experiment (n = 70) in a counseling-style interaction context, comparing three virtual human conditions: a neutral dialogue-based agent, a dialogue-based empathic agent, and a video-based empathic agent that incorporates users' facial cues. Participants engaged in a 15-minute interaction and subsequently evaluated their experience using subjective measures of empathy and interaction quality. Results from analysis of variance (ANOVA) revealed significant differences across conditions in affective empathy, perceived naturalness of facial movement, and appropriateness of facial expression. The video-based empathic expression condition elicited significantly higher affective empathy than the neutral baseline (p < .001) and marginally higher levels than the dialogue-based condition (p < .10). In contrast, cognitive empathy did not differ significantly across conditions. These findings indicate that empathic expression in virtual humans should be conceptualized as a graded design variable, rather than a binary capability, with visually grounded cues playing a decisive role in shaping affective user experience.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20220v1,Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning,arXiv,2025-12-23,"Summary: We study offline multitask reinforcement learning in settings where multiple tasks share a low-rank representation of their action-value functions. In this regime, a learner is provided with fixed datasets collected from several related tasks, without access to further online interaction, and seeks to exploit shared structure to improve statistical efficiency and generalization. We analyze a multitask variant of fitted Q-iteration that jointly learns a shared representation and task-specific value functions via Bellman error minimization on offline data. Under standard realizability and coverage assumptions commonly used in offline reinforcement learning, we establish finite-sample generalization guarantees for the learned value functions. Our analysis explicitly characterizes how pooling data across tasks improves estimation accuracy, yielding a $1/\sqrt{nT}$ dependence on the total number of samples across tasks, while retaining the usual dependence on the horizon and concentrability coefficients arising from distribution shift. In addition, we consider a downstream offline setting in which a new task shares the same underlying representation as the upstream tasks. We study how reusing the representation learned during the multitask phase affects value estimation for this new task, and show that it can reduce the effective complexity of downstream learning relative to learning from scratch. Together, our results clarify the role of shared representations in multitask offline Q-learning and provide theoretical insight into when and how multitask structure can improve generalization in model-free, value-based reinforcement learning.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20218v1,Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud,arXiv,2025-12-23,"Summary: Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20216v1,"Quantitative Financial Modeling for Sri Lankan Markets: Approach Combining NLP, Clustering and Time-Series Forecasting",arXiv,2025-12-23,"Summary: This research introduces a novel quantitative methodology tailored for quantitative finance applications, enabling banks, stockbrokers, and investors to predict economic regimes and market signals in emerging markets, specifically Sri Lankan stock indices (S&P SL20 and ASPI) by integrating Environmental, Social, and Governance (ESG) sentiment analysis with macroeconomic indicators and advanced time-series forecasting. Designed to leverage quantitative techniques for enhanced risk assessment, portfolio optimization, and trading strategies in volatile environments, the architecture employs FinBERT, a transformer-based NLP model, to extract sentiment from ESG texts, followed by unsupervised clustering (UMAP/HDBSCAN) to identify 5 latent ESG regimes, validated via PCA. These regimes are mapped to economic conditions using a dense neural network and gradient boosting classifier, achieving 84.04% training and 82.0% validation accuracy. Concurrently, time-series models (SRNN, MLP, LSTM, GRU) forecast daily closing prices, with GRU attaining an R-squared of 0.801 and LSTM delivering 52.78% directional accuracy on intraday data. A strong correlation between S&P SL20 and S&P 500, observed through moving average and volatility trend plots, further bolsters forecasting precision. A rule-based fusion logic merges ESG and time-series outputs for final market signals. By addressing literature gaps that overlook emerging markets and holistic integration, this quant-driven framework combines global correlations and local sentiment analysis to offer scalable, accurate tools for quantitative finance professionals navigating complex markets like Sri Lanka.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20210v1,Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs,arXiv,2025-12-23,"Summary: The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20206v1,TongSIM: A General Platform for Simulating Intelligent Machines,arXiv,2025-12-23,"Summary: As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20204v1,Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings,arXiv,2025-12-23,"Summary: Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.
  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20190v1,Pricing of wrapped Bitcoin and Ethereum on-chain options,arXiv,2025-12-23,"Summary: This paper measures price differences between Hegic option quotes on Arbitrum and a model-based benchmark built on Black--Scholes model with regime-sensitive volatility estimated via a two-regime MS-AR-(GJR)-GARCH model. Using option-level feasible GLS, we find benchmark prices exceed Hegic quotes on average, especially for call options. The price spread rises with order size, strike, maturity, and estimated volatility, and falls with trading volume. By underlying, wrapped Bitcoin options show larger and more persistent spreads, while Ethereum options are closer to the benchmark. The framework offers a data-driven analysis for monitoring and calibrating on-chain option pricing logic.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20188v1,Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation,arXiv,2025-12-23,"Summary: Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20184v1,Reaching Agreement Among Reasoning LLM Agents,arXiv,2025-12-23,"Summary: Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.
  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20182v1,FaithLens: Detecting and Explaining Faithfulness Hallucination,arXiv,2025-12-23,"Summary: Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20181v1,Competing or Collaborating? The Role of Hackathon Formats in Shaping Team Dynamics and Project Choices,arXiv,2025-12-23,"Summary: Hackathons have emerged as dynamic platforms for fostering innovation, collaboration, and skill development in the technology sector. Structural differences across hackathon formats raise important questions about how event design can shape student learning experiences and engagement. This study examines two distinct hackathon formats: a gender-specific hackathon (GS) and a regular institutional hackathon (RI). Using a mixed-methods approach, we analyze variations in team dynamics, project themes, role assignments, and environmental settings. Our findings indicate that GS hackathon foster a collaborative and supportive atmosphere, emphasizing personal growth and community learning, with projects often centered on health and well-being. In contrast, RI hackathon tend to promote a competitive, outcome-driven environment, with projects frequently addressing entertainment and environmental sustainability. Based on these insights, we propose a hybrid hackathon model that combines the strengths of both formats to balance competition with inclusivity. This work contributes to the design of more engaging, equitable, and pedagogically effective hackathon experiences.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20179v1,RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making,arXiv,2025-12-23,"Summary: Current LLM-based driving agents that rely on unstructured plain-text memory suffer from low-precision scene retrieval and inefficient reflection. To address this limitation, we present RESPOND, a structured decision-making framework for LLM-driven agents grounded in explicit risk patterns. RESPOND represents each ego-centric scene using a unified 5 by 3 matrix that encodes spatial topology and road constraints, enabling consistent and reliable retrieval of spatial risk configurations. Based on this representation, a hybrid rule and LLM decision pipeline is developed with a two-tier memory mechanism. In high-risk contexts, exact pattern matching enables rapid and safe reuse of verified actions, while in low-risk contexts, sub-pattern matching supports personalized driving style adaptation. In addition, a pattern-aware reflection mechanism abstracts tactical corrections from crash and near-miss frames to update structured memory, achieving one-crash-to-generalize learning. Extensive experiments demonstrate the effectiveness of RESPOND. In highway-env, RESPOND outperforms state-of-the-art LLM-based and reinforcement learning based driving agents while producing substantially fewer collisions. With step-wise human feedback, the agent acquires a Sporty driving style within approximately 20 decision steps through sub-pattern abstraction. For real-world validation, RESPOND is evaluated on 53 high-risk cut-in scenarios extracted from the HighD dataset. For each event, intervention is applied immediately before the cut-in and RESPOND re-decides the driving action. Compared to recorded human behavior, RESPOND reduces subsequent risk in 84.9 percent of scenarios, demonstrating its practical feasibility under real-world driving conditions. These results highlight RESPONDs potential for autonomous driving, personalized driving assistance, and proactive hazard mitigation.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20178v1,SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication,arXiv,2025-12-23,"Summary: Distributed Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental operation in numerous high-performance computing and deep learning applications. The major performance bottleneck in distributed SpMM lies in the substantial communication overhead, which limits both performance and scalability. In this paper, we identify and analyze sources of inefficient communication in existing distributed SpMM implementations at two levels and address these inefficiencies by proposing: (1) a fine-grained, sparsity-aware communication strategy that reduces communication overhead by exploiting the sparsity pattern of the sparse matrix, and (2) a hierarchical communication strategy that integrates the sparsity-aware strategy with the common two-tier network architectures in GPU-accelerated systems, to reduce redundant communication across slow network links. We implement these optimizations in a comprehensive distributed SpMM framework, \method{}. Extensive evaluations on real-world datasets show that our framework demonstrates strong scalability up to 128 GPUs, achieving geometric mean speedups of 221.5$\times$, 56.0$\times$, 23.4$\times$, and 8.8$\times$ over four state-of-the-art baselines (CAGNET, SPA, BCL, and CoLa, respectively) at this scale.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20177v1,NeuralCrop: Combining physics and machine learning for improved crop yield predictions,arXiv,2025-12-23,"Summary: Global gridded crop models (GGCMs) simulate daily crop growth by explicitly representing key biophysical processes and project end-of-season yield time series. They are a primary tool to quantify the impacts of climate change on agricultural productivity and assess associated risks for food security. Despite decades of development, state-of-the-art GGCMs still have substantial uncertainties in simulating complex biophysical processes due to limited process understanding. Recently, machine learning approaches trained on observational data have shown great potential in crop yield predictions. However, these models have not demonstrated improved performance over classical GGCMs and are not suitable for simulating crop yields under changing climate conditions due to problems in generalizing outside their training distributions. Here we introduce NeuralCrop, a hybrid GGCM that combines the strengths of an advanced process-based GGCM, resolving important processes explicitly, with data-driven machine learning components. The model is first trained to emulate a competitive GGCM before it is fine-tuned on observational data. We show that NeuralCrop outperforms state-of-the-art GGCMs across site-level and large-scale cropping regions. Across moisture conditions, NeuralCrop reproduces the interannual yield anomalies in European wheat regions and the US Corn Belt more accurately during the period from 2000 to 2019 with particularly strong improvements under drought extremes. When generalizing to conditions unseen during training, NeuralCrop continues to make robust projections, while pure machine learning models exhibit substantial performance degradation. Our results show that our hybrid crop modelling approach offers overall improved crop modeling and more reliable yield projections under climate change and intensifying extreme weather conditions.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20174v1,Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark,arXiv,2025-12-23,"Summary: Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20173v1,Offline Safe Policy Optimization From Heterogeneous Feedback,arXiv,2025-12-23,"Summary: Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20172v1,Collaborative Group-Aware Hashing for Fast Recommender Systems,arXiv,2025-12-23,"Summary: The fast online recommendation is critical for applications with large-scale databases; meanwhile, it is challenging to provide accurate recommendations in sparse scenarios. Hash technique has shown its superiority for speeding up the online recommendation by bit operations on Hamming distance computations. However, existing hashing-based recommendations suffer from low accuracy, especially with sparse settings, due to the limited representation capability of each bit and neglected inherent relations among users and items. To this end, this paper lodges a Collaborative Group-Aware Hashing (CGAH) method for both collaborative filtering (namely CGAH-CF) and content-aware recommendations (namely CGAH) by integrating the inherent group information to alleviate the sparse issue. Firstly, we extract inherent group affinities of users and items by classifying their latent vectors into different groups. Then, the preference is formulated as the inner product of the group affinity and the similarity of hash codes. By learning hash codes with the inherent group information, CGAH obtains more effective hash codes than other discrete methods with sparse interactive data. Extensive experiments on three public datasets show the superior performance of our proposed CGAH and CGAH-CF over the state-of-the-art discrete collaborative filtering methods and discrete content-aware recommendations under different sparse settings.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20169v1,Learning to Reason in LLMs by Expectation Maximization,arXiv,2025-12-23,"Summary: Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20168v1,Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography,arXiv,2025-12-23,"Summary: By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20164v1,AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications,arXiv,2025-12-23,"Summary: Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by ""adversarial instructions"" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20163v1,Population Protocols Revisited: Parity and Beyond,arXiv,2025-12-23,"Summary: For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.
  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\log^3 n)$ states and achieve silent stabilisation in $O(\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20162v1,Concept Generalization in Humans and Large Language Models: Insights from the Number Game,arXiv,2025-12-23,"Summary: We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20161v1,A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers,arXiv,2025-12-23,"Summary: Data centers account for significant global energy consumption and a carbon footprint. The recent increasing demand for edge computing and AI advancements drives the growth of data center storage capacity. Energy efficiency is a cost-effective way to combat climate change, cut energy costs, improve business competitiveness, and promote IT and environmental sustainability. Thus, optimizing data center energy management is the most important factor in the sustainability of the world. Power Usage Effectiveness (PUE) is used to represent the operational efficiency of the data center. Predicting PUE using Neural Networks provides an understanding of the effect of each feature on energy consumption, thus enabling targeted modifications of those key features to improve energy efficiency. In this paper, we have developed Bidirectional Gated Recurrent Unit (BiGRU) based PUE prediction model and compared the model performance with GRU. The data set comprises 52,560 samples with 117 features using EnergyPlus, simulating a DC in Singapore. Sets of the most relevant features are selected using the Recursive Feature Elimination with Cross-Validation (RFECV) algorithm for different parameter settings. These feature sets are used to find the optimal hyperparameter configuration and train the BiGRU model. The performance of the optimized BiGRU-based PUE prediction model is then compared with that of GRU using mean squared error (MSE), mean absolute error (MAE), and R-squared metrics.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20159v1,AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration,arXiv,2025-12-23,"Summary: Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.
  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20156v1,Fun-Audio-Chat Technical Report,arXiv,2025-12-23,"Summary: Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20145v1,Retrieval-augmented Prompt Learning for Pre-trained Foundation Models,arXiv,2025-12-23,"Summary: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20144v1,Multi-hop Reasoning via Early Knowledge Alignment,arXiv,2025-12-23,"Summary: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20140v1,Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection,arXiv,2025-12-23,"Summary: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20136v1,M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation,arXiv,2025-12-23,"Summary: Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20135v1,MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization,arXiv,2025-12-23,"Summary: Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed ""thinking"" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open ""thinking"" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed ""thinking"" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20129v1,"Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs",arXiv,2025-12-23,"Summary: Authoring 3D scenes is a central task for spatial computing applications. Competing visions for lowering existing barriers are (1) focus on immersive, direct manipulation of 3D content or (2) leverage AI techniques that capture real scenes (3D Radiance Fields such as, NeRFs, 3D Gaussian Splatting) and modify them at a higher level of abstraction, at the cost of high latency. We unify the complementary strengths of these approaches and investigate how to integrate generative AI advances into real-time, immersive 3D Radiance Field editing. We introduce Dreamcrafter, a VR-based 3D scene editing system that: (1) provides a modular architecture to integrate generative AI algorithms; (2) combines different levels of control for creating objects, including natural language and direct manipulation; and (3) introduces proxy representations that support interaction during high-latency operations. We contribute empirical findings on control preferences and discuss how generative AI interfaces beyond text input enhance creativity in scene editing and world building.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20116v1,/UnmuteAll: Modeling Verbal Communication Patterns of Collaborative Contexts in MOBA Games,arXiv,2025-12-23,"Summary: Team communication plays a vital role in supporting collaboration in multiplayer online games. Therefore, numerous studies were conducted to examine communication patterns in esports teams. While non-verbal communication has been extensively investigated, research on assessing voice-based verbal communication patterns remains relatively understudied. In this study, we propose a framework that automatically assesses verbal communication patterns by constructing networks with utterances transcribed from voice recordings. Through a data collection study, we obtained 84 game sessions from five League of Legends teams and subsequently investigated how verbal communication patterns varied across different conditions. As a result, we revealed that esports players exhibited broader and more balanced participation in collaborative situations, increased utterances over time with the largest rise in decision making, and team-level differences that were contingent on effective professional training. Building upon these findings, this study provides a generalizable tool for analyzing effective team communication.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20115v1,Sample-Efficient Policy Constraint Offline Deep Reinforcement Learning based on Sample Filtering,arXiv,2025-12-23,"Summary: Offline reinforcement learning (RL) aims to learn a policy that maximizes the expected return using a given static dataset of transitions. However, offline RL faces the distribution shift problem. The policy constraint offline RL method is proposed to solve the distribution shift problem. During the policy constraint offline RL training, it is important to ensure the difference between the learned policy and behavior policy within a given threshold. Thus, the learned policy heavily relies on the quality of the behavior policy. However, a problem exists in existing policy constraint methods: if the dataset contains many low-reward transitions, the learned will be contained with a suboptimal reference policy, leading to slow learning speed, low sample efficiency, and inferior performances. This paper shows that the sampling method in policy constraint offline RL that uses all the transitions in the dataset can be improved. A simple but efficient sample filtering method is proposed to improve the sample efficiency and the final performance. First, we evaluate the score of the transitions by average reward and average discounted reward of episodes in the dataset and extract the transition samples of high scores. Second, the high-score transition samples are used to train the offline RL algorithms. We verify the proposed method in a series of offline RL algorithms and benchmark tasks. Experimental results show that the proposed method outperforms baselines.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20112v1,Evolutionary Neural Architecture Search with Dual Contrastive Learning,arXiv,2025-12-23,"Summary: Evolutionary Neural Architecture Search (ENAS) has gained attention for automatically designing neural network architectures. Recent studies use a neural predictor to guide the process, but the high computational costs of gathering training data -- since each label requires fully training an architecture -- make achieving a high-precision predictor with { limited compute budget (i.e., a capped number of fully trained architecture-label pairs)} crucial for ENAS success. This paper introduces ENAS with Dual Contrastive Learning (DCL-ENAS), a novel method that employs two stages of contrastive learning to train the neural predictor. In the first stage, contrastive self-supervised learning is used to learn meaningful representations from neural architectures without requiring labels. In the second stage, fine-tuning with contrastive learning is performed to accurately predict the relative performance of different architectures rather than their absolute performance, which is sufficient to guide the evolutionary search. Across NASBench-101 and NASBench-201, DCL-ENAS achieves the highest validation accuracy, surpassing the strongest published baselines by 0.05\% (ImageNet16-120) to 0.39\% (NASBench-101). On a real-world ECG arrhythmia classification task, DCL-ENAS improves performance by approximately 2.5 percentage points over a manually designed, non-NAS model obtained via random search, while requiring only 7.7 GPU-days.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20111v1,ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language,arXiv,2025-12-23,"Summary: As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20097v1,A Novel Graph-Sequence Learning Model for Inductive Text Classification,arXiv,2025-12-23,"Summary: Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20096v1,Information-directed sampling for bandits: a primer,arXiv,2025-12-23,"Summary: The Multi-Armed Bandit problem provides a fundamental framework for analyzing the tension between exploration and exploitation in sequential learning. This paper explores Information Directed Sampling (IDS) policies, a class of heuristics that balance immediate regret against information gain. We focus on the tractable environment of two-state Bernoulli bandits as a minimal model to rigorously compare heuristic strategies against the optimal policy. We extend the IDS framework to the discounted infinite-horizon setting by introducing a modified information measure and a tuning parameter to modulate the decision-making behavior. We examine two specific problem classes: symmetric bandits and the scenario involving one fair coin. In the symmetric case we show that IDS achieves bounded cumulative regret, whereas in the one-fair-coin scenario the IDS policy yields a regret that scales logarithmically with the horizon, in agreement with classical asymptotic lower bounds. This work serves as a pedagogical synthesis, aiming to bridge concepts from reinforcement learning and information theory for an audience of statistical physicists.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20094v1,Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning,arXiv,2025-12-23,"Summary: In this paper, we investigate how the widely existing contextual and structural divergence may influence the representation learning in rich-text graphs. To this end, we propose Jensen-Shannon Divergence Message-Passing (JSDMP), a new learning paradigm for rich-text graph representation learning. Besides considering similarity regarding structure and text, JSDMP further captures their corresponding dissimilarity by Jensen-Shannon divergence. Similarity and dissimilarity are then jointly used to compute new message weights among text nodes, thus enabling representations to learn with contextual and structural information from truly correlated text nodes. With JSDMP, we propose two novel graph neural networks, namely Divergent message-passing graph convolutional network (DMPGCN) and Divergent message-passing Page-Rank graph neural networks (DMPPRG), for learning representations in rich-text graphs. DMPGCN and DMPPRG have been extensively texted on well-established rich-text datasets and compared with several state-of-the-art baselines. The experimental results show that DMPGCN and DMPPRG can outperform other baselines, demonstrating the effectiveness of the proposed Jensen-Shannon Divergence Message-Passing paradigm",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20092v1,Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents,arXiv,2025-12-23,"Summary: Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20088v1,Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts,arXiv,2025-12-23,"Summary: Fashion style classification is a challenging task because of the large visual variation within the same style and the existence of visually similar styles.
  Styles are expressed not only by the global appearance, but also by the attributes of individual items and their combinations.
  In this study, we propose an item region-based fashion style classification network (IRSN) to effectively classify fashion styles by analyzing item-specific features and their combinations in addition to global features.
  IRSN extracts features of each item region using item region pooling (IRP), analyzes them separately, and combines them using gated feature fusion (GFF).
  In addition, we improve the feature extractor by applying a dual-backbone architecture that combines a domain-specific feature extractor and a general feature extractor pre-trained with a large-scale image-text dataset.
  In experiments, applying IRSN to six widely-used backbones, including EfficientNet, ConvNeXt, and Swin Transformer, improved style classification accuracy by an average of 6.9% and a maximum of 14.5% on the FashionStyle14 dataset and by an average of 7.6% and a maximum of 15.1% on the ShowniqV3 dataset. Visualization analysis also supports that the IRSN models are better than the baseline models at capturing differences between similar style classes.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20086v1,Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection,arXiv,2025-12-23,"Summary: Spatio-temporal graph neural networks (ST-GNNs) have achieved notable success in structured domains such as road traffic and public transportation, where spatial entities can be naturally represented as fixed nodes. In contrast, many real-world systems including maritime traffic lack such fixed anchors, making the construction of spatio-temporal graphs a fundamental challenge. Anomaly detection in these non-grid environments is particularly difficult due to the absence of canonical reference points, the sparsity and irregularity of trajectories, and the fact that anomalies may manifest at multiple granularities. In this work, we introduce a novel benchmark dataset for anomaly detection in the maritime domain, extending the Open Maritime Traffic Analysis Dataset (OMTAD) into a benchmark tailored for graph-based anomaly detection. Our dataset enables systematic evaluation across three different granularities: node-level, edge-level, and graph-level anomalies. We plan to employ two specialized LLM-based agents: \emph{Trajectory Synthesizer} and \emph{Anomaly Injector} to construct richer interaction contexts and generate semantically meaningful anomalies. We expect this benchmark to promote reproducibility and to foster methodological advances in anomaly detection for non-grid spatio-temporal systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20084v1,QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption,arXiv,2025-12-23,"Summary: Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa.
  To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (\textbf{Q}wen) with an E(3)-equivariant graph Transformer (\textbf{E}quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20082v1,"Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches",arXiv,2025-12-23,"Summary: Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20080v1,CBA: Communication-Bound-Aware Cross-Domain Resource Assignment for Pipeline-Parallel Distributed LLM Training in Dynamic Multi-DC Optical Networks,arXiv,2025-12-23,"Summary: We propose a communication-bound-aware cross-domain resource assignment framework for pipeline-parallel distributed training over multi-datacenter optical networks, which lowers iteration time by 31.25% and reduces 13.20% blocking requests compared to baselines.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20074v1,Reason2Decide: Rationale-Driven Multi-Task Learning,arXiv,2025-12-23,"Summary: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20068v1,Change Point Detection and Mean-Field Dynamics of Variable Productivity Hawkes Processes,arXiv,2025-12-23,"Summary: Many self-exciting systems change because endogenous amplification, as opposed to exogenous forcing, varies. We study a Hawkes process with fixed background rate and kernel, but piecewise time-varying productivity. For exponential kernels we derive closed-form mean-field relaxation after a change and a deterministic surrogate for post-change Fisher information, revealing a boundary layer in which change time information localises and saturates, while post-change level information grows linearly beyond a short transient. These results motivate a Bayesian change point procedure that stabilizes inference on finite windows. We illustrate the method on invasive pneumococcal disease incidence in The Gambia, identifying a decline in productivity aligned with pneumococcal conjugate vaccine rollout.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20064v1,FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling,arXiv,2025-12-23,"Summary: Matrix Product State (MPS) is a versatile tensor network representation widely applied in quantum physics, quantum chemistry, and machine learning, etc. MPS sampling serves as a critical fundamental operation in these fields. As the problems become more complex, the scale of MPS is rapidly increasing. Traditional data parallelism is limited by memory and heavy I/O in large-scale MPS. Model parallelism that can handle large-scale MPS imposes rigid process bindings and lacks scalability. This work proposes Fast-MPS, a multi-level parallel framework for scalable MPS sampling. Our design combines data parallelism across samples with tensor parallelism along bond dimensions. We eliminate memory and I/O pressure through compression and overlapping, and revive data parallel in large-scale MPS sampling. We evaluate our approach on Gaussian Boson Sampling, a representative and demanding application. Fast-MPS achieves over 10x speedup compared to existing simulators, scales to thousands of processes, and enables simulations with 8,176 sites and bond dimension chi = 10^4, significantly outperforming the state of the art. Fast-MPS has demonstrated great potential in high-performance tensor network applications.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20063v1,PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models,arXiv,2025-12-23,"Summary: We introduce $\texttt{PairFlow}$, a lightweight preprocessing step for training Discrete Flow Models (DFMs) to achieve few-step sampling without requiring a pretrained teacher. DFMs have recently emerged as a new class of generative models for discrete data, offering strong performance. However, they suffer from slow sampling due to their iterative nature. Existing acceleration methods largely depend on finetuning, which introduces substantial additional training overhead. $\texttt{PairFlow}$ addresses this issue with a lightweight preprocessing step. Inspired by ReFlow and its extension to DFMs, we train DFMs from coupled samples of source and target distributions, without requiring any pretrained teacher. At the core of our approach is a closed-form inversion for DFMs, which allows efficient construction of paired source-target samples. Despite its extremely low cost, taking only up to 1.7% of the compute needed for full model training, $\texttt{PairFlow}$ matches or even surpasses the performance of two-stage training involving finetuning. Furthermore, models trained with our framework provide stronger base models for subsequent distillation, yielding further acceleration after finetuning. Experiments on molecular data as well as binary and RGB images demonstrate the broad applicability and effectiveness of our approach.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20062v1,On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities,arXiv,2025-12-23,"Summary: Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20061v1,Scaling Reinforcement Learning for Content Moderation with Large Language Models,arXiv,2025-12-23,"Summary: Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20059v1,DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion,arXiv,2025-12-23,"Summary: Student engagement is a critical factor influencing academic success and learning outcomes. Accurately predicting student engagement is essential for optimizing teaching strategies and providing personalized interventions. However, most approaches focus on single-dimensional feature analysis and assessing engagement based on individual student factors. In this work, we propose a dual-stream multi-feature fusion model based on hypergraph convolutional networks (DS-HGCN), incorporating social contagion of student engagement. DS-HGCN enables accurate prediction of student engagement states by modeling multi-dimensional features and their propagation mechanisms between students. The framework constructs a hypergraph structure to encode engagement contagion among students and captures the emotional and behavioral differences and commonalities by multi-frequency signals. Furthermore, we introduce a hypergraph attention mechanism to dynamically weigh the influence of each student, accounting for individual differences in the propagation process. Extensive experiments on public benchmark datasets demonstrate that our proposed method achieves superior performance and significantly outperforms existing state-of-the-art approaches.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20058v1,Deep Eigenspace Network and Its Application to Parametric Non-selfadjoint Eigenvalue Problems,arXiv,2025-12-23,"Summary: We consider operator learning for efficiently solving parametric non-selfadjoint eigenvalue problems. To overcome the spectral instability and mode switching inherent in non-selfadjoint operators, we introduce a hybrid framework that learns the stable invariant eigensubspace mapping rather than individual eigenfunctions. We proposed a Deep Eigenspace Network (DEN) architecture integrating Fourier Neural Operators, geometry-adaptive POD bases, and explicit banded cross-mode mixing mechanisms to capture complex spectral dependencies on unstructured meshes. We apply DEN to the parametric non-selfadjoint Steklov eigenvalue problem and provide theoretical proofs for the Lipschitz continuity of the eigensubspace with respect to the parameters. In addition, we derive error bounds for the reconstruction of the eigenspace. Numerical experiments validate DEN's high accuracy and zero-shot generalization capabilities across different discretizations.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20057v1,Structure-Preserving Nonlinear Sufficient Dimension Reduction for Tensors,arXiv,2025-12-23,"Summary: We introduce two nonlinear sufficient dimension reduction methods for regressions with tensor-valued predictors. Our goal is two-fold: the first is to preserve the tensor structure when performing dimension reduction, particularly the meaning of the tensor modes, for improved interpretation; the second is to substantially reduce the number of parameters in dimension reduction, thereby achieving model parsimony and enhancing estimation accuracy. Our two tensor dimension reduction methods echo the two commonly used tensor decomposition mechanisms: one is the Tucker decomposition, which reduces a larger tensor to a smaller one; the other is the CP-decomposition, which represents an arbitrary tensor as a sequence of rank-one tensors. We developed the Fisher consistency of our methods at the population level and established their consistency and convergence rates. Both methods are easy to implement numerically: the Tucker-form can be implemented through a sequence of least-squares steps, and the CP-form can be implemented through a sequence of singular value decompositions. We investigated the finite-sample performance of our methods and showed substantial improvement in accuracy over existing methods in simulations and two data applications.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20617v1,SpatialTree: How Spatial Abilities Branch Out in MLLMs,arXiv,2025-12-23,"Summary: Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive ""thinking"" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20615v1,Active Intelligence in Video Avatars via Closed-loop World Modeling,arXiv,2025-12-23,"Summary: Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20613v1,Variational (matrix) product states for combinatorial optimization,arXiv,2025-12-23,"Summary: To compute approximate solutions for combinatorial optimization problems, we describe variational methods based on the product state (PS) and matrix product state (MPS) ansatzes. We perform variational energy minimization with respect to a quantum annealing Hamiltonian and utilize randomness by embedding the approaches in the metaheuristic iterated local search (ILS). The resulting quantum-inspired ILS algorithms are benchmarked on maximum cut problems of up to 50000 variables. We show that they can outperform traditional (M)PS methods, classical ILS, the quantum approximate optimization algorithm and other variational quantum-inspired solvers.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20611v1,"Single-LED-pumped, room-temperature, solid-state maser",arXiv,2025-12-23,"Summary: Through their ability to achieve `cryogenic' levels of noise performance while operating at room temperature, optically-pumped, solid-state (OPSS) masers show great promise as quantum sensors, oscillators, and amplifiers. We here demonstrate maser oscillation in a microwave cavity containing a crystal of pentacene-doped \textit{para}-terphenyl (ptc:ptp) pumped by a single, chip-scale LED. Here, unlike previous work, the size of the pump source no longer dominates the size of the maser system as a whole. This miniaturization is achieved through invasive optical pumping in the form of a waveguide, the tip of which is embedded into the maser crystal. Combining experimental measurements with ray-tracing analysis, we find that our approach offers at least a factor of 2 enhancement in the cooperativity over end-on optical excitation.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20591v1,LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing,arXiv,2025-12-23,"Summary: Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20583v1,Making Sense of Private Advertising: A Principled Approach to a Complex Ecosystem,arXiv,2025-12-23,"Summary: In this work, we model the end-to-end pipeline of the advertising ecosystem, allowing us to identify two main issues with the current trajectory of private advertising proposals. First, prior work has largely considered ad targeting and engagement metrics individually rather than in composition. This has resulted in privacy notions that, while reasonable for each protocol in isolation, fail to compose to a natural notion of privacy for the ecosystem as a whole, permitting advertisers to extract new information about the audience of their advertisements. The second issue serves to explain the first: we prove that \textit{perfect} privacy is impossible for any, even minimally, useful advertising ecosystem, due to the advertisers' expectation of conducting market research on the results.
  Having demonstrated that leakage is inherent in advertising, we re-examine what privacy could realistically mean in advertising, building on the well-established notion of \textit{sensitive} data in a specific context. We identify that fundamentally new approaches are needed when designing privacy-preserving advertising subsystems in order to ensure that the privacy properties of the end-to-end advertising system are well aligned with people's privacy desires.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20571v1,Composing Mini Oscilloscope on Embedded Systems,arXiv,2025-12-23,"Summary: In this paper, our goal is to reproduce the basic functionalities of a regular oscilloscope, using the Nuvoton NUC-140 embedded systems development platform as the front-end and display method. A custom-built daughter board connects the NUC-140 to a variety of peripherals, including two BNC scope-probe connections, an external nine-button keypad, and a calibration signal. The LCD of the NUC-140 development board serves as the waveform display. From the experimental results, it is demonstrated that our proposed system became a very competent debugging tool. It implements 90% of the features we typically use on original oscilloscopes, including: automatic, edge-triggered, and single modes; waveform visualization using vertical and horizontal scaling; probe calibration.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20561v1,FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models,arXiv,2025-12-23,"Summary: Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.
  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.
  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20557v1,Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models,arXiv,2025-12-23,"Summary: Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20556v1,Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios,arXiv,2025-12-23,"Summary: Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20535v1,ARBITER: AI-Driven Filtering for Role-Based Access Control,arXiv,2025-12-23,"Summary: Role-Based Access Control (RBAC) struggles to adapt to dynamic enterprise environments with documents that contain information that cannot be disclosed to specific user groups. As these documents are used by LLM-driven systems (e.g., in RAG) the problem is exacerbated as LLMs can leak sensitive data due to prompt truncation, classification errors, or loss of system context. We introduce \our, a system designed to provide RBAC in RAG systems. \our implements layered input/output validation, role-aware retrieval, and post-generation fact-checking. Unlike traditional RBAC approaches that rely on fine-tuned classifiers, \our uses LLMs operating in few-shot settings with prompt-based steering for rapid deployment and role updates. We evaluate the approach on 389 queries using a synthetic dataset. Experimental results show 85\% accuracy and 89\% F1-score in query filtering, close to traditional RBAC solutions. Results suggest that practical RBAC deployment on RAG systems is approaching the maturity level needed for dynamic enterprise environments.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20522v1,An Instrument for Physical Vapor Deposition onto Cryo-EM Samples for Microsecond Time-Resolved Cryo-EM,arXiv,2025-12-23,"Summary: Laser flash melting and revitrification experiments have recently improved the time resolution of cryo-electron microscopy (cryo-EM) to the microsecond timescale, making it fast enough to observe many of the protein motions that are associated with function. The technique has also opened up a new dimension for cryo-EM sample preparation, making it possible to deposit compounds onto a cryo-EM sample while it is frozen, so that upon flash melting, the embedded particles experience an altered environment. For example, we have recently shown that depositing ultrathin silicon dioxide membranes onto a cryo-EM sample causes particles to detach from the interface upon flash melting, removing preferred particle orientation. These experiments also point towards a new strategy for initiating protein dynamics in time resolved experiments by depositing reagents, which will then mix with the sample upon flash melting. Here, we describe an apparatus for physical vapor deposition of compounds onto cryo-EM samples, detailing its design and operation. As a demonstration, we determine that the minimum thickness of silicon dioxide sealing membranes in a laser flash melting experiment is just over two monolayers. We propose that our design can form the basis for an integrated platform for microsecond time-resolved cryo-EM experiments.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20501v1,Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition,arXiv,2025-12-23,"Summary: This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.
  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.
  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.
  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.
  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.
  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.
  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20468v1,Calibration Method of Spacecraft-Inertial Sensor Center-of-Mass Offset for the Taiji Gravitational Wave Detection Mission under Science Mode,arXiv,2025-12-23,"Summary: Accurately calibrating the center-of-mass (CoM) offset between the spacecraft (SC) and the inertial sensor test mass (TM) is crucial for space-based gravitational-wave (GW) antennas, such as LISA and Taiji. Current calibration methods require additional spacecraft maneuvers that disrupt science data continuity and inter-satellite links, compromising the coherence of gravitational wave signals. Here, we present a maneuver-free calibration scheme that directly estimates the CoM offset vector using only standard science-mode measurements from inertial sensors, interferometers, and differential wavefront sensors. By embedding the CoM offset induced coupling acceleration as an extended state in a model-based adaptive Kalman filter, we achieve estimation accuracy of 0.01-1.5 mm across all axes with a maximum error below 1%. This approach enables continuous, high-precision calibration during nominal observation runs, ensuring continuous and coherent gravitational wave data collection while maintaining the required precision, and also facilitating advanced DFACS functions such as performance evaluations and fault diagnosis. For LISA-like missions, where data continuity is paramount for detecting faint gravitational wave signals, this method will enhance scientific output and reliability.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20449v1,Studying nuclear medium modification using the Gerasimov-Drell-Hearn sum rule,arXiv,2025-12-23,"Summary: The Gerasimov-Drell-Hearn sum rule is a generic relation that has been used to make significant contributions to research in hadronic physics. It connects the spin-dependent cross-section for photoproduction off a particle to the squared ratio of the particle's anomalous magnetic moment and its mass, $(κ/M)^2$. Thus, for a nucleon embedded in a nucleus, the sum rule relates the cross-section to $κ/M$ averaged quadratically over the nucleons comprising the nucleus. This quadratic averaging can be used to constrain the mechanism responsible for the medium modification of the nucleon. We also point out that the global properties of the embedded nucleon like its axial charge, mass or magnetic moment are observables measurable through sum rules.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20405v1,ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected,arXiv,2025-12-23,"Summary: Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical systems. This research explores both the offensive and defensive sides of this growing threat. On the attack side, we demonstrate how an author can inject hidden prompts inside a PDF that secretly guide or ""jailbreak"" LLM reviewers into giving overly positive feedback and biased acceptance. On the defense side, we propose an ""inject-and-detect"" strategy for editors, where invisible trigger prompts are embedded into papers; if a review repeats or reacts to these triggers, it reveals that the review was generated by an LLM, not a human. This method turns prompt injections from vulnerability into a verification tool. We outline our design, expected model behaviors, and ethical safeguards for deployment. The goal is to expose how fragile today's peer-review process becomes under LLM influence and how editorial awareness can help restore trust in scientific evaluation.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20385v1,Generalized method of L-moment estimation for stationary and nonstationary extreme value models,arXiv,2025-12-23,"Summary: Precisely estimating out-of-sample upper quantiles is very important in risk assessment and in engineering practice for structural design to prevent a greater disaster. For this purpose, the generalized extreme value (GEV) distribution has been broadly used. To estimate the parameters of GEV distribution, the maximum likelihood estimation (MLE) and L-moment estimation (LME) methods have been primarily employed. For a better estimation using the MLE, several studies considered the generalized MLE (penalized likelihood or Bayesian) methods to cooperate with a penalty function or prior information for parameters. However, a generalized LME method for the same purpose has not been developed yet in the literature. We thus propose the generalized method of L-moment estimation (GLME) to cooperate with a penalty function or prior information. The proposed estimation is based on the generalized L-moment distance and a multivariate normal likelihood approximation. Because the L-moment estimator is more efficient and robust for small samples than the MLE, we reasonably expect the advantages of LME to continue to hold for GLME. The proposed method is applied to the stationary and nonstationary GEV models with two novel (data-adaptive) penalty functions to correct the bias of LME. A simulation study indicates that the biases of LME are considerably corrected by the GLME with slight increases in the standard error. Applications to US flood damage data and maximum rainfall at Phliu Agromet in Thailand illustrate the usefulness of the proposed method. This study may promote further work on penalized or Bayesian inferences based on L-moments.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20371v1,Non linear Regge trajectories of quarkonia from holography,arXiv,2025-12-23,"Summary: We propose a holographic model for quarkonia using the WKB approximation with the Langer correction to properly reproduce nonlinear Regge trajectories of the form $m_n^2 = β(n + c_0)^{2/3} + c_1$. This form is expected from previous studies involving the solution of Cornell Potential for heavy quark anti-quark interactions using a model based on the quadratic form of the spinless Salpeter-type equation (QSSE). The model fits experimental masses with very good accuracy. As a by product, the corresponding decay constants also show a reasonable agreement with the results obtained from experimental data.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20362v1,CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation,arXiv,2025-12-23,"Summary: Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.
  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.
  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20361v1,From the planar Ising model to quasiconformal mappings,arXiv,2025-12-23,"Summary: We identify the scaling limit of full-plane Kadanoff-Ceva fermions on generic, non-degenerate $s$-embeddings. In this broad setting, the scaling limits are described in terms of solutions to conjugate Beltrami equations with prescribed singularities. For the underlying Ising model, this leads to the scaling limit of the energy-energy correlations and reveals a connection between the scaling limits of (near-)critical planar Ising models and quasiconformal mappings. For grids approximating bounded domains in the complex plane, we establish, in the scaling regime, the conformal covariance of the energy density on critical doubly periodic graphs. We complement this result with an analogous statement in the case where the limiting conformal structure generates a maximal surface $(z,\vartheta)$ in Minkowski space $\mathbb{R}^{(2,1)}$. All scaling factors obtained are local and expressed in terms of the geometry of the embedding, even in situations where they vary drastically from one region to another.
  These results confirm the predictions of Chelkak and highlight that the scaling limits of generic (near-)critical Ising models naturally live on a substantially richer conformal structure than the classical Euclidean one.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20355v1,FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration,arXiv,2025-12-23,"Summary: Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20334v1,Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation,arXiv,2025-12-23,"Summary: With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.
  This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20317v1,Towards Energy Independence: Critical Minerals in the Indian Context,arXiv,2025-12-23,"Summary: The global impetus for extracting rare earth elements (REEs) is shaping the future of green technologies. From high-efficiency magnets in wind turbines to advanced batteries and solar photovoltaics, REEs are indispensable for a greener world through the energy transition. However, supply chains remain a barrier for the majority of the global population. India is mainly dependent on imports for most REEs. Innovation and recycling efforts in most REEs are still in their early stages. For India, aspiring to Viksit Bharat by 2047, securing sustainable REE access is critical to national energy security and technological independence. This paper explores India's opportunities and challenges in the REE domain, high-lighting underutilized resources such as copper tailings, fly ash, and e-waste. We argue for circular economy pathways that can reduce environmental impacts and strengthen domestic supply. Hindustan Copper Limited (HCL), as India's sole vertically integrated copper producer, is uniquely positioned to pioneer co-recovery of REEs, advance research and development partnerships, and build a comprehensive supply chain. By embedding sustainability, ESG principles, and community trust in its strategy, HCL can evolve into a national champion in this domain.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20313v1,"Finite-Temperature Thermally-Assisted-Occupation Density Functional Theory, Ab Initio Molecular Dynamics, and Quantum Mechanics/Molecular Mechanics Methods",arXiv,2025-12-23,"Summary: Recently, thermally-assisted-occupation density functional theory (TAO-DFT) [J.-D. Chai, J. Chem. Phys. 136, 154104 (2012)] has been demonstrated to be an efficient and accurate electronic structure method for studying the ground-state properties of large multi-reference (MR) systems at absolute zero. To explore the thermal equilibrium properties of large MR systems at finite electronic temperatures, in the present work, we propose the finite-temperature (FT) extension of TAO-DFT, denoted as FT-TAO-DFT. Besides, to unlock the dynamical information of large MR systems at finite temperatures, FT-TAO-DFT is combined with ab initio molecular dynamics, leading to FT-TAO-AIMD. In addition, we also develop FT-TAO-DFT-based quantum mechanics/molecular mechanics (QM/MM), denoted as FT-TAO-QM/MM, to provide a cost-effective description of the thermal equilibrium properties of a QM subsystem with MR character embedded in an MM environment at finite temperatures. Moreover, the FT-TAO-DFT, FT-TAO-AIMD, and FT-TAO-QM/MM methods are employed to explore the radical nature and infrared (IR) spectra of n-acenes (n = 2--6), consisting of n linearly fused benzene rings, in vacuum and in an argon (Ar) matrix at finite temperatures. According to our calculations, for n-acenes at 1000 K or below, the electronic temperature effects on the radical nature and IR spectra are very minor, while the nuclear temperature effects on these properties are noticeable. For n-acene in an Ar matrx at absolute zero, the Ar matrix has minimal impact on the radical nature of n-acene, while the co-deposition procedure of n-acene and Ar atoms may affect the IR spectrum of n-acene.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20303v1,From the Two-Capacitor Paradox to Electromagnetic Side-Channel Mitigation in Digital Circuits,arXiv,2025-12-23,"Summary: The classical two-capacitor paradox of the lost energy is revisited from an electronic circuit security stand-point. The paradox has been solved previously by various researchers, and the energy lost during the charging of capacitors has been primarily attributed to the heat and radiation. We analytically prove this for various standard resistor-capacitor (RC) and resistor-inductor-capacitor (RLC) circuit models. From the perspective of electronic system security, electromagnetic (EM) side-channel analysis (SCA) has recently gained significant prominence with the growth of resource-constrained, internet connected devices. This article connects the energy lost due to capacitor charging to the EM SCA leakage in electronic devices, leading to the recovery of the secret encryption key embedded within the device. Finally, with an understanding of how lost energy relates to EM radiation, we propose adiabatic charging as a solution to minimize EM leakage, thereby paving the way towards low-overhead EM SCA resilience.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20290v1,A non-compact QCD axion,arXiv,2025-12-23,"Summary: We investigate the cosmology of an axion that is fundamentally non-compact. During inflation, fluctuations of the effectively massless field populate many QCD vacua, thereby evading conventional isocurvature constraints while generating domain walls -- without accompanying cosmic strings. A small non-QCD contribution to the axion potential is required to trigger the timely collapse of domain walls; as a consequence, a residual amount of CP violation in the strong sector must exist, potentially within reach of planned experiments. Non-compact axions can account for the entirety of the dark matter abundance, and the collapse of domain walls sources a stochastic gravitational-wave background at nanohertz frequencies. Such axion dynamics can be embedded in top-down constructions -- such as Weyl-invariant Einstein-Cartan gravity -- where the tilting of the axion potential arises automatically.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20257v1,LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation,arXiv,2025-12-23,"Summary: With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20255v1,BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation,arXiv,2025-12-23,"Summary: High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20217v1,LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation,arXiv,2025-12-23,"Summary: 3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20213v1,JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement,arXiv,2025-12-23,"Summary: Given the complexity of underwater environments and the variability of water as a medium, underwater images are inevitably subject to various types of degradation. The degradations present nonlinear coupling rather than simple superposition, which renders the effective processing of such coupled degradations particularly challenging. Most existing methods focus on designing specific branches, modules, or strategies for specific degradations, with little attention paid to the potential information embedded in their coupling. Consequently, they struggle to effectively capture and process the nonlinear interactions of multiple degradations from a bottom-up perspective. To address this issue, we propose JDPNet, a joint degradation processing network, that mines and unifies the potential information inherent in coupled degradations within a unified framework. Specifically, we introduce a joint feature-mining module, along with a probabilistic bootstrap distribution strategy, to facilitate effective mining and unified adjustment of coupled degradation features. Furthermore, to balance color, clarity, and contrast, we design a novel AquaBalanceLoss to guide the network in learning from multiple coupled degradation losses. Experiments on six publicly available underwater datasets, as well as two new datasets constructed in this study, show that JDPNet exhibits state-of-the-art performance while offering a better tradeoff between performance, parameter size, and computational cost.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20205v1,Active Brownian particles in power-law viscoelastic media,arXiv,2025-12-23,"Summary: Many active particles are embedded in environments that exhibit viscoelastic properties. An important class of such media lacks a single characteristic relaxation timescale when subjected to a time-dependent stress. Rather, the stress response spans a broad continuum of timescales, a behavior naturally described by a scale-free, fractal-like power-law relaxation modulus. Using a generalization of the fractional Langevin equation, we investigate an active Brownian particle embedded in a power-law viscoelastic environment with translational and rotational dynamics governed by independent fractional orders. We solve the model analytically, develop a numerical scheme to validate the theoretical predictions, and provide tools that can be used in further studies. A rich variety of diffusion regimes emerges, which modify the intermediate-time behavior of the mean squared displacement. Notably, we find that the competition between translational and rotational contributions favors a superdiffusive persistence over the standard ballistic motion, and over-stretches its characteristic timescale, fundamentally altering the standard relation between persistence and propulsion in active matter.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20201v1,Joint Design of Embedded Index Coding and Beamforming for MIMO-based Distributed Computing via Multi-Agent Reinforcement Learning,arXiv,2025-12-23,"Summary: In distributed computing systems, reducing the communication load during the data shuffling phase is a critical challenge, as excessive inter-node transmissions are a major performance bottleneck. One promising approach to alleviate this burden is Embedded Index Coding (EIC), which exploits cached data at user nodes to encode transmissions more efficiently. However, most prior work on EIC has focused on minimizing code length in wired, error-free environments-an objective often suboptimal for wireless multiple-input multiple-output (MIMO) systems, where channel conditions and spatial multiplexing gains must be considered. This paper investigates the joint design of EIC and transmit beamforming in MIMO systems to minimize total transmission time, an NP-hard problem. We first present a conventional optimization method that determines the optimal EIC via exhaustive search. To address its prohibitive complexity and adapt to dynamic wireless environments, we propose a novel, low-complexity multi-agent reinforcement learning (MARL) framework. The proposed framework enables decentralized agents to act on local observations while effectively managing the hybrid action space of discrete EIC selection and continuous beamforming design. Simulation results demonstrate that the proposed MARL-based approach achieves near-optimal performance with significantly reduced complexity, underscoring its effectiveness and practicality for real-world wireless systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20166v1,LoLA: Long Horizon Latent Action Learning for General Robot Manipulation,arXiv,2025-12-23,"Summary: The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable ""embodiment-anchored"" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20152v1,Origins and Nature of Macroeconomic Instability in Vector Autoregressions,arXiv,2025-12-23,"Summary: For a general class of dynamic and stochastic structural models, we show that (i) non-linearity in economic dynamics is a necessary and sufficient condition for time-varying parameters (TVPs) in the reduced-form VARMA process followed by observables, and (ii) all parameters' time-variation is driven by the same, typically few sources of stochasticity: the structural shocks. Our results call into question the common interpretation that TVPs are due to ""structural instabilities"". Motivated by our theoretical analysis, we model a set of macroeconomic and financial variables as a TVP-VAR with a factor-structure in TVPs. This reveals that most instabilities are driven by a few factors, which comove strongly with measures of macroeconomic uncertainty and the contribution of finance to real economic activity, commonly emphasized as important sources of non-linearities in macroeconomics. Furthermore, our model yields improved forecasts relative to the standard TVP-VAR where TVPs evolve as independent random walks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20141v1,Worth the Effort? An Examination on the Effect of Higher Diligence Calculations of the Sound Shell Model,arXiv,2025-12-23,"Summary: The gravitational wave spectrum arising from using the full velocity profile is well known to differ qualitatively from analytic fits to a broken power law. Former studies have shown that unlike the uncertainties arising from thermal field theory, more diligence in the hydrodynamics can sometimes have limited benefit. However, this was shown in the context of broken power law fits. We test the benefits of some recent calculations in modeling the spectrum, including new developments in adjustments of the low frequency tail to be consistent with causality, but we use the full velocity profile. We find the spectral shape information has a heightened sensitivity to the speed of sound which can be demonstrated analytically, however for our benchmark model this still results in a modest difference. The reason for a heightened sensitivity is because the velocity at the boundary is quite sensitive to the speed of sound, which in turn means a small change to the speed of sound can have a large change to the shape of the velocity profile. Furthermore, even modest changes in the product $ακ$ can make non-trivial changes to the shape around the peak. Finally, there are many points where adjusting the infrared behavior to be consistent with causality is affecting the spectrum near its peak. All this implies that the spectrum is sensitive to five thermal parameters rather than four which gives hope that an observation of a gravitational wave spectrum from a first order cosmological phase transition could eventually give even more information about the underlying microphysics responsible.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20128v1,milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion,arXiv,2025-12-23,"Summary: Millimeter-wave radar offers a privacy-preserving and lighting-invariant alternative to RGB sensors for Human Pose Estimation (HPE) task. However, the radar signals are often sparse due to specular reflection, making the extraction of robust features from radar signals highly challenging. To address this, we present milliMamba, a radar-based 2D human pose estimation framework that jointly models spatio-temporal dependencies across both the feature extraction and decoding stages. Specifically, given the high dimensionality of radar inputs, we adopt a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer sequences with linear complexity. A Spatio-Temporal-Cross Attention decoder then predicts joint coordinates across multiple frames. Together, this spatio-temporal modeling pipeline enables the model to leverage contextual cues from neighboring frames and joints to infer missing joints caused by specular reflections. To reinforce motion smoothness, we incorporate a velocity loss alongside the standard keypoint loss during training. Experiments on the TransHuPR and HuPR datasets demonstrate that our method achieves significant performance improvements, exceeding the baselines by 11.0 AP and 14.6 AP, respectively, while maintaining reasonable complexity. Code: https://github.com/NYCU-MAPL/milliMamba",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20117v1,DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation,arXiv,2025-12-23,"Summary: Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20113v1,Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection,arXiv,2025-12-23,"Summary: Deteriorating civil infrastructure requires automated inspection techniques overcoming limitations of visual assessment. While Ground Penetrating Radar and Infrared Thermography enable subsurface defect detection, single modal approaches face complementary constraints radar struggles with moisture and shallow defects, while thermography exhibits weather dependency and limited depth. This paper presents a multi modal attention network fusing radar temporal patterns with thermal spatial signatures for bridge deck delamination detection. Our architecture introduces temporal attention for radar processing, spatial attention for thermal features, and cross modal fusion with learnable embeddings discovering complementary defect patterns invisible to individual sensors. We incorporate uncertainty quantification through Monte Carlo dropout and learned variance estimation, decomposing uncertainty into epistemic and aleatoric components for safety critical decisions. Experiments on five bridge datasets reveal that on balanced to moderately imbalanced data, our approach substantially outperforms baselines in accuracy and AUC representing meaningful improvements over single modal and concatenation based fusion. Ablation studies demonstrate cross modal attention provides critical gains beyond within modality attention, while multi head mechanisms achieve improved calibration. Uncertainty quantification reduces calibration error, enabling selective prediction by rejecting uncertain cases. However, under extreme class imbalance, attention mechanisms show vulnerability to majority class collapse. These findings provide actionable guidance: attention based architecture performs well across typical scenarios, while extreme imbalance requires specialized techniques. Our system maintains deployment efficiency, enabling real time inspection with characterized capabilities and limitations.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20107v1,UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis,arXiv,2025-12-23,"Summary: Novel view synthesis (NVS) seeks to render photorealistic, 3D-consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion-based methods hallucinate plausible content yet incur heavy training- and inference-time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi-view image tokens and Plucker-ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed-forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end-to-end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state-of-the-art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20104v1,Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models,arXiv,2025-12-23,"Summary: Human Activity Recognition (HAR) plays a vital role in healthcare, surveillance, and innovative environments, where reliable action recognition supports timely decision-making and automation. Although deep learning-based HAR systems are widely adopted, the impact of Activation Functions (AFs) and Model Optimizers (MOs) on performance has not been sufficiently analyzed, particularly regarding how their combinations influence model behavior in practical scenarios. Most existing studies focus on architecture design, while the interaction between AF and MO choices remains relatively unexplored. In this work, we investigate the effect of three commonly used activation functions (ReLU, Sigmoid, and Tanh) combined with four optimization algorithms (SGD, Adam, RMSprop, and Adagrad) using two recurrent deep learning architectures, namely BiLSTM and ConvLSTM. Experiments are conducted on six medically relevant activity classes selected from the HMDB51 and UCF101 datasets, considering their suitability for healthcare-oriented HAR applications. Our experimental results show that ConvLSTM consistently outperforms BiLSTM across both datasets. ConvLSTM, combined with Adam or RMSprop, achieves an accuracy of up to 99.00%, demonstrating strong spatio-temporal learning capabilities and stable performance. While BiLSTM performs reasonably well on UCF101, with accuracy approaching 98.00%, its performance drops to approximately 60.00% on HMDB51, indicating limited robustness across datasets and weaker sensitivity to AF and MO variations. This study provides practical insights for optimizing HAR systems, particularly for real-world healthcare environments where fast and precise activity detection is critical.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20053v1,An Optimal Policy for Learning Controllable Dynamics by Exploration,arXiv,2025-12-23,"Summary: Controllable Markov chains describe the dynamics of sequential decision making tasks and are the central component in optimal control and reinforcement learning. In this work, we give the general form of an optimal policy for learning controllable dynamics in an unknown environment by exploring over a limited time horizon. This policy is simple to implement and efficient to compute, and allows an agent to ``learn by exploring"" as it maximizes its information gain in a greedy fashion by selecting controls from a constraint set that changes over time during exploration. We give a simple parameterization for the set of controls, and present an algorithm for finding an optimal policy. The reason for this policy is due to the existence of certain types of states that restrict control of the dynamics; such as transient states, absorbing states, and non-backtracking states. We show why the occurrence of these states makes a non-stationary policy essential for achieving optimal exploration. Six interesting examples of controllable dynamics are treated in detail. Policy optimality is demonstrated using counting arguments, comparing with suboptimal policies, and by making use of a sequential improvement property from dynamic programming.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20047v1,Markov Chain Model of Entanglement Setup in Noisy Dynamic LEO Satellite Networks,arXiv,2025-12-23,"Summary: Quantum entanglement routing in dynamic Low Earth Orbit (LEO) satellite networks is important for achieving scalable and high-fidelity quantum communication. However, the dynamic characteristics of satellite network topology, limited quantum resources, and strict coherence time constraints pose significant challenges to reliable entanglement routing. An entanglement distribution analysis model for this unique environment is critical and helpful for entanglement routing research. We address the fundamental challenge of establishing and maintaining quantum entanglement links between satellites operating in free space, where links are subject to both transmission losses and quantum memory decoherence. This paper presents a comprehensive Markov chain model with a state space defined by link storage age and physical distance for analyzing entanglement distribution in noisy dynamic LEO satellite quantum networks. We construct transition matrices that capture system dynamics under varying request arrival rates, and derive analytical expressions for key performance metrics, including request satisfaction rate, average waiting time, link utilization efficiency, and average consumed link fidelity. Our analysis reveals that the critical trade-offs of higher request rates lead to faster link consumption with higher fidelity but potentially lower satisfaction rates, while lower request rates allow longer storage times at the cost of lower fidelity of increased decoherence effect. Moreover, this paper proves it is reasonable to leave out polarization rotation when the transmission distance is very short (40-50 km). In summary, this work provides theoretical foundations for designing and optimizing quantum entanglement distribution strategies in satellite networks, with applications to global-scale quantum communications.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20042v1,Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva,arXiv,2025-12-23,"Summary: Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20034v1,VSA:Visual-Structural Alignment for UI-to-Code,arXiv,2025-12-23,"Summary: The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20029v1,$\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning,arXiv,2025-12-23,"Summary: Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20028v1,DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics,arXiv,2025-12-23,"Summary: Accurate and interpretable forecasting of multivariate time series is crucial for understanding the complex dynamics of cryptocurrency markets in digital asset systems. Advanced deep learning methodologies, particularly Transformer-based and MLP-based architectures, have achieved competitive predictive performance in cryptocurrency forecasting tasks. However, cryptocurrency data is inherently composed of long-term socio-economic trends and local high-frequency speculative oscillations. Existing deep learning-based 'black-box' models fail to effectively decouple these composite dynamics or provide the interpretability needed for trustworthy financial decision-making. To overcome these limitations, we propose DecoKAN, an interpretable forecasting framework that integrates multi-level Discrete Wavelet Transform (DWT) for decoupling and hierarchical signal decomposition with Kolmogorov-Arnold Network (KAN) mixers for transparent and interpretable nonlinear modeling. The DWT component decomposes complex cryptocurrency time series into distinct frequency components, enabling frequency-specific analysis, while KAN mixers provide intrinsically interpretable spline-based mappings within each decomposed subseries. Furthermore, interpretability is enhanced through a symbolic analysis pipeline involving sparsification, pruning, and symbolization, which produces concise analytical expressions offering symbolic representations of the learned patterns. Extensive experiments demonstrate that DecoKAN achieves the lowest average Mean Squared Error on all tested real-world cryptocurrency datasets (BTC, ETH, XMR), consistently outperforming a comprehensive suite of competitive state-of-the-art baselines. These results validate DecoKAN's potential to bridge the gap between predictive accuracy and model transparency, advancing trustworthy decision support within complex cryptocurrency markets.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20026v1,MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis,arXiv,2025-12-23,"Summary: Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20025v1,A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments,arXiv,2025-12-23,"Summary: Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20014v1,Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting,arXiv,2025-12-23,"Summary: While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as ""bring my cup"", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20013v1,SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images,arXiv,2025-12-23,"Summary: Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20006v1,Orthogonal Activation with Implicit Group-Aware Bias Learning for Class Imbalance,arXiv,2025-12-23,"Summary: Class imbalance is a common challenge in machine learning and data mining, often leading to suboptimal performance in classifiers. While deep learning excels in feature extraction, its performance still deteriorates under imbalanced data. In this work, we propose a novel activation function, named OGAB, designed to alleviate class imbalance in deep learning classifiers. OGAB incorporates orthogonality and group-aware bias learning to enhance feature distinguishability in imbalanced scenarios without explicitly requiring label information. Our key insight is that activation functions can be used to introduce strong inductive biases that can address complex data challenges beyond traditional non-linearity. Our work demonstrates that orthogonal transformations can preserve information about minority classes by maintaining feature independence, thereby preventing the dominance of majority classes in the embedding space. Further, the proposed group-aware bias mechanism automatically identifies data clusters and adjusts embeddings to enhance class separability without the need for explicit supervision. Unlike existing approaches that address class imbalance through preprocessing data modifications or post-processing corrections, our proposed approach tackles class imbalance during the training phase at the embedding learning level, enabling direct integration with the learning process. We demonstrate the effectiveness of our solution on both real-world and synthetic imbalanced datasets, showing consistent performance improvements over both traditional and learnable activation functions.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20004v1,IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense,arXiv,2025-12-23,"Summary: Since the Internet of Things (IoT) is widely adopted using Android applications, detecting malicious Android apps is essential. In recent years, Android graph-based deep learning research has proposed many approaches to extract relationships from applications as graphs to generate graph embeddings. First, we demonstrate the effectiveness of graph-based classification using a Graph Neural Network (GNN)-based classifier to generate API graph embeddings. The graph embeddings are combined with Permission and Intent features to train multiple machine learning and deep learning models for Android malware detection. The proposed classification approach achieves an accuracy of 98.33 percent on the CICMaldroid dataset and 98.68 percent on the Drebin dataset. However, graph-based deep learning models are vulnerable, as attackers can add fake relationships to evade detection by the classifier. Second, we propose a Generative Adversarial Network (GAN)-based attack algorithm named VGAE-MalGAN targeting graph-based GNN Android malware classifiers. The VGAE-MalGAN generator produces adversarial malware API graphs, while the VGAE-MalGAN substitute detector attempts to mimic the target detector. Experimental results show that VGAE-MalGAN can significantly reduce the detection rate of GNN-based malware classifiers. Although the model initially fails to detect adversarial malware, retraining with generated adversarial samples improves robustness and helps mitigate adversarial attacks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20002v1,LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models,arXiv,2025-12-23,"Summary: Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length temporal windows, which include substantial high-frequency noise and obscure long-term trends. Moreover, auxiliary variables containing rich domain-specific information are often underutilized, especially in few-shot settings. To address these challenges, we propose LoFT-LLM, a frequency-aware forecasting pipeline that integrates low-frequency learning with semantic calibration via a large language model (LLM). Firstly, a Patch Low-Frequency forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. Secondly, a residual learner then models high-frequency variations. Finally, a fine-tuned LLM refines the predictions by incorporating auxiliary context and domain knowledge through structured natural language prompts. Extensive experiments on financial and energy datasets demonstrate that LoFT-LLM significantly outperforms strong baselines under both full-data and few-shot regimes, delivering superior accuracy, robustness, and interpretability.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19995v1,Schoenfeld's Anatomy of Mathematical Reasoning by Language Models,arXiv,2025-12-23,"Summary: Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19992v1,S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test,arXiv,2025-12-23,"Summary: The integration of embodied agents into human environments demands embodied social intelligence: reasoning over both social norms and physical constraints. However, existing evaluations fail to address this integration, as they are limited to either disembodied social reasoning (e.g., in text) or socially-agnostic physical tasks. Both approaches fail to assess an agent's ability to integrate and trade off both physical and social constraints within a realistic, embodied context. To address this challenge, we introduce Spatially Situated Social Intelligence Test (S$^{3}$IT), a benchmark specifically designed to evaluate embodied social intelligence. It is centered on a novel and challenging seat-ordering task, requiring an agent to arrange seating in a 3D environment for a group of large language model-driven (LLM-driven) NPCs with diverse identities, preferences, and intricate interpersonal relationships. Our procedurally extensible framework generates a vast and diverse scenario space with controllable difficulty, compelling the agent to acquire preferences through active dialogue, perceive the environment via autonomous exploration, and perform multi-objective optimization within a complex constraint network. We evaluate state-of-the-art LLMs on S$^{3}$IT and found that they still struggle with this problem, showing an obvious gap compared with the human baseline. Results imply that LLMs have deficiencies in spatial intelligence, yet simultaneously demonstrate their ability to achieve near human-level competence in resolving conflicts that possess explicit textual cues.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19990v1,A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping,arXiv,2025-12-23,"Summary: Cross-resolution land cover mapping aims to produce high-resolution semantic predictions from coarse or low-resolution supervision, yet the severe resolution mismatch makes effective learning highly challenging. Existing weakly supervised approaches often struggle to align fine-grained spatial structures with coarse labels, leading to noisy supervision and degraded mapping accuracy. To tackle this problem, we propose DDTM, a dual-branch weakly supervised framework that explicitly decouples local semantic refinement from global contextual reasoning. Specifically, DDTM introduces a diffusion-based branch to progressively refine fine-scale local semantics under coarse supervision, while a transformer-based branch enforces long-range contextual consistency across large spatial extents. In addition, we design a pseudo-label confidence evaluation module to mitigate noise induced by cross-resolution inconsistencies and to selectively exploit reliable supervisory signals. Extensive experiments demonstrate that DDTM establishes a new state-of-the-art on the Chesapeake Bay benchmark, achieving 66.52\% mIoU and substantially outperforming prior weakly supervised methods. The code is available at https://github.com/gpgpgp123/DDTM.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19983v1,IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation,arXiv,2025-12-23,"Summary: Multimodal recommender systems (MRSs) are critical for various online platforms, offering users more accurate personalized recommendations by incorporating multimodal information of items. Structure-based MRSs have achieved state-of-the-art performance by constructing semantic item graphs, which explicitly model relationships between items based on modality feature similarity. However, such semantic item graphs are often noisy due to 1) inherent noise in multimodal information and 2) misalignment between item semantics and user-item co-occurrence relationships, which introduces false links and leads to suboptimal recommendations. To address this challenge, we propose Item Graph Diffusion for Multimodal Recommendation (IGDMRec), a novel method that leverages a diffusion model with classifier-free guidance to denoise the semantic item graph by integrating user behavioral information. Specifically, IGDMRec introduces a Behavior-conditioned Graph Diffusion (BGD) module, incorporating interaction data as conditioning information to guide the denoising of the semantic item graph. Additionally, a Conditional Denoising Network (CD-Net) is designed to implement the denoising process with manageable complexity. Finally, we propose a contrastive representation augmentation scheme that leverages both the denoised item graph and the original item graph to enhance item representations. \LL{Extensive experiments on four real-world datasets demonstrate the superiority of IGDMRec over competitive baselines, with robustness analysis validating its denoising capability and ablation studies verifying the effectiveness of its key components.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19980v1,"Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?",arXiv,2025-12-23,"Summary: Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19972v1,"Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions",arXiv,2025-12-23,"Summary: Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19967v1,Lyman Continuum escaping from in-situ formed stars in a tidal bridge at z = 3,arXiv,2025-12-23,"Summary: In order to account for reionization of the early Universe, galaxies at that time must have had significantly higher escape fractions of Lyman Continuum (LyC) than observed in the present Universe. Any explanation invoked to explain LyC escape must agree with this dramatic cosmic evolution. Galaxy mergers are often suggested as such a regulating mechanism. They occur an order or magnitude more frequently at $z \ge 3$ than in the local Universe, and they can trigger LyC escape either by inducing strong nuclear starbursts, or by tidally displacing the neutral ISM from the bulk of the stars. In the local Universe, LyC escape has been found to correlate with a range of physical and observable properties closely associated with strong star formation. For this reason, interest in mergers as drivers of LyC escape have been mainly focused on their capacity to induce strong star formation. However, at $z \ge 2$, these correlations are weaker, and we observe a much more diverse Lyman Continuum Emitter (LCE) population. This suggests that processes external to the LCE galaxies are more important for facilitating the escape at higher redshifts, which makes tidal displacement an interesting explanatory model; however, this has only been conclusively observed once before. In this letter, we present archival JWST/NIRSpec IFU and HST UVIS and IR imaging observations of the z = 3 Lyman-Continuum emitter LACES104037. We find that its Lyman-Continuum escape originates in a tidal bridge in the direction towards an interacting companion galaxy first identified in this work. LyC escape from tidal stripping or in-situ formed stars in tidal features could help explain both the higher cosmic LyC escape fraction and the greater diversity of LCE galaxy properties at higher redshifts.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19957v1,Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification,arXiv,2025-12-23,"Summary: This paper presents an approach developed to address the PlantClef 2025 challenge, which consists of a fine-grained multi-label species identification, over high-resolution images. Our solution focused on employing class prototypes obtained from the training dataset as a proxy guidance for training a segmentation Vision Transformer (ViT) on the test set images. To obtain these representations, the proposed method extracts features from training dataset images and create clusters, by applying K-Means, with $K$ equals to the number of classes in the dataset. The segmentation model is a customized narrow ViT, built by replacing the patch embedding layer with a frozen DinoV2, pre-trained on the training dataset for individual species classification. This model is trained to reconstruct the class prototypes of the training dataset from the test dataset images. We then use this model to obtain attention scores that enable to identify and localize areas of interest and consequently guide the classification process. The proposed approach enabled a domain-adaptation from multi-class identification with individual species, into multi-label classification from high-resolution vegetation plots. Our method achieved fifth place in the PlantCLEF 2025 challenge on the private leaderboard, with an F1 score of 0.33331. Besides that, in absolute terms our method scored 0.03 lower than the top-performing submission, suggesting that it may achieved competitive performance in the benchmark task. Our code is available at \href{https://github.com/ADAM-UEFS/PlantCLEF2025}{https://github.com/ADAM-UEFS/PlantCLEF2025}.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19945v1,Energy-Efficient Multi-LLM Reasoning for Binary-Free Zero-Day Detection in IoT Firmware,arXiv,2025-12-23,"Summary: Securing Internet of Things (IoT) firmware remains difficult due to proprietary binaries, stripped symbols, heterogeneous architectures, and limited access to executable code. Existing analysis methods, such as static analysis, symbolic execution, and fuzzing, depend on binary visibility and functional emulation, making them unreliable when firmware is encrypted or inaccessible. To address this limitation, we propose a binary-free, architecture-agnostic solution that estimates the likelihood of conceptual zero-day vulnerabilities using only high-level descriptors. The approach integrates a tri-LLM reasoning architecture combining a LLaMA-based configuration interpreter, a DeepSeek-based structural abstraction analyzer, and a GPT-4o semantic fusion model. The solution also incorporates LLM computational signatures, including latency patterns, uncertainty markers, and reasoning depth indicators, as well as an energy-aware symbolic load model, to enhance interpretability and operational feasibility. In addition, we formally derive the mathematical foundations of the reasoning pipeline, establishing monotonicity, divergence, and energy-risk coupling properties that theoretically justify the model's behavior. Simulation-based evaluation reveals that high exposure conditions increase the predicted zero-day likelihood by 20 to 35 percent across models, with GPT-4o demonstrating the strongest cross-layer correlations and the highest sensitivity. Energy and divergence metrics significantly predict elevated risk (p < 0.01), reinforcing the effectiveness of the proposed reasoning framework.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19943v1,SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction,arXiv,2025-12-23,"Summary: While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19936v1,GIMLET: Generalizable and Interpretable Model Learning through Embedded Thermodynamics,arXiv,2025-12-22,"Summary: We develop a data-driven framework for discovering constitutive relations in models of fluid flow and scalar transport. Our approach infers unknown closure terms in the governing equations (gray-box discovery) under the assumption that the temporal derivative, convective transport, and pressure-gradient contributions are known. The formulation is rooted in a variational principle from nonequilibrium thermodynamics, where the dynamics is defined by a free-energy functional and a dissipation functional. The unknown constitutive terms arise as functional derivatives of these functionals with respect to the state variables. To enable a flexible and structured model discovery, the free-energy and dissipation functionals are parameterized using neural networks, while their functional derivatives are obtained via automatic differentiation. This construction enforces thermodynamic consistency by design, ensuring monotonic decay of the total free energy and non-negative entropy production. The resulting method, termed GIMLET (Generalizable and Interpretable Model Learning through Embedded Thermodynamics), avoids reliance on a predefined library of candidate functions, unlike sparse regression or symbolic identification approaches. The learned models are generalizable in that functionals identified from one dataset can be transferred to distinct datasets governed by the same underlying equations. Moreover, the inferred free-energy and dissipation functions provide direct physical interpretability of the learned dynamics. The framework is demonstrated on several benchmark systems, including the viscous Burgers equation, the Kuramoto--Sivashinsky equation, and the incompressible Navier--Stokes equations for both Newtonian and non-Newtonian fluids.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19935v1,Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress,arXiv,2025-12-22,"Summary: Machine learning models used in financial decision systems operate in nonstationary economic environments, yet adversarial robustness is typically evaluated under static assumptions. This work introduces Conditional Adversarial Fragility, a regime dependent phenomenon in which adversarial vulnerability is systematically amplified during periods of macroeconomic stress. We propose a regime aware evaluation framework for time indexed tabular financial classification tasks that conditions robustness assessment on external indicators of economic stress. Using volatility based regime segmentation as a proxy for macroeconomic conditions, we evaluate model behavior across calm and stress periods while holding model architecture, attack methodology, and evaluation protocols constant. Baseline predictive performance remains comparable across regimes, indicating that economic stress alone does not induce inherent performance degradation. Under adversarial perturbations, however, models operating during stress regimes exhibit substantially greater degradation across predictive accuracy, operational decision thresholds, and risk sensitive outcomes. We further demonstrate that this amplification propagates to increased false negative rates, elevating the risk of missed high risk cases during adverse conditions. To complement numerical robustness metrics, we introduce an interpretive governance layer based on semantic auditing of model explanations using large language models. Together, these results demonstrate that adversarial robustness in financial machine learning is a regime dependent property and motivate stress aware approaches to model risk assessment in high stakes financial deployments.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19934v1,Vehicle-centric Perception via Multimodal Structured Pre-training,arXiv,2025-12-22,"Summary: Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19933v1,PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation,arXiv,2025-12-22,"Summary: Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19926v1,Developers' Experience with Generative AI -- First Insights from an Empirical Mixed-Methods Field Study,arXiv,2025-12-22,"Summary: With the rise of AI-powered coding assistants, firms and programmers are exploring how to optimize their interaction with them. Research has so far mainly focused on evaluating output quality and productivity gains, leaving aside the developers' experience during the interaction. In this study, we take a multimodal, developer-centered approach to gain insights into how professional developers experience the interaction with Generative AI (GenAI) in their natural work environment in a firm. The aim of this paper is (1) to demonstrate a feasible mixed-method study design with controlled and uncontrolled study periods within a firm setting, (2) to give first insights from complementary behavioral and subjective experience data on developers' interaction with GitHub Copilot and (3) to compare the impact of interaction types (no Copilot use, in-code suggestions, chat prompts or both in-code suggestions and chat prompts) on efficiency, accuracy and perceived workload whilst working on different task categories. Results of the controlled sessions in this study indicate that moderate use of either in-code suggestions or chat prompts improves efficiency (task duration) and reduces perceived workload compared to not using Copilot, while excessive or combined use lessens these benefits. Accuracy (task completion) profits from chat interaction. In general, subjective perception of workload aligns with objective behavioral data in this study. During the uncontrolled period of the study, both higher cognitive load and productivity were perceived when interacting with AI during everyday working tasks. This study motivates the use of comparable study designs, in e.g. workshop or hackathon settings, to evaluate GenAI tools holistically and realistically with a focus on the developers' experience.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19920v1,Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning,arXiv,2025-12-22,"Summary: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19918v1,Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs,arXiv,2025-12-22,"Summary: User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19904v1,Three-dimensional atom-by-atom mapping of nanoscale precipitates in single Te inclusions in Cd0.9Zn0.1Te crystal,arXiv,2025-12-22,"Summary: The complexity and richness of phenomena governing alloy crystal growth can be unraveled by examining the three-dimensional atomic-level distribution of elements and impurities incorporated during growth. These species act as atomic fingerprints, revealing the thermodynamic constraints that shape material structure and composition. Herein, we combine transmission electron microscopy and atom probe of tellurium (Te) inclusions within cadmium zinc telluride (CZT) single crystals. The correlative analysis uncovers nanoscale precipitates embedded within Te inclusions, consisting of CZT nanocrystals with a Zn content of 1.5 at.%. Surrounding these precipitates, an around 10 nm-thick shell is observed, enriched with copper and indium impurities. In addition, traces of sodium and sulfur are detected within the nanocrystals. These findings provide direct evidence of the complex segregation and precipitation processes occurring during CZT crystal growth, reflecting the interplay of thermodynamic driving forces and kinetic constraints that govern solute redistribution. The resulting insights contribute to a deeper understanding of impurity behavior and phase separation mechanisms in CZT alloys. This work establishes a framework for modeling and optimization of growth strategies of higher-quality CZT crystals for next-generation infrared and radiation detection technologies.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19895v1,A ruled narrow elastic strip model with corrected energy,arXiv,2025-12-22,"Summary: We present a new one-dimensional model for elastic strips based on a nondevelopable ruled surface. An auxiliary field regularizes the Sadowsky narrow-strip model to allow nonzero twist with vanishing curvature. The energy exhibits the scalings derived by Freddi and co-workers, and for a certain choice of parameter, convexifies the Sadowsky energy without patching. We present the kinematics and energetics of the model, and employ a variational approach featuring a rotation tensor to derive equilibrium equations. We perform a regular perturbation expansion to study the model behavior close to inflection points. When the energy is convex, curvature and moment are continuous at inflection points, while the auxiliary function suffers a jump, leading to a discontinuity in the ruled embedding for any finite width.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19871v1,HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction,arXiv,2025-12-22,"Summary: 3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19864v1,HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data,arXiv,2025-12-22,"Summary: Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19850v1,RANSAC Scoring Functions: Analysis and Reality Check,arXiv,2025-12-22,"Summary: We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.
  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.
  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.
  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19844v1,Bosonization solution of the Kondo lattice in a Luttinger liquid,arXiv,2025-12-22,"Summary: We address the physics of a regular arrangement of independent magnetic impurities embedded in a band of interacting electrons. We focus on the one-dimensional case that can be studied using bosonization and in which the electron bulk is described by a Luttinger liquid. The impurity spins interact with the electrons via magnetic exchange that introduces the possibility of Kondo and RKKY physics. We find that for two special values of the interactions, the model can be refermionized as a non-interacting electron band hybridized with a regular array of resonant levels. These solvable limits provide access to impurity correlators that correspond to either extended algebraic order or local screening. A physical picture emerges of how the interelectron interactions can stabilize either Kondo or RKKY physics depending on the sign of the interaction.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19841v1,A Multi-Agent Retrieval-Augmented Framework for Work-in-Progress Predictio,arXiv,2025-12-22,"Summary: Work-in-Progress (WiP) prediction is critical for predictive process monitoring, enabling accurate anticipation of workload fluctuations and optimized operational planning. This paper proposes a retrieval-augmented, multi-agent framework that combines retrieval-augmented generation (RAG) and collaborative multi-agent reasoning for WiP prediction. The narrative generation component transforms structured event logs into semantically rich natural language stories, which are embedded into a semantic vector-based process memory to facilitate dynamic retrieval of historical context during inference. The framework includes predictor agents that independently leverage retrieved historical contexts and a decision-making assistant agent that extracts high-level descriptive signals from recent events. A fusion agent then synthesizes predictions using ReAct-style reasoning over agent outputs and retrieved narratives. We evaluate our framework on two real-world benchmark datasets. Results show that the proposed retrieval-augmented multi-agent approach achieves competitive prediction accuracy, obtaining a Mean Absolute Percentage Error (MAPE) of 1.50\% on one dataset, and surpassing Temporal Convolutional Networks (TCN), Long Short-Term Memory (LSTM), and persistence baselines. The results highlight improved robustness, demonstrating the effectiveness of integrating retrieval mechanisms and multi-agent reasoning in WiP prediction.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19833v1,Universal Seesaw Leptogenesis,arXiv,2025-12-22,"Summary: We study the implications for leptogenesis in a class of left-right symmetric model, where all fermion masses are induced through the Universal Seesaw mechanism. Unlike conventional analyses, we do not use the decays of the neutrino embedded in the right-chiral lepton doublet, but rather those of the gauge-singlet mediators required for neutrino mass generation in the canonical Type-I seesaw. Due to the generalized parity symmetry that doubles the fermionic degrees of freedom in this model, we can generate the required $CP$ violation in the heavy fermion decays with only a single generation of mediators. One of the distinct features of our scenario is that the bounds from thermalization or washout via gauge interactions typically encountered in the canonical left-right symmetric models do not apply here. Moreover, the heavy mediators can decay to both the left and the right-chiral neutrinos, leading to a cancellation in the resulting baryon asymmetry for decays above the left-right symmetry breaking scale. We discuss ways to avoid this cancellation and show that low scale left-right symmetry breaking above the current collider limits is viable. The right chiral neutrinos also obtain their masses from the seesaw mechanism, and the lightest one turns out to have a sub-eV scale mass. We find that its abundance is consistent with standard cosmology, and it acts as potentially observable dark radiation.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19826v1,Designing low-loss cavities across the band-gap of photonic crystal slabs,arXiv,2025-12-22,"Summary: Photonic crystal cavities (PCCs) are defects in host photonic crystals (PCs) which create bound states in the PC band gap. These bound states are resonant states of the electromagnetic field with a complex resonance frequency and can have very small mode volumes. PCCs are attractive for a variety of applications, from cavity quantum electrodynamics to biosensing. A PC slab geometry is advantageous given its superior manufacturability compared to three-dimensional crystals, and the accessibility of the surface allows sensing and coupling. However, the emission into the half spaces above and below the slab limits the bound state lifetime. Controlling this emission is thus crucial for applications, most of which benefiting from a long lifetime. A range of methods to find defect geometries suppressing the emission to increase the lifetime have been demonstrated in the past. However, they do not cater for a designed resonant frequency covering a wide addressable range, as needed for multiplexed devices. Here, we demonstrate a design method controlling both resonance frequency and emission, by minimising a cost function including both losses and target frequency. We show applications on PCCs in GaAs PC slabs immersed in water, relevant for biosensing. The reduced refractive index contrast in these structures compared to previously studied PCCs embedded in vacuum renders the emission suppression more challenging. We optimize the quality factor of a standard L3 cavity from 1000 to 10^4-10^5, with an addressable resonance frequency range covering 12% relative bandwidth, spanning more than half of the band gap. We furthermore report optimised structures of H1 cavities, and provide the optimisation code for widespread use.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19821v1,How to choose my stochastic volatility parameters? A review,arXiv,2025-12-22,"Summary: Based on the existing literature, this article presents the different ways of choosing the parameters of stochastic volatility models in general, in the context of pricing financial derivative contracts. This includes the use of stochastic volatility inside stochastic local volatility models.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19810v1,Predicting Student Actions in a Procedural Training Environment,arXiv,2025-12-22,"Summary: Data mining is known to have a potential for predicting user performance. However, there are few studies that explore its potential for predicting student behavior in a procedural training environment. This paper presents a collective student model, which is built from past student logs. These logs are firstly grouped into clusters. Then an extended automaton is created for each cluster based on the sequences of events found in the cluster logs. The main objective of this model is to predict the actions of new students for improving the tutoring feedback provided by an intelligent tutoring system. The proposed model has been validated using student logs collected in a 3D virtual laboratory for teaching biotechnology. As a result of this validation, we concluded that the model can provide reasonably good predictions and can support tutoring feedback that is better adapted to each student type.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19799v1,PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research,arXiv,2025-12-22,"Summary: Advances in LLMs have produced agents with knowledge and operational capabilities comparable to human scientists, suggesting potential to assist, accelerate, and automate research. However, existing studies mainly evaluate such systems on well-defined benchmarks or general tasks like literature retrieval, limiting their end-to-end problem-solving ability in open scientific scenarios. This is particularly true in physics, which is abstract, mathematically intensive, and requires integrating analytical reasoning with code-based computation. To address this, we propose PhysMaster, an LLM-based agent functioning as an autonomous theoretical and computational physicist. PhysMaster couples absract reasoning with numerical computation and leverages LANDAU, the Layered Academic Data Universe, which preserves retrieved literature, curated prior knowledge, and validated methodological traces, enhancing decision reliability and stability. It also employs an adaptive exploration strategy balancing efficiency and open-ended exploration, enabling robust performance in ultra-long-horizon tasks. We evaluate PhysMaster on problems from high-energy theory, condensed matter theory to astrophysics, including: (i) acceleration, compressing labor-intensive research from months to hours; (ii) automation, autonomously executing hypothesis-driven loops ; and (iii) autonomous discovery, independently exploring open problems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19789v1,Residual Symmetries and Scalar Multiplet Vacuum Alignment in Non-Abelian Flavour Models,arXiv,2025-12-22,"Summary: We demonstrate that, upon minimizing a renormalizable, single-scalar potential invariant under a non-Abelian symmetry, special orientations in the associated vacuum alignment of the scalar multiplet correspond to the preservation of a discrete residual flavour symmetry in the broken phase of the theory. Conversely, we show that these special scalar alignments are perturbed when additional Lagrangian operators (e.g. renormalizable, multi-flavon operators and/or effective, higher-dimensional operators) are present that break said residual symmetry, leading to a vacuum reorientation and phenomenological consequences. We therefore construct a one-to-one correspondence principle between broken residual symmetries and vacuum alignment corrections, providing a mechanism to identify (and correct) a subtle but persistent form of phenomenologically relevant fine-tuning embedded in -- but often ignored by -- most successful non-Abelian flavour models. We first establish this correspondence in a set of toy models based on the S4 permutation symmetry, and then apply the lessons learned to the more realistic A4 Altarelli-Feruglio and $Δ(27)$ Universal Texture Zero models.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19687v1,Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning,arXiv,2025-12-22,"Summary: We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19686v1,Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models,arXiv,2025-12-22,"Summary: Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19683v1,From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs,arXiv,2025-12-22,"Summary: While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19675v1,Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918),arXiv,2025-12-22,"Summary: We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19673v1,Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies,arXiv,2025-12-22,"Summary: Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19671v1,CORE: Compensable Reward as a Catalyst for Improving Offline RL in Wireless Networks,arXiv,2025-12-22,"Summary: Real-world wireless data are expensive to collect and often lack sufficient expert demonstrations, causing existing offline RL methods to overfit suboptimal behaviors and exhibit unstable performance. To address this issue, we propose CORE, an offline RL framework specifically designed for wireless environments. CORE identifies latent expert trajectories from noisy datasets via behavior embedding clustering, and trains a conditional variational autoencoder with a contrastive objective to separate expert and non-expert behaviors in latent space. Based on the learned representations, CORE constructs compensable rewards that reflect expert-likelihood, effectively guiding policy learning under limited or imperfect supervision. More broadly, this work represents one of the early systematic explorations of offline RL in wireless networking, where prior adoption remains limited. Beyond introducing offline RL techniques to this domain, we further examine intrinsic wireless data characteristics and develop a domain-aligned algorithm that explicitly accounts for their structural properties. While offline RL has not yet been fully established as a standard methodology in the wireless community, our study aims to provide foundational insights and empirical evidence to support its broader acceptance.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19664v1,Quantum upper triangular matrix algebras,arXiv,2025-12-22,"Summary: Following the ideas in~\cite{yM88} and some inspiration from~\cite{KO24}, we construct a bialgebra $T_q(n)$ and a pointed Hopf algebra $UT_q(n)$ which quantize the coordinate rings of the algebra of upper triangular matrices and of the group of invertible upper triangular matrices of size $n\geq 2$, respectively, where $q$ is a nonzero parameter. The resulting structure on $UT_q(n)$ is neither commutative nor cocommutative, so we obtain a quantum group. The motivation comes from the idea of quantizing the incidence algebra of a finite poset, as the latter can be embedded as a subalgebra of the algebra of upper triangular matrices. After defining the bialgebra $T_q(n)$ and the Hopf algebra $UT_q(n)$, we study and compare their Lie algebras of derivations, their automorphism groups and their low degree Hochschild cohomology, in case $n=2$.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19663v1,Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis,arXiv,2025-12-22,"Summary: Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19651v1,Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting,arXiv,2025-12-22,"Summary: Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19650v1,A Composite Theory of Higgs and Flavour,arXiv,2025-12-22,"Summary: We introduce a composite Higgs model in which a flavour deconstructed gauge group is embedded in the strong sector. The pattern of global symmetry breaking yields, as pseudo-Nambu-Goldstone (pNGB) bosons, both a Standard Model (SM)-like Higgs and the link field whose vacuum expectation value breaks the flavour non-universal gauge group down to the SM. Inevitably, a third pNGB appears, transforming as a second Higgs doublet coupled only to the light generations. This heavy Higgs naturally mediates suppressed light Yukawa couplings, providing a solution to the flavour puzzle. At the same time, new physics contributions to flavour violating observables are CKM- and chirally suppressed, while electric dipole moment bounds are evaded through an automatic mass alignment in the light fermion sector. The result is a natural framework for addressing the origin of both Higgs and flavour hierarchies, with reduced Higgs mass tuning and minimised impact in flavour observables, that is best tested by high-$p_T$ and precision electroweak measurements.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19609v1,MapTrace: Scalable Data Generation for Route Tracing on Maps,arXiv,2025-12-22,"Summary: While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19605v1,KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning,arXiv,2025-12-22,"Summary: Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19602v1,No Data? No Problem: Robust Vision-Tabular Learning with Missing Values,arXiv,2025-12-22,"Summary: Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19585v1,Increasing the Thinking Budget is Not All You Need,arXiv,2025-12-22,"Summary: Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.19577v1,Deep Learning for Primordial $B$-mode Extraction,arXiv,2025-12-22,"Summary: The search for primordial gravitational waves is a central goal of cosmic microwave background (CMB) surveys. Isolating the characteristic $B$-mode polarization signal sourced by primordial gravitational waves is challenging for several reasons: the amplitude of the signal is inherently small; astrophysical foregrounds produce $B$-mode polarization contaminating the signal; and secondary $B$-mode polarization fluctuations are produced via the conversion of $E$ modes. Current and future low-noise, multi-frequency observations enable sufficient precision to address the first two of these challenges such that secondary $B$ modes will become the bottleneck for improved constraints on the amplitude of primordial gravitational waves. The dominant source of secondary $B$-mode polarization is gravitational lensing by large scale structure. Various strategies have been developed to estimate the lensing deflection and to reverse its effects the CMB, thus reducing confusion from lensing $B$ modes in the search for primordial gravitational waves. However, a few complications remain. First, there may be additional sources of secondary $B$-mode polarization, for example from patchy reionization or from cosmic polarization rotation. Second, the statistics of delensed CMB maps can become complicated and non-Gaussian, especially when advanced lensing reconstruction techniques are applied. We previously demonstrated how a deep learning network, ResUNet-CMB, can provide nearly optimal simultaneous estimates of multiple sources of secondary $B$-mode polarization. In this paper, we show how deep learning can be applied to estimate and remove multiple sources of secondary $B$-mode polarization, and we further show how this technique can be used in a likelihood analysis to produce nearly optimal, unbiased estimates of the amplitude of primordial gravitational waves.",2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2512.20949,Neural Probe-Based Hallucination Detection for Large Language Models,arXiv,2025-12-24,"Token-level hallucination detection via lightweight MLP probes over hidden states, aiming for real-time detection without retrieval or multi-sample verification.",2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2512.13979,ReflCtrl: Controlling LLM Reflection via Representation Engineering,arXiv,2025-12-16,"Identifies and steers a latent “reflection direction” to control self-reflection frequency, reducing reasoning tokens while maintaining accuracy.",2025-12-25
reasoning_and_planning,openai,https://alignment.anthropic.com/2025/inverse-scaling/,Inverse Scaling in Test-Time Compute,Anthropic Alignment Science Blog,2025-07-22,"Shows concrete task families where longer reasoning degrades accuracy (and can amplify risky behaviors), mapping failure modes of test-time compute scaling.",2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2507.14417,Inverse Scaling in Test-Time Compute,arXiv,2025-07-19 (revised 2025-12-15),Preprint (later revised) providing systematic evaluations and failure mode taxonomy for cases where extra inference-time reasoning hurts performance.,2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2512.08892,Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders,arXiv,2025-12-09,Uses sparse autoencoders to isolate internal activation features tied to RAG unfaithfulness; proposes a lightweight detector (RAGLens) based on internal representations.,2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2512.07141,Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models,arXiv,2025-12-08,"Trains a multi-pass reflection loop to self-correct unsafe reasoning/output (not just answer filtering), relevant to “self-reflection for robustness/safety” mechanisms.",2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2512.17911,Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models,arXiv,2025-11-26,Introduces a benchmark for reasoning-trace leakage in unlearning and a training-free inference-time steering method to forget while preserving reasoning capability.,2025-12-25
reasoning_and_planning,openai,https://aclanthology.org/2025.emnlp-main.532/,DSG-MCTS: A Dynamic Strategy-Guided Monte Carlo Tree Search for Diversified Reasoning in Large Language Models,ACL Anthology (EMNLP 2025),2025-11,MCTS variant aimed at diversified reasoning traces and improved accuracy/efficiency on hard reasoning benchmarks.,2025-12-25
reasoning_and_planning,openai,https://aclanthology.org/2025.findings-emnlp.440/,LRPLAN: A Multi-Agent Collaboration of Large Language and Reasoning Models for Planning with Implicit & Explicit Constraints,ACL Anthology (Findings of EMNLP 2025),2025-11,"Combines LLM and “reasoning model” agents to handle explicit+implicit constraints in planning tasks, addressing consistency and commonsense gaps.",2025-12-25
reasoning_and_planning,openai,https://aclanthology.org/2025.emnlp-industry.116/,Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning,ACL Anthology (EMNLP 2025 Industry Track),2025-11,RL framework (RLTR) that rewards tool-use completeness to train planning behavior without needing verifiable final answers—important for real-world agent planning.,2025-12-25
reasoning_and_planning,openai,https://aclanthology.org/2025.findings-acl.1266/,Tool learning via Inference-time Scaling and Cycle Verifier,ACL Anthology (Findings of ACL 2025),2025-07,"Uses inference-time scaling plus cycle verification to synthesize data for tool learning, linking test-time compute and planning/tool-use skill acquisition.",2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2510.02919,Self-Reflective Generation at Test Time,arXiv,2025-10-03,A test-time reflection mechanism that triggers at high-uncertainty points during generation to reduce cascading errors in reasoning.,2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2506.12217,From Emergence to Control: Probing and Modulating Self-Reflection in Language Models,arXiv,2025-06-13,"Probes latent self-reflection and constructs an activation-space control vector to up/down-regulate reflection, trading off accuracy vs compute.",2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2509.17995,Variation in Verification: Understanding Verification Dynamics in Large Language Models,arXiv,2025-09-22,Systematic study of generative verifiers across difficulty and model scaling; clarifies when verification helps and where it bottlenecks.,2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2505.17155,TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling,arXiv,2025-05-22,"Inference-time method to detect/truncate redundant reasoning using a verifier, improving throughput for long-CoT reasoning models.",2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2504.01005,"When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",arXiv,2025-04-01,Analyzes compute-optimal allocation between generating more solutions (self-consistency) vs spending compute on verification (GenRM-style).,2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2502.12118,Scaling Test-Time Compute Without Verification or RL is Suboptimal,arXiv,2025-02-17,Argues (theoretically + empirically) that verifier-guided RL/search scales better than verifier-free trace distillation as inference budgets grow.,2025-12-25
reasoning_and_planning,openai,https://github.com/RyanLiu112/GenPRM,GenPRM (official codebase),GitHub,2025-04-03 (repo release; updated through 2025-11-08),"Reference implementation + released models/data for GenPRM, useful for reproducing scalable process-verification and critic-style training.",2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2501.03200,The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input,arXiv,2025-01-06,Benchmark/leaderboard for long-context grounding and faithfulness—useful for evaluating factuality/grounding in long-form responses.,2025-12-25
reasoning_and_planning,openai,https://arxiv.org/abs/2505.00506,"HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection",arXiv,2025-05-01,A multi-domain hallucination detection benchmark stressing long contexts and real-world settings—useful for evaluating detectors and RAG reliability.,2025-12-25
reasoning_and_planning,anthropic,https://sakana.ai/ab-mcts/,Inference-Time Scaling and Collective Intelligence for Frontier AI (AB-MCTS),Sakana AI,recent,Novel MCTS variant combining frontier models (o4-mini + Gemini 2.5 + R1) for collective reasoning on ARC-AGI-2.,2025-12-25
reasoning_and_planning,anthropic,https://arxiv.org/abs/2410.01707,Interpretable Contrastive Monte Carlo Tree Search Reasoning (SC-MCTS*),arXiv,December 2024,Novel MCTS algorithm improving both reasoning accuracy and speed for LLMs through contrastive learning.,2025-12-25
reasoning_and_planning,anthropic,https://news.mit.edu/2025/smarter-way-large-language-models-think-about-hard-problems-1204,A smarter way for large language models to think about hard problems,MIT News,December 2025,MIT research on instance-adaptive inference scaling with calibrated process reward models.,2025-12-25
reasoning_and_planning,anthropic,https://lilianweng.github.io/posts/2025-05-01-thinking/,Why We Think - Test-Time Compute and Chain-of-Thought,Lilian Weng's Blog,May 2025,"Comprehensive technical deep-dive on test-time compute, CoT reasoning, and DeepSeek R1 from OpenAI researcher.",2025-12-25
reasoning_and_planning,anthropic,https://arxiv.org/abs/2412.06769,Training Large Language Models to Reason in a Continuous Latent Space (Coconut),arXiv,December 2024,Meta AI research on latent reasoning enabling BFS-style exploration without token generation.,2025-12-25
reasoning_and_planning,anthropic,https://arxiv.org/abs/2501.09686,Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models,arXiv,January 2025,Comprehensive survey on RL-based reasoning covering automated data construction and test-time scaling.,2025-12-25
reasoning_and_planning,anthropic,https://github.com/EdinburghNLP/awesome-hallucination-detection,Awesome Hallucination Detection - Papers and Methods,GitHub,recent,"Curated collection of hallucination detection papers including HaluCheck, semantic density, and interventions.",2025-12-25
reasoning_and_planning,anthropic,https://aclanthology.org/2024.acl-long.506/,InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated Answers,ACL Anthology,August 2024,ACL paper on detecting 87% hallucination rate in Llama-2 without external knowledge.,2025-12-25
reasoning_and_planning,anthropic,https://lilianweng.github.io/posts/2023-06-23-agent/,LLM Powered Autonomous Agents,Lilian Weng's Blog,June 2023,"Influential framework defining agent architecture: planning, memory, and tool use components.",2025-12-25
reasoning_and_planning,anthropic,https://lilianweng.github.io/posts/2024-07-07-hallucination/,Extrinsic Hallucinations in LLMs,Lilian Weng's Blog,July 2024,"Comprehensive technical overview of hallucination causes, detection, and mitigation strategies.",2025-12-25
reasoning_and_planning,anthropic,https://arxiv.org/abs/2406.09136,Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs,arXiv (NeurIPS 2024),October 2024,NeurIPS paper combining Tree-of-Thought search with CoT training for better reasoning.,2025-12-25
reasoning_and_planning,anthropic,https://venturebeat.com/ai/how-test-time-scaling-unlocks-hidden-reasoning-abilities-in-small-language-models/,How test-time scaling unlocks hidden reasoning abilities in small language models,VentureBeat,August 2025,Coverage of Shanghai AI Lab research showing 1B models can outperform 405B with optimal TTS.,2025-12-25
reasoning_and_planning,anthropic,https://arxiv.org/abs/2503.10814,Thinking Machines: A Survey of LLM based Reasoning Strategies,arXiv,March 2025,"Survey covering O1, DeepSeek R1, and evolution of reasoning strategies in LLMs.",2025-12-25
reasoning_and_planning,anthropic,https://sebastianraschka.com/blog/2024/llm-research-papers-the-2024-list.html,LLM Research Papers: The 2024 List,Sebastian Raschka Blog,December 2024,Curated list of significant LLM papers from 2024 including reasoning and scaling research.,2025-12-25
reasoning_and_planning,exa,https://arxiv.org/abs/2503.20757,MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search,Exa,2025-03-26,"The webpage describes **MCTS-RAG**, a novel approach that enhances the reasoning capabilities of small language models (LMs) on knowledge-intensive tasks by combining **Retrieval-Augmented Generation (RAG)** with **Monte Carlo Tree Search (MCTS)**.

Key points related to your query:

*   **Reasoning LLMs & Planning with LLMs:** MCTS-RAG enhances reasoning by using MCTS to refine reasoning paths through an iterative decision-making process, integrating structured reasoning with adaptive retrieval.
*   **MCTS (Monte Carlo Tree Search) for language models:** The core of the method is leveraging MCTS to guide the integration of retrieval and reasoning.
*   **Inference-time compute:** The method achieves performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute.
*   **Hallucination reduction and detection & Grounding, factuality:** MCTS-RAG is shown to reduce hallucinations and ensure improved factual accuracy and response consistency by dynamically integrating retrieval and reasoning, unlike standard RAG or MCTS methods that rely solely on internal knowledge.

The paper focuses on improving reasoning and factuality in LMs using MCTS and RAG.",2025-12-25
reasoning_and_planning,exa,https://inovex.de/de/blog/mcts-meets-llms-enabling-complex-reasoning-and-strategic-planning,MCTS meets LLMs: Enabling Complex Reasoning and Strategic Planning,Exa,2024-06-19,"The webpage discusses a novel framework that integrates **Monte Carlo Tree Search (MCTS)** with **Large Language Models (LLMs)** to enhance their capabilities in complex **reasoning** and **strategic planning**, particularly demonstrated through a case study in Visual Question Answering (VQA).

Key points related to your query:

*   **Reasoning LLMs & Chain-of-Thought (CoT):** The text acknowledges the need for LLMs to reason and mentions **Chain-of-Thought (CoT)** prompting as a technique to elicit more thoughtful responses, contrasting it with a simple, incorrect answer.
*   **MCTS for Language Models (Planning with LLMs):** The core of the framework uses **MCTS** as the environment dynamics manager. The LLM acts as the agent, making decisions (actions like ""Retrieve"" or ""Answer"") at each node of the MCTS tree. This iterative exploration allows the agent to foresee consequences and refine its decision-making, similar to trial-and-error learning.
*   **Inference-time Compute & Test-time Scaling:** The MCTS process inherently involves iterative computation during inference (tree growth and simulation/rollouts) to determine the best sequence of actions, which relates to inference-time compute. The framework's flexibility allows for adjusting parameters governing the tree search, balancing speed and exploration.
*   **Hallucination Reduction and Detection/Grounding/Factuality:** While not explicitly using the terms ""hallucination reduction"" or ""detection,"" the framework addresses related issues:
    *   The MCTS approach is described as more **failure-tolerant** than CoT because it allows the LLM to explore multiple paths and **correct itself** if a path is derailed, unlike CoT where recovery from mistakes is difficult.
    *   The ""Retrieve"" action, which uses a vector database and a vision-language model to gather external evidence, serves to **ground** the LLM's response in external data, mitigating reliance solely on internal knowledge.
    *   However, the text explicitly states that **hallucinations and overconfidence persist as challenges** despite the advancements.
*   **Self-reflection:** The LLM evaluates the utility of actions and assesses the quality of states, which is a form of iterative self-assessment within the planning loop.

**In summary:** The page details how MCTS provides a structured planning mechanism around an LLM",2025-12-25
reasoning_and_planning,exa,https://www.researchgate.net/publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights,Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights,Exa,2025-02-18,"The webpage summarizes a study titled ""Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights,"" which examines the reasoning and planning capabilities of Large Language Models (LLMs) using inference-time techniques.

The study focuses on:
*   **Reasoning and Planning in LLMs:** Analyzing their ability to solve complex tasks across five categories: arithmetic, logical, common sense, algorithmic reasoning, and planning.
*   **Inference-Time Techniques:** Evaluating methods like **Chain-of-Thought (CoT)**, **Self-Consistency (SC)**, **Tree-of-Thought (ToT)**, and **Reasoning as Planning with World Models (RAP)** (which uses **MCTS**).
*   **Benchmark Creation:** Introducing **Sys2Bench**, a comprehensive benchmark with eleven diverse tasks to evaluate these techniques.
*   **Key Findings:** The study concludes that **simply scaling inference-time computation has limitations**, as no single technique consistently performs well across all reasoning and planning tasks. Tree search methods (ToT, RAP) often struggle with increasing complexity or tasks requiring self-verification, while CoT and SC perform well on arithmetic tasks. Large Reasoning Models (LRMs) like OpenAI's o1 show strong performance, especially in arithmetic and planning, but still struggle with tasks requiring advanced spatial reasoning (like Rubik's Cube). The research suggests a need for more diverse approaches beyond just scaling computation.

The user query asks about several specific concepts: **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, factuality.**

The page directly addresses:
*   **Reasoning LLMs:** The entire paper is about their reasoning and planning capabilities.
*   **Chain-of-Thought (CoT):** Explicitly mentioned and evaluated as a baseline inference-time technique.
*   **Inference-Time Compute/Scaling:** The core focus is on examining and finding limitations in scaling inference-time techniques.
*   **Planning with LLMs:** A major category of tasks evaluated in Sys2Bench.
*   **MCTS for Language Models:** Mentioned as part of the RAP technique, which uses Monte Carlo Tree Search.
*   **Factuality/Hallucination:** The discussion section notes that LLMs",2025-12-25
reasoning_and_planning,exa,https://medium.com/verimsabanci/inference-time-compute-scaling-enhancing-reasoning-in-llms-without-additional-training-63417b1febfc,Inference-Time Compute Scaling: Enhancing Reasoning in LLMs Without Additional Training,Exa,2025-05-23,"The webpage discusses **Inference-Time Compute Scaling** as a method to enhance **Reasoning LLMs** without requiring additional training.

Key points related to your query:

*   **Reasoning LLMs:** These models aim to solve complex, multi-step tasks by generating intermediate thought processes.
*   **Inference-Time Compute Scaling:** This involves increasing the computational effort during inference (model usage) rather than training. Techniques mentioned include:
    *   **Chain-of-Thought (CoT) prompting:** Encouraging intermediate reasoning steps.
    *   **Self-consistency sampling:** Sampling multiple paths and choosing the most consistent result.
    *   **Monte Carlo Tree Search (MCTS):** Exploring different output paths for higher quality.
*   **Self-reflection and Planning with LLMs:** While the page focuses on inference-time scaling, it mentions MCTS as a technique used for exploring different output paths, which relates to planning.
*   **Hallucination Reduction and Detection, Grounding, Factuality:** These specific topics are **not** explicitly detailed in the provided text, although improving reasoning generally aims to address these issues.
*   **Test-time scaling:** This is synonymous with the main topic, **Inference-Time Compute Scaling**.

**In summary, the page covers Reasoning LLMs, Chain-of-Thought, Inference-Time Compute Scaling (Test-time scaling), MCTS for language models, and briefly touches upon the goal of better reasoning.** It does not provide specific details on self-reflection, grounding, or hallucination reduction/detection beyond the general context of improving reasoning.",2025-12-25
reasoning_and_planning,exa,https://arxiv.org/pdf/2310.04406,Untitled,Exa,2024-06-07,"The user query asks for a summary related to several concepts in the field of **Reasoning and Planning with Large Language Models (LLMs)**, including: Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, Monte Carlo Tree Search (MCTS) for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.

The provided webpage introduces **Language Agent Tree Search (LATS)**, a general framework that synergizes LLM capabilities in **reasoning, acting, and planning**.

Here is a summary of how LATS addresses the concepts in the query:

*   **Reasoning LLMs & Chain-of-Thought (CoT):** LATS builds upon and extends reasoning techniques like CoT by framing the problem as a search over possible reasoning and acting steps. It uses the base LLM ($p_\theta$) for reasoning.
*   **Planning with LLMs & MCTS (Monte Carlo Tree Search):** LATS's core contribution is integrating a variant of **MCTS** to enable deliberate planning, moving beyond the simple, reflexive acting processes of prior methods. This allows LATS to search over a combinatorial space of reasoning and acting steps.
*   **Self-Reflection:** LATS incorporates a **Reflection** operation. Upon reaching an unsuccessful terminal node, the LLM generates a verbal self-reflection summarizing errors, which is stored and used as additional context in future trials, enabling the agent to learn from trial and error without explicit training.
*   **Grounding & Factuality (and Hallucination Reduction):** LATS enhances sensibility and addresses limitations of purely internal reasoning (which risks hallucination) by incorporating **external feedback** (observations from the environment). The value function also incorporates a **self-consistency score** to improve value assignment.
*   **Inference-Time Compute & Test-Time Scaling:** LATS involves a higher computational cost than simpler prompting methods (like ReAct) because of the tree search. However, the paper notes that LATS achieves better performance and, in some comparisons, requires fewer overall nodes/tokens upon success than other tree-based search methods (like ToT and RAP), suggesting a more efficient search mechanism. The authors acknowledge the higher compute cost but expect it to decrease over time.

In summary, LATS unifies reasoning, acting, and planning by using **

The provided webpage text is a list of references and appendices detailing the methodology, ablation studies, and prompts used in a research paper, likely about ""Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models"" (LATS).

The user query asks for a summary related to: **'reasoning\_and\_planning: Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, factuality'**

The text mentions several relevant concepts:

*   **Reasoning LLMs and Chain-of-Thought (CoT):** References to ""Chain of thought prompting elicits reasoning in large language models"" (Wei et al., 2022) and ""Self-consistency improves chain of thought reasoning"" (Wang et al., 2022) are present. The LATS algorithm itself is a planning/reasoning method.
*   **Planning with LLMs and MCTS:** The core subject is **LATS (Language Agent Tree Search)**, which unifies reasoning, acting, and planning. The text explicitly mentions MCTS (Monte Carlo Tree Search) in the context of LATS (referencing Kocsis and Szepesvari, 2006, and Swiechowski et al., 2021) and its application to language models (e.g., ""Mastering the game of Go with deep neural networks and tree search"" by Silver et al., 2016, is cited as inspiration).
*   **Self-reflection:** The text discusses **Self-Refine** (Madaan et al., 2023) and includes a ""Reflection Prompt"" section for LATS, indicating the use of self-reflection/self-feedback.
*   **Inference-time compute/Test-time scaling:** Section B discusses the **Computational cost** of LATS compared to simpler methods like ReAct, noting that LATS has a higher cost but can be mitigated by adjusting the number of sampled nodes ($n$).
*   **Grounding/Factuality:** The environment details mention **HotPotQA**, a dataset for question-answering requiring reasoning over supporting documents, which relates to grounding and factuality. The **WebShop** environment also involves grounded language understanding",2025-12-25
reasoning_and_planning,exa,https://aclanthology.org/2023.emnlp-main.507/,Reasoning with Language Model is Planning with World Model - ACL Anthology,Exa,2025-12-10,"The webpage describes a paper titled ""Reasoning with Language Model is Planning with World Model,"" which introduces a framework called **Reasoning via Planning (RAP)**.

This framework addresses the limitations of Large Language Models (LLMs) in complex reasoning tasks (like plan generation, math, and logic) that stem from their lack of an internal ""world model"" for predicting future states and simulating outcomes.

RAP overcomes this by repurposing the LLM to act as **both a world model and a reasoning agent**. It incorporates a planning algorithm based on **Monte Carlo Tree Search (MCTS)** to strategically explore the reasoning space, balancing exploration and exploitation to find high-reward reasoning paths.

The paper notes that LLMs show remarkable reasoning capabilities with **Chain-of-Thought (CoT)** prompting, but RAP is shown to be superior to CoT and other baselines in challenging reasoning problems.",2025-12-25
reasoning_and_planning,exa,https://arxiv.org/abs/2305.14078,Computer Science > Robotics,Exa,2023-05-23,"The webpage discusses using Large Language Models (LLMs) as a **commonsense knowledge** source for **Large-Scale Task Planning**.

Specifically, it highlights:
*   LLMs can provide a **world model** (commonsense knowledge) in addition to acting as a policy.
*   This world model and policy can be combined within a search algorithm like **Monte Carlo Tree Search (MCTS)** (MCTS for language models is mentioned).
*   The LLM-induced world model provides a **commonsense prior belief** for MCTS, leading to effective **reasoning**.
*   The LLM-induced policy acts as a heuristic to guide the search, improving efficiency.
*   The proposed **LLM-MCTS** algorithm outperforms MCTS alone and policies induced solely by LLMs (GPT2 and GPT3.5) on complex tasks.
*   The findings suggest that using the LLM as a world model for model-based planning is beneficial when the description length of the world model is substantially smaller than that of the policy (related to **minimum description length (MDL)**).

While the query covers many topics like ""chain-of-thought,"" ""self-reflection,"" ""test-time scaling,"" and ""hallucination reduction,"" the paper directly addresses **planning with LLMs** and **MCTS for language models**, leveraging LLMs for **reasoning** through a world model.",2025-12-25
reasoning_and_planning,exa,https://arxiv.org/pdf/2305.14992,Untitled,Exa,2023-10-23,"The user query asks for a summary related to several topics in LLM reasoning: **reasoning and planning, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**

The provided webpage text introduces a framework called **Reasoning via Planning (RAP)**, which addresses limitations in current LLM reasoning, particularly the lack of an internal world model needed for deliberate planning.

Here is a summary of how the text relates to the query topics:

*   **Reasoning and Planning / Planning with LLMs:** The core of the paper is the RAP framework, which repurposes the LLM as both a **world model** (to predict future states) and a **reasoning agent** to perform strategic planning. This directly addresses the need for better planning capabilities in LLMs, contrasting with the purely autoregressive nature of methods like CoT.
*   **Chain-of-Thought (CoT):** CoT is mentioned as a baseline method that generates reasoning steps sequentially but struggles with complex, multi-step problems because it lacks simulation capabilities. RAP is shown to significantly outperform CoT in plan generation and math reasoning tasks.
*   **MCTS (Monte Carlo Tree Search) for Language Models:** RAP explicitly incorporates **Monte Carlo Tree Search (MCTS)** as the principled planning algorithm to efficiently explore the vast reasoning space, balancing **exploration vs. exploitation** (a key aspect of inference-time compute/strategy).
*   **Self-Reflection:** The reward design in RAP includes **""Self-evaluation by the LLM,""** where the model criticizes its own reasoning step (""Is this reasoning step correct?""), using the resulting probability as a reward signal.
*   **Grounding and Factuality:** The introduction of the **world model** (which predicts the next state based on actions) helps LLMs produce a more **grounded and coherent inference** by simulating the environment's state, which is crucial for maintaining factuality in multi-step reasoning. In logical reasoning tasks, RAP achieves high proof accuracy, suggesting improved grounding in the provided facts and rules.
*   **Inference-time Compute / Test-time Scaling:** The use of MCTS inherently involves managing **inference-time compute** by iterating a set number of times (e.g., RAP(1

The webpage discusses **Reasoning via Planning (RAP)**, a novel framework that equips Large Language Models (LLMs) with strategic planning abilities, allowing them to act as both a world model and a reasoning agent.

Key aspects related to your query:

*   **Reasoning LLMs & Chain-of-Thought (CoT):** RAP is presented as an improvement over the **chain-of-thought (CoT)** prompting baseline, showing superior performance, especially when many steps are required.
*   **Planning with LLMs & MCTS:** RAP achieves effective balance between exploration and exploitation by using **Monte Carlo Tree Search (MCTS)** for language models. The document details modifications made to classic MCTS to handle the large reasoning space and high computational cost of LLMs, including sampling potential actions and using lightweight local rewards during selection.
*   **Inference-time compute & Test-time scaling:** The framework operates during the inference stage. The performance comparison against CoT shows that RAP maintains a high success rate where CoT drops significantly when more steps are needed, suggesting better scaling with reasoning depth.
*   **Self-reflection (Self-evaluation reward):** The framework utilizes a **self-evaluation reward** as part of its reward structure. Experiments show that the self-evaluation reward is crucial for guiding reasoning, especially in mathematical reasoning where detecting errors post-generation is feasible.
*   **Grounding & Hallucination Reduction:** While not explicitly using the terms ""grounding"" or ""hallucination reduction,"" the framework's use of an LLM as a **world model** to simulate states and anticipate outcomes, combined with planning, serves to structure the reasoning process, which is a method to improve factuality and reduce errors compared to simple CoT. The text also mentions the importance of grounding in future work (combining external tools).
*   **Reward Choice:** The effectiveness of different rewards (action likelihood, task-specific, self-evaluation) is explored, showing that combinations, particularly those including self-evaluation, boost performance across different tasks (Blocksworld and math reasoning).

The document focuses heavily on the RAP framework, its comparison to CoT, and its implementation using MCTS and various reward mechanisms to enhance reasoning.",2025-12-25
reasoning_and_planning,exa,https://llm-mcts.github.io/,SOCIAL MEDIA TITLE TAG,Exa,2023-01-01,"The webpage describes a paper titled ""Large Language Models as Commonsense Knowledge for Large-Scale Task Planning."" It focuses on using Large Language Models (LLMs) within the **Monte Carlo Tree Search (MCTS)** framework for large-scale task planning.

Specifically, the paper proposes an **LLM-MCTS algorithm** where:
1.  The LLM acts as a **commonsense world model**, providing a prior belief for MCTS to enable effective **reasoning**.
2.  The LLM also acts as a **heuristic policy** to guide the search, improving efficiency.

The results show that this LLM-MCTS approach outperforms MCTS alone and policies induced solely by LLMs for complex tasks. The authors suggest that using the LLM as a world model for model-based planning is likely better than using it only as a policy when the description length of the world model is substantially smaller than that of the policy (related to the **minimum description length (MDL)** principle).

While the user query covers a broad range of topics related to **reasoning and planning with LLMs** (including chain-of-thought, self-reflection, inference-time compute, hallucination reduction, grounding, etc.), this specific page directly addresses **planning with LLMs** using **MCTS** and leveraging the LLM as a **world model** for **reasoning**.",2025-12-25
reasoning_and_planning,exa,https://github.com/1989Ryan/llm-mcts,"GitHub - 1989Ryan/llm-mcts: [NeurIPS 2023] We use large language models as commonsense world model and heuristic policy within Monte-Carlo Tree Search, enabling better-reasoned decision-making for daily task planning problems.",Exa,2023-05-22,"The webpage describes the **llm-mcts** GitHub repository, which implements a method for **task planning** using **Large Language Models (LLMs)** within **Monte Carlo Tree Search (MCTS)**.

Specifically, the LLMs are used as both the **commonsense world model** and the **heuristic policy** within MCTS. This approach aims to enable **better-reasoned decision-making for daily task planning problems** by providing MCTS with a commonsense prior belief of states and guiding the search to reduce complexity. The work is associated with a **NeurIPS 2023** paper.

The query covers several topics related to advanced LLM capabilities, including: *reasoning, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction, grounding, and factuality*.

The page directly addresses **planning with LLMs** and **MCTS (Monte Carlo Tree Search) for language models** in the context of task planning. It does not explicitly detail reasoning techniques like chain-of-thought, inference-time compute specifics, self-reflection, test-time scaling, or methods for hallucination reduction/detection, grounding, or factuality beyond the general goal of ""better-reasoned decision-making.""",2025-12-25
reasoning_and_planning,exa,https://arxiv.org/abs/2305.02556,Faithful Question Answering with Monte-Carlo Planning,Exa,2023-05-04,"The webpage describes a paper titled ""Faithful Question Answering with Monte-Carlo Planning"" which proposes **FAME** (FAithful question answering with MontE-carlo planning). This method aims to answer questions based on **faithful reasoning steps** organized as a structured entailment tree. It formulates the task as a discrete decision-making problem solved by a controller interacting with a reasoning environment. A **Monte-Carlo planning algorithm** is introduced to perform look-ahead search and select actions that lead to high-quality steps.

While the user query covers broad topics like **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality**, the paper specifically focuses on:

*   **Planning with LLMs** (using Monte-Carlo planning for action selection).
*   **MCTS (Monte Carlo Tree Search) for language models** (explicitly using a Monte-Carlo planning algorithm).
*   **Faithfulness** and producing valid reasoning steps (related to grounding and factuality in the context of QA).

The paper does not explicitly detail reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, test-time scaling, or hallucination reduction/detection beyond the scope of producing faithful reasoning steps for question answering.",2025-12-25
reasoning_and_planning,exa,https://arxiv.org/html/2506.18183v1,"Reasoning about Uncertainty: 
 Do Reasoning Models Know When They Don’t Know?",Exa,2025-12-17,"The user query asks for a summary related to several topics concerning Reasoning LLMs, including chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction/detection, grounding, and factuality.

The provided webpage focuses heavily on **Reasoning LLMs**, **chain-of-thought (CoT)**, **inference-time compute** (referred to as ""deeper reasoning"" or ""inference-time scaling""), **self-reflection** (introduced as ""introspective uncertainty quantification""), **hallucination reduction and detection** (addressed via uncertainty quantification and calibration), and **factuality** (assessed using benchmarks like SimpleQA).

Here is a summary of the key findings from the page relevant to the query:

**Reasoning LLMs and Chain-of-Thought (CoT):**
*   Reasoning language models (like Claude 3.7 Sonnet, DeepSeek R1, Gemini 2 Flash Thinking, and o3-Mini) achieve SOTA performance by using multi-step reasoning, often induced via reinforcement learning or CoT prompting.
*   The study investigates the calibration (trustworthiness) of these models when they self-verbalize their confidence.

**Inference-Time Compute / Deeper Reasoning (Test-Time Scaling):**
*   Deeper reasoning (scaling reasoning effort/tokens) generally leads to **higher accuracy**.
*   However, as accuracy saturates, deeper reasoning causes models to become **even more overconfident** (calibration errors increase, especially ECE), particularly on wrongly-answered questions.
*   Increased reasoning depth can also lead to a **decrease in the completion rate** as models struggle with format instructions.

**Self-Reflection (Introspective Uncertainty Quantification - IUQ):**
*   The paper introduces **Introspective Uncertainty Quantification (IUQ)**, where a model reasons about its own chain-of-thought trace to update its confidence, mimicking human self-checking.
*   More critical introspection (IUQ-Medium and IUQ-High, which explicitly ask the model to find flaws) generally **improves calibration** (reduces miscalibration/overconfidence) for models like o3-Mini and DeepSeek R1, especially on challenging datasets.
*   However, this introspection **degrades the calibration of Claude 3.7 Sonnet**.

**Hall

The user query asks for a summary related to several topics in **reasoning and planning with LLMs**, including: Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction/detection, grounding, and factuality.

The provided webpage focuses on the **calibration and uncertainty quantification (UQ)** of **reasoning models**.

Here is a summary of the relevant points from the text concerning the user's query topics:

*   **Reasoning LLMs & Calibration:** The paper examines the calibration of reasoning models, finding that they are generally **overconfident** and become even more so with deeper reasoning, especially when accuracy doesn't improve.
*   **Chain-of-Thought (CoT) Prompting:** The study explored CoT prompting alongside basic, multi-step, and top-K prompts. It found that **CoT prompting and multi-step prompting do not significantly outperform the basic prompt strategy** for reasoning models. This is because reasoning models are already trained to generate reasoning traces by default, unlike non-reasoning models where CoT is effective for eliciting reasoning.
*   **Self-Reflection (Introspection):** Critical introspection (a form of self-reflection) **enables models to identify flaws and hallucinations** in their reasoning traces, leading to improved calibration by reducing overconfidence (though Claude became more overconfident).
*   **Hallucination Reduction/Detection:** Introspection is shown to help models identify flaws and **hallucinations** in their reasoning traces, which aids in confidence reassessment.
*   **Planning with LLMs / MCTS:** While the paper discusses reasoning models and their traces, it does not explicitly detail planning with LLMs or the use of MCTS (Monte Carlo Tree Search) for language models.
*   **Factuality/Grounding:** The paper focuses on *confidence* calibration rather than explicitly measuring grounding or factuality, although improving calibration is related to reducing incorrect outputs (hallucinations).
*   **Inference-time Compute / Test-time Scaling:** These topics are not directly addressed, although introspection and multi-step prompting relate to increased inference-time computation.

**In summary:** The page primarily discusses the calibration of reasoning models, finding that specialized prompt strategies like Chain-of-Thought do not significantly improve performance over basic prompting for these",2025-12-25
reasoning_and_planning,exa,https://arxiv.org/abs/2404.05966,Computer Science > Computation and Language,Exa,2024-04-09,"The provided webpage describes **THOUGHTSCULPT**, a general reasoning and search method for tasks where outputs can be broken down into components.

Key aspects relevant to your query include:

*   **Reasoning and Search:** THOUGHTSCULPT explores a search tree of potential solutions using **Monte Carlo Tree Search (MCTS)**.
*   **Intermediate Revision:** A critical feature is the inclusion of **revision actions** in the action space, allowing the model to revise previous output components rather than just continuing to build the solution.

While the paper focuses on a specific reasoning method (THOUGHTSCULPT using MCTS and revision), it directly addresses concepts related to **Reasoning LLMs**, **MCTS for language models**, and **planning with LLMs** (as MCTS is a planning technique). It does not explicitly mention chain-of-thought, inference-time compute, self-reflection, test-time scaling, hallucination reduction/detection, grounding, or factuality.",2025-12-25
reasoning_and_planning,exa,https://dl.acm.org/doi/10.5555/3600270.3602070,Chain-of-thought prompting elicits reasoning in large language models,Exa,2022-11-28,"The webpage is an abstract and citation information for a research article titled **""Chain-of-thought prompting elicits reasoning in large language models""**.

The abstract states that generating a **chain of thought** (a series of intermediate reasoning steps) significantly improves the ability of large language models (LLMs) to perform complex reasoning. This is achieved through **chain-of-thought prompting**, where a few chain-of-thought demonstrations are provided as examples in the prompt. Experiments showed this method improves performance on arithmetic, commonsense, and symbolic reasoning tasks, with striking empirical gains, such as achieving state-of-the-art accuracy on the GSM8K benchmark for math word problems using a PaLM 540B model with just eight chain-of-thought exemplars.

The user query covers several topics related to LLM reasoning and planning: *reasoning & planning, Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, factuality*.

**Summary relevant to the query:**

The page directly addresses **Reasoning LLMs** and **chain-of-thought** prompting, showing that providing intermediate reasoning steps significantly improves complex reasoning abilities in LLMs across arithmetic, commonsense, and symbolic tasks.

**Topics not explicitly covered in the abstract:** inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.",2025-12-25
reasoning_and_planning,exa,https://ar5iv.labs.arxiv.org/html/2309.15402,"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future",Exa,2022-01-01,"The user query asks for a summary covering several topics related to reasoning and planning with Large Language Models (LLMs), including: **Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**

This webpage is a survey titled ""A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future,"" which focuses heavily on **Chain-of-Thought (CoT)** reasoning (referred to broadly as X-of-Thought or XoT).

Here is a summary of how the page addresses the concepts in the query:

*   **Reasoning LLMs & Chain-of-Thought (CoT):** The survey is fundamentally about CoT reasoning, which enhances LLMs' capabilities for complex reasoning tasks (like mathematical and commonsense reasoning) by generating step-by-step rationales. It covers the introduction of CoT, its mathematical formulation, and taxonomies based on construction (Manual, Automatic, Semi-automatic), structure (Chain, Tree, Graph), and enhancement methods.
*   **Planning with LLMs:** Planning is explicitly mentioned as a **Frontier Application** of XoT (§5.2). Examples provided include ToT (Tree-of-Thought) and ReAct.
*   **Self-Reflection & Hallucination Reduction/Detection & Factuality:** These concepts are strongly covered under **XoT Enhancement Methods**, particularly in the **Verify and Refine** section (§4.3.1). This section discusses strategies like Self-Refine, Reflexion (which uses reinforcement learning for reflection), and methods that use verification (like RCoT or FOBAR) to reduce cascading errors and factual mistakes (hallucinations). External knowledge integration (§4.3.3) also aims to reduce factual errors.
*   **MCTS (Monte Carlo Tree Search) for Language Models:** This is addressed under **XoT Structural Variants** (§4.2) in the **Tree Structure** subsection, where Tree-of-Thought (ToT) methods are discussed, which often incorporate tree search algorithms to explore reasoning paths.
*   **Inference-time compute & Test-time scaling:** While the survey discusses methods that increase complexity (like Tree and Graph structures) and methods that improve efficiency (like SoT

The webpage provides information related to several aspects of your query concerning **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, hallucination reduction and detection, and grounding/factuality**.

Here is a summary of the relevant sections:

**Reasoning LLMs & Chain-of-Thought (CoT):**
*   The document surveys **Chain of Thought (CoT) Reasoning** and its generalized form, **X-of-Thought (XoT)**.
*   **CoT Distillation (§5.3)** discusses methods where reasoning chains generated by larger LLMs are distilled into smaller models to enhance their reasoning capabilities.
*   **Faithfulness (§6.2)** addresses the issue of **hallucination** (factual mistakes and contextual inconsistencies) in CoT reasoning, mentioning techniques that use external knowledge for evaluation, reflection mechanisms for correction, and question decomposition.
*   **CoT Theory (§6.3)** explores the empirical and theoretical underpinnings of why CoT works, noting its reliance on semantic knowledge and its potential to reduce the complexity of in-context learning.

**Inference-Time Compute & Efficiency:**
*   Section **4.3.5 Efficiency** discusses the expensive overheads of LLM reasoning. Methods mentioned to reduce inference costs include dynamically adjusting the number of samples (self-consistency), parallel question decomposition, and selectively skipping intermediate layers during reasoning.

**Self-Reflection:**
*   In **§5.2 Planning**, methods like **Self-Refine** (where the model evaluates and feeds back on its own output) and **Reflexion** (which reflects on and rectifies previous errors) are discussed as techniques to improve error correction.

**Planning with LLMs & MCTS for Language Models:**
*   Section **§5.2 Planning** details advanced planning techniques beyond basic CoT:
    *   **Tree-of-Thought (ToT)** allows LLMs to explore multiple reasoning paths in a tree structure and self-evaluate.
    *   **Reasoning via Planning (RAP)** uses the **Monte Carlo Tree Search (MCTS)** algorithm, employing LLMs as both the world model and the reasoning agent.
    *   **Graph of Thought (GoT)** uses graph nodes for thoughts and external Graph Neural Networks for organization.
    *",2025-12-25
reasoning_and_planning,exa,https://www.emergentmind.com/articles/2201.11903,"Chain of Thought Prompting Elicits Reasoning in Large Language Models
     
      (2201.11903v1)",Exa,unknown,"The webpage summarizes the paper ""Chain of Thought Prompting Elicits Reasoning in Large Language Models"" (2201.11903).

**Summary relevant to the user query:**

The paper introduces **Chain-of-Thought (CoT) prompting**, a technique that elicits **reasoning** in Large Language Models (LLMs) by including intermediate reasoning steps in few-shot examples.

*   **Reasoning LLMs & Chain-of-Thought:** CoT prompting guides LLMs to generate a ""chain of thought""—a series of short sentences mimicking a human reasoning process—before providing the final answer. This significantly improves performance on multi-step reasoning tasks like arithmetic, commonsense, and symbolic manipulation, which large models often struggle with using standard prompting.
*   **Emergent Ability & Inference-Time Compute:** CoT reasoning was found to be an **emergent ability** tied to model scale; it only consistently improved performance for models larger than approximately 100 billion parameters. The increased output length implies increased **inference-time compute**.
*   **Hallucination Reduction and Detection (Factuality):** While CoT improves performance, the paper notes a limitation: there is **no guarantee that the generated chains of thought are factually correct or logically sound**, even if they lead to a correct answer. Manual analysis showed that errors in reasoning (major errors) and calculation/logic (minor errors) still occur, highlighting an open challenge in ensuring the **factuality** of the generated reasoning.
*   **Planning with LLMs:** The paper tested CoT on commonsense reasoning tasks like StrategyQA and SayCan robot **planning**, showing applicability beyond pure arithmetic.
*   **Grounding:** The paper does not explicitly discuss grounding in the context of external knowledge bases, but the manual analysis suggests that major errors stem from semantic understanding issues, which relates to how well the model grounds its reasoning in the problem's context.

**Topics not explicitly covered or detailed:**

*   MCTS (Monte Carlo Tree Search) for language models.
*   Self-reflection (though CoT is a form of self-correction/step-by-step verification).
*   Test-time scaling (beyond the observation that CoT emerges at scale).",2025-12-25
reasoning_and_planning,exa,https://aclanthology.org/2025.coling-main.719.pdf,Untitled,Exa,2024-12-08,"This webpage provides a comprehensive survey of **Chain-of-X (CoX) paradigms for Large Language Models (LLMs)**, which generalize the widely used Chain-of-Thought (CoT) prompting method.

The summary covers:

*   **Chain-of-X (CoX) Definition:** CoX extends CoT by constructing a sequence of problem-related components (the 'X' or 'node') that either compose a solution or iteratively refine outputs.
*   **Taxonomy by Nodes (Components):** CoX methods are categorized based on the type of node used in the chain:
    *   **Chain-of-Intermediates:** Involves breaking down problems (Problem Decomposition, e.g., classic CoT, Least-to-Most) or accumulating information (Knowledge Composition, e.g., Chain-of-Knowledge).
    *   **Chain-of-Augmentation:** Augments the chain with external data, including **Instructions**, **Histories**, **Retrievals** (e.g., ReAct, Self-Ask), and **Tools** (e.g., MultiToolCoT).
    *   **Chain-of-Feedback:** Uses feedback interlaced during generation, categorized as **Self Feedback** (e.g., Self-Refine, Chain-of-Verification) or **External Feedback**.
    *   **Chain-of-Models:** Leverages the distinct strengths of multiple LLMs working sequentially (e.g., Chain-of-Experts, Chain-of-Discussion).
*   **Taxonomy by Tasks (Applications):** CoX methods are applied across diverse areas:
    *   **Multi-Modal Interaction:** Handling text alongside images, tables, code, or speech.
    *   **Factuality & Safety:** Reducing **hallucination** (via verification or knowledge grounding) and improving **alignment** with human preferences.
    *   **Multi-Step Reasoning:** Solving complex problems requiring logical progression.
    *   **Instruction Following:** Enhancing the ability to follow complex, sequential instructions.
    *   **LLMs as Agents:** Boosting the **planning** abilities of LLM-based agents.
    *   **Evaluation Tools:** Using CoX structures to probe and evaluate LLM vulnerabilities and performance.
*   **Future Directions:** The paper suggests future research should focus on causal analysis of intermediate steps, reducing the",2025-12-25
reasoning_and_planning,exa,https://www.reddit.com/r/LocalLLaMA/comments/1mkza1b/new_paper_reveals_chainofthought_reasoning_of/,New paper reveals Chain-of-Thought reasoning of LLMs a mirage,Exa,2025-08-08,"The webpage discusses a new paper suggesting that the Chain-of-Thought (CoT) reasoning observed in Large Language Models (LLMs) might be a ""mirage."" The paper indicates that LLMs could be ""thinking"" in latent space, which effectively separates their internal reasoning process from the visible context tokens they output.

This directly relates to the user's query regarding **reasoning and planning in LLMs**, specifically **chain-of-thought reasoning**.",2025-12-25
reasoning_and_planning,exa,https://par.nsf.gov/biblio/10463284-faithful-chain-thought-reasoning,Faithful Chain-of-Thought Reasoning | NSF Public Access Repository,Exa,2023-11-01,"The webpage describes a research paper titled ""Faithful Chain-of-Thought Reasoning."" This work proposes a framework called **Faithful CoT** to address the issue that standard Chain-of-Thought (CoT) reasoning chains generated by Language Models (LMs) may not faithfully reflect how the model arrived at the answer.

Faithful CoT involves two stages:
1.  **Translation:** Converting the Natural Language query into a **symbolic reasoning chain** using an LM.
2.  **Problem Solving:** Using a **deterministic solver** on the reasoning chain to derive the final answer.

This approach guarantees that the reasoning chain provides a faithful explanation. Empirically, Faithful CoT outperforms standard CoT on 9 out of 10 benchmarks across diverse domains, showing significant accuracy gains in areas like **Math Word Problems (MWP)**, **Planning**, **Multi-hop Question Answering (QA)**, and **Relational Inference**. It also achieved new state-of-the-art few-shot performance on several datasets when tested with GPT-4 and Codex.

While the user query covers a broad range of topics related to Reasoning LLMs (including MCTS, self-reflection, grounding, and hallucination reduction), the provided text specifically focuses on **Chain-of-Thought (CoT) reasoning** and improving its **faithfulness** and **empirical performance**.",2025-12-25
reasoning_and_planning,exa,https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/,Language Models Perform Reasoning via Chain of Thought,Exa,2024-05-30,"The provided webpage discusses **Chain of Thought (CoT) Prompting** as a method to enable large language models (LLMs) to perform multi-step reasoning tasks, such as arithmetic word problems and commonsense reasoning.

Key points related to your query:

*   **Reasoning LLMs and Chain-of-Thought:** CoT prompting involves prompting the model to produce intermediate reasoning steps before giving the final answer, mimicking an intuitive thought process. This method is shown to improve reasoning abilities in LLMs of sufficient scale ($\sim$100B parameters).
*   **Inference-time Compute/Scaling:** The benefits of CoT prompting are an **emergent property of model scale**; performance improvements are only seen with larger models (around 100B parameters or more).
*   **Planning with LLMs:** While the text focuses on reasoning decomposition, the concept of breaking down a problem into intermediate steps is foundational to planning.
*   **Performance Improvements:** CoT prompting led to state-of-the-art performance on the GSM8K arithmetic reasoning benchmark when combined with the 540B parameter PaLM model. Performance also improved on commonsense reasoning tasks.

The page **does not** explicitly discuss:
*   MCTS (Monte Carlo Tree Search) for language models.
*   Test-time scaling (beyond the observation that CoT is scale-dependent).
*   Hallucination reduction and detection.
*   Grounding or factuality (though improved reasoning might indirectly help).
*   Self-reflection.",2025-12-25
reasoning_and_planning,exa,https://paperswithcode.com/paper/chain-of-thought-prompting-elicits-reasoning,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,Exa,2022-01-28,"The webpage discusses **Chain-of-Thought Prompting**, a method that enhances the **reasoning capabilities** of **Large Language Models (LLMs)** by having them generate a series of intermediate reasoning steps. This technique shows significant improvement on **arithmetic, commonsense, and symbolic reasoning tasks**. The text specifically mentions that providing a few chain-of-thought demonstrations as exemplars in the prompt elicits these reasoning abilities in sufficiently large LLMs. It also notes that this method, when applied to a 540B-parameter model, achieved state-of-the-art accuracy on the GSM8K benchmark for math word problems.

While the user query covers a broad range of topics related to reasoning and planning in LLMs (including MCTS, self-reflection, hallucination reduction, etc.), this specific page focuses primarily on **Chain-of-Thought Prompting** as a method for eliciting **reasoning** in LLMs.",2025-12-25
reasoning_and_planning,exa,https://medium.com/@myschang/chain-of-thought-cot-in-large-language-models-introduction-and-applications-910363d82431,Chain-of-Thought (CoT) in Large Language Models: Introduction and Applications,Exa,2023-04-26,"The provided webpage focuses on **Chain-of-Thought (CoT)** in Large Language Models (LLMs) as a technique to enhance their **reasoning ability**.

Key points related to your query include:

*   **Reasoning LLMs:** The text highlights that sufficiently large LLMs exhibit ""emerging abilities"" like human-like reasoning, allowing them to break down complex tasks.
*   **Chain-of-Thought (CoT):** This is the central topic, described as a method to ""excite the reasoning ability of LLMs"" by providing reasoning examples (Few-shot CoT) or simple prompts like ""Let’s think step by step"" (Zero-shot CoT), enabling models to generate explicit reasoning steps.
*   **Planning with LLMs:** The reasoning ability of LLMs is noted as being applicable as a ""planer or decomposer in many downstream tasks."" An example is given where an LLM is prompted to suggest possible goals for an RL agent (ELLM method).

The page **does not explicitly discuss** the following terms from your query: *inference-time compute, self-reflection, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, or factuality*.",2025-12-25
reasoning_and_planning,exa,https://arxiv.org/html/2505.16782v1,A Comprehensive Survey on Latent Chain-of-Thought Reasoning,Exa,2024-01-01,"The provided web page is a comprehensive survey on **Latent Chain-of-Thought (CoT) Reasoning** in Large Language Models (LLMs). It focuses on moving reasoning beyond explicit, verbalized natural language steps into internal, latent spaces for richer representations, faster inference, and handling abstract reasoning.

Here is a summary of the page content relevant to your query:

**Reasoning and Planning with LLMs (Focusing on Latent CoT):**

*   **Chain-of-Thought (CoT) and Latent CoT:** The paper contrasts conventional explicit CoT (reasoning steps verbalized in natural language) with **Latent CoT**, where inference occurs in latent spaces, decoupling reasoning from language.
*   **Token-wise Strategies:** These methods use special tokens to streamline reasoning:
    *   **Discrete Tokens:** Symbolic markers (like ""pause tokens"" or ""planning tokens"") that segment or control internal computation.
    *   **Continuous Tokens:** Learned embeddings in latent spaces that represent intermediate reasoning states implicitly (e.g., COCONUT, CODI, SoftCoT).
*   **Internal Mechanisms:** These focus on how reasoning emerges implicitly within the model's structure or representations:
    *   **Structural CoT:** Uses architectural depth, recurrence, and looping computations (e.g., CoTFormer, Huginn, RELAY) to simulate iterative reasoning steps.
    *   **Representational CoT:** Internalizes explicit CoT directly into latent hidden states, often via knowledge distillation or fine-tuning (e.g., STaR, ICoT).
*   **Planning with LLMs:** While the paper focuses on *latent* reasoning, it discusses methods that use specialized tokens (like ""planning tokens"") to improve coherence and precision, which relates to planning. Furthermore, the future directions mention that latent CoT is expected to lead to more compact and faster **planning and decision-making** for LLM Agents.
*   **Inference-Time Compute & Test-Time Scaling:** Latent CoT aims for computational efficiency by using compressed representations (continuous tokens) or reusing layers (recurrent structures), which directly impacts inference-time compute. **Test-time scaling** is explicitly mentioned in the context of SoftCoT++, which perturbs the latent space for diverse exploration.
*   **Self-Reflection:** The analysis section mentions **CoE (Chain-",2025-12-25
reasoning_and_planning,exa,https://huggingface.co/papers/2501.14304,MASTER: A Multi-Agent System with LLM Specialized MCTS,Exa,2025-02-20,"The MASTER framework is a novel approach that improves problem-solving in Large Language Models (LLMs) by integrating a **Multi-Agent System** with **Tactical Execution** and a specialized **MCTS (Monte Carlo Tree Search)** algorithm.

The paper addresses limitations of using standard MCTS with LLMs, specifically:
1.  The difficulty in obtaining **objective rewards** for tasks like question answering, unlike games like Go.
2.  The **excessive token usage and time consumption** required for statistically significant reward estimations (typically needing over 30 simulations).

MASTER uses LLM-specialized MCTS to coordinate agent recruitment and communication, autonomously adjusting the number of agents based on task complexity and ensuring focused communication. Experiments show it achieves state-of-the-art performance on HotpotQA (76% accuracy) and WebShop (80% accuracy).

While the query covers broad topics like **reasoning and planning, chain-of-thought, self-reflection, grounding, and hallucination reduction**, the paper specifically focuses on enhancing **planning capability** using a specialized **MCTS** within a **Multi-Agent System** framework.",2025-12-25
reasoning_and_planning,exa,https://arxiv.org/html/2505.00610v1,Combining LLMs with Logic-Based Framework to Explain MCTS,Exa,2019-06-30,"The webpage describes a framework that combines **Large Language Models (LLMs)** with a **Logic-Based Framework** to provide explanations for the **Monte Carlo Tree Search (MCTS)** algorithm, particularly in sequential planning problems like paratransit routing.

Key aspects related to your query:

*   **Reasoning and Planning:** The core focus is on explaining decisions made by MCTS, a planning algorithm used in complex sequential planning problems.
*   **MCTS (Monte Carlo Tree Search) for language models:** The framework is specifically designed to explain MCTS decisions.
*   **Grounding and Factuality:** The framework ensures that explanations are factually consistent with the underlying environmental dynamics and constraints by transforming user queries into logic statements that are evaluated against the MCTS search tree. The evaluation section explicitly measures **factual consistency** (using FactCC) and relevance (using BERTScore), showing significant improvement over using basic LLMs alone.
*   **LLMs:** LLMs are used in multiple components: interpreting user queries, generating logic statements, and generating the final natural language explanations.

The page does not explicitly detail concepts like ""chain-of-thought,"" ""inference-time compute,"" ""self-reflection,"" ""test-time scaling,"" or ""hallucination reduction and detection"" in the context of general LLM reasoning, but it addresses the need for **factuality** and **grounding** in explanations derived from a planning process.",2025-12-25
reasoning_and_planning,exa,https://ui.adsabs.harvard.edu/abs/arXiv:2501.14304,MASTER: A Multi-Agent System with LLM Specialized MCTS - ADS,Exa,unknown,"The webpage mentions that recent studies have incorporated the **Monte Carlo Tree Search (MCTS)** algorithm to augment the **planning capacity of LLMs**, specifically referencing an ""LLM specialized MCTS.""

This directly relates to the user query's interest in **planning with LLMs** and **MCTS (Monte Carlo Tree Search) for language models**.

No other specific topics from the query (like reasoning, chain-of-thought, self-reflection, hallucination reduction, etc.) are explicitly detailed in the provided text snippet.",2025-12-25
reasoning_and_planning,exa,https://arxiv.org/abs/2501.14304,Computer Science > Artificial Intelligence,Exa,2025-01-24,"The webpage describes a paper titled ""MASTER: A Multi-Agent System with LLM Specialized MCTS."" This work addresses the challenges of using **Monte Carlo Tree Search (MCTS)** to augment the **strategic planning capability** of **Large Language Models (LLMs)**.

Specifically, it notes that MCTS struggles with tasks like question answering where objective rewards are unavailable, and that achieving statistically significant results requires excessive token usage and time.

The proposed solution, MASTER, is a novel framework that coordinates agent recruitment and communication using **LLM specialized MCTS**. This system autonomously adjusts the number of agents based on task complexity.

While the query covers broad topics like **reasoning and planning**, **chain-of-thought**, **inference-time compute**, **self-reflection**, **planning with LLMs**, **MCTS for language models**, **test-time scaling**, **hallucination reduction and detection**, **grounding**, and **factuality**, the provided text focuses primarily on **planning with LLMs** and **MCTS for language models**, and demonstrates improved performance on question-answering tasks (HotpotQA and WebShop).",2025-12-25
agents_and_finance,openai,https://arxiv.org/abs/2512.16433,Emergent Bias and Fairness in Multi-Agent Decision Systems,arXiv,2025-12-18,Analyzes emergent bias in multi-agent decision systems and proposes evaluation approaches for high-stakes financial tasks like credit scoring.,2025-12-25
agents_and_finance,openai,https://newsroom.morningstar.com/newsroom/news-archive/press-release-details/2025/Morningstar-and-PitchBook-Bring-Trusted-Investing-Intelligence-to-Apps-in-ChatGPT/default.aspx,Morningstar and PitchBook Bring Trusted Investing Intelligence to Apps in ChatGPT,Morningstar (press release),2025-12-17,Announces live MCP app integrations in ChatGPT so licensed users can query Morningstar + PitchBook public/private market data via natural language prompts.,2025-12-25
agents_and_finance,openai,https://www.globenewswire.com/news-release/2025/12/16/3206057/0/en/FactSet-Meets-Demand-for-AI-Ready-Data-First-to-Announce-MCP-Sans-Intermediary.html,"FactSet Meets Demand for AI-Ready Data, First to Announce MCP Sans Intermediary",GlobeNewswire (FactSet press release),2025-12-16,FactSet launches a production-grade MCP server for agentic/enterprise AI systems to access curated FactSet datasets without custom integrations.,2025-12-25
agents_and_finance,openai,https://www.businesswire.com/news/home/20251216371604/en/Hebbia-Empowers-Platform-Users-with-Preqin-Data-Transforming-Private-Markets-Workflows,Hebbia Empowers Platform Users with Preqin Data; Transforming Private Markets Workflows,Business Wire,2025-12-16,"Adds Preqin private markets datasets into Hebbia for diligence, deal screening, benchmarking, fundraising relationship mapping, and portfolio monitoring.",2025-12-25
agents_and_finance,openai,https://www.blackrock.com/aladdin/discover/press-release-lseg-expands-partnership-with-blackrock,"LSEG expands partnership with BlackRock, strengthening private markets data offering with new Preqin data feeds",BlackRock Aladdin (press release),2025-10-29,"Expands LSEG Workspace/Data & Feeds with Preqin data access, directly relevant to private markets intelligence and downstream AI/agentic consumption.",2025-12-25
agents_and_finance,openai,https://pitchbook.com/short-links/273vY7,PitchBook’s Industry Leading Private Capital Market Intelligence is Now Accessible in Claude,PitchBook (press release),2025-10-28,"General availability of PitchBook private markets data in Anthropic Claude, enabling finance professionals to run conversational research grounded in licensed data.",2025-12-25
agents_and_finance,openai,https://www.junipersquare.com/news/juniper-square-adds-preqin-data-to-its-services-to-help-gps-accelerate-capital-raising,Juniper Square Adds Preqin Data to its Services to Help GPs Accelerate Capital Raising,Juniper Square,2025-10-28,"Integrates Preqin data into Juniper Square’s AI CRM for Investor Relations to enrich LP data, improve targeting, and speed fundraising workflows.",2025-12-25
agents_and_finance,openai,https://www.salesforce.com/ap/news/press-releases/2025/10/16/anthropic-regulated-industries-partnership-expansion-announcement/,"Salesforce, Anthropic Expand Partnership Supporting Regulated Industries",Salesforce (press release),2025-10-16,Describes deeper Claude + Agentforce integration (including financial services) for compliant agentic workflows using CRM context and domain expertise.,2025-12-25
agents_and_finance,openai,https://www.prnewswire.com/news-releases/sp-global-launches-sp-capital-iq-pro-document-intelligence-on-salesforces-agentexchange-transforming-document-analysis-with-generative-ai-302582333.html,"S&P Global Launches S&P Capital IQ Pro Document Intelligence on Salesforce's AgentExchange, Transforming Document Analysis with Generative AI",PRNewswire,2025-10-14,CapIQ Pro “Document Intelligence” on Salesforce’s agent marketplace enables extraction/sentiment from transcripts and filings directly in enterprise workflows.,2025-12-25
agents_and_finance,openai,https://www.alpha-sense.com/press/alphasense-surpasses-500m-in-arr/,AlphaSense Surpasses $500M in ARR as Adoption of Applied AI Workflows Surges,AlphaSense (press release),2025-10-07,"Highlights AlphaSense’s agentic workflow orchestration (e.g., Deep Research, AI Agent Interviewer) and adoption in high-stakes market intelligence.",2025-12-25
agents_and_finance,openai,https://arxiv.org/abs/2510.04643,QuantAgents: Towards Multi-agent Financial System via Simulated Trading,arXiv,2025-10-06,Multi-agent simulated trading framework with role-specialized agents (risk/news/manager) and feedback loops—relevant to agent teams for investing.,2025-12-25
agents_and_finance,openai,https://arxiv.org/abs/2510.01664,GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents,arXiv,2025-10-02,Shows how to encode investor philosophies as prompt-guided agents and evaluate via backtests—useful for portfolio construction and research automation.,2025-12-25
agents_and_finance,openai,https://github.com/yejining99/GuruAgents,yejining99/GuruAgents,GitHub,recent,Open-source code/data for reproducing the GuruAgents investor-persona agent framework and backtesting experiments.,2025-12-25
agents_and_finance,openai,https://www.farsight-ai.com/news/farsight-integrates-with-pitchbook,Farsight Integrates with PitchBook to Accelerate AI-Powered Financial Workflows,Farsight,2025-09-30,Brings PitchBook data into Farsight’s automation engine to accelerate pitch/CIM and client-ready output generation for deal teams.,2025-12-25
agents_and_finance,openai,https://github.com/stefanoamorelli/sec-edgar-mcp,SEC EDGAR MCP (Model Context Protocol) Server,GitHub,2025-09,"MCP server exposing SEC EDGAR filings (XBRL, sections, insider trades) with verifiable URLs—high-value building block for diligence and reporting agents.",2025-12-25
agents_and_finance,openai,https://www.anthropic.com/news/claude-for-financial-services,Claude for Financial Services,Anthropic,2025-07-15,"Anthropic’s finance-focused solution for research/analysis, unifying market + internal data sources for investment and banking workflows.",2025-12-25
agents_and_finance,openai,https://arxiv.org/abs/2505.15155,R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization,arXiv,2025-05-21,"Proposes a coordinated multi-agent pipeline automating quant factor mining, model iteration, and evaluation/backtesting loops.",2025-12-25
agents_and_finance,openai,https://github.com/microsoft/RD-Agent,microsoft/RD-Agent,GitHub,recent,"Codebase for Microsoft’s RD-Agent framework; useful for building multi-agent R&D loops in quant finance (factor discovery, code-gen, backtests).",2025-12-25
agents_and_finance,openai,https://www.v7labs.com/agents/ai-investment-memo-generation-agent,AI Investment Memo Generation Agent | Automate Deal Memos,V7 Labs,recent,Commercial example of a diligence agent that synthesizes CIM/data-room documents to draft IC memos in firm-specific formats (PE/VC workflows).,2025-12-25
agents_and_finance,anthropic,https://rpc.cfainstitute.org/research/the-automation-ahead-content-series/agentic-ai-for-finance,"Agentic AI for Finance: Workflows, Tips, and Case Studies",CFA Institute Research Foundation,recent,"Comprehensive guide from CFA Institute on agentic AI in investment workflows, showing 73% of Y Combinator investment startups are now agentic AI related.",2025-12-25
agents_and_finance,anthropic,https://www.prnewswire.com/news-releases/alphasense-surpasses-500m-in-arr-as-adoption-of-applied-ai-workflows-surges-302576369.html,AlphaSense Surpasses $500M in ARR as Adoption of Applied AI Workflows Surges,PR Newswire,October 2025,AlphaSense milestone showing massive adoption of agentic workflows for financial research with AI Workflow Agents automating due diligence.,2025-12-25
agents_and_finance,anthropic,https://www.getdynamiq.ai/post/ai-agents-for-private-equity-use-cases-kpis-and-deployment-strategy,"AI Agents for Private Equity: Use Cases, KPIs, and Deployment Strategy",Dynamiq AI,recent,"Practical guide for PE firms deploying AI agents for due diligence, financial modeling, and portfolio monitoring with ROI metrics.",2025-12-25
agents_and_finance,anthropic,https://www.prnewswire.com/news-releases/sp-global-transforms-sp-capital-iq-pro-experience-with-the-launch-of-new-generative-ai-powered-capabilities-302300387.html,S&P Global Transforms S&P Capital IQ Pro with ChatIQ and Document Intelligence,S&P Global / PR Newswire,November 2024,S&P's GenAI-powered ChatIQ assistant trained on proprietary Capital IQ data for banking and buy-side analyst workflows.,2025-12-25
agents_and_finance,anthropic,https://www.sciencedirect.com/science/article/pii/S105752192500835X,Generative AI-powered Venture Screening: Can Large Language Models Help Venture Capitalists?,ScienceDirect,November 2025,Academic study showing LLM agents operate 537x faster than human VC analysts without sacrificing categorization quality.,2025-12-25
agents_and_finance,anthropic,https://www.newsletter.datadrivenvc.io/p/the-multi-agent-future-of-vc-will,The Multi-Agent Future of VC: Will We See One-Person Billion-Dollar Funds?,Data Driven VC Newsletter,September 2025,Thought leadership on how multi-agent systems could enable unprecedented VC fund scalability through automation.,2025-12-25
agents_and_finance,anthropic,https://digiqt.com/blog/ai-agents-for-venture-capital/,AI Agents in Venture Capital: Proven Growth Wins,Digiqt Blog,September 2025,"Practical overview of VC agent implementations including EQT's Motherbrain, AlphaSense/Tegus integrations, and workflow automation.",2025-12-25
agents_and_finance,anthropic,https://cloud.google.com/transform/new-research-shows-how-ai-agents-are-driving-value-for-financial-services,New Research Shows How AI Agents Are Driving Value for Financial Services,Google Cloud Blog,September 2025,Google Cloud research showing 77% of financial services executives report positive ROI from gen AI within first year.,2025-12-25
agents_and_finance,anthropic,https://www.toltiq.com,ToltIQ - AI for Private Equity Due Diligence,ToltIQ,recent,"Specialized AI platform for PE due diligence with VDR integration, prompt libraries for financial/legal/ops analysis.",2025-12-25
agents_and_finance,anthropic,https://rogo.ai/news/announcing-our-partnership-with-preqin,Rogo and Preqin Partner to Accelerate Private-Markets Intelligence,Rogo AI,recent,Partnership enabling AI-driven due diligence and LP updates using Preqin's private market data within Rogo's platform.,2025-12-25
agents_and_finance,anthropic,https://www.ravenpack.com/blog/ravenpack-and-preqin-bring-private-markets-intelligence-to-ai-workflows-on-bigdatacom,RavenPack and Preqin Bring Private Markets Intelligence to AI Workflows,RavenPack,October 2025,Integration of Preqin's 135K+ funds data into Bigdata.com's agentic AI platform for unified public/private market analysis.,2025-12-25
agents_and_finance,anthropic,https://tradingagents-ai.github.io/,TradingAgents: Multi-Agents LLM Financial Trading Framework,TradingAgents Project Site,recent,Research project website with detailed architecture for multi-agent trading firm simulation with debate-driven decision making.,2025-12-25
agents_and_finance,anthropic,https://www.pwc.com/us/en/tech-effect/ai-analytics/ai-agents-for-finance.html,How AI Agents Help Drive a New Finance Operating Model,PwC,recent,"PwC guide on AI agents for CFO operations including invoice processing, treasury operations, and forecasting.",2025-12-25
agents_and_finance,anthropic,https://arxiv.org/html/2403.19735v1,Enhancing Anomaly Detection in Financial Markets with LLM-based Multi-Agent Framework,arXiv,March 2024,Research paper on collaborative AI agent network for automated anomaly validation in S&P 500 data.,2025-12-25
agents_and_finance,anthropic,https://www.cbinsights.com/research/report/top-fintech-startups-2025/,Fintech 100: The Most Promising Fintech Startups of 2025,CB Insights,October 2025,"Annual ranking highlighting 17 companies using AI for accounting/treasury automation, plus agentic payments rails development.",2025-12-25
agents_and_finance,anthropic,https://www.fairviewcapital.com/insights/agenticai,Preparing for the Agentic Era in Venture Capital,Fairview Capital,recent,LP perspective on how agentic AI will transform VC operations from sourcing to due diligence to investment memos.,2025-12-25
agents_and_finance,anthropic,https://kpmg.com/us/en/frv/reference-library/2024/guide-ai-and-automation-in-financial-reporting.html,Guide: AI and Automation in Financial Reporting,KPMG,November 2024,Comprehensive KPMG guide on AI governance and internal controls for financial reporting automation.,2025-12-25
agents_and_finance,anthropic,https://www.ibm.com/think/topics/ai-agents-in-finance,AI Agents in Finance,IBM,November 2025,IBM perspective on multi-agent collaboration across finance functions with watsonx Orchestrate case study.,2025-12-25
agents_and_finance,anthropic,https://techcrunch.com/2025/11/26/here-are-the-49-us-ai-startups-that-have-raised-100m-or-more-in-2025/,49 US AI Startups That Have Raised $100M+ in 2025,TechCrunch,November 2025,Comprehensive funding tracker including Sierra ($350M for customer service agents) and major AI infrastructure deals.,2025-12-25
agents_and_finance,exa,https://www.alpha-sense.com/solutions/financial-services/investment-and-corporate-banking/,Give every deal an  agentic AI edge,Exa,2025-05-05,"The webpage describes how **AlphaSense** provides an **agentic AI edge** for investment and corporate banking to transform tasks like pitch preparation, market and comps analysis, buyer discovery, and due diligence.

Key functionalities mentioned include:
*   **Pitch Book Preparation & Market Research:** Building pitches and ramping up on markets using AI-synthesized insights from expert call transcripts, exclusive broker research, and precedent deal notes.
*   **Buyer and Target Identification:** Identifying high-value buyers faster using AI analysis of M&A activity patterns across 40M+ entities globally.
*   **Transaction Comps & Valuation Analysis:** Sourcing accurate transaction comps and multiples quickly, including real-time trading analysis and sector-specific benchmarking.
*   **Deal Prep & Due Diligence:** Accelerating deal execution with risk analysis, expert call insights on threats, and instant analysis of documents like CIMs and VDRs (""Ask in Doc"" functionality).

The benefits highlighted are winning more mandates with deal-winning insights and executing deals faster by automating compilation tasks.

**Regarding the specific terms in your query:**

*   **Multi-agent systems for finance, private equity, venture capital:** The page focuses on **Investment and Corporate Banking** using AI/agentic tools, but does not explicitly detail multi-agent systems for PE/VC, though the capabilities overlap with due diligence and deal sourcing.
*   **Due diligence automation:** Explicitly covered under ""Deal Prep & Due Diligence.""
*   **Investment opportunity analysis, deal sourcing:** Covered by buyer/target identification and market research.
*   **Portfolio monitoring:** Not explicitly mentioned.
*   **Agents for data analysis, financial report generation:** AI synthesis and data extraction (like GenGrid) are mentioned, which relates to data analysis, but specific ""financial report generation"" is not detailed.
*   **PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations:** **AlphaSense** is the platform being advertised. It mentions using exclusive content, including **broker research** and **precedent deal notes**, which often come from sources like PitchBook or Preqin, but **direct integrations** with PitchBook, Preqin, CapIQ, or Bloomberg are **not explicitly listed** in the provided text.",2025-12-25
agents_and_finance,exa,https://www.a.team/solutions/investment-due-diligence-agent,AI Agents for Investment Firms,Exa,unknown,"The webpage describes the use of **AI Agents for Investment Firms**, focusing on how they can accelerate workflows and surface better insights in areas like **private equity**.

Key applications mentioned include:

*   **Investment Due Diligence:** Agents automatically extract KPIs, normalize financial statements, flag anomalies, and pre-populate investment committee materials. Benefits include accelerating decision-making, processing thousands of documents daily, identifying risks (like revenue recognition issues), and auto-generating first-draft IC memos.
*   **Market Position Validation:** Agents analyze target companies against competitive landscapes and verify market size claims, providing real-time competitive intelligence and thesis risk assessment.
*   **Portfolio Reporting:** Agents provide continuous monitoring across holdings, benchmark performance against peers, and identify optimization opportunities through a cross-portfolio KPI dashboard and early warning system.

The system utilizes a team of proprietary agents, including an **AI Data Engineer**, **AI Data Scientist**, and **AI Controller** (for traceability and compliance). The approach emphasizes rapid prototyping (1-3 days) and integration with existing systems.

**Regarding the specific integrations mentioned in your query:** The page mentions **Secure API integrations** with existing systems and lists various AI solutions like **Market Data Agents**, but it **does not explicitly list** integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.",2025-12-25
agents_and_finance,exa,https://www.v7labs.com/agents/ai-private-equity-due-diligence-agent,AI Private Equity Due Diligence Agent,Exa,2025-11-06,"The AI Private Equity Due Diligence Agent automates the first-pass diligence process for Private Equity, Growth Equity, and Venture Capital firms. It reads the entire virtual data room (VDR), extracts and normalizes financials, identifies commercial and legal risks based on the firm's playbook, and generates a structured first draft of the investment committee memo. This process can reduce diligence time from weeks to hours, saving up to 95% of the average time spent.

The agent handles various file types (PDFs, Excel, Word, PowerPoint) and complex tables, synthesizing findings into a comprehensive, cited diligence memo. It also offers other specialized finance agents, such as an **AI Financial Due Diligence Agent**, an **AI Investment Analysis Agent**, and a **Financial Valuation Agent**.",2025-12-25
agents_and_finance,exa,https://www.tensorway.com/projects/deal-sourcing-ai-agent-private-equity,"How We Built  AI Agent That Analyzes 5,000 Investment Opportunities   in a Few Hours",Exa,2000-01-01,"The webpage describes the development of an **Agentic AI Solution** for a **Private Equity Fund** to automate and enhance their **deal sourcing process**.

The solution is a **multi-agent architecture** designed for private equity workflows, featuring:
*   **LLM Core:** Uses models like Azure OpenAI, Gemini, and Anthropic to process documents and evaluate opportunities.
*   **Orchestration Layer:** A lead agent that assigns tasks to specialized sub-agents.
*   **Knowledge Integration:** Connects to internal documents (using GraphRAG), investment guidelines, financial databases, CRM systems, and web data.
*   **Specialized AI Agents:** Includes a **Research Agent** (for target identification and ranking), an **Evaluation Agent** (for standardized assessment), a **Report Agent** (for generating reports), a **Financial Extraction Agent** (for processing unstructured financial documents), and a **Presentation Agent** (for creating executive decks).

The key outcomes achieved include:
*   Analyzing over **5,000 company profiles** in hours.
*   **80% time reduction** for initial screening.
*   **8X faster** investment deck preparation.
*   **12X faster** data visualization.
*   Eliminating manual effort on routine tasks, allowing professionals to focus on strategic analysis.

The system addresses challenges like high resource allocation, slow decision-making due to data volume, fragmented data sources, and lack of standardization in assessments.

**Regarding your specific query:** The page details the use of **multi-agent systems for private equity, due diligence automation, investment opportunity analysis, and deal sourcing**. It mentions the integration of **financial databases** and the generation of **reports** and **investment decks**. However, it **does not explicitly mention** the use of agents for **venture capital**, **portfolio monitoring**, **data analysis** (beyond the specialized agents performing specific analysis tasks), **financial report generation** (though reports are created), or integrations with specific external platforms like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.",2025-12-25
agents_and_finance,exa,https://dealflowagent.com/pevc,AI Agents For Private Equity Funds - Automate Deal Origination And Due Diligence,Exa,2025-04-28,"This page describes customizable AI Agents designed for Private Equity funds to automate deal origination, due diligence, and portfolio monitoring.

**Key functionalities and agents mentioned:**

*   **Deal Origination (James AI):** Monitors 1,000+ marketplaces, 4,000+ news sites, and social media 24/7 to surface and qualify deals based on search criteria, identify intent-to-sell signals, and prepare personalized outreach messages.
*   **Due Diligence (Mary AI):** Analyzes financials, operations, and external risk signals across multiple sources, processes financial reports and filings, flags potential red flags, and structures due diligence findings.
*   **Portfolio Growth & Monitoring (Kunal AI):** Monitors key financial, operational, and cultural indicators post-acquisition, benchmarks KPIs, surfaces integration friction points, and highlights automation opportunities.
*   **General Capabilities:** AI scans filings, databases, and market signals to identify high-intent sellers, score and rank targets, and automate outreach. It can analyze thousands of pages instantly to flag risks and structure findings.

**Integrations:** The process mentions ensuring seamless integration with tools like **PitchBook, CapIQ**, and internal databases.

The system focuses on using multi-agent systems to automate complex tasks in finance, specifically for M&A activities, aiming for increased efficiency and better insights.",2025-12-25
agents_and_finance,exa,https://www.v7labs.com/blog/private-equity-analysis-tools,Private Equity Analysis Tools: A Complete Guide,Exa,unknown,"The webpage discusses the evolution of Private Equity analysis tools, focusing heavily on the shift towards **Agentic AI** systems that can execute multi-step analytical workflows.

**Regarding your specific query on 'agents\_and\_finance':**

The text explicitly details the role of **AI Agents** in finance and private equity analysis, covering several areas mentioned in your query:

*   **Multi-agent systems for finance/Private Equity/Venture Capital:** The core theme is the move to ""Agentic AI systems that can execute multi-step analytical workflows the way a junior analyst would.""
*   **Due diligence automation:** Agents are shown automating data room analysis, extracting financials, normalizing data, and identifying red flags (e.g., the ""AI Financial Due Diligence Agent"").
*   **Investment opportunity analysis:** An ""AI Investment Analysis Agent"" is mentioned that synthesizes reports and earnings calls to build investment theses and bull/bear cases.
*   **Deal sourcing:** A ""Deal Screening Agent"" within V7 Go is described that extracts key metrics from pitch decks to flag companies meeting investment criteria.
*   **Portfolio monitoring:** A ""Portfolio Monitoring Workflow"" using a ""Financial Reporting Agent"" is described to extract, normalize, and populate data from various financial reports.
*   **Agents for data analysis/Financial report generation:** Agents are shown to extract, normalize, and reconcile financial data across multiple sources, and automated report generation (like IC memos) is mentioned in relation to DiligentIQ/ToltIQ.
*   **Integrations (PitchBook, CapIQ, etc.):** The platform **Dili** is mentioned as being able to connect to third-party data providers like **PitchBook** and **Capital IQ** to run workflows over that combined data. Legacy platforms like **PitchBook**, **FactSet**, and **Capital IQ** are discussed as incumbents that AI challengers often integrate alongside.

In summary, the page strongly supports the concept of using agentic AI for nearly all aspects of finance, private equity, and venture capital analysis mentioned in your query, highlighting specific agents and platforms that perform these functions.",2025-12-25
agents_and_finance,exa,https://businessinsider.com/ai-agent-fintech-auquans-pitch-deck-seed-round-2025-1,"This startup is bringing AI agents to banks and money managers including UBS, Blue Owl Capital, and T. Rowe Price. Here's the deck it used to raise $8 million.",Exa,2025-01-31,"Auquan is a startup that uses generative AI agents to automate research and data processing tasks for finance firms, aiming to reduce the manual work typically done by junior analysts or bankers.

**Key aspects related to your query:**

*   **Multi-agent systems for finance:** Auquan uses an ""agent super orchestrator"" that breaks down jobs and organizes several ""mini agents"" to handle specific tasks, such as data searching, analysis, and report generation.
*   **Due diligence automation:** Due-diligence reports are a major use case; the startup automates the creation of these reports for clients.
*   **Investment opportunity analysis/Deal sourcing:** The technology is used across various divisions, including private-market investing and investment banking, to produce documents like investment committee memos and pitch books.
*   **Agents for data analysis/Financial report generation:** The system automates gathering and processing data and putting that information into written templates (e.g., PowerPoint presentations or Google Docs).
*   **Integrations:** Auquan pulls data from providers like **CapIQ** and **Pitchbook**, as well as public data sets and client internal file systems. (Note: **AlphaSense**, **Preqin**, and **Bloomberg** are not explicitly mentioned as integrations in the provided text, though FactSet is mentioned alongside CapIQ and PitchBook).
*   **Clients:** Auquan has secured major clients including UBS, Blue Owl Capital, and T. Rowe Price.

The system is designed to mimic human analysts by accessing raw data, understanding user intent via templates, and automatically generating the required output.",2025-12-25
agents_and_finance,exa,https://www.hopkins.pe/,AI Agent for Private Equity,Exa,2025-07-10,"The webpage describes **Hopkins**, an AI Agent designed for **Private Equity** to automate and accelerate the entire diligence process, deal sourcing, and portfolio monitoring.

It functions as an AI partner for faster, smarter deal analysis, tackling tasks such as:

*   **Deal Sourcing:** Continuous crawling of deal databases, press, and social media to surface thesis-fit targets.
*   **Due Diligence & IC:** Analyzing data rooms (VDR Red-Flagger), auditing leadership track records, and finding customer references.
*   **Document Analysis & Reporting:** Converting Confidential Information Memorandums (CIMs) into partner-ready investment PPT decks and two-page summaries.
*   **Portfolio Monitoring:** Tracking KPIs, detecting variance, and building market maps.
*   **Other Functions:** Exit prep, reviewing passed investments, fundraising readiness checks, and composing meeting briefs.

Hopkins integrates with data rooms, CRMs, and document stores to learn the investment playbook and deliver decision-ready outputs, significantly reducing human time spent on these tasks (e.g., VDR review reduced from 1 day to 20 minutes).

While the query lists several specific platforms like PitchBook, AlphaSense, Preqin, CapIQ, and Bloomberg, the webpage **does not explicitly mention** integrations with these specific third-party data providers.",2025-12-25
agents_and_finance,exa,https://menlovc.com/perspective/backing-trove-ai-agents-for-private-equity/,Backing Trove AI: Agents for Private Equity,Exa,2025-10-14,"Trove (formerly Mako) is an AI associate purpose-built for private equity firms, aiming to deliver associate-level AGI to free up professionals for judgment-driven work. It addresses the manual nature of financial workflows by directly integrating with a firm's apps and data sources (deal/portfolio files, emails, CRM) to create a searchable knowledge base.

Trove's capabilities include:
*   **Unlocking firm knowledge:** Indexing internal data for quick surfacing of metrics, files, and insights.
*   **Investor-grade intelligence:** Independently executing private equity workstreams like analyzing data rooms, pulling metrics from past deals, and generating portfolio reports in the firm's style, using a specialized agent capable of complex reasoning and adaptation.
*   **Security:** Utilizing a ""cloud-prem"" deployment model, allowing the application to reside entirely within the customer's private cloud.

The company recently secured a \$7.1M seed round led by Menlo Ventures. The founders are Danny Goldman (CEO), who has private equity experience from Bain, and Shivaal Roy (CTO), who has experience building enterprise AI at Glean and Scale AI.",2025-12-25
agents_and_finance,exa,https://www.alpha-sense.com/,Accelerate your workflow with  AI insights you can trust,Exa,unknown,"The webpage describes **AlphaSense**, an enterprise-grade platform for financial and business professionals that uses AI to accelerate workflows and provide trusted insights.

Regarding your query about **multi-agent systems for finance and related activities**:

*   **AI Workflow Agents:** AlphaSense features **AI Workflow Agents** that automate time-consuming workflows like ramp-ups, earnings prep, competitive benchmarking, and **diligence** (which relates to due diligence). With a single click, these agents search, synthesize, and deliver reports or insights.
*   **Investment Focus:** The platform is used by professionals in **Investment Banking**, **Hedge Funds**, and **Private Equity**.
*   **Data Sources:** It integrates various data sets, including standardized financials (from Canalyst), expert transcripts (Tegus), broker research, company filings, and public/private financial data.
*   **Due Diligence:** Royalty Pharma is mentioned as using AlphaSense and Expert Insights for **due diligence**.
*   **Integrations/Specific Tools:** While the page heavily features **AlphaSense** itself and mentions **Tegus** and **Canalyst**, it **does not explicitly mention** integrations with **PitchBook, Preqin, CapIQ, or Bloomberg**, nor does it detail agents specifically for deal sourcing, investment opportunity analysis, portfolio monitoring, or financial report generation beyond the general capabilities of the Workflow Agents.

In summary, AlphaSense offers AI agents to automate research and diligence workflows relevant to finance and private equity, but it does not confirm support for all the specific external platforms listed in your query.",2025-12-25
agents_and_finance,exa,https://beam.ai/use-cases/private-equity,Automate Private Equity with AI Agents: How Beam AI Transforms Fund Management | Beam AI,Exa,unknown,"Beam AI utilizes specialized AI agents to automate and transform private equity fund management. These agents address challenges in areas like **budget management** (automating financial tracking, reporting, and budget comparison across portfolio companies) and, for real estate focused firms, **property management** (automating maintenance, tenant communication, and financial tracking).

The platform also supports **automated fund valuations** for enhanced decision-making, real-time monitoring, and risk management. Furthermore, Beam AI's **Budget Proposal tool** streamlines the creation of funding requests for firms raising capital or assisting portfolio companies.

Beam AI emphasizes **multi-agent capabilities** for comprehensive automation and **seamless integration** with existing tools. Future applications mentioned include **advanced due diligence**, **portfolio optimization**, and **investor relations** automation. The text focuses on private equity but also mentions applications for venture capital processes (streamlining budget proposals).

The page **does not explicitly mention** agents for deal sourcing, investment opportunity analysis, portfolio monitoring (beyond general valuation/optimization), or integrations with specific data providers like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",2025-12-25
agents_and_finance,exa,https://hollandmountain.com/ai-agents-in-private-equity/,AI Agents in Private Equity: Hype or Reality? - Holland Mountain,Exa,2025-05-28,"AI agents can be utilized in Private Equity primarily in two areas: **investment processes** and **investor relations**.

In **investment processes**, agentic workflows can greatly benefit complex tasks like **deal origination** (collecting, filtering, and scoring data).

In **investor relations**, integrating AI-powered add-ins into tools like CRM, Word, PowerPoint, Excel, and Outlook can streamline workflows. A compelling use case mentioned is **automating responses to DD (due diligence) questionnaires** using an AI agent with access to a historical knowledge base.

The text describes AI agents as systems capable of executing specific actions, with more advanced forms involving **agentic workflows** where a primary agent coordinates multiple specialized assistants (e.g., one for legal interpretation, another for financial modeling) to accomplish complex tasks collaboratively.

The text **does not explicitly mention** agents for:
*   Venture capital (only Private Equity is discussed)
*   Portfolio monitoring
*   Specific integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.
*   Financial report generation.",2025-12-25
agents_and_finance,exa,https://www.multimodal.dev/agentic-ai-for-private-equity-portfolio-companies,Agentic AI for Private Equity Portfolio Companies,Exa,unknown,"The webpage describes an Agentic AI solution called **AgentFlow** designed to automate and improve operational efficiency for **Private Equity (PE) portfolio companies**, particularly those in the **finance and insurance** sectors.

The solution focuses on addressing issues like document chaos, decision lag, and compliance risks associated with manual processes.

**Key features and capabilities mentioned include:**

*   **Specialized AI Agents:**
    *   **Unstructured AI:** Transforms unstructured data into machine-friendly JSON.
    *   **Document AI:** Speeds up document processing.
    *   **Conversational AI:** Provides chat-based access for due diligence (e.g., underwriting, loans).
    *   **Database AI:** Enables natural language queries against large datasets.
    *   **Decision AI:** Automates decisions, reducing underwriting errors.
    *   **Report AI:** Generates compliance reports, memos, and summaries quickly.
*   **Targeted Workflows:** Automation examples are provided for **Finance** (Mortgage Underwriting, KYB/KYC) and **Insurance** (Claims Processing, Underwriting).
*   **Deployment & Security:** The platform is deployed on-premises or in a Virtual Private Cloud (VPC) to ensure data security, privacy, and compliance, with no client data used to train public models.
*   **PE Firm Benefits:** Focuses on measurable value creation, rapid ROI (full integration in less than 12 weeks), and a Human-in-the-Loop design compliant with regulations like SEC Rule 15l-2.

**Regarding the specific terms in your query:**

*   **Multi-agent systems for finance, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring:** The platform uses multiple AI Agents (AgentFlow) and explicitly mentions use cases like due diligence (via Conversational AI) and automating finance workflows.
*   **agents for data analysis, financial report generation:** Covered by Database AI and Report AI, respectively.
*   **PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations:** **No direct mention** of integration with these specific data providers is found on the page.",2025-12-25
agents_and_finance,exa,https://openai.com/index/hebbia/,"Hebbia’s deep research automates 90% of finance and legal work, powered by OpenAI",Exa,2025-03-20,"Hebbia's Matrix platform is a **multi-agent AI system** powered by OpenAI models (including o3-mini, o1, and GPT-4o) designed to automate complex financial and legal workflows, aiming to automate **90% of finance and legal work**.

This system uses **agent swarms** to break down complex queries, route tasks to the best AI model, process full documents, and synthesize answers with citations, effectively giving the models an ""infinite"" context window for deep research over offline data.

Specific applications and value delivered in finance include:
*   **Private equity firms** saving 20–30 hours per deal on screening, **due diligence**, and expert network research.
*   **Investment bankers** saving 30–40 hours per deal on marketing materials and client prep.
*   **Private credit teams** automating the extraction of loan terms and covenants.

The platform focuses on achieving high accuracy for deep research tasks over private, offline information, which is a key limitation for standard RAG tools.",2025-12-25
agents_and_finance,exa,https://www.leewayhertz.com/ai-agent-for-private-equity/,"AI agent for private equity: Key components, applications, implementation and benefits",Exa,2024-07-11,"AI agents are transforming the private equity sector by optimizing decision-making, enhancing due diligence, and providing deeper insights through advanced data analytics. They can automate tasks like **deal sourcing and screening**, **pre-screening and risk analysis**, **portfolio management**, **exit strategies**, **data aggregation and analysis**, **real-time monitoring and alerts**, **customizable dashboards and reports**, **scenario planning**, **contract management**, **cash flow estimation**, **audits**, and **anti-fraud checks**.

Key components of these agents include input processing, cognitive functions (profiling, memory, knowledge, planning), and execution. They address challenges such as manual data analysis, inefficient lead generation, limited predictive insights, risk management difficulties, operational inefficiencies, and assessing market sentiment.

The article focuses on AI agents in **private equity** and how they can be built, but it **does not specifically mention** the use of agents for **venture capital**, **PitchBook**, **AlphaSense**, **Preqin**, **CapIQ**, or **Bloomberg integrations**.",2025-12-25
agents_and_finance,exa,https://www.linkedin.com/posts/tarikmoody_how-jp-morgan-built-an-ai-agent-for-investment-activity-7333955130725175298-nxMN,Untitled,Exa,2025-05-29,"The webpage describes a post by Tarik Moody detailing how David Odomirok and Zheng Xue from JP Morgan Chase Private Bank built ""Ask David,"" a sophisticated **multi-agent AI system** designed to **automate investment research** for thousands of financial products.

The system uses a **multi-agent architecture** including supervisor agents, specialized sub-agents, and human-in-the-loop workflows. The development evolved from simple ReAct agents to complex sub-graph systems, improving accuracy from 50% to over 90% for domain-specific queries. The post highlights the necessity of **human SME oversight** for achieving 100% accuracy in high-stakes environments and discusses lessons on handling structured vs. unstructured data.

While the post focuses on **multi-agent systems for investment research** within a major financial institution (JP Morgan), it **does not explicitly mention** private equity, venture capital, due diligence automation, deal sourcing, portfolio monitoring, financial report generation, or integrations with specific platforms like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",2025-12-25
agents_and_finance,exa,https://v7labs.com/agents/ai-due-diligence-agent,AI Financial Due Diligence Agent,Exa,2025-09-03,"The webpage describes an **AI Financial Due Diligence Agent** designed to automate the first-pass due diligence process for **Private Equity, Venture Capital, and Corporate Development**.

Key capabilities relevant to your query include:
*   **Due Diligence Automation:** It navigates Virtual Data Rooms (VDRs), extracts and normalizes financials, analyzes commercial contracts, and flags risks.
*   **Financial Analysis:** It performs **Financial Statement Spreading** (extracting data from income statements, balance sheets, and cash flow statements) and populates standardized financial model templates.
*   **Risk Identification:** It proactively searches for diligence risks like customer concentration, margin erosion, and problematic contract clauses.
*   **Cap Table Analysis:** It parses capitalization tables to validate ownership and check for liquidation preference issues.
*   **Data Integration:** It mentions the ability to **Import your files** from sources including **PitchBook** and **S&P Capital IQ** (CapIQ).

The page also mentions other related agents, such as an **AI Investment Analysis Agent** and a **Financial Valuation Agent**, which cover aspects like investment opportunity analysis and deal sourcing/analysis.",2025-12-25
agents_and_finance,exa,https://www.akira.ai/ai-agents/due-diligence-process-automation-ai-agents,Due Diligence Process Automation AI Agents,Exa,2024-12-10,"The webpage describes **Due Diligence Process Automation AI Agents** designed to streamline the due diligence process in areas like investments, partnerships, or acquisitions.

These agents automate:
*   **Data Collection:** Aggregating data from various sources using web scraping and APIs.
*   **Document Analysis:** Extracting key clauses and compliance issues from legal and financial documents using NLP.
*   **Risk Assessment:** Providing more accurate risk understanding by analyzing historical data and market patterns.
*   **Reporting:** Generating real-time insights and reports.
*   **Continuous Monitoring:** Tracking relevant data sources for changes impacting investments post-assessment.

**Use Cases** mentioned include Mergers and Acquisitions, **Venture Capital Investments**, Compliance, Real Estate Transactions, Supply Chain Management, and Corporate Governance.

The user query specifically asks about **agents\_and\_finance** covering private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, data analysis, financial report generation, and integrations with platforms like PitchBook, AlphaSense, Preqin, CapIQ, and Bloomberg.

The webpage directly addresses:
*   **Due diligence automation** (the core topic).
*   **Venture Capital Investments** (a specific use case).
*   **Investment opportunity analysis** (implied through risk assessment and decision-making).
*   **Agents for data analysis** and **financial report generation** (covered by automated reporting and analysis capabilities).

However, the page **does not explicitly mention or detail** the following aspects of the query:
*   **Private Equity** (only VC is explicitly listed).
*   **Deal sourcing**.
*   **Portfolio monitoring**.
*   Specific integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.

While the ""Agentic Finance and Procurement"" section mentions monitoring spend, vendors, and contracts, it is a general feature and not specifically tied to the due diligence agents or the requested financial data platforms.

**Conclusion:** The page covers several aspects of the query related to AI agents in finance and due diligence, but it **does not provide a complete answer** regarding private equity specifics, deal sourcing, portfolio monitoring, or the named third-party financial data integrations.",2025-12-25
agents_and_finance,exa,https://relevanceai.com/agent-templates-tasks/due-diligence-process-automation-ai-agents-ai-agents,Due Diligence Process Automation AI Agents AI Agents,Exa,2024-09-12,"The webpage discusses the use of **AI agents for Due Diligence Process Automation**, which is transforming how financial institutions handle complex transactions.

**Key aspects covered include:**

*   **What it is:** Using AI agents to streamline and enhance the investigation of business or investment opportunities by sifting through data, spotting patterns, and generating insights quickly.
*   **Benefits:** Dramatically accelerating the process, providing consistency and scalability, freeing up human experts for high-value tasks, and continuously improving accuracy over time.
*   **Use Cases:** AI agents are shown to be effective in **Venture Capital** for evaluating startups (digesting financial statements, flagging red flags, identifying market trends) and in **Real Estate** for property acquisition due diligence (analyzing documents, forecasting property value appreciation).
*   **Tasks AI Agents can handle:** Document analysis, risk assessment, financial statement analysis, market research, competitor intelligence, compliance checking, and scenario modeling.
*   **Challenges:** Technical hurdles like data integration and accuracy, and operational hurdles like change management, maintaining human oversight, regulatory compliance, and high implementation costs.
*   **Future:** The future of finance involves **Augmented Intelligence**, where the symbiosis of AI capabilities and human expertise leads to smarter, faster decision-making.

**Regarding your specific query about finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, and specific integrations (PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg):**

The page explicitly covers **due diligence automation**, **venture capital**, **investment opportunity analysis** (implied through due diligence), **agents for data analysis**, and **financial report generation** (implied through financial statement analysis and preparation).

However, the page **does not mention** private equity, deal sourcing, portfolio monitoring, or specific integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.

**Summary relevant to your query:**
AI agents are transforming due diligence in finance, particularly for **Venture Capital**, by automating data analysis, risk assessment, and financial statement review to accelerate deal evaluation and uncover insights. They handle tasks like document analysis and financial modeling, augmenting human experts. While the page confirms the use of AI agents for financial due diligence and data analysis, it does not detail their application in private equity, deal sourcing, portfolio monitoring, or integration",2025-12-25
agents_and_finance,exa,https://dealflowagent.com/dd,AI Agents For Due Diligence - Automate Deep Research On Companies And Markets,Exa,2025-04-28,"The webpage describes customizable AI Agents designed to automate and enhance various aspects of the investment and M&A process, which aligns with the user query about **agents and finance**, including **due diligence automation**, **deal sourcing**, and **portfolio monitoring**.

Here is a summary based on the user query:

The service offers **AI Deal Assistants** (like ""Mary - Due Diligence"") that leverage thousands of sources to uncover deals, analyze risks, and optimize integrations.

**Key functionalities mentioned include:**

*   **Due Diligence:** AI analyzes financials, operations, and external risk signals across multiple sources, processes financial reports and filings to surface red flags, identifies inconsistencies, and structures findings. This aims to reduce manual workload and speed up risk assessment.
*   **Deal Origination & Intent Analysis:** AI scans filings, databases, and market signals to surface investor-ready opportunities, identifies high-intent sellers (tracking leadership shifts, funding, etc.), scores and ranks targets, and automates outreach.
*   **Portfolio Management & Efficiency:** AI monitors financials, risks, and synergies in real-time post-acquisition, tracking portfolio health, detecting early risks, and optimizing synergy tracking.
*   **Integrations:** The process involves integrating workflows with existing tools, specifically mentioning integration with **PitchBook**, **CapIQ**, Salesforce, and HubSpot. (AlphaSense and Preqin are not explicitly mentioned, but the general scope covers database integration).

The overall goal is to transform fragmented tools into an AI-driven process, completing days of work in minutes.",2025-12-25
agents_and_finance,exa,https://v7labs.com/agents/ai-data-room-analysis,Data Room Analysis Agent,Exa,2025-09-03,"The webpage describes the **Data Room Analysis Agent** by V7 Go, which is designed to automate the first-pass review of data rooms for due diligence, primarily targeting **Private Equity, Investment Banking, and M&A Teams**.

Key features relevant to your query include:
*   **Due Diligence Automation:** It automatically indexes files, identifies key commercial and financial documents, extracts critical data, and flags risks to accelerate go/no-go decisions, reducing review time from days to hours (98% time saved).
*   **Financial Data Extraction:** It consolidates key financials from various documents (CIM, audited statements, management presentations) into a single summary table.
*   **Risk Detection:** It proactively searches documents for predefined deal-breakers and risks, such as problematic contract clauses or customer concentration issues.
*   **Diligence Checklist Automation:** It can search the data room to find answers and supporting documents for items on a standard diligence checklist.
*   **Natural Language Q&A:** The indexed data room becomes a knowledge hub where users can ask complex questions in plain language and receive instant, cited answers.
*   **Document Handling:** It supports importing files from VDRs like Intralinks and Merrill Datasite, and handles various formats including PDFs, spreadsheets (Excel), tables, and graphs, supporting over 50 languages and low-quality scans.

While the page focuses heavily on due diligence automation and financial data extraction within a data room context, it **does not explicitly mention** multi-agent systems for general finance, private equity/venture capital deal sourcing, portfolio monitoring, financial report generation, or direct integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**. It does mention other related agents like an ""AI Investment Analysis Agent"" and a ""Financial Valuation Agent.""",2025-12-25
agents_and_finance,exa,https://zbrain.ai/agents/Utilities/all/Due-Diligence/ai-due-diligence-agent/,Agents-internal,Exa,2024-09-13,"The webpage describes the **AI Due Diligence Agent**, which automates company research and analysis for due diligence.

While the user query is broad, focusing on **""agents\_and\_finance: Multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations""**, the provided text specifically details an agent for **due diligence automation** and **financial analysis**.

**Summary relevant to the query:**

The **AI Due Diligence Agent** automates company research by gathering and analyzing data from multiple sources to streamline due diligence, providing real-time insights, financial analysis, and risk monitoring. It addresses challenges like manual research limitations and data incompleteness by orchestrating searches across various databases and professional networks to generate comprehensive, standardized reports.

The agent's process includes:
1.  **Initial Company Research:** Domain discovery and profile verification.
2.  **Multi-Source Data Collection:** Gathering organizational data (from sources like Apollo and LinkedIn), financial/competitor insights, employee sentiment (Glassdoor), news monitoring (Google News API), and patent research.
3.  **Knowledge Base Enhancement:** Integrating historical data.
4.  **Report Generation:** Synthesizing data into structured, actionable reports.
5.  **Human Feedback Integration:** Continuous improvement based on user input.

The agent emphasizes **scalability, accuracy, and seamless integration** with financial APIs, professional networks, and databases.

**Note on specific integrations mentioned in the query:** The text mentions working with ""financial APIs, professional networks, and databases"" and specifically names **Glassdoor** and **LinkedIn** as data sources, but it **does not explicitly mention** PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg integrations.",2025-12-25
agents_and_finance,exa,https://www.stack-ai.com/blog/how-to-build-a-due-diligence-ai-agent,How to Build a Due Diligence AI Agent,Exa,2025-11-20,"The provided webpage details how to build a **Due Diligence AI Agent** using Stack AI to automate the time-consuming due diligence process common before major financial transactions like mergers.

This AI agent is designed for the **Finance** industry and the **Investment Research Department**, with an **Investment Analyst** persona. It addresses the problem of manual due diligence taking too long by using five instances of GPT-4o, leveraging data sources like **Web Search, Yahoo Stock Ticker, and LinkedIn** to quickly output:
*   Company size
*   Revenue in 2024
*   Key people
*   Business overview
*   Industry

The benefits include reducing comparison time from 4 hours to 15 minutes, allowing analysts to focus on key tasks, and helping firms avoid bad investments.

While the page focuses on **due diligence automation** within finance, it **does not mention** multi-agent systems for private equity, venture capital, investment opportunity analysis, deal sourcing, portfolio monitoring, financial report generation, or integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.",2025-12-25
agents_and_finance,exa,https://www.solulab.com/ai-agents-for-due-diligence/,Transform Due Diligence with Automation AI Agents,Exa,2025-01-24,"The webpage discusses the use of **AI Agents for Due Diligence Automation**, focusing on how they transform processes in areas like investment analysis.

**Key points relevant to your query:**

*   **AI Agents for Data Analysis:** AI agents are used to automate the examination of massive datasets, improving accuracy and providing deeper insights into potential risks and opportunities.
*   **Due Diligence Automation:** AI can cut document review time by up to 70%, analyze financial data for trends and anomalies, and automate processes like regulatory monitoring, document management, risk assessment, contract review, and data extraction.
*   **Financial Context:** While the page focuses heavily on due diligence (which is critical in finance, private equity, and venture capital), it **does not explicitly mention** the specific platforms you listed: **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg integrations.**

**Summary:** The page details how AI agents automate and enhance the due diligence process through advanced data analysis, document review, and risk assessment, which are core activities in finance, private equity, and venture capital. However, it does not confirm the use of specific third-party financial data platforms like PitchBook or Bloomberg.

**No answer found** for the specific integrations mentioned in your query.",2025-12-25
agents_and_finance,exa,https://www.leewayhertz.com/ai-agents-for-due-diligence/,"AI agents for due diligence: Role, use cases and applications, benefits, and implementation",Exa,2024-07-26,"The provided webpage discusses the role, use cases, applications, benefits, and implementation of **AI agents in due diligence**.

While the user query is very specific about **""agents\_and\_finance: Multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations,""** the webpage focuses primarily on **AI agents for due diligence** in general, covering:

*   **Due Diligence Automation:** AI agents streamline the process by analyzing vast amounts of data (contracts, financial statements) quickly and accurately, reducing review time by up to 70%.
*   **Use Cases:** Includes automated screening, deal sourcing, data automation/analysis, risk identification, financial analysis (ratio analysis, peer comparison), and compliance checks.
*   **Financial Analysis:** AI agents can analyze financial statements to identify risks and opportunities, and forecast financial performance.
*   **Key Components:** Details the structure of these agents (Core, Planning Module, Memory/RAG, Tools, Databases).

**However, the webpage does not explicitly mention or detail the integration or use of AI agents specifically with the following platforms listed in the query:**

*   Private equity or venture capital specific workflows (beyond general deal sourcing/due diligence).
*   Portfolio monitoring.
*   Financial report generation (though data analysis is mentioned).
*   Specific data providers like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg integrations.**

**Summary relative to the query:**

The page confirms that AI agents are transforming **due diligence automation** and are used for **deal sourcing** and **data analysis** within the financial context (e.g., analyzing financial statements). However, it does not provide information on multi-agent systems specifically tailored for private equity/VC, portfolio monitoring, or integrations with specific financial data platforms like PitchBook or Bloomberg.

**No answer found** for the specific integrations and specialized finance applications mentioned in the query.

The webpage focuses on building and implementing **AI agents for due diligence**, detailing the steps involved, challenges, and the services offered by LeewayHertz.

Regarding your specific query about **agents and finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg**:

*   **Due Diligence Automation & Financial Report Generation:** The page extensively covers how LLM-powered AI agents can automate due diligence tasks, including **document review** (analyzing financial statements) and **financial statement analysis** as a custom development task.
*   **Data Analysis & Investment Opportunity Analysis:** The text mentions developing **reasoning and analysis capabilities** for data analysis, pattern recognition, and trend identification, which supports investment analysis.
*   **Finance/PE/VC Context:** While the page discusses due diligence generally, it explicitly mentions tailoring agents for domains like **mergers and acquisitions** and **financial analysis**. LeewayHertz's custom development includes tasks like **Financial statement analysis** and **Market and competitor intelligence gathering**, which are highly relevant to Private Equity and Venture Capital.
*   **Integrations (PitchBook, AlphaSense, etc.):** The page discusses the need for **Knowledge integration systems** and **Seamless integration** with existing systems and data repositories. However, it **does not specifically mention** integrations with external financial data providers like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.

**Summary:** The page provides a comprehensive guide on building AI agents for due diligence, covering automation, financial analysis, and data analysis relevant to finance/investment. However, it **does not explicitly confirm** the integration capabilities with the specific financial data platforms you listed (PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg).",2025-12-25
agent_infrastructure,openai,https://agentskills.io/specification,Agent Skills Specification,Agent Skills (open standard),"Dec 18, 2025 (announced)",Canonical spec for the SKILL.md-based portable “Agent Skills” format adopted across multiple agent products.,2025-12-25
agent_infrastructure,openai,https://developers.openai.com/codex/skills,Agent Skills (Codex),OpenAI Developers (Codex),recent,"OpenAI’s documentation for creating, storing, and invoking skills in Codex CLI/IDE extensions using the Agent Skills standard.",2025-12-25
agent_infrastructure,openai,https://aaif.io/news/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation-aaif-anchored-by-new-project-contributions-including-model-context-protocol-mcp-goose-and-agents-md/,"Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF), Anchored by New Project Contributions Including MCP, goose and AGENTS.md",Agentic AI Foundation (AAIF),"Dec 9, 2025",Major governance/standardization milestone: MCP + goose + AGENTS.md moved under a neutral Linux Foundation umbrella.,2025-12-25
agent_infrastructure,openai,https://github.com/openai/agents.md,"agentsmd/agents.md — AGENTS.md, a simple open format for guiding coding agents",GitHub,Aug 2025 (released),Reference implementation/spec for AGENTS.md—a repo-level convention to provide consistent guidance to coding agents across toolchains.,2025-12-25
agent_infrastructure,openai,https://modelcontextprotocol.io/specification/2025-11-25/changelog,Key Changes - Model Context Protocol (2025-11-25),Model Context Protocol,"Nov 25, 2025","Authoritative MCP spec changelog covering auth discovery, schema dialect, elicitation updates, and governance/process changes.",2025-12-25
agent_infrastructure,openai,https://github.com/modelcontextprotocol/modelcontextprotocol,modelcontextprotocol/modelcontextprotocol,GitHub,recent,Primary repo for MCP specification + schemas + official docs—core reference for any MCP client/server implementation.,2025-12-25
agent_infrastructure,openai,https://github.com/modelcontextprotocol/servers,modelcontextprotocol/servers,GitHub,recent,"Reference MCP servers maintained by the steering group (filesystem, git, memory, fetch, everything) for testing and examples.",2025-12-25
agent_infrastructure,openai,https://www.cloudflare.com/press/press-releases/2025/cloudflare-accelerates-ai-agent-development-remote-mcp/,Cloudflare Accelerates AI Agent Development With The Industry's First Remote MCP Server,Cloudflare Press Release,"Apr 7, 2025","Infrastructure enabling hosted/remote MCP servers (not just local), with auth partnerships and durable workflows for long-running agents.",2025-12-25
agent_infrastructure,openai,https://arxiv.org/abs/2510.16558,Toward Understanding Security Issues in the Model Context Protocol Ecosystem,arXiv,"Oct 18, 2025","Security analysis of MCP hosts/registries/servers (trust boundaries, output verification gaps, hijacking risks) with proposed defenses.",2025-12-25
agent_infrastructure,openai,https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents,Effective harnesses for long-running agents,Anthropic Engineering,"Nov 26, 2025","Practical patterns for durable, multi-session agents (initializer + incremental agent) addressing context-window and continuity problems.",2025-12-25
agent_infrastructure,openai,https://platform.openai.com/docs/guides/agents-sdk,Agents SDK,OpenAI API Docs,recent,"Official docs for OpenAI’s Agents SDK primitives (handoffs, guardrails, sessions, tracing) and production agent architecture.",2025-12-25
agent_infrastructure,openai,https://docs.langchain.com/oss/javascript/langgraph/add-memory,Memory (LangGraph),LangChain Docs,recent,Implementation guidance for short-term + long-term memory in LangGraph via checkpointers/thread persistence—core agent infra.,2025-12-25
agent_infrastructure,openai,https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/,Agents (LlamaIndex),LlamaIndex Docs,recent,"Agent framework docs covering tools, memory configuration, and multimodal agent workflows in LlamaIndex.",2025-12-25
agent_infrastructure,anthropic,https://cloud.google.com/blog/products/ai-machine-learning/announcing-official-mcp-support-for-google-services,Announcing official MCP support for Google services,Google Cloud Blog,"December 10, 2025",Official Google announcement of MCP server support integrated with Gemini 3 and Apigee for enterprise API-to-MCP transformation,2025-12-25
agent_infrastructure,anthropic,https://github.com/anthropics/claude-agent-sdk-python,GitHub - anthropics/claude-agent-sdk-python,GitHub,recent,"Official Anthropic Claude Agent SDK Python repository with custom tools, MCP integration, and hooks for production agent development",2025-12-25
agent_infrastructure,anthropic,https://github.com/openai/openai-agents-python/releases,Releases · openai/openai-agents-python,GitHub,December 2025,"Latest OpenAI Agents SDK releases including GPT-5.1 support, SIP protocol for realtime agents, Python 3.14 compatibility",2025-12-25
agent_infrastructure,anthropic,https://changelog.langchain.com/,LangChain Changelog,LangChain,December 2025,"LangChain updates: MCP Adapters 0.2.0, LangSmith Polly AI assistant, Pairwise Annotation Queues, LangSmith Fetch CLI tool",2025-12-25
agent_infrastructure,anthropic,https://docs.langchain.com/oss/python/releases/changelog,LangChain Python Changelog,LangChain Docs,December 2025,"LangChain v1.1.0 with create_agent provider-specific tool params, Google GenAI SDK rewrite, model profiles feature",2025-12-25
agent_infrastructure,anthropic,https://www.llamaindex.ai/blog/improved-long-and-short-term-memory-for-llamaindex-agents,Improved Long & Short-Term Memory for LlamaIndex Agents,LlamaIndex Blog,recent,LlamaIndex introduces enhanced Memory component with FactExtractionMemoryBlock and VectorMemoryBlock for agent long-term memory,2025-12-25
agent_infrastructure,anthropic,https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/,Memory | LlamaIndex Python Documentation,LlamaIndex Docs,recent,"Comprehensive LlamaIndex agent memory documentation including ChatMemoryBuffer, token limits, and memory block configuration",2025-12-25
agent_infrastructure,anthropic,https://www.startuphub.ai/ai-news/artificial-intelligence/2025/contextforge-mcp-gateway-the-mcp-router-for-ai-agents/,ContextForge MCP Gateway: the MCP router for AI agents,StartupHub AI,"December 16, 2025","IBM's open-source ContextForge MCP Gateway - enterprise MCP router with 30+ security plugins, multi-tenant workspaces, observability",2025-12-25
agent_infrastructure,anthropic,https://github.com/mem0ai/mem0,GitHub - mem0ai/mem0: Universal memory layer for AI Agents,GitHub,recent,"Mem0 open-source agent memory with 41,000 GitHub stars - graph-based memory, 26% accuracy improvement over OpenAI memory",2025-12-25
agent_infrastructure,anthropic,https://techcrunch.com/2025/10/28/mem0-raises-24m-from-yc-peak-xv-and-basis-set-to-build-the-memory-layer-for-ai-apps/,Mem0 raises $24M to build the memory layer for AI apps,TechCrunch,"October 28, 2025","Mem0 raises $24M Series A, AWS selects as exclusive memory provider for Agent SDK, 186M API calls in Q3 2025",2025-12-25
agent_infrastructure,anthropic,https://github.com/huggingface/smolagents,GitHub - huggingface/smolagents: a barebones library for agents,GitHub,recent,"Hugging Face smolagents - minimalist 1000-line agent framework with CodeAgent, sandboxed execution, Hub integration",2025-12-25
agent_infrastructure,anthropic,https://huggingface.co/blog/smolagents,Introducing smolagents: simple agents that write actions in code,Hugging Face Blog,recent,Official smolagents launch announcement - code agents outperform JSON tool-calling by 30% fewer steps and LLM calls,2025-12-25
agent_infrastructure,anthropic,https://simonwillison.net/tags/ai-agents/,Simon Willison on ai-agents,Simon Willison Blog,recent,"Simon Willison's AI agent insights including lethal trifecta security concerns, MCP risks, and context plumbing concepts",2025-12-25
agent_infrastructure,anthropic,https://arxiv.org/abs/2512.02228,"STRIDE: A Systematic Framework for Selecting AI Modalities - Agentic AI, AI Assistants, or LLM Calls",arXiv,"December 1, 2025","Research paper on when to use agentic AI vs assistants vs LLM calls - 92% accuracy in modality selection, 45% reduction in unnecessary agent deployments",2025-12-25
agent_infrastructure,anthropic,https://arxiv.org/html/2512.08296v1,Towards a Science of Scaling Agent Systems,arXiv,December 2025,"First universal equation for agentic systems - scaling principles across 180 configurations, 87% correct architecture selection",2025-12-25
agent_infrastructure,anthropic,https://arxiv.org/abs/2504.19413,Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory,arXiv,April 2025,Academic paper on Mem0 memory architecture achieving 91% lower p95 latency and 90% token savings vs full-context approach,2025-12-25
agent_infrastructure,anthropic,https://learn.microsoft.com/en-us/agent-framework/overview/agent-framework-overview,Introduction to Microsoft Agent Framework,Microsoft Learn,recent,"Microsoft Agent Framework merging Semantic Kernel and AutoGen - multi-agent orchestration, MCP clients, state management",2025-12-25
agent_infrastructure,anthropic,https://karpathy.bearblog.dev/year-in-review-2025/,2025 LLM Year in Review,Karpathy Blog,December 2025,"Andrej Karpathy's year review - Claude Code as first convincing LLM agent, RLVR emergence, vibe coding paradigm shift",2025-12-25
agent_infrastructure,anthropic,https://www.whatllm.org/blog/state-of-llms-december-2025,The State of LLMs: December 2025,What LLM,December 2025,"Analysis of 114 models: benchmarks saturating, agentic capabilities as new frontier, DeepSeek V3.2-Speciale tradeoffs",2025-12-25
agent_infrastructure,exa,https://github.com/langchain-ai/agent-protocol,"Search code, repositories, users, issues, pull requests...",Exa,2024-11-12,"The webpage describes the **Agent Protocol**, which aims to codify framework-agnostic APIs for serving LLM agents in production.

The protocol centers around three core concepts:

1.  **Runs:** APIs for executing an agent, supporting both stateless (one-shot) and background execution paradigms, including waiting for output or streaming results.
2.  **Threads:** APIs for organizing multi-turn agent executions, providing persistent state, history tracking, and concurrency controls.
3.  **Store:** APIs for working with long-term memory, allowing for customizable memory scopes and flexible storage (text and structured data) with CRUD and search capabilities.

The protocol also defines endpoints for **Agent Introspection** (getting agent capabilities) and first-class support for **Messages**, defining a Message spec based on formats used by providers like OpenAI and Anthropic.

While the page details the structure and endpoints of the Agent Protocol (Runs, Threads, Store), it **does not explicitly mention or detail** the following terms from your query: **MCP servers, tool use, agent memory (beyond the Store concept), agentic memory, agent frameworks (other than mentioning LangGraph implements a superset), LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, or agent orchestration.**",2025-12-25
agent_infrastructure,exa,https://github.com/hideya/mcp-langchain-tools-usage,GitHub - hideya/langchain-mcp-tools-ts-usage: MCP Tools Usage From LangChain ReAct Agent / Example in TypeScript,Exa,2025-01-06,"The webpage describes a TypeScript project demonstrating the usage of **Model Context Protocol (MCP) servers** with a **LangChain ReAct Agent**.

Key points related to your query:

*   **Agent Frameworks/Libraries:** It specifically uses **LangChain** (in TypeScript) and leverages the `@h1deya/langchain-mcp-tools` library.
*   **Tool Use/Function Calling:** It shows how to convert MCP server tools into **LangChain-compatible tools** (`StructuredTool[]`) using the `convertMcpToLangchainTools()` utility function.
*   **LLMs/Providers:** It uses **Google GenAI's `gemini-2.5-flash`** as the LLM, with commented-out code for OpenAI and Anthropic LLMs.
*   **Structured Outputs:** The conversion process handles LLM provider-specific schema transformations to prevent compatibility issues, implying support for structured tool definitions.

The page focuses on the integration between LangChain agents and external tools exposed via MCP servers, but it does not explicitly detail general concepts like **agent memory**, **agent orchestration**, or specific SDKs like **OpenAI Agents SDK**, **Anthropic Agents SDK**, or **Google SDK** beyond using Google GenAI for the LLM.",2025-12-25
agent_infrastructure,exa,https://servicesground.com/blog/function-calling-structured-outputs/,3 Powerful Ways Agentic AI Function Calling Transforms Tool Use & Structured Outputs,Exa,2025-11-07,"The provided web page focuses on **tool use, function calling, and structured outputs** as the core mechanisms transforming Agentic AI from text generation to action.

It explains how these features, supported by frameworks like **OpenAI, LangChain, Gemini, and Semantic Kernel**, ensure that AI actions are safe, consistent, and machine-readable by enforcing schemas (often via JSON Schema or Pydantic).

The summary of relevant concepts from your query is:

*   **Tool Use & Function Calling:** The mechanism allowing agents to trigger external actions safely.
*   **Structured Outputs:** Ensures model responses conform to a known schema, making them reliable for downstream systems.
*   **Agent Frameworks:** **LangChain** and **Semantic Kernel** are explicitly mentioned as frameworks that implement and orchestrate tool use. **OpenAI** and **Google (Gemini)** SDKs are discussed for their native function calling capabilities.
*   **Agent Orchestration:** Semantic Kernel is highlighted for its role in enterprise tool orchestration, and LangChain is shown managing multi-step execution with structured tools.

**Concepts from your query that are NOT explicitly detailed in the text:**

*   `agent_infrastructure: MCP servers`
*   `agent memory` / `agentic memory` (Though a related guide on memory is linked, the main text focuses on action/tool use.)
*   `LlamaIndex` (Mentioned in related guides, but not in the main body discussing tool use frameworks.)
*   `Anthropic Agents SDK`
*   `Google SDK` (The text discusses the **Gemini API**, which is part of the Google SDK, but not the SDK generally.)",2025-12-25
agent_infrastructure,exa,https://imdeepmind.com/docs/ml/mcp/langchain,LangChain | imdeepmind,Exa,2025-08-15,"LangChain is a framework that enables Large Language Models (LLMs) to interact with external data sources, tools, and APIs in a structured manner, which is particularly useful when working with **MCP servers**.

**Key functions of LangChain:**
*   **Wrapping APIs/tools:** It registers APIs (including MCP servers) as ""tools"" so LLMs can understand and call them.
*   **Tool Selection:** It decides which tool to call and when, based on user requests.
*   **Orchestration:** It handles multi-step reasoning, allowing LLMs to chain different tool calls together.

**LangChain's role with MCP servers:**
LangChain acts as a middle layer, a tool manager, and an orchestrator between the LLM and one or more **MCP servers**. It registers the functions exposed by the MCP servers (e.g., `get_weather`) as LangChain tools, handling argument parsing and result formatting. This allows the LLM to execute complex, multi-step queries involving different MCP capabilities without requiring custom integration code.

The document focuses on **LangChain** and its use with **MCP servers** for tool use and orchestration, using a weather example. It does not explicitly detail **agent memory**, **agentic memory**, **agent frameworks** beyond LangChain, **LlamaIndex**, **OpenAI Agents SDK**, **Anthropic Agents SDK**, **Google SDK**, **function calling** (though it describes tool calling), **structured outputs**, or **agent orchestration** beyond the context of LangChain managing tool calls.",2025-12-25
agent_infrastructure,exa,https://apipie.ai/docs/blog/top-10-opensource-ai-agent-frameworks-may-2025,Top 10 Open-Source AI Agent Frameworks of May 2025,Exa,2025-05-01,"The webpage discusses the top open-source AI agent frameworks of May 2025 and the capabilities they provide, such as planning, tool use, memory maintenance, and multi-agent collaboration.

Here is a summary of the information relevant to your query:

*   **Agent Frameworks:** The article lists and details 10 top frameworks, including **LangChain/LangGraph**, **CrewAI**, **AG2**, **OpenAI Agents SDK**, **Google Agent Development Kit (ADK)**, **Microsoft Semantic Kernel (SK)**, **Hugging Face SmolAgents**, **LlamaIndex**, **Pydantic AI**, and **Agno**.
*   **Tool Use/Function Calling:** Most frameworks support tool use.
    *   **LangChain/LangGraph** has an extensive tool library.
    *   **OpenAI Agents SDK** focuses on first-class function calling.
    *   **Google ADK** supports OpenAPI specs as tools.
    *   **Pydantic AI** enforces structured outputs via schema-validated function calling.
*   **Agent Memory:** Memory support is a key feature:
    *   **LangChain/LangGraph** supports multiple memory types and persistent context.
    *   **LlamaIndex** specializes in memory via vector stores and indices (RAG focus).
    *   **Semantic Kernel** offers rich abstractions for memory.
*   **Agent Orchestration/Multi-Agent:** Several frameworks focus on orchestration:
    *   **CrewAI** uses role-based collaboration and offers self-organizing or scripted flows.
    *   **AG2** uses an event-driven, multi-agent conversation framework.
    *   **Google ADK** provides explicit constructs for Sequential, Loop, and Parallel agents.
*   **Agentic Memory/Agent Frameworks:** The entire article is about agent frameworks, which inherently deal with agentic memory and reasoning structures.
*   **Structured Outputs:** **Pydantic AI** is highlighted for bringing type safety and schema enforcement to LLM outputs, ensuring structured results.
*   **MCP Servers (Model Context Protocol):** The future trends section mentions the emerging **MCP (Model Context Protocol)**, and the **Google ADK** is noted for supporting it.
*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** **LangChain",2025-12-25
agent_infrastructure,exa,https://playbooks.com/mcp/rectalogic-langchain,LangChain Integration MCP server,Exa,2024-11-26,"The webpage describes the **LangChain Integration MCP server**, which allows **LangChain-powered applications** to use **MCP tools** (Model Context Protocol) for controlled interaction with local resources like the filesystem.

Key points relevant to your query:

*   **Agent Infrastructure/Frameworks:** It specifically integrates with **LangChain** to provide **tool calling capabilities**.
*   **Tool Use/Function Calling:** The MCP server acts as a tool that language models can invoke (e.g., to read files).
*   **MCP Servers:** The core topic is setting up and using this specific MCP server (`langchain-mcp`).
*   **Integration:** Instructions are provided for integrating this server with specific AI environments like **Claude Code**, **Cursor**, and **Claude Desktop**.

The page **does not** explicitly detail:
*   MCP servers in general beyond this specific LangChain integration.
*   Agent memory or agentic memory concepts.
*   OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK.
*   Agent orchestration.
*   Structured outputs (though tool use implies structured interaction).",2025-12-25
agent_infrastructure,exa,https://docs.langchain.com/langgraph-platform/server-mcp,MCP endpoint in Agent Server,Exa,2025-05-12,"The webpage describes the **Model Context Protocol (MCP) endpoint in LangChain's Agent Server**.

**Key Summary Points:**

*   **What MCP is:** MCP is an open protocol for describing tools and data sources in a model-agnostic format, allowing LLMs to discover and use them via a structured API.
*   **Implementation:** Agent Server implements MCP using the Streamable HTTP transport. This allows **LangGraph agents** to be exposed as **MCP tools**.
*   **Endpoint Location:** The MCP endpoint is available at `/mcp` on the Agent Server.
*   **Usage:** MCP-compliant clients (supporting Streamable HTTP) can connect to the Agent Server to use these exposed agents as tools. Examples are provided for connecting via Python (using `langchain-mcp-adapters`) and JavaScript/TypeScript.
*   **Exposing Agents as Tools:** Agents are exposed with their name, description, and input schema. It is recommended to define custom agents with explicit input/output schemas rather than relying on the general `AnyMessage` state.
*   **User-Scoped Tools:** Custom authentication middleware can be used to populate user context, allowing user-scoped MCP tools to be accessed within a LangSmith deployment.
*   **Limitations:** The current LangGraph MCP implementation **does not support sessions**; each `/mcp` request is stateless.
*   **Disabling MCP:** The endpoint can be disabled by setting `""disable_mcp"": true` in the configuration file.

**Regarding the User Query:**

The query lists several concepts related to agent infrastructure, frameworks, and SDKs: *agent\_infrastructure: MCP servers, tool use, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration*.

The webpage directly addresses **MCP servers**, **tool use** (exposing agents as tools), **LangChain** (specifically LangGraph/Agent Server), and **structured outputs** (via defining explicit schemas).

It **does not** provide specific information on:
*   Agent memory or agentic memory.
*   LlamaIndex.
*   OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK.
*   Agent orchestration (beyond the context of LangGraph).

**Summary tailored to the query:**

The page details the **",2025-12-25
agent_infrastructure,exa,https://openai.github.io/openai-agents-python/tools/,Tools - OpenAI Agents SDK,Exa,2011-06-09,"The provided text is documentation for **Tools in the OpenAI Agents SDK**. It details how agents can take actions using three classes of tools:

1.  **Hosted tools:** Built-in tools running on LLM servers, such as `WebSearchTool`, `FileSearchTool`, `ComputerTool`, `CodeInterpreterTool`, `ImageGenerationTool`, and the `HostedMCPTool` (which exposes a remote MCP server's tools).
2.  **Function calling:** Allows using any Python function as a tool, with automatic schema and docstring parsing.
3.  **Agents as tools:** Enables agents to call other agents, facilitating agent orchestration.

The documentation also covers:
*   Returning images or files from function tools.
*   Creating custom function tools directly without using a Python function decorator.
*   Automatic parsing of function arguments and docstrings using `inspect`, `griffe`, and `pydantic`.
*   Customizing agent tools with configuration like `custom_output_extractor` and handling streaming results.
*   Conditionally enabling/disabling tools at runtime using the `is_enabled` parameter, which can accept booleans or callable functions that check context.
*   Handling errors during function tool invocation using a `failure_error_function`.

Regarding your specific query terms:

*   **agent\_infrastructure:** The document describes the tooling aspect of agent infrastructure within the OpenAI SDK.
*   **MCP servers:** Mentioned via the `HostedMCPTool`.
*   **tool use:** This is the central topic of the document.
*   **agent memory, agentic memory:** Not explicitly discussed.
*   **agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** The document focuses specifically on the **OpenAI Agents SDK**. Other frameworks are not mentioned.
*   **function calling:** Covered extensively.
*   **structured outputs:** Implied through Pydantic/schema creation for function arguments, and the ability to return specific types like images/files from tools.
*   **agent orchestration:** Covered under ""Agents as tools,"" where one agent can call others.

**Summary relevant to the query:**

The OpenAI Agents SDK supports **tool use** through three mechanisms: **hosted tools** (including the **HostedMCPTool** for remote **MCP servers**), **function calling** (which supports **structured outputs",2025-12-25
agent_infrastructure,exa,https://docs.retool.com/agents/guides/tools/connect-to-mcp-server,Connect an MCP server to an agent,Exa,unknown,"The webpage explains how to connect an **MCP (Module Context Protocol) server** to a **Retool Agent**.

**Key points:**

*   **MCP Server:** An open standard introduced by Anthropic that allows LLM-applications (clients) to call tools from a remote system (server).
*   **Functionality:** Connecting an MCP server allows an agent to dynamically retrieve, select, and invoke tools from external data sources (like GitHub, Slack, etc.) provided by the server.
*   **Connection Steps:** You add the MCP server as a new tool within the agent's configuration in Retool, providing the server URL and authentication details if necessary.
*   **Authentication:** Retool supports OAuth 2.0 and Basic Auth for MCP resources.
*   **Protocol Versions:** The page lists supported MCP protocol versions, with the latest being ""2025-06-18"".
*   **Exposing Local Servers:** Instructions are provided on using tools like `supergateway` and `ngrok` to expose locally running MCP servers (which often run over `stdio`) via an HTTP gateway so Retool can connect to them.

The content directly relates to **agent infrastructure**, **tool use**, and **agent frameworks** (specifically within the Retool ecosystem).",2025-12-25
agent_infrastructure,exa,https://docs.nvidia.com/aiqtoolkit/1.1.0/index.html,NVIDIA Agent Intelligence Toolkit Overview #,Exa,unknown,"The NVIDIA Agent Intelligence (AIQ) toolkit is a flexible, lightweight, and unifying library designed to easily connect existing enterprise agents to data sources and tools across any framework.

Key features relevant to your query include:

*   **Agent Frameworks:** It is **Framework Agnostic**, working alongside existing agentic frameworks such as **LangChain**, **LlamaIndex**, CrewAI, and Microsoft Semantic Kernel.
*   **Agent Infrastructure/Tool Use:** It supports **Full MCP Support**, allowing you to use the toolkit as an **MCP client** to connect to and use tools served by remote **MCP servers**, or as an **MCP server** to publish tools via MCP.
*   **Tool Use/Function Calling:** Every agent, tool, and workflow exists as a **function call** that can be composed.
*   **Agent Memory/Agentic Memory:** The toolkit **complements any existing agentic framework or memory tool** you are using and is not tied to any specific long-term memory or data source.

The page does not explicitly mention ""tool use"" beyond the function call structure, ""structured outputs,"" ""agent orchestration,"" ""OpenAI Agents SDK,"" ""Anthropic Agents SDK,"" or ""Google SDK.""",2025-12-25
agent_infrastructure,exa,https://becomingahacker.org/integrating-agentic-rag-with-mcp-servers-technical-implementation-guide-1aba8fd4e442?gi=9921cbf26fe6,Integrating Agentic RAG with MCP Servers: Technical Implementation Guide,Exa,2025-03-23,"The webpage details the integration of **Agentic Retrieval-Augmented Generation (RAG)** with **Model Context Protocol (MCP) Servers**.

Here is a summary addressing the components mentioned in your query:

*   **agent\_infrastructure: MCP servers, tool use, agent memory, agentic memory, agent frameworks, function calling, structured outputs, agent orchestration:**
    *   **MCP Servers:** MCP is an open standard that **standardizes how applications provide context to LLMs**, acting as a universal interface (""USB-C port for AI applications""). **MCP servers** are lightweight programs exposing specific capabilities (data sources or tools) via this standard protocol. They are crucial for connecting agents to external data and services consistently.
    *   **Agent Memory/Agentic Memory:** MCP servers are explicitly mentioned as a way to provide **extended memory** for AI agents, managing **long-term contextual data** (like conversation history or user preferences) that exceeds the LLM's context window.
    *   **Tool Use/Function Calling:** In the integrated architecture, **MCP servers essentially serve as the agent’s toolset**. The agent's logic (often implemented via LLM **function calling** abilities or an **agent framework**) decides when to invoke a server via standardized MCP API calls (which appear as function calls to the agent).
    *   **Agent Frameworks/Orchestration:** The system relies on an **Agent (LLM) with planning logic** to orchestrate the flow, deciding what data to retrieve, which MCP server to query, and how to integrate the context. The text discusses various agent types (Routing, Query Planning, ReAct, Plan-and-Execute) that contribute to this **orchestration**.
    *   **Structured Outputs:** While not explicitly detailed as ""structured outputs,"" the data flow involves MCP servers returning results (often in JSON format, as suggested by the RPC/JSON-RPC mention) which are then integrated into the LLM's prompt.

*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:**
    *   The text mentions that if using an agent framework like **LangChain**, the MCP server query can be registered as a **tool function**.
    *   It notes that **Anthropic** introduced the concept of MCP.
    *   It mentions that SDKs exist for Python and TypeScript to implement the MCP",2025-12-25
agent_infrastructure,exa,https://openai.github.io/openai-agents-python/agents/,Agents - OpenAI Agents SDK,Exa,2011-06-09,"The provided web page details the **OpenAI Agents SDK**, focusing on the core building block: the **Agent**.

Here is a summary of the concepts mentioned in your query that are covered by the page:

*   **Agent Frameworks/SDKs:** The page describes the **OpenAI Agents SDK** itself.
*   **Agent Configuration:** Agents are configured with a `name`, `instructions` (system prompt), `model`, and **`tools`**.
*   **Tool Use/Function Calling:** Agents can be equipped with tools (defined via `@function_tool`). The page discusses how to force tool use using the `tool_choice` setting (options include `auto`, `required`, or specifying a tool name).
*   **Structured Outputs:** Agents can be configured to produce specific output types using the `output_type` parameter, which leverages Pydantic objects and forces the model to use **structured outputs** instead of plain text.
*   **Agent Orchestration (Multi-agent systems):** Two design patterns are described:
    1.  **Manager (agents as tools):** A central agent invokes specialized sub-agents exposed as tools.
    2.  **Handoffs:** Peer agents delegate control to a specialized agent that takes over the conversation.
*   **Agent Memory/Context:** The concept of **`context`** is introduced as a dependency-injection tool that serves as a ""grab bag of dependencies and state for the agent run,"" passed to every agent, tool, and handoff.

**Concepts from your query *not* explicitly detailed or named in the text:**

*   MCP servers
*   Agent memory (though context serves as state)
*   Agentic memory
*   LangChain
*   LlamaIndex
*   Anthropic Agents SDK
*   Google SDK

**Summary:**

The OpenAI Agents SDK defines an Agent as an LLM configured with instructions and tools. It supports **tool use** (function calling) and **structured outputs** via Pydantic models. Multi-agent systems can be designed using **manager/orchestration** patterns or **handoffs**. Agents maintain state and dependencies through a configurable **context** object.",2025-12-25
agent_infrastructure,exa,https://research.aimultiple.com/agentic-frameworks/,Top 5 Open-Source Agentic Frameworks,Exa,2025-11-11,"The webpage discusses the **Top 5 Open-Source Agentic Frameworks** (CrewAI, LangChain, OpenAI Swarm, and LangGraph are benchmarked, with AutoGen also mentioned).

Here is a summary of how the mentioned concepts relate to the frameworks discussed:

*   **Agent Frameworks:** The entire page is a comparison of agentic frameworks (LangGraph, AutoGen, CrewAI, OpenAI Swarm, LangChain).
*   **Tool Use/Function Calling:** All frameworks support **tool use**, which enables agents to call external APIs or functions. CrewAI, LangGraph, and AutoGen use **structured functions** (often via annotations). OpenAI Swarm infers function behavior via **docstrings**. LangChain uses explicit interfaces.
*   **Agent Memory:**
    *   **LangGraph** supports in-thread (short-term) and cross-thread (long-term) memory.
    *   **CrewAI** provides layered memory out of the box, using ChromaDB and SQLite for short-term and long-term storage, and supports entity memory.
    *   **LangChain** supports both short-term (in-memory buffers) and long-term memory (integrating with external vector stores).
    *   **AutoGen** uses a contextual memory model for short-term context but lacks built-in persistent memory.
    *   **OpenAI Swarm** is stateless and does not manage memory natively.
*   **Agent Orchestration:**
    *   **LangGraph** excels at complex workflows using a **graph-based (DAG)** approach for fine-grained orchestration.
    *   **CrewAI** uses a role-based, declarative architecture, but its orchestration is limited to linear or loop-based flows.
    *   **LangChain** operates primarily through single-agent patterns, requiring manual orchestration for multi-agent setups.
    *   **AutoGen** uses free-form, asynchronous message passing for collaboration.
    *   **OpenAI Swarm** currently operates via a single-agent control loop using routines.
*   **Structured Outputs:** The page notes that most frameworks include built-in handling for **structured outputs** (e.g., JSON).
*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** **LangChain** is benchmarked and discussed extensively. **Llama",2025-12-25
agent_infrastructure,exa,https://reference.langchain.com/python/langchain_mcp_adapters/,langchain-mcp-adapters ¶,Exa,unknown,"The provided web page details the `langchain-mcp-adapters` package, which facilitates connecting LangChain applications to **MCP servers**.

Here is a summary of how the page relates to your query:

*   **agent\_infrastructure: MCP servers**: The core of the documentation is about connecting to and interacting with **MCP servers** using the `MultiServerMCPClient`.
*   **tool use**: The package includes functionality to load **LangChain-compatible tools** from MCP servers (`get_tools`, `load_mcp_tools`) and manage tool execution via **interceptors** (`ToolCallInterceptor`).
*   **agent frameworks, LangChain**: The package explicitly adapts MCP components (tools, prompts, resources) to be **LangChain-compatible**.
*   **function calling, structured outputs**: The `MCPToolArtifact` indicates that structured content is returned from MCP tool calls, which relates to structured outputs often used in function calling patterns.
*   **agent memory, agentic memory, agent orchestration, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK**: **No direct information** is provided in this document regarding agent memory, agentic memory, orchestration, or specific SDKs like LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK. The focus is purely on the adapter layer between LangChain and MCP servers.

**In summary:** This page describes the LangChain adapter for interacting with **MCP servers** to load **tools** and **prompts**, which are components of an agent infrastructure. It does not cover agent memory, agentic memory, or the other specific agent frameworks/concepts listed in your query.",2025-12-25
agent_infrastructure,exa,https://apidog.com/blog/langchain-mcp-adapters/,How to Integrate LangChain with MCP Servers Using langchain-mcp-adapters,Exa,2025-04-14,"The webpage details how to integrate tools exposed via the **Model Context Protocol (MCP)** servers into **LangChain** applications using the `langchain-mcp-adapters` library.

This library acts as a bridge, converting MCP tools into LangChain-compatible `BaseTool` objects. Key components include:

*   **MCP Server:** Exposes functions (tools) using decorators (`@()`) and can run via `stdio` (subprocess) or `sse` (web service).
*   **`langchain-mcp-adapters`:** Provides the `MultiServerMCPClient` to connect to one or more MCP servers simultaneously, handling transport details (`stdio` or `sse`).
*   **Tool Conversion:** Functions like `load_mcp_tools` fetch tool definitions and wrap them into LangChain `StructuredTool` objects.
*   **Agent Integration:** These converted tools can then be passed directly to **LangGraph** agents (e.g., via `create_react_agent`).

The article provides step-by-step examples for setting up a single math server (`stdio`) and connecting to multiple servers (math via `stdio` and a weather server via `sse`) using the `MultiServerMCPClient`. It also shows how to integrate this setup into a persistent **LangGraph API Server** using an async context manager for client lifecycle management.",2025-12-25
agent_infrastructure,exa,https://mcp.so/server/langchain-mcp-client/datalayer,MCP Servers,Exa,2025-02-20,"The webpage describes the **LangChain MCP Client**, which allows users to connect to **Model Context Protocol (MCP) servers** and utilize **LangChain-compatible language models**.

While the user query lists many related concepts like ""tool use,"" ""agent memory,"" ""agent frameworks,"" ""function calling,"" and various SDKs, the provided text specifically focuses on:

*   **MCP servers** (connecting to them).
*   **LangChain** (the client is LangChain-compatible).

The text **does not** explicitly detail: tool use, agent memory, agentic memory, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, structured outputs, or agent orchestration.

**Summary based on the query and page content:**

The page details the **LangChain MCP Client**, a tool for connecting to **MCP servers** and using **LangChain-compatible LLMs**.",2025-12-25
agent_infrastructure,exa,https://mcpmarket.com/server/langchain-client,LangChain Client: Connect ReAct Agents to MCP Servers,Exa,2025-01-01,"The LangChain MCP Client facilitates the integration of **LangChain agents** with **Model Context Protocol (MCP) servers**. It converts MCP server tools into **LangChain-compatible tools** using the `langchain_mcp_tools` library, allowing for dynamic interaction via a CLI. Configuration involves `.env` and `llm_mcp_config.json5` files for API keys, LLM parameters, and MCP server details. Key features include using any **LangChain-compatible LLM**, connecting to any MCP server, and parallel initialization of multiple servers. Use cases involve automating tasks by chaining MCP server tools and building conversational AI applications that leverage these external services.",2025-12-25
agent_infrastructure,exa,https://docs.langchain.com/use-these-docs,Use docs programmatically,Exa,2025-04-01,"The webpage describes how to use the LangChain documentation programmatically, primarily through the **Model Context Protocol (MCP) server**.

Key points related to your query:

*   **agent_infrastructure:** The page focuses on making the documentation accessible to AI tools and workflows.
*   **agent memory/agentic memory:** The MCP server allows AI applications to query the latest docs in real-time, effectively providing a source of up-to-date knowledge for agents.
*   **agent frameworks:** The documentation is for **LangChain** and **LangGraph**.
*   **tool use/function calling/structured outputs:** While the page doesn't detail these concepts directly, it shows how to connect the documentation as a data source (via MCP) to AI assistants/tools (like Claude Code, Codex CLI, Cursor, VS Code) that would utilize these capabilities.
*   **agent orchestration:** The ability to connect documentation to various tools suggests integration into broader agent workflows.
*   **agent frameworks (LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK):** The page is specifically about **LangChain** documentation. It shows integration examples with **Claude** (Anthropic) and mentions **OpenAI Codex CLI**.
*   **agent_infrastructure: MCP servers:** The core feature discussed is the built-in **Model Context Protocol (MCP) server** provided by the LangChain docs.

**Summary:**

This page explains how to programmatically access the LangChain documentation using its built-in **Model Context Protocol (MCP) server**. This server allows AI applications (like those built with **LangChain** or using tools like **Claude** or **Codex CLI**) to query the documentation in real-time, serving as a source of current information for agents. It provides specific instructions on how to connect this MCP server to various tools, including **Claude Code**, **Claude Desktop**, **Codex CLI**, **Cursor/VS Code**, and **Antigravity**.",2025-12-25
agent_infrastructure,exa,https://docs.oap.langchain.com/setup/mcp-server,MCP Server - Docs by LangChain,Exa,2025-05-12,"The provided web page is documentation from **LangChain** specifically about setting up and configuring an **MCP Server** within the **Open Agent Platform**.

It details how to connect agents to MCP servers that support **Streamable HTTP requests**, covering two main configuration methods:

1.  **Unauthenticated Servers:** Set the `NEXT_PUBLIC_MCP_SERVER_URL` environment variable to the server's URL.
2.  **Authenticated Servers:** Requires setting `NEXT_PUBLIC_MCP_AUTH_REQUIRED=true`. This involves a proxy route (`/api/oap_mcp`) that uses a JWT (like Supabase's) to exchange for an MCP access token, which is then used for subsequent requests.

The page also explains how to **change the MCP Server URL** using a script located in `apps/web/scripts/`, which updates agent configurations via LangSmith authentication.

**Regarding your query:** The page focuses specifically on **MCP Servers** within the LangChain ecosystem. It does **not** provide information on: tool use, agent memory, agentic memory, other agent frameworks (besides LangChain itself), LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, or general agent orchestration.",2025-12-25
agent_infrastructure,exa,https://arxiv.org/html/2504.08525v2,Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks,Exa,2024-01-01,"The user query lists several concepts related to LLM agents, infrastructure, frameworks, and capabilities, such as: `agent_infrastructure: MCP servers`, `tool use`, `agent memory`, `agentic memory`, `agent frameworks`, `LangChain`, `LlamaIndex`, `OpenAI Agents SDK`, `Anthropic Agents SDK`, `Google SDK`, `function calling`, `structured outputs`, and `agent orchestration`.

The provided webpage describes the **Task Memory Engine (TME)**, a structured memory framework for multi-step LLM agent tasks.

Here is a summary of how the page relates to the query terms:

*   **Agent Memory/Agentic Memory:** The core focus of the paper is TME, which is a novel memory architecture designed to overcome the limitations of linear history and shallow buffers by providing a structured, hierarchical representation of task state via the **Task Memory Tree (TMT)**.
*   **Agent Frameworks/Orchestration:** TME is presented as a memory module that enhances task reasoning and execution consistency for LLM-based agents, supporting non-linear workflows, backtracking, and dynamic updates, which are key aspects of agent orchestration.
*   **Structured Outputs:** TME inherently deals with structured state modeling (the TMT nodes store structured input, output, status, etc.) and uses a **Prompt Synthesizer** to generate concise, context-aware prompts based on this structure.
*   **Tool Use/Function Calling:** The related work section mentions **AutoGPT** which tracks tool calls, and the TME architecture is designed to be integrated into agent control loops that execute actions (which often involve tool use or function calling).
*   **LangChain/LlamaIndex/SDKs:** The related work section explicitly mentions **Prompt-based memory (e.g., LangChain’s BufferMemory)** as a traditional approach that TME seeks to improve upon. However, the paper does not discuss or integrate with LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK directly, nor does it mention ""MCP servers.""

**Conclusion:**

The document details a structured memory framework (**TME**) that addresses issues related to **agent memory**, **structured state tracking**, and **token-efficient prompt construction** for LLM agents. It contrasts its approach with existing memory methods, including those used in frameworks like LangChain. However, it **does not mention",2025-12-25
agent_infrastructure,exa,https://arxiv.org/html/2412.15266v1,On the Structural Memory of LLM Agents,Exa,2023-01-01,"The provided web page focuses on the **Structural Memory of LLM Agents**, investigating how different memory structures and retrieval methods impact agent performance across various tasks (multi-hop QA, single-hop QA, dialogue understanding, and reading comprehension).

Here is a summary of the page content relevant to your query terms:

*   **Agent Memory / Agentic Memory:** The core topic of the paper is the memory module in LLM-based agents, exploring its structure and retrieval.
*   **Memory Structures:** The paper evaluates four main structures: **chunks**, **knowledge triples**, **atomic facts**, and **summaries**, as well as **mixed memory** (a combination of all four).
    *   **Chunks** and **Summaries** excel in tasks with lengthy contexts (reading comprehension, dialogue).
    *   **Knowledge Triples** and **Atomic Facts** are effective for relational reasoning and precision (QA tasks).
    *   **Mixed memory** delivers the most balanced and resilient performance, especially in noisy environments.
*   **Memory Retrieval Methods:** Three methods are evaluated: **single-step retrieval**, **reranking**, and **iterative retrieval**.
    *   **Iterative retrieval** is found to be the most effective method across most tasks, as it refines the query over multiple steps.
*   **Agent Frameworks / Tool Use / Function Calling / Structured Outputs / Agent Orchestration:** The paper mentions that LLM-based agent research focuses on areas like planning, reflection, and **external tools utilization** (which relates to tool use/function calling). The memory structures themselves (like knowledge triples) represent a form of **structured output** from the LLM processing raw data. However, the paper does not specifically detail or compare frameworks like LangChain, LlamaIndex, or SDKs (OpenAI, Anthropic, Google), nor does it deeply discuss agent orchestration beyond the memory module's role in the overall agent framework.
*   **agent\_infrastructure: MCP servers:** This specific term is **not mentioned** in the text.
*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** **LangChain** is mentioned only in the implementation details as the tool used for storing vectorized memories. The other specific SDKs and frameworks are **not mentioned**.

**In summary:** The page details an experimental study on memory structures (chunks, triples, facts,",2025-12-25
agent_infrastructure,exa,https://docs.swarms.world/en/latest/swarms/examples/agent_structured_outputs/,Agent Structured Outputs ¶,Exa,2025-08-27,"The provided web page explains how to use **structured outputs** with **Swarms agents** by leveraging **function calling schemas**, which follow the **OpenAI function calling format**.

This feature allows you to define exactly how agents should structure their responses using function schemas, making the outputs easier to parse. The page details the structure of these schemas, provides prerequisites (Python 3.7+, OpenAI API key, Swarms library), and includes a comprehensive code example demonstrating how to define multiple tools (function schemas) for an agent designed for financial analysis. It also outlines the supported schema types and properties, including basic types, formats, enums, and support for nested objects and arrays.

**Regarding your specific query terms:**

The page directly discusses **tool use** and **function calling** in the context of agents, specifically using the Swarms framework. It also touches upon **structured outputs**.

It **does not** mention: **MCP servers**, **agent memory**, **agentic memory**, **agent frameworks** (other than Swarms), **LangChain**, **LlamaIndex**, **OpenAI Agents SDK**, **Anthropic Agents SDK**, **Google SDK**, or **agent orchestration**.",2025-12-25
agent_infrastructure,exa,https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms,The guide to structured outputs and function calling with LLMs,Exa,2025-09-10,"The user query lists several concepts related to **agent infrastructure** and **agent frameworks**, including: `MCP servers`, `tool use`, `agent memory`, `agentic memory`, `agent frameworks`, `LangChain`, `LlamaIndex`, `OpenAI Agents SDK`, `Anthropic Agents SDK`, `Google SDK`, `function calling`, `structured outputs`, and `agent orchestration`.

The provided webpage is titled ""The guide to structured outputs and function calling with LLMs.""

**Summary based on the query:**

The webpage focuses heavily on **structured outputs** and **function calling**, which are key components of building AI agents.

*   **Structured Outputs:** It details methods for getting reliable, machine-readable data (like JSON) from LLMs using API-native features (JSON mode, JSON Schema enforcement) and non-API-native libraries like **Pydantic**, **Instructor**, and **Outlines**. It shows implementation examples for **OpenAI**, **Anthropic (Claude)**, and **Google (Gemini)**.
*   **Function Calling (Tool Use):** It explains that function calling allows LLMs to interact with external tools/APIs by generating structured JSON parameters. It outlines the workflow (Define Schema, Send Request, Extract Call, Execute Function) and discusses automating schema generation from Python code (using `inspect` or **Pydantic**).
*   **Agent Frameworks/SDKs:** The guide explicitly mentions and shows code examples using the **OpenAI SDK**, **Anthropic SDK**, and **Google SDK** in the context of structured outputs and function calling. It also mentions **Pydantic** as a core library for defining structures used by agents.

**What is missing or not explicitly covered:**

The page does **not** discuss:
*   `MCP servers`
*   `agent memory` or `agentic memory` (beyond the context of function calling as a step in an agent chain)
*   Specific mentions of **LangChain** or **LlamaIndex** as frameworks (though the concepts discussed are central to them).
*   `agent orchestration` (beyond the implication of chaining function calls).

**Conclusion:**

The page provides detailed information on **structured outputs**, **function calling**, and the use of **OpenAI SDK**, **Anthropic SDK**, and **Google SDK** for these tasks, which directly addresses several key terms in the user query related to agent

The user query is a list of terms related to building and deploying AI agents, including infrastructure components, frameworks, memory concepts, and specific techniques like function calling and structured outputs.

The provided webpage focuses heavily on **structured outputs** and **function calling** with LLMs, detailing the workflow, schema definition (using JSON Schema and Pydantic), and specific libraries like **Pydantic AI**, **Outlines**, and **Instructor** for achieving reliable, structured data from models like those from OpenAI, Anthropic, and Google.

Here is a summary of how the page addresses the query terms:

*   **Tool use / Function calling:** This is a major topic, detailing the 5-step workflow for using function calling to enable LLMs to interact with external tools and APIs.
*   **Structured outputs:** This is the central theme, explaining why it's needed (for data storage, UI display, and function calling) and covering API-native methods (JSON mode, JSON Schema mode) and libraries.
*   **Agent frameworks / LangChain / LlamaIndex / OpenAI Agents SDK / Anthropic Agents SDK / Google SDK:** The page mentions libraries that help create agents and handle structured outputs/function calling, specifically **Pydantic AI** (which creates agents), and implicitly supports the SDKs of OpenAI, Anthropic, and Google through its discussion of their respective structured output features. It does not explicitly detail LangChain or LlamaIndex.
*   **Agent memory / Agentic memory:** These terms are **not mentioned** in the text.
*   **Agent orchestration:** The concept of chaining steps (like in the function calling workflow) relates to orchestration, and the conclusion mentions that getting structured output is ""just step one"" before ensuring the LLM returns the *right* function call, which is key to orchestration. However, the term itself is **not explicitly defined or detailed**.
*   **MCP servers:** This term is **not mentioned**.

**Summary:**

The guide extensively covers **structured outputs** and **function calling**, explaining the workflow, schema definition (using JSON Schema and Pydantic), and relevant libraries (**Pydantic AI, Outlines, Instructor**) that work with major LLM providers (OpenAI, Anthropic, Google). It touches upon building **agents** and the need for reliable function calls, which is part of **agent orchestration**. However, it does not discuss **MCP servers**,",2025-12-25
agent_infrastructure,exa,https://docs.langchain.com/oss/javascript/langchain/structured-output,Structured output,Exa,2025-04-01,"Structured output in LangChain allows agents to return data in a specific, predictable, and typed format instead of natural language.

Key aspects of structured output:

*   **Implementation:** LangChain's prebuilt ReAct agent (`createAgent`) handles this automatically. The desired schema (Zod or JSON Schema) is set, and the resulting structured data is captured in the `structuredResponse` key of the agent's state.
*   **Response Format Control:** You can define the schema using a Zod schema or a JSON Schema.
*   **Strategy Selection:**
    *   **Provider Strategy:** Used when model providers (like OpenAI, Grok, Gemini) support native structured output via their APIs. This is the most reliable method.
    *   **Tool Calling Strategy:** Used for models that support tool calling but not native structured output. LangChain uses tool calling to enforce the structure.
*   **Customization:** You can customize the message content returned in the conversation history when structured output is generated using the `toolMessageContent` parameter.
*   **Error Handling:** LangChain provides intelligent retry mechanisms for common errors:
    *   **Multiple Structured Outputs Error:** If a model incorrectly calls multiple structured output tools when only one is expected, the agent provides error feedback and prompts a retry.
    *   **Schema Validation Error:** If the output does not match the expected schema (e.g., a rating outside the allowed range), the agent provides specific error feedback detailing the validation failure and retries.
    *   Error handling can be customized by providing a custom message, handling specific exceptions, or disabling retries entirely (`handleError: false`).

The page also mentions connecting documentation to Claude, VSCode, and more via **MCP (Model Context Protocol)** for real-time answers.",2025-12-25
agent_infrastructure,exa,https://medium.com/@akankshasinha247/function-calling-the-engine-behind-structured-outputs-real-world-ai-actions-f370a0b28c4f,Function Calling: The Engine Behind Structured Outputs & Real-World AI Actions,Exa,2025-04-13,"The provided web page focuses on **Function Calling** as the core mechanism that enables Large Language Models (LLMs) to move beyond text generation and perform real-world actions.

**Key takeaways relevant to your query:**

*   **Function Calling:** This capability allows LLMs to invoke external tools, APIs, or scripts using structured inputs (like JSON) and integrate the results back into a coherent response. It is the engine behind structured outputs and real-world AI actions.
*   **Tool Use:** Function calling is synonymous with enabling LLMs to use tools on demand.
*   **Agent Frameworks/Orchestration:** The article mentions using **LangGraph** and **LangChain** to build multi-step agents that utilize function calling (e.g., checking weather, finding events). It also hints at future topics involving **multi-agent systems** and **orchestration**.
*   **Structured Outputs:** Function calling directly supports structured output, often via JSON mode.

**Information not explicitly detailed or missing:**

The page does not specifically detail:
*   **agent\_infrastructure: MCP servers**
*   **agent memory** or **agentic memory** (though it mentions these will be covered in a future article combined with function calling).
*   **OpenAI Agents SDK, Anthropic Agents SDK, Google SDK** (though it mentions OpenAI and Gemini support function calling).
*   **LlamaIndex**",2025-12-25
agent_infrastructure,exa,https://medium.com/@bhavya4995bansal/ai-agents-3-0-deep-dive-into-openai-anthropic-and-google-966ea887ce05,"AI Agents 3.0: Deep Dive into OpenAI, Anthropic, and Google",Exa,2025-04-13,"The user query is a list of keywords related to agent infrastructure and frameworks. The webpage discusses the new agentic AI offerings from **OpenAI, Anthropic, and Google**.

Here is a summary mapping the query terms to the content:

*   **agent_infrastructure / agent frameworks:** The entire article focuses on the new agentic AI offerings from OpenAI, Anthropic, and Google, which serve as new frameworks for building agents.
*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** These specific third-party or SDK names are **not mentioned** in the text.
*   **tool use / function calling:** **OpenAI** highlights **Function Calling** as a key feature for invoking external functions via JSON-like output.
*   **agent memory / agentic memory:** **Anthropic** focuses on the **Model Context Protocol (MCP)** designed to accommodate **longer, more dynamic contexts**, which relates to maintaining memory/context over extended interactions.
*   **agent orchestration:** OpenAI is noted for its **Ease of Orchestration** in handling multi-step flows.
*   **structured outputs:** This is implied by OpenAI's **Function Calling**, which generates a JSON-like output.
*   **MCP servers:** The text mentions the **Model Context Protocol (MCP)** from Anthropic, but not specifically ""MCP servers.""
*   **structured outputs:** Implied by OpenAI's function calling generating JSON-like output.
*   **agent orchestration:** OpenAI is noted for its **Ease of Orchestration** in handling multi-step flows.

**Summary:**

The page details the agentic AI approaches from OpenAI, Anthropic, and Google. **OpenAI** emphasizes **function calling** for easy integration and rapid prototyping. **Anthropic** focuses on the **Model Context Protocol (MCP)** for handling longer contexts and **Tracing Thoughts** for model interpretability. **Google** offers an **agentic ecosystem** tightly integrated with Google Cloud for enterprise readiness. The article does not mention specific frameworks like LangChain or LlamaIndex, nor does it detail specific SDKs beyond the general offerings from the three major providers.

**No answer found** for: LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, MCP servers.",2025-12-25
agent_infrastructure,exa,https://openai.github.io/openai-agents-python/mcp/,Model context protocol (MCP),Exa,2016-06-09,"The webpage describes the **Model Context Protocol (MCP)**, an open standard that dictates how applications expose tools and context to language models, likened to a ""USB-C port for AI applications.""

The **OpenAI Agents SDK** understands multiple MCP transports, allowing agents to reuse existing MCP servers or build new ones to expose tools (like filesystem, HTTP, or connector-backed tools).

Key integration options discussed include:

1.  **Hosted MCP server tools (`HostedMCPTool`):** Tool execution is pushed into OpenAI's infrastructure via the Responses API. This supports streaming results, optional approval flows for sensitive operations, and integration with OpenAI connectors (e.g., Google Calendar).
2.  **Streamable HTTP MCP servers (`MCPServerStreamableHttp`):** For connecting to HTTP servers where the network connection is managed by the user, offering low latency.
3.  **HTTP with SSE MCP servers (`MCPServerSse`):** Similar to Streamable HTTP but specifically for servers implementing HTTP with Server-Sent Events (SSE).
4.  **stdio MCP servers (`MCPServerStdio`):** Used for local subprocesses, communicating over stdin/stdout, useful for quick proofs of concept.

The page also details features like **Tool filtering** (static or dynamic based on context) to control which tools are exposed, the ability for MCP servers to provide **dynamic prompts** to generate agent instructions, and **caching** of tool lists to reduce latency. Tracing is automatically captured for MCP activity.

**Regarding your query:**

The page focuses on the **Model Context Protocol (MCP)** and its implementation within the **OpenAI Agents SDK** for providing tools and context to agents. It mentions concepts related to agent infrastructure like **tool use** and **function calling** (via MCP tools).

However, it **does not explicitly detail or define**:
*   **agent\_infrastructure** (as a general concept)
*   **agent memory** or **agentic memory**
*   **agent frameworks** (LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK are mentioned only in the context of the SDK being discussed or as potential external frameworks, but not defined or compared)
*   **structured outputs** (though tool results can have structured content, the general concept isn't the focus)
*   **agent orchestration**

No answer found",2025-12-25
agent_infrastructure,exa,https://cookbook.openai.com/examples/structured_outputs_multi_agent,Structured Outputs for Multi-Agent Systems,Exa,2025-03-15,"The webpage describes how to use **Structured Outputs** (enforcing a strict schema using the `strict: true` parameter, building on JSON mode and function calling) to build **multi-agent systems**.

The example demonstrates a four-agent system for a data analysis task:
1.  **Triaging agent:** Routes the user query to the relevant specialized agents.
2.  **Data pre-processing Agent:** Handles data cleaning, transformation, and aggregation using tools like `clean_data`.
3.  **Data Analysis Agent:** Performs statistical analysis using tools like `stat_analysis`.
4.  **Data Visualization Agent:** Creates charts using tools like `create_line_chart`.

The core benefit highlighted is that Structured Outputs guarantee that tool calls adhere to the provided schema, increasing robustness and reducing the need for manual argument validation.

**Regarding your specific query terms:**

*   **agent\_infrastructure:** The page details the setup of a multi-agent system, including agent roles and tool definitions.
*   **tool use, function calling, structured outputs:** These are the central themes of the cookbook.
*   **agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, agent orchestration:** These specific terms or frameworks are **not explicitly mentioned** in the provided text, although the concepts described (multi-agent systems, tool use) are foundational to these areas.
*   **MCP servers:** Not mentioned.

**Summary relevant to your query:**

The page focuses on using **Structured Outputs** to build a **multi-agent system** for data analysis, detailing the setup of specialized agents (Triaging, Data Processing, Analysis, Visualization) and their associated **tool use** and **function calling** capabilities to ensure reliable execution via schema enforcement.

**No answer found** for: MCP servers, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK.",2025-12-25
agent_infrastructure,exa,https://doc.agentscope.io/build_tutorial/structured_output.html,Structured Output — AgentScope Doc documentation,Exa,2025-01-01,"The provided web page is documentation for **Structured Output** within the AgentScope framework. It details how to configure an agent to output data in a structured format, specifically JSON dictionaries, using various parsers like `MarkdownJsonDictParser` and `RegexTaggedContentParser`.

Key aspects covered include:
*   **Defining a Parser:** Using `content_hint` and `required_keys` to guide the LLM's output format.
*   **Parsing the Output:** Using the parser's `parse` method on a `ModelResponse` object to extract the structured data.
*   **Error Handling:** How the parser raises errors if the output does not match the expected format.
*   **Advanced Usage:** Using parsers for more complex content (like code snippets) via regular expressions, and **Auto Post-Processing** to automatically route parts of the parsed dictionary to agent memory (`keys_to_memory`), message content (`keys_to_content`), or metadata (`keys_to_metadata`).

The user query lists many topics related to agent infrastructure, frameworks, and SDKs (e.g., `agent_infrastructure: MCP servers`, `LangChain`, `agent memory`, `function calling`, `agent orchestration`).

**Summary relative to the query:**

The document specifically addresses **structured outputs** and **function calling** concepts by showing how to enforce structured responses from LLMs, which is a core component of modern agent frameworks. It also demonstrates how to route parts of the structured output to **agent memory** and message content.

However, the page **does not** discuss:
*   MCP servers
*   Tool use (beyond structured output for function-like responses)
*   Agent frameworks like LangChain or LlamaIndex
*   Specific SDKs (OpenAI, Anthropic, Google)
*   Agent orchestration

**Conclusion:** The page provides specific details on structured output generation and memory integration within the AgentScope context, which partially overlaps with the user's broad query about agent infrastructure components.

**No answer found** for the majority of the specific infrastructure components listed in the query.",2025-12-25
retrieval_and_embeddings,openai,https://docs.cohere.com/release-notes/,Release Notes | Cohere — Cohere’s Rerank v4.0 Model is Here!,Cohere Docs,"Dec 11, 2025","Announces Cohere Rerank v4.0 (pro/fast) with 32k context, multilingual + JSON reranking—major new production reranker for RAG pipelines.",2025-12-25
retrieval_and_embeddings,openai,https://aws.amazon.com/about-aws/whats-new/2025/10/coheres-embed-v4-multimodal-embeddings-bedrock/,Cohere’s Embed v4 multimodal embeddings model now available on Amazon Bedrock,AWS What's New,"Oct 2, 2025",Signals production availability of Cohere Embed v4 for Bedrock users building enterprise semantic search and RAG.,2025-12-25
retrieval_and_embeddings,openai,https://blog.google/technology/developers/file-search-gemini-api/,Introducing the File Search Tool in Gemini API,Google Blog (Google DeepMind),"Nov 6, 2025","Managed RAG feature that handles chunking, embeddings, vector search, and citations—important “RAG without infra” alternative for many teams.",2025-12-25
retrieval_and_embeddings,openai,https://developers.googleblog.com/en/gemini-embedding-available-gemini-api/,Gemini Embedding now generally available in the Gemini API,Google Developers Blog,"Jul 14, 2025",Official GA for gemini-embedding-001 with MRL (dimension scaling) and multilingual support—key proprietary embedding option.,2025-12-25
retrieval_and_embeddings,openai,https://qdrant.tech/blog/qdrant-1.15.x/,Qdrant 1.15 — Smarter Quantization & better Text Filtering,Qdrant Blog,"Jul 18, 2025",Release highlights include MMR reranking plus text-index upgrades and quantization modes—practical improvements for hybrid/vector search stacks.,2025-12-25
retrieval_and_embeddings,openai,https://www.businesswire.com/news/home/20250715309806/en/Qdrant-Launches-Qdrant-Cloud-Inference-to-Unify-Embeddings-and-Vector-Search-Across-Multiple-Modalities,Qdrant Launches Qdrant Cloud Inference to Unify Embeddings and Vector Search Across Multiple Modalities,Business Wire,"Jul 15, 2025","Adds managed embedding generation inside the vector DB workflow, simplifying hybrid/multimodal retrieval architectures.",2025-12-25
retrieval_and_embeddings,openai,https://blog.voyageai.com/2025/08/11/rerank-2-5/,rerank-2.5 and rerank-2.5-lite: instruction-following rerankers,Voyage AI Blog,"Aug 11, 2025",Primary announcement of Voyage rerank-2.5 series—strong reranker option emphasizing instruction-following and long context.,2025-12-25
retrieval_and_embeddings,openai,https://fireworks.ai/blog/embeddings-and-reranking-announcement,Announcing Embeddings and Reranking On Fireworks AI,Fireworks AI Blog,"Oct 9, 2025",Launches serverless endpoints for embeddings + reranking (Qwen3 embedding/rerank models) for scalable RAG workloads.,2025-12-25
retrieval_and_embeddings,openai,https://huggingface.co/nvidia/llama-embed-nemotron-8b,nvidia/llama-embed-nemotron-8b,Hugging Face (Model Card),"Oct 21, 2025",High-performing multilingual text embedding model for retrieval/reranking; useful for state-of-the-art open(-weights) embedding baselines.,2025-12-25
retrieval_and_embeddings,openai,https://huggingface.co/blog/nvidia/llama-embed-nemotron-8b,Llama‑Embed‑Nemotron‑8B Text Embedding Model Ranks First on Multilingual MTEB Leaderboard,Hugging Face Blog,"Oct 21, 2025","Technical overview + evaluation context for llama-embed-nemotron-8b, helpful for model selection and deployment decisions.",2025-12-25
retrieval_and_embeddings,openai,https://huggingface.co/Qwen/Qwen3-Embedding-8B,Qwen/Qwen3-Embedding-8B,Hugging Face (Model Card),recent,Open model card for a strong multilingual embedding model (Qwen3 embedding series) used widely for retrieval and RAG.,2025-12-25
retrieval_and_embeddings,openai,https://github.com/QwenLM/Qwen3-Embedding,Qwen3-Embedding (Embedding + Reranker series repo),GitHub (QwenLM),recent,"Central repo for Qwen3 embedding + reranker families, benchmarks, and usage—useful for implementing open embedding/rerank stacks.",2025-12-25
retrieval_and_embeddings,openai,https://arxiv.org/abs/2509.25085,jina-reranker-v3: Last but Not Late Interaction for Document Reranking,arXiv,"Sep 29, 2025",Introduces Jina’s reranker-v3 architecture for multilingual reranking—relevant for efficient high-quality second-stage ranking.,2025-12-25
retrieval_and_embeddings,openai,https://arxiv.org/abs/2510.08252,ReasonEmbed: Enhanced Text Embeddings for Reasoning-Intensive Document Retrieval,arXiv,"Oct 9, 2025","Proposes embedding training methods tailored to reasoning-heavy retrieval (e.g., BRIGHT), relevant for harder RAG retrieval scenarios.",2025-12-25
retrieval_and_embeddings,openai,https://arxiv.org/abs/2510.22733,E2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker,arXiv,"Oct 26, 2025",Shows how to extend embedding models to do listwise reranking efficiently—useful for simplifying retrieval + rerank stacks.,2025-12-25
retrieval_and_embeddings,openai,https://arxiv.org/abs/2512.05411,A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems,arXiv,"Dec 5, 2025","Practical framework combining chunking strategies, embeddings, and metadata enrichment for improved enterprise RAG retrieval quality.",2025-12-25
retrieval_and_embeddings,openai,https://arxiv.org/abs/2512.00367,Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation,arXiv,"Nov 29, 2025",New semantic chunking methods + evaluation framework; directly relevant to chunking strategies for higher-quality RAG retrieval.,2025-12-25
retrieval_and_embeddings,openai,https://aws.amazon.com/about-aws/whats-new/2025/04/amazon-bedrock-knowledge-bases-hybrid-search-aurora-postgresql-mongo-db-atlas-vector-stores,Amazon Bedrock Knowledge Bases now supports hybrid search for Aurora PostgreSQL and MongoDB Atlas vector stores,AWS What's New,"Apr 10, 2025",Adds hybrid (dense + lexical) retrieval to Bedrock Knowledge Bases for popular vector-store backends—useful for managed RAG deployments.,2025-12-25
retrieval_and_embeddings,openai,https://developers.cloudflare.com/changelog/2025-03-17-new-workers-ai-models/,New models in Workers AI,Cloudflare Changelog,"Mar 17, 2025",Adds bge-m3 embeddings + bge reranker to Workers AI—easy-to-deploy dense+sparse+rerank components for RAG on Cloudflare.,2025-12-25
retrieval_and_embeddings,openai,https://ir.elastic.co/news/news-details/2025/Elasticsearch-Open-Inference-API-now-Supports-Jina-AI-Embeddings-and-Rerank-Model/default.aspx,Elasticsearch Open Inference API now Supports Jina AI Embeddings and Rerank Model,Elastic (Press Release),"Feb 20, 2025",Elasticsearch integration for Jina embeddings/rerankers—important for enterprises building RAG on Elastic’s vector capabilities.,2025-12-25
retrieval_and_embeddings,openai,https://arxiv.org/abs/2508.01405,Balancing the Blend: An Experimental Analysis of Trade-offs in Hybrid Search,arXiv,"Aug 2, 2025",Large empirical benchmark of hybrid search architectures and reranking/fusion tradeoffs—useful for designing hybrid retrieval stacks.,2025-12-25
retrieval_and_embeddings,openai,https://arxiv.org/abs/2505.07233,DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation,arXiv,"May 12, 2025",Agent/RL approach that dynamically adjusts reranking and k-selection based on LLM feedback—relevant for adaptive retrieval and context sizing.,2025-12-25
retrieval_and_embeddings,anthropic,https://aws.amazon.com/blogs/aws/amazon-s3-vectors-now-generally-available-with-increased-scale-and-performance/,Amazon S3 Vectors now generally available with increased scale and performance,AWS News Blog,December 2025,"Major AWS announcement: native vector storage in S3 with up to 90% cost savings versus specialized vector databases, supporting 2 billion vectors per index.",2025-12-25
retrieval_and_embeddings,anthropic,https://cohere.com/blog/rerank-4,Introducing Rerank 4: Cohere's most powerful reranker yet,Cohere Blog,"December 11, 2025","Latest Cohere reranker with 32K context window (4x increase), self-learning capability, and cross-encoder architecture for enterprise search.",2025-12-25
retrieval_and_embeddings,anthropic,https://venturebeat.com/ai/coheres-rerank-4-quadruples-the-context-window-to-cut-agent-errors-and-boost,Cohere's Rerank 4 quadruples the context window to cut agent errors and boost enterprise search accuracy,VentureBeat,"December 22, 2025","Breaking news on Cohere Rerank 4 with self-learning features and benchmarks against Qwen, Jina, and Voyage rerankers.",2025-12-25
retrieval_and_embeddings,anthropic,https://www.llamaindex.ai/blog/llamaindex-newsletter-2025-12-23,LlamaIndex Newsletter 2025-12-23,LlamaIndex Blog,"December 23, 2025",Latest LlamaIndex updates including LlamaParse v2 with 50% cost reduction and new LlamaSplit API for automatic document separation.,2025-12-25
retrieval_and_embeddings,anthropic,https://www.infoq.com/news/2025/12/pinecone-drn-vector-workloads/,Pinecone Introduces Dedicated Read Nodes in Public Preview for Predictable Vector Workloads,InfoQ,December 2025,New Pinecone feature for enterprise-grade predictable performance with hourly per-node pricing for high-throughput applications.,2025-12-25
retrieval_and_embeddings,anthropic,https://aws.amazon.com/blogs/aws/amazon-opensearch-service-improves-vector-database-performance-and-cost-with-gpu-acceleration-and-auto-optimization/,Amazon OpenSearch Service improves vector database performance with GPU acceleration and auto-optimization,AWS News Blog,December 2025,AWS OpenSearch now offers 10x faster vector index building with GPU acceleration and auto-optimization for billion-scale databases.,2025-12-25
retrieval_and_embeddings,anthropic,https://developers.googleblog.com/en/introducing-embeddinggemma/,Introducing EmbeddingGemma: The Best-in-Class Open Model for On-Device Embeddings,Google Developers Blog,"September 4, 2025",Google's new 308M parameter embedding model optimized for on-device RAG with <15ms inference time and sub-200MB memory.,2025-12-25
retrieval_and_embeddings,anthropic,https://blog.voyageai.com/2025/07/23/voyage-context-3/,Introducing voyage-context-3: focused chunk-level details with global document context,Voyage AI Blog,"July 23, 2025","Novel contextualized chunk embedding model that outperforms OpenAI, Cohere, and Jina late chunking by 14-24% on retrieval tasks.",2025-12-25
retrieval_and_embeddings,anthropic,https://arxiv.org/abs/2501.09136,Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG,arXiv,January 2025,"Comprehensive survey on Agentic RAG covering reflection, planning, tool use, and multi-agent collaboration patterns.",2025-12-25
retrieval_and_embeddings,anthropic,https://arxiv.org/html/2506.00054v1,"Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers",arXiv,"May 28, 2025","Major RAG survey covering RAG-Fusion, KRAGEN, LongRAG, SimRAG, and hallucination-aware decoding constraints.",2025-12-25
retrieval_and_embeddings,anthropic,https://arxiv.org/abs/2409.04701,Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models,arXiv,"July 7, 2025 (v3)",Influential Jina AI paper on late chunking technique that preserves document context in chunk embeddings without additional training.,2025-12-25
retrieval_and_embeddings,anthropic,https://arxiv.org/abs/2504.19754,Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation,arXiv,"April 28, 2025",Rigorous comparison of late chunking vs contextual retrieval showing trade-offs between efficiency and semantic coherence.,2025-12-25
retrieval_and_embeddings,anthropic,https://github.com/DEEP-PolyU/Awesome-GraphRAG,Awesome-GraphRAG: A curated list of resources on graph-based retrieval-augmented generation,GitHub,recent,"Comprehensive resource list including HippoRAG2, PIKE-RAG, E²GraphRAG, and latest GraphRAG benchmarks released May 2025.",2025-12-25
retrieval_and_embeddings,anthropic,https://microsoft.github.io/graphrag/,Microsoft GraphRAG Documentation,Microsoft Research,October 2025,Official documentation for Microsoft's GraphRAG combining knowledge graphs with community summaries for enhanced retrieval.,2025-12-25
retrieval_and_embeddings,anthropic,https://ragflow.io/blog/rag-at-the-crossroads-mid-2025-reflections-on-ai-evolution,RAG at the Crossroads - Mid-2025 Reflections on AI's Incremental Evolution,RAGFlow Blog,"July 2, 2025","Industry reflection on RAG's evolution including memory mechanisms, reasoning integration, and multimodality trends.",2025-12-25
retrieval_and_embeddings,anthropic,https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking,Optimizing RAG with Hybrid Search & Reranking,VectorHub by Superlinked,recent,Practical guide on combining keyword and vector search with semantic reranking using Cohere and cross-encoder models.,2025-12-25
retrieval_and_embeddings,anthropic,https://qdrant.tech/articles/hybrid-search/,Hybrid Search Revamped - Building with Qdrant's Query API,Qdrant Blog,recent,"Technical deep-dive on hybrid search with Matryoshka embeddings, sparse vectors, and late interaction reranking in Qdrant.",2025-12-25
retrieval_and_embeddings,anthropic,https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/azure-ai-search-outperforming-vector-search-with-hybrid-retrieval-and-reranking/3929167,Azure AI Search: Outperforming vector search with hybrid retrieval and reranking,Microsoft Tech Community,November 2024,Quantitative benchmarks showing hybrid retrieval + semantic ranking outperforms pure vector search across query types.,2025-12-25
retrieval_and_embeddings,anthropic,https://dev.to/datastax/the-best-embedding-models-for-information-retrieval-in-2025-3dp5,The Best Embedding Models for Information Retrieval in 2025,DEV Community / DataStax,January 2025,"Benchmarks of Voyage-3-large, ModernBERT Embed, Stella, and other embedding models with real-world retrieval performance data.",2025-12-25
retrieval_and_embeddings,anthropic,https://langcopilot.com/posts/2025-10-11-document-chunking-for-rag-practical-guide,Document Chunking for RAG: 9 Strategies Tested (70% Accuracy Boost 2025),LLM Practical Experience Hub,December 2025,Practical guide testing 9 chunking strategies with code examples; semantic chunking with 256-512 tokens optimal.,2025-12-25
retrieval_and_embeddings,anthropic,https://weaviate.io/blog/chunking-strategies-for-rag,Chunking Strategies to Improve Your RAG Performance,Weaviate Blog,"September 4, 2025",Comprehensive overview of pre-chunking vs post-chunking strategies and LLM-based chunking for high-value documents.,2025-12-25
retrieval_and_embeddings,anthropic,https://arxiv.org/html/2506.10408v1,Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic RAG for Industry Challenges,arXiv,"June 12, 2025",Survey categorizing RAG approaches into predefined vs agentic reasoning paradigms with industry application focus.,2025-12-25
retrieval_and_embeddings,anthropic,https://arxiv.org/abs/2509.04139,Enhancing Technical Documents Retrieval for RAG,arXiv,"September 4, 2025",Novel Technical-Embeddings framework with query expansion and contextual summarization for technical documentation RAG.,2025-12-25
retrieval_and_embeddings,anthropic,https://towardsdatascience.com/graphrag-in-practice-how-to-build-cost-efficient-high-recall-retrieval-systems/,"GraphRAG in Practice: How to Build Cost-Efficient, High-Recall Retrieval Systems",Towards Data Science,December 2025,Practical guide on building GraphRAG systems with cost-effective entity extraction and iterative search optimization.,2025-12-25
retrieval_and_embeddings,anthropic,https://finance.yahoo.com/news/vector-database-market-8-945-150100035.html,"Vector Database Market $8,945.7 million by 2030",Yahoo Finance / MarketsandMarkets,"December 4, 2025","Market analysis projecting 27.5% CAGR for vector databases driven by AI, LLMs, and multimodal applications.",2025-12-25
retrieval_and_embeddings,anthropic,https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/introducing-cohere-rerank-4-0-in-microsoft-foundry/4477076,Introducing Cohere Rerank 4.0 in Microsoft Foundry,Microsoft Tech Community,December 2025,"Cohere Rerank 4.0 integration with Azure Search, Agent Service, and multi-LLM support for enterprise RAG.",2025-12-25
retrieval_and_embeddings,exa,https://huggingface.co/blog/embeddinggemma,"Welcome EmbeddingGemma, Google's new efficient embedding model",Exa,2025-07-01,"The webpage introduces **EmbeddingGemma**, Google's new efficient, state-of-the-art multilingual embedding model, perfect for on-device use cases.

Here is a summary of how it relates to your query topics:

*   **Embeddings (new efficient models):** EmbeddingGemma is a new, efficient model with only **308M parameters** and a **2K context window**. It ranks as the highest text-only multilingual embedding model under 500M on the MTEB benchmark.
*   **Vector Databases / Retrieval:** The model is designed for retrieval tasks, using a Gemma3 transformer backbone modified with bi-directional attention (making it an encoder). It supports **Matryoshka Representation Learning (MRL)**, allowing the 768-dimensional output vector to be truncated to 512, 256, or 128 dimensions for faster and cheaper downstream processing like similarity search in vector databases.
*   **RAG Architectures / RAG Alternatives:** The model is explicitly mentioned as unlocking new possibilities for **mobile RAG pipelines** and agents.
*   **Rerankers:** The model supports a specific prompt for **Reranking** tasks (`""Reranking"": ""task: search result | query: ""`).
*   **Hybrid Search:** Not explicitly mentioned, but the model's strong retrieval capabilities support the embedding component of hybrid search.
*   **Chunking Strategies:** Not explicitly mentioned, though the model has a 2048-token context window, which is relevant to chunking decisions in RAG.

The page also details extensive **Usage** instructions across various frameworks relevant to RAG pipelines, including **LangChain**, **LlamaIndex**, **Haystack**, and **txtai**, and provides deployment options via **Text Embeddings Inference (TEI)**. Furthermore, it covers **Finetuning** the model for domain-specific tasks.",2025-12-25
retrieval_and_embeddings,exa,https://openai.com/blog/new-and-improved-embedding-model/,New and improved embedding model,Exa,2022-12-15,"The webpage announces the release of a **new and improved embedding model, `text-embedding-ada-002`**, which replaces five previous models for text and code search.

Key improvements and features mentioned include:
*   **Stronger Performance:** It outperforms all old embedding models on text search, code search, and sentence similarity tasks.
*   **Unification of Capabilities:** It merges five separate models into a single interface.
*   **Longer Context:** Context length increased from 2048 to 8192.
*   **Smaller Embedding Size:** New embeddings have 1536 dimensions, making them more cost-effective for **vector databases**.
*   **Reduced Price:** Priced 99.8% lower than the previous most capable model (Davinci) while achieving similar or better performance.

The page also mentions that embeddings are numerical representations used in applications for personalization, recommendation, and **search** (relevant to **retrieval**). Examples of usage include Kalendar AI matching sales pitches and Notion improving search.

**Regarding the full user query:**
The page directly discusses **embeddings (new efficient models)** and their use in **vector databases** and **search** (which relates to **retrieval**). However, it **does not mention** vector databases specifically (other than cost-effectiveness for them), rerankers, RAG architectures, RAG alternatives, hybrid search, or chunking strategies.

**Summary based on the query:**
The page details the release of the `text-embedding-ada-002` model, highlighting its improved performance, unification of capabilities, longer context, smaller embedding size (benefiting vector databases), and significantly reduced price compared to previous models.",2025-12-25
retrieval_and_embeddings,exa,https://www.digitalocean.com/community/tutorials/beyond-vector-databases-rag-without-embeddings,Beyond Vector Databases: RAG Architectures Without Embeddings,Exa,2025-08-25,"The webpage discusses **Retrieval-Augmented Generation (RAG)** architectures, focusing on alternatives to the standard approach that relies on **vector databases** and **embeddings**.

Here is a summary addressing the components of your query:

*   **Vector databases & Embeddings (new efficient models):** Traditional RAG uses embeddings (numeric vector representations of text) stored in a vector database for semantic search. The article notes limitations like semantic gaps, reduced accuracy, lack of interpretability, and high infrastructure costs associated with this method.
*   **Rerankers:** While not explicitly detailed as a separate section, the concept of improving retrieval accuracy is addressed. Some embedding-free methods, like lexical search followed by an LLM re-rank, are suggested as alternatives to purely vector-based ranking.
*   **RAG Architectures & RAG Alternatives:** The core of the article explores **RAG without embeddings**. Alternatives include:
    *   **Lexical or Keyword-Based Retrieval** (e.g., using BM25 against a full-text engine).
    *   **LLM-based Iterative Search** (e.g., ELITE), where the LLM guides the search process iteratively.
    *   **Structured Knowledge and Graph-Based Retrieval** (e.g., GraphRAG), which uses knowledge graphs to represent relationships between entities.
    *   **Prompt-Based Retrieval (Prompt-RAG)**, which uses the LLM to select relevant document sections based on a structured Table-of-Contents.
*   **Hybrid Search:** The future of RAG is predicted to involve **Hybrid & Adaptive Pipelines**, mixing fast vector search for common queries with fallback methods like graph or agent retrieval for complex reasoning tasks.
*   **Chunking Strategies:** The traditional method involves splitting documents into chunks before embedding. Embedding-free methods sometimes leverage document structure (like headings/TOCs) or utilize larger context windows to potentially reduce the reliance on explicit chunking strategies.

In summary, the page details the limitations of standard vector-based RAG and presents several **embedding-free RAG architectures** that offer benefits like lower cost, better interpretability, and improved precision in specialized domains, suggesting a future of hybrid, adaptive systems.",2025-12-25
retrieval_and_embeddings,exa,https://gpt-trainer.com/blog/rag+chunking+strategy,RAG Chunking Strategy,Exa,2025-05-16,"The webpage provides a detailed overview of **Retrieval-Augmented Generation (RAG)** architectures, focusing heavily on the **chunking strategies** used during document ingestion.

Here is a summary of the topics mentioned in your query that are covered on the page:

*   **Vector databases & Embeddings:** The page describes using **vector search** over an embedded knowledge base, where documents are converted into **embedding vectors** using models (like SentenceTransformers) and stored in a **vector database** for efficient semantic search.
*   **RAG Architectures:** The core RAG pipeline is detailed, involving document ingestion, chunking, embedding, indexing in a vector DB, **retrieval** (online phase), optional **reranking**, and final **generation** with the retrieved context.
*   **Chunking Strategies:** This is the main focus. Strategies discussed include:
    *   Fixed-Length Chunking (with overlap).
    *   Context-Aware (Structure-Based) Chunking (using recursive splitting by delimiters like paragraphs or headings).
    *   **Semantic Chunking** (using similarity checks between sentence embeddings to determine boundaries).
    *   Adaptive/ML-Guided Chunking.
    *   **Hybrid Chunking Strategies** (combining approaches or using context-enriched chunks).
*   **Rerankers:** The optional step of **reranking** is mentioned, where a model re-scores candidate chunks retrieved by vector similarity to improve relevance before feeding them to the final LLM.
*   **Hybrid Search:** The page mentions **hybrid retrieval** combining keyword search (lexical retrieval) with vector search to capture both exact matches and semantic matches.

The page does **not** explicitly discuss ""RAG alternatives"" in detail, though it focuses entirely on one specific, popular RAG implementation (vector-based).",2025-12-25
retrieval_and_embeddings,exa,https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR,"""9 Chunking Strategies for RAG Models""",Exa,2025-05-13,"The webpage provides information on **Chunking Strategies for RAG Models** and discusses several related advanced RAG concepts mentioned in the user query through comments and related posts.

Here is a summary addressing the user query components:

*   **Chunking Strategies:** The main post details **9 Chunking Strategies**: Fixed-Size, Sentence-Based, Semantic-Based, Recursive, Sliding-Window, Hierarchical, Topic-Based, Modality-Specific, and Agentic Chunking. Another post mentions **section-aware chunking** and **semantic chunking** (splitting by idea). A third post discusses **contextual summaries** added to chunks.
*   **Embeddings (new efficient models):** One related post mentions **Fine Tuned Embeddings** for domain-specific improvements and **Instruction Embedding Models** for zero-shot adaptation.
*   **Rerankers:** One related post explicitly mentions **Rerankers** (like Cohere Rerank 3.5) used to reorder top candidates from the vector database for better precision.
*   **RAG Architectures:** The page discusses several architectures:
    *   The main topic is **RAG**.
    *   Related posts mention **Agentic RAG**, **Graph RAG**, and **CAG (Cache Augmented Generation)** as an alternative. Another post mentions 10 RAG architectures including Standard, Agentic, and Multi-Modal.
*   **RAG Alternatives:** **CAG (Cache Augmented Generation)** is mentioned as the opposite of RAG, preloading context instead of retrieving it.
*   **Hybrid Search:** One related post mentions **Hybrid Search** (combining keyword search like BM25 with embeddings) and another mentions **Hybrid retrieval (BM25 + vectors)** as a best practice.
*   **Vector Databases:** While not detailed, vector databases are implied as the storage mechanism for embeddings in RAG systems, and one post mentions **Vector DB comparisons** in a cheat sheet.

**Vector Databases:** Mentioned as part of the RAG system where embeddings are stored, with comparisons noted in a linked cheat sheet.",2025-12-25
retrieval_and_embeddings,exa,https://www.fingoweb.com/blog/what-are-the-best-rag-alternatives/,What are the best RAG Alternatives?,Exa,2025-07-28,"The webpage discusses several promising alternatives to Retrieval-Augmented Generation (RAG) for improving the accuracy and relevance of large language model outputs.

The main RAG alternatives covered are:

1.  **Prompt Engineering with Context Windows:** Injecting domain knowledge directly into the prompt using structured templates. This avoids external retrieval systems, leading to lower latency and complexity, but is limited by context window size and is not scalable for frequently updated knowledge.
2.  **Toolformer and API-Calling Models:** Allowing the language model to autonomously decide when to invoke external tools (like APIs, databases, or calculators) via function calling. This provides adaptive reasoning and improved response quality by pulling in precise, up-to-date information only when necessary.
3.  **LangChain Agents and Function-Calling Architectures:** Using agents as orchestrators to chain reasoning, information retrieval, and external tool execution in iterative loops. This is suitable for complex, multi-step workflows that require dynamic decision-making beyond simple similarity search.
4.  **Fine-tuning with Domain-Specific Data:** Directly training the model on curated, niche data. This is superior to RAG when responses must be highly consistent, deterministic, or when latency is critical, especially in narrow domains. Open-source tools like LoRA, PEFT, and Axolotl facilitate efficient fine-tuning.

The article also notes that the most lightweight alternative often involves combining simple retrieval mechanisms with advanced prompt engineering.",2025-12-25
retrieval_and_embeddings,exa,https://arxiv.org/html/2509.00100v1,MODE: Mixture of Document Experts for RAG,Exa,2025-01-01,"The webpage describes **MODE (Mixture of Document Experts)**, a lightweight and efficient alternative framework for Retrieval-Augmented Generation (RAG), particularly suited for small to medium-sized datasets.

Here is a summary addressing the components mentioned in your query:

*   **Vector databases:** MODE **eliminates the need for dedicated vector databases** by replacing fine-grained vector search with a cluster-and-route mechanism.
*   **Embeddings (new efficient models):** The framework uses embeddings for initial chunking and for calculating cluster centroids, but the core efficiency gain comes from avoiding large-scale vector search over these embeddings during inference.
*   **Rerankers:** MODE **eliminates the need for re-rankers** entirely, as the cluster-and-route mechanism aims to provide highly topical context directly, reducing latency.
*   **RAG architectures:** MODE is presented as an **architectural alternative** to the standard ""index-retrieve-rerank"" pipeline. It partitions the corpus into semantically coherent clusters (""document experts"") and routes queries to the most relevant cluster using fast centroid-based matching.
*   **RAG alternatives:** MODE itself is the proposed alternative, focusing on simplicity and efficiency for non-web-scale corpora.
*   **Hybrid search:** The paper does not explicitly discuss hybrid search (combining sparse and dense retrieval). It focuses on replacing dense retrieval with a clustering approach.
*   **Chunking strategies:** The ingestion phase includes a step for **Chunking and Embedding**. The paper mentions using 300-token windows with 15% overlap as a default, and defines corpus sizes based on the number of chunks (small=100, medium=200, large=500).

In essence, MODE achieves efficiency by performing **cluster-based routing** instead of nearest-neighbor search across a massive vector index, thereby simplifying the RAG architecture significantly.",2025-12-25
retrieval_and_embeddings,exa,https://www.pinecone.io/learn/series/rag/rerankers/,Rerankers and Two-Stage Retrieval,Exa,unknown,"The webpage discusses **Rerankers and Two-Stage Retrieval** as a method to improve **Retrieval Augmented Generation (RAG)** pipelines when out-of-the-box RAG performance is suboptimal.

Key points related to your query:

*   **RAG Architectures:** It focuses on a two-stage retrieval system where the first stage uses a fast **vector search** (relying on **embeddings**/bi-encoders) to retrieve a larger set of documents, and the second stage uses a slower but more accurate **reranker** to reorder and select the most relevant documents before passing them to the LLM.
*   **Embeddings:** It mentions that the first stage uses embedding models (like `multilingual-e5-large`) to transform text into vectors for similarity search in a **vector database** (Pinecone is used in the example).
*   **Rerankers:** Rerankers (cross-encoders) are significantly more accurate than embedding models because they analyze the query and document pair directly, avoiding the information loss inherent in compressing text into a single vector. They are used to maximize LLM recall by minimizing noise in the context window.
*   **Chunking Strategies:** The example uses pre-chunked data from the `jamescalam/ai-arxiv-chunked` dataset, where each record is 1-2 paragraphs long.

The page does not explicitly detail **new efficient models** for embeddings, **alternatives** to RAG, or **hybrid search** strategies, although it implies that the first stage of retrieval could use sparse embedding models alongside vector search.",2025-12-25
retrieval_and_embeddings,exa,https://www.singlegrain.com/blog-posts/link-building/llm-retrieval-optimization-for-reliable-rag-systems/,LLM Retrieval Optimization for Reliable RAG Systems,Exa,2025-12-05,"The webpage provides a comprehensive guide on **LLM Retrieval Optimization for Reliable RAG Systems**, covering many of the concepts mentioned in your query.

Here is a summary mapping the concepts from your query to the content of the page:

*   **Vector databases / Vector search:** The page discusses **Vector search** as a core component of the retrieval stack, contrasting it with sparse search and mentioning its use in hybrid retrieval. It also mentions **vector stores** in the FAQ regarding building vs. buying a stack.
*   **Embeddings (new efficient models):** The page covers **choosing embeddings** as part of data preparation, noting decisions around model family, dimensionality, and the trade-off between quality and cost.
*   **Rerankers:** **Rerankers** are explicitly listed as a key component in the retrieval stack, used to reorder candidates with higher precision after initial retrieval.
*   **RAG architectures:** The page details the **End-to-end RAG request flow** and the **Components in the retrieval stack**, providing a clear architectural overview.
*   **RAG alternatives:** While the page focuses heavily on optimizing RAG, it contrasts RAG with **standalone LLMs** and discusses how retrieval optimization integrates into broader **AI search strategy**, which implies optimizing the overall information delivery mechanism.
*   **Hybrid search:** **Hybrid search** (combining sparse and dense retrieval) is discussed as a primary retrieval backend option, often yielding the highest quality.
*   **Chunking strategies:** The section on **Data preparation and indexing strategies** details several effective **Chunking strategies** (fixed window, structure-aware, use-case-specific).

In summary, the page thoroughly addresses **Vector databases (via vector search), embeddings, rerankers, RAG architectures, hybrid search, and chunking strategies** within the context of optimizing Retrieval-Augmented Generation (RAG).",2025-12-25
retrieval_and_embeddings,exa,https://www.applied-ai.com/briefings/enterprise-rag-architecture/,Enterprise RAG Architecture: A Practitioner's Guide,Exa,unknown,"The webpage provides a practitioner's guide to **Enterprise RAG Architecture**, detailing the evolution from simple RAG to sophisticated production systems.

Key topics covered relevant to your query include:

*   **Vector Databases:** Discusses the decision matrix, noting that **pgvector** is often sufficient for datasets under 5 million vectors due to its **hybrid query capability** (SQL filtering alongside vector search). Purpose-built databases like **Qdrant**, **Weaviate**, and **Milvus** are recommended for larger scales or specific needs (e.g., multi-modal search, massive scale).
*   **Embeddings:** The guide implies the use of dense embeddings but highlights that **embedding model mismatch** (general models failing on specialized domains) is a common failure mode, suggesting domain-specific models or fine-tuning.
*   **Rerankers:** **Reranking is described as essential** for production systems. The guide details **Two-Stage Retrieval with Reranking** using cross-encoders (like Cohere Rerank) to significantly improve precision after initial retrieval.
*   **RAG Architectures:** It outlines a maturity spectrum:
    *   **Naive RAG** (prototyping baseline) is insufficient for production.
    *   **Advanced RAG** (production standard) incorporates **Hybrid Search** (Vector + Keyword, often using RRF), **Late Interaction (ColBERT)**, and **Two-Stage Retrieval with Reranking**.
    *   **GraphRAG** is covered for queries requiring relationship understanding.
    *   **Agentic RAG** represents the frontier, involving AI agents for multi-step orchestration.
*   **Hybrid Search:** Explicitly stated as **beating pure semantic search**, combining BM25/keyword methods with dense retrieval.
*   **Chunking Strategies:** Discusses **Chunking Strategy Failures**, recommending **structure-aware chunking** (preserving document elements) and **Parent Document Retrieval** (embedding small chunks but retrieving larger context).
*   **RAG Alternatives:** The guide lists scenarios where RAG is **NOT** recommended, such as when content is static and small enough for **fine-tuning** a model, or for exact match requirements best handled by traditional search.",2025-12-25
retrieval_and_embeddings,exa,https://arxiv.org/html/2510.20296v1,"RAG-Stack : Co-Optimizing RAG Quality and Performance 
 From the Vector Database Perspective",Exa,unknown,"The webpage discusses **RAG-Stack**, a blueprint for co-optimizing the **quality and performance** of Retrieval-Augmented Generation (RAG) systems, moving beyond optimizing vector databases in isolation.

The user query lists several key components and concepts related to RAG: *Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies*.

Here is how the webpage addresses these topics:

*   **Vector databases & RAG Architectures:** The paper positions RAG as a prominent application of vector databases and focuses on optimizing the end-to-end RAG pipeline, which includes the database.
*   **Embeddings & Chunking Strategies:** These are explicitly mentioned as part of the **Algorithm Configuration Space** (Table 1) that influences generation quality. Specifically, the choice of **embedding model** and **document chunking strategies** are tunable knobs.
*   **Rerankers & Hybrid Search:** **Result reranking** is listed as an algorithmic factor influencing quality. **Hybrid search** (combining vector search with keyword-based matching like BM25) is listed under the ""Retrieval method"" configuration.
*   **New efficient models & RAG alternatives:** While the paper acknowledges the continuous emergence of new retrieval techniques and models, it focuses on a framework (**RAG-Stack**) to evaluate them rather than introducing specific new models or alternatives to RAG itself.

**Summary of RAG-Stack's approach to these components:**

RAG-Stack addresses the complexity by systematically exploring the vast configuration space, which includes:

1.  **Algorithmic Factors (Quality-related):** Document chunking, choice of embedding models, Top-K, query rewriting, and result reranking.
2.  **System Factors (Performance-related):** Hardware choices (CPU/GPU), software frameworks, and database indexing/quantization.

The framework uses three pillars to manage this complexity:
*   **RAG-IR:** An abstraction layer that captures the performance-relevant attributes of the algorithm configuration (including chunking, embedding model parameters, Top-K, etc.) without capturing quality-only details.
*   **RAG-CM:** A cost model to predict system performance based on the RAG-IR representation.
*   **RAG-PE:** A plan exploration algorithm to efficiently search for",2025-12-25
retrieval_and_embeddings,exa,https://airev.us/retrieval-augmented-generation,Retrieval-Augmented Generation,Exa,2025-01-01,"The webpage provides a technical deep dive into Retrieval-Augmented Generation (RAG), covering several components mentioned in your query:

*   **Vector databases:** These are specialized databases (like Faiss, Qdrant, Pinecone, Weaviate) used to store chunk embeddings for efficient Approximate Nearest Neighbor (ANN) similarity search.
*   **Embeddings (new efficient models):** The text discusses the centrality of embeddings for semantic similarity and mentions popular solutions like **OpenAI's text-embedding-3**, **Cohere's Embed 4** (noted for state-of-the-art accuracy and multi-modality), and the open-weights model **e5-large-v2**.
*   **Rerankers:** While the term ""rerankers"" is not explicitly detailed as a separate section, the concept of improving retrieval quality is implied in advanced workflows, such as using **Multi-Vector Retrieval (MMR)** in the LangChain example, which is a technique often used to select diverse and relevant chunks.
*   **RAG Architectures:** The page covers the **High-Level Workflow** (Retrieval then Augmented Generation), **Iterative & Adaptive Retrieval** (multi-hop queries), and **Agentic RAG** (where the LLM acts as an agent using function calling to dynamically decide when and how to retrieve data).
*   **Hybrid search:** This specific term is **not explicitly mentioned** in the text.
*   **Chunking strategies:** Detailed strategies are provided based on document type:
    *   General rule: Chunks should be around **10-20% of the LLM's context window** (e.g., 1k tokens for an 8k window).
    *   For **Books:** Break chapters into $\sim 10-20\%$ of the context window.
    *   For **Wikipedia:** Split by headings to keep related knowledge together.
    *   For **Phone Call Transcripts:** Segment by time and speaker turns.",2025-12-25
retrieval_and_embeddings,exa,https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-information-retrieval,Information retrieval,Exa,2025-10-01,"This page focuses on the **Information Retrieval Phase** of a Retrieval-Augmented Generation (RAG) solution, covering how to configure a search index, choose search approaches, and refine queries.

Here is a summary of the topics relevant to your query:

*   **Vector Databases & Embeddings:** The article discusses configuring the search index in a vector store, specifically mentioning vector search algorithms like **exhaustive k-nearest neighbors (KNN)** and **Hierarchical Navigable Small World (HNSW)** for approximate nearest neighbor (ANN) search. It also notes that the vector column's dimensions must match the output of the embedding model (e.g., 1,536 for `text-embedding-3-small`).
*   **Rerankers:** Reranking strategies are covered, including using a **language model** or a **cross-encoder** (like Roberta) to re-evaluate and rank aggregated search results for better relevance. Azure AI Search also offers proprietary **semantic ranking**.
*   **RAG Architectures & Hybrid Search:** The page details different search approaches: **vector search**, **full-text search**, and **hybrid search** (which combines vector and text searches, often using Reciprocal Rank Fusion for reranking in AI Search). It also describes a complex RAG pipeline that combines query augmentation, decomposition, rewriting, and execution steps.
*   **Chunking Strategies:** While the page focuses on the retrieval phase, it references the **Chunking Phase** documentation for details on chunking strategies.
*   **RAG Alternatives:** The **HyDE (Hypothetical Document Embeddings)** technique is presented as an alternative retrieval method where a generated answer (not the query) is embedded to find similar documents.

The page does not explicitly detail ""new efficient models"" for embeddings, nor does it provide a dedicated section on ""chunking strategies"" (referring to external documentation for that).",2025-12-25
retrieval_and_embeddings,exa,https://milvus.io/docs/v2.0.x/overview.md,Milvus vector database documentation,Exa,2025-02-24,"The Milvus documentation covers several aspects related to your query, particularly concerning vector databases and search:

*   **Vector Databases:** Milvus is a vector database, and the documentation provides information on how to install, use, and deploy it.
*   **Hybrid Search:** ""Hybrid Search"" is listed under ""Recommended articles - Use.""
*   **Rerankers:** The ""What's new in docs"" section mentions guidance on how to use a **decay ranker**.
*   **Embeddings:** The ""What's new in docs"" section mentions guidance on how to use an **embedding function**.

The documentation does not explicitly detail: **new efficient models**, **RAG architectures**, **RAG alternatives**, or **chunking strategies**.",2025-12-25
retrieval_and_embeddings,exa,https://medium.com/@vladris/embeddings-and-vector-databases-732f9927b377,Embeddings and Vector Databases,Exa,2023-08-18,"The user query asks for a summary covering several topics related to Retrieval-Augmented Generation (RAG): **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, and chunking strategies.**

This webpage excerpt focuses primarily on **Embeddings** and **Vector Databases** as a method for implementing memory in Large Language Models (LLMs), which is a core component of RAG.

Here is a summary of the relevant points covered in the text:

*   **Embeddings:** These are numerical representations (vectors) of words or text sequences that capture semantic meaning and relationships. The text discusses using OpenAI's `text-embedding-ada-002` model. Similarity between embeddings is measured using **cosine similarity** (or cosine distance).
*   **Memory based on Embeddings (Basic RAG Concept):** Embeddings are used to implement memory by computing the embedding of user input and comparing it (using cosine distance) to pre-computed embeddings of stored data chunks to retrieve the most relevant context for the LLM prompt.
*   **Vector Databases:** For large datasets, iterating through all embeddings becomes slow. A **vector database** is introduced as a specialized database optimized for storing and efficiently querying high-dimensional vector data. The text demonstrates using **Chroma** as an example vector database to store and retrieve documents based on similarity search.
*   **Other Vector Database Options:** The text lists several other vector database options, including Weaviate, Qdrant, Milvus, Pinecone, and extensions for existing databases like Redis and Postgres (pgvector).

**Topics not covered in detail or mentioned:**

*   **Rerankers:** Not discussed.
*   **RAG Architectures:** The text describes a basic retrieval mechanism but does not detail formal RAG architectures.
*   **RAG Alternatives:** Not discussed.
*   **Hybrid Search:** Not discussed.
*   **Chunking Strategies:** The text mentions that the unit of data (chunk size) is up to the user but does not detail specific strategies.
*   **New Efficient Models (for embeddings):** While `text-embedding-ada-002` is mentioned as the model used, the text does not compare it against newer or more efficient models.",2025-12-25
retrieval_and_embeddings,exa,https://huggingface.co/blog/qdrddr/what-are-embeddings-and-vector-databases,What are Embeddings and Vector Databases?,Exa,2024-08-20,"The webpage explains **Embeddings** as numerical representations of information that allow computers to determine similarity for tasks like search and classification. These embeddings act like ""digital fingerprints"" (vectors) for data, enabling fast, semantic search by finding data points whose vectors are mathematically closest to the query's vector.

The text also covers:
*   **Vector Databases (Vector DB):** Where the encoded numerical representations (vectors) of a dataset are stored, allowing for quick retrieval of relevant information chunks, often as the first phase in a **RAG (Retrieval-Augmented Generation)** application.
*   **How Embeddings Models Work:** They are trained on large datasets to find correlations (e.g., recognizing that ""Pride and Prejudice"" is semantically related to ""First Impressions"").
*   **Why Use Embeddings:** It is faster and easier for computers to process and understand relationships numerically than by searching raw text directly.
*   **Advantages & Disadvantages:** A key advantage is simplifying initial data retrieval without needing a schema. A major disadvantage is the **lack of transitivity** (if A is similar to B, and B is similar to C, A is not necessarily similar to C) and difficulty in synthesizing summarized concepts over large datasets, which can lead to less than 100% accurate RAG results.
*   **Retrieval:** Vector database retrieval is presented as one method, and the text notes that vector search can be just the first step before using more complex techniques.

The query asks about **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies**.

The page directly discusses **Vector databases**, **embeddings**, **RAG architectures** (by describing the two phases of RAG), and **chunking strategies** (mentioning that information is split into overlapping chunks before being stored).

The page **does not** explicitly mention:
*   New efficient embedding models (it only mentions that models are typically flavors of BERT).
*   Rerankers.
*   RAG alternatives.
*   Hybrid search.

Since the page does not cover all the requested topics, the summary focuses on what is present:

**Summary:**
The page defines **Embeddings** as numerical vectors representing data for fast, semantic similarity search, and **Vector Databases** as the storage system for these vectors, often used in the first phase",2025-12-25
retrieval_and_embeddings,exa,https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/,Vector Databases: from Embeddings to Applications,Exa,2023-11-08,"The webpage describes a DeepLearning.AI short course titled ""Vector Databases: from Embeddings to Applications.""

The course covers:
*   **Vector Databases:** Understanding their role, especially with LLMs, and when to apply them.
*   **Embeddings:** How to form vector representations of data.
*   **Search Techniques:** Exploring algorithms for fast searches, including **sparse, dense, and hybrid search**.
*   **Applications:** Building applications ranging from **RAG** to multilingual search.

While the course focuses heavily on vector databases, embeddings, and RAG architectures, it **does not explicitly mention** new efficient models for embeddings, rerankers, RAG alternatives, or specific chunking strategies.",2025-12-25
retrieval_and_embeddings,exa,https://aws.amazon.com/what-is/vector-databases/,What is a Vector Database?,Exa,2025-03-11,"The provided webpage focuses on **Vector Databases**, explaining what they are, why they are important, how they are used, who uses them, and their benefits.

Here is a summary of the topics mentioned in the user query that are covered on the page:

*   **Vector databases:** The entire page is dedicated to explaining vector databases, which store and retrieve vectors (embeddings) for efficient nearest-neighbor lookups.
*   **Embeddings (new efficient models):** The text mentions that embedding models encode data into vectors that capture meaning and context, allowing for similarity searches.
*   **RAG architectures (implied):** The page notes that vector databases can complement generative AI models by providing an **external knowledge base** for chatbots, which is a core component of Retrieval-Augmented Generation (RAG).
*   **Hybrid search:** The text explicitly states that developers can index metadata alongside vectors to enable **hybrid search** on both keywords and vectors, and mentions using hybrid relevancy scoring models that blend traditional term frequency models (like BM25) with vector scores.

The following topics from the user query are **not explicitly detailed** on the page:

*   Rerankers
*   RAG alternatives
*   Chunking strategies",2025-12-25
retrieval_and_embeddings,exa,https://learn.microsoft.com/en-us/fabric/real-time-intelligence/vector-database,Vector databases,Exa,2024-11-19,"The webpage provides an overview of **Vector Databases**, explaining that they store and manage data as **vectors** (numerical arrays) to enable complex queries like vector similarity search, quantization, and clustering, which traditional databases struggle with due to high-dimensional data.

Key concepts covered include:
*   **Vector Similarity:** Measuring how similar or different two vectors are using distance metrics like Euclidean distance or cosine similarity.
*   **Embeddings:** Mathematical representations of data (text, images, etc.) that capture semantic meaning. These are often generated by Large Language Models (LLMs) and are stored in the vector database.

The general workflow for using a vector database is:
1.  **Embed data** using a model.
2.  **Store vectors** in the database.
3.  **Embed the query** using the same model.
4.  **Query vectors** using similarity search to find matches.

The article then focuses on using **Eventhouse in Microsoft Fabric as a vector database**, highlighting features like the `dynamic` data type for storage, the `Vector16` encoding type for reduced storage and faster processing, and the `series_cosine_similarity` function for searching. It also details steps to **optimize for scale** by setting the encoding to `Vector16` and adjusting sharding and merging policies.",2025-12-25
retrieval_and_embeddings,exa,https://vespa.ai/vector-database/,"The Fastest, Most Scalable Vector Database",Exa,2025-12-10,"The webpage describes **Vespa** as a fast and scalable **Vector Database** that functions as a unified AI Search Platform.

Here is a summary of how Vespa addresses the components mentioned in your query:

*   **Vector Databases:** Vespa is presented as a high-performance vector database capable of handling billions of documents with sub-millisecond latency.
*   **Embeddings (new efficient models):** Vespa supports storing and computing on tensors, which includes vectors (embeddings). It allows for multimodal and multi-vector support.
*   **Rerankers:** Vespa supports **Multi-Phase and Model-Driven Ranking**, allowing users to deploy ranking models (using ONNX, XGBoost, or custom functions) directly within the serving layer to re-rank results after initial recall, maximizing accuracy.
*   **RAG Architectures:** Vespa is explicitly positioned for **GenAI (RAG)**, unifying the necessary components (vector search, text search, ranking) into a single engine for scalable RAG pipelines.
*   **RAG Alternatives:** While not detailing specific alternatives, the page contrasts Vespa's unified architecture against fragmented systems that require stitching together separate vector stores, keyword indexes, and re-ranking layers.
*   **Hybrid Search:** Vespa natively supports **Unified Vector, Text, and Structured Retrieval**, allowing users to combine dense embeddings, keyword signals, and metadata filters in a single query. Its tensor-native architecture supports both dense and sparse features for hybrid search.
*   **Chunking Strategies:** The page does not explicitly detail specific chunking strategies, though it mentions that new documents and embeddings become searchable immediately due to its real-time indexing capabilities.",2025-12-25
retrieval_and_embeddings,exa,https://www.linkedin.com/pulse/alternatives-rag-systems-kag-cag-soroush-mozooni-6b5ge,Alternatives to RAG Systems: KAG and CAG,Exa,2025-01-07,"The webpage discusses alternatives to Retrieval-Augmented Generation (RAG) systems, specifically **Cache-Augmented Generation (CAG)** and **Knowledge-Augmented Generation (KAG)**.

*   **CAG** simplifies the process by preloading necessary knowledge into the LLM's context using a precomputed ""cache,"" eliminating the need for a separate retrieval pipeline. It is best for tasks with small or manageable knowledge bases where speed and simplicity are key.
*   **KAG** integrates **knowledge graphs (KGs)** with LLMs to enable logical reasoning and provide more professional, accurate answers grounded in structured, verified information. It is ideal for complex, knowledge-intensive tasks requiring accuracy and domain expertise (e.g., legal or medical analysis).

The article does not specifically detail vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives (other than KAG/CAG), hybrid search, or chunking strategies, although it discusses the challenges of RAG that these components often address.",2025-12-25
retrieval_and_embeddings,exa,https://raghunaathan.medium.com/exploring-retrieval-augmented-generation-rag-and-its-alternatives-bf9e2f337f88,Exploring Retrieval-Augmented Generation (RAG) and Its Alternatives,Exa,2024-04-30,"The webpage provides a comprehensive overview of Retrieval-Augmented Generation (RAG), its architecture, types, benefits, and comparisons with fine-tuning.

Here is a summary of the components mentioned in your query:

*   **Vector databases (Vector Store):** These are part of the Knowledge Base, where document embeddings are indexed and stored, forming the foundation for the retrieval stage.
*   **Embeddings:** The process involves transforming chunked text content into embeddings using an embedding model before storing them in the vector store. The text also mentions the influence of the embedding model on chunking strategies.
*   **Rerankers:** Reranking the retriever results is a key component of the **Advanced RAG** implementation and the **Reader** function. Rerankers evaluate candidate documents based on relevance using mechanisms like cross-attention to refine the initial retrieval.
*   **RAG Architectures:** The article details three main types: **Naive**, **Advanced**, and **Modular** RAG, highlighting that Advanced RAG includes pre- and post-retrieval steps like query rewriting and candidate reranking to improve quality.
*   **RAG Alternatives:** The article primarily contrasts RAG with **Fine-tuning** LLMs, and also discusses **Hybrid Approaches** (RAG + fine-tuning) as alternatives or enhancements.
*   **Hybrid Search:** This is covered under the **Hybrid Approach** section, specifically mentioning methods like **RAG-guided Fine-tuning** and **RAG with Attention-based Fusion**.
*   **Chunking Strategies:** The text mentions that content is chunked based on the **embedding model and the LLM context window** during the Knowledge Base creation process.

The article also discusses retrieval mechanisms (Sparse, Dense, Contextual retrieval), prompt compression, prompt generation, guardrails, and evaluation metrics.",2025-12-25
retrieval_and_embeddings,exa,https://dev.to/naresh_007/beyond-vanilla-rag-the-7-modern-rag-architectures-every-ai-engineer-must-know-4l0c,Beyond Vanilla RAG: The 7 Modern RAG Architectures Every AI Engineer Must Know,Exa,2025-12-10,"The webpage, titled ""Beyond Vanilla RAG: The 7 Modern RAG Architectures Every AI Engineer Must Know,"" is likely to discuss various Retrieval-Augmented Generation (RAG) architectures.

Based on your query, the page is **highly relevant** as it is expected to cover:

*   **RAG architectures** (specifically modern ones beyond ""vanilla"").
*   It is very likely to touch upon **vector databases**, **embeddings** (especially with new efficient models), **rerankers**, **hybrid search**, and **chunking strategies**, as these are core components of modern RAG systems.

However, the provided text snippet is only the title and navigation elements, not the main content. Therefore, while the topic strongly suggests the content you are looking for, I cannot confirm the exact details without the body of the article.

**Summary based on the title and query:** The page discusses **7 Modern RAG Architectures** and is expected to cover key components like **vector databases, embeddings, rerankers, hybrid search, and chunking strategies** relevant to advanced RAG implementation.",2025-12-25
retrieval_and_embeddings,exa,https://medium.com/@tj.ruesch/understanding-modern-rag-architectures-from-simple-to-complex-6eef17f702ba,Understanding Modern RAG Architectures: From Simple to Complex,Exa,2025-01-05,"The webpage provides a detailed overview of modern Retrieval-Augmented Generation (RAG) architectures, covering foundational concepts and advanced patterns.

Here is a summary addressing the components mentioned in your query:

*   **Vector databases:** These are referred to as the **Document Store** in Naive RAG, which holds documents and their vector representations, enabling semantic search. Scaling vector stores is noted as a critical implementation consideration.
*   **Embeddings (new efficient models):** The **Embedding Model** is described as the system's ""librarian,"" converting documents and queries into vectors. The text mentions powerful models like OpenAI's `text-embedding-ada-002` and `E5`, as well as lighter alternatives like `MPNet`.
*   **Rerankers:** These are central to the **Retrieve-and-Rerank** architecture. After initial broad retrieval, a more sophisticated model evaluates the retrieved chunks using complex relevance signals to improve precision. Reranking is also mentioned as being incorporated into Graph RAG.
*   **RAG Architectures:** The page details several architectures:
    *   **Naive RAG:** The foundation, involving query processing, retrieval, context combination, and generation.
    *   **Retrieve-and-Rerank:** A two-stage approach for precision.
    *   **Multimodal RAG:** Handles diverse content types (images, code) by unifying their embeddings in a shared vector space.
    *   **Graph RAG:** Enhances vector retrieval with graph traversal to understand relationships and hierarchies.
    *   **Hybrid RAG:** Combines initial retrieval with subsequent reranking.
    *   **Agentic Approaches (Single-Router and Multi-Agent RAG):** Represent the future, involving intelligent routing and specialized agents coordinating knowledge work.
*   **Hybrid search:** This concept is implicitly covered by the **Hybrid RAG** architecture, which combines different relevance signals (like semantic search and graph traversal, or initial retrieval and reranking).
*   **Chunking strategies:** These are explicitly mentioned as a **critical** component within the **Document Store** of Naive RAG, noting that improper chunking leads to loss of granularity or context.
*   **RAG alternatives:** The text focuses on the evolution *within* RAG architectures (from Naive to Agentic) rather than explicitly listing alternatives *to* RAG",2025-12-25
retrieval_and_embeddings,exa,https://qdrant.tech/documentation/overview,Introduction,Exa,2001-01-01,"The webpage introduces **Vector Databases** as a new way to interact with abstract data representations (vectors or **embeddings**) derived from machine learning models. It highlights their use in applications like **semantic search** and **recommendation systems**.

The page focuses on **Qdrant**, describing it as a vector similarity search engine for storing, searching, and managing points (vectors with an optional payload).

Key concepts covered include:
*   **Vector Databases:** Optimized for storing and querying high-dimensional vectors efficiently, often using indexing techniques like HNSW.
*   **Distance Metrics:** The page details the three most common metrics used for similarity search: **Cosine Similarity**, **Dot Product**, and **Euclidean Distance**.
*   **Qdrant Architecture Components:** **Collections** (named sets of points), **Points** (vectors with an ID and payload), **Distance Metrics**, and **Storage** options (In-memory or Memmap).

The user query asks about: *Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies*.

**Summary relative to the query:**
The page extensively covers **Vector databases** and **embeddings** (referring to them as abstract data representations derived from ML models). It also mentions **Distance Metrics** which are crucial for similarity search. However, the page **does not mention** **rerankers**, **RAG architectures**, **RAG alternatives**, **hybrid search**, or **chunking strategies**.

**Conclusion:** The page provides information on vector databases and embeddings but does not answer the parts of the query related to RAG, rerankers, hybrid search, or chunking strategies.

No answer found (for the complete query).",2025-12-25
retrieval_and_embeddings,exa,https://www.youtube.com/watch?v=dN0lsF2cvm4,Vector Databases simply explained! (Embeddings \u0026 Indexes),Exa,2023-05-06,"This page provides a simple explanation of **Vector Databases**, covering what they are, how they work, and their use cases. It specifically mentions **vector embeddings** and **indexes**. The video also lists several **different vector database options**, including Pinecone, Weaviate, Chroma, Redis, Qdrant, Milvus, and Vespa.

However, the page **does not** explicitly discuss:
*   New efficient embedding models
*   Rerankers
*   RAG architectures or alternatives
*   Hybrid search
*   Chunking strategies

Therefore, for the full scope of your query, the answer is **No answer found**.",2025-12-25
retrieval_and_embeddings,exa,https://elastic.co/what-is/vector-database,What is a vector database?,Exa,2023-01-26,"The webpage provides a comprehensive overview of **vector databases** and related concepts crucial for modern information retrieval, particularly in the context of Large Language Models (LLMs) and Retrieval Augmented Generation (RAG).

Here is a summary addressing the components mentioned in your query:

*   **Vector Databases:** Specialized databases designed to store, manage, and search high-dimensional **vector embeddings**. They act as an external knowledge base to ""ground"" LLM responses, mitigating hallucination by enabling **semantic similarity searches**.
*   **Embeddings (new efficient models):** Vector embeddings are numerical arrays representing data (text, images, etc.) generated by ML models. The page discusses **dense vectors** (high-dimensional, non-zero elements, common in transformer models) and **sparse vectors** (mostly zero elements, optimized for storage and compatible with traditional search like TF-IDF/BM25).
*   **Rerankers:** **Semantic reranking** is a second-stage process in a ""retrieve-and-rerank"" pipeline. It uses a more accurate, computationally intensive model (like a **cross-encoder**) to reorder a smaller set of initial candidate documents retrieved by a faster method (like a **bi-encoder** or BM25) to improve final relevance.
*   **RAG Architectures:** **Retrieval Augmented Generation (RAG)** is listed as a common use case for vector databases, where LLMs are provided with external, up-to-date knowledge to generate factual answers. The page details the two-stage **""retrieve-and-rerank""** process often used in these pipelines.
*   **RAG Alternatives:** While the page focuses heavily on RAG, it implies alternatives by discussing the core components. The use of **hybrid search** (combining lexical and vector search) and the ability to build complex, modular search pipelines offer flexibility beyond a simple RAG setup.
*   **Hybrid Search:** This is enabled by integrating vector storage with traditional database functionalities. It allows for a **single, unified query** that performs lexical search (keyword matching), vector search (semantic matching), and metadata filtering simultaneously, leading to more relevant results.
*   **Chunking Strategies:** **No answer found** regarding specific chunking strategies. (The page mentions that embeddings are generated from data like ""words, phrases, or entire documents,"" but does not detail methods for splitting documents into chunks.)",2025-12-25
retrieval_and_embeddings,exa,https://www.exxactcorp.com/blog/deep-learning/alternative-rag-models,"Three Alternative RAG Models - SQL, Knowledge Bases, & APIs",Exa,2025-04-17,"The webpage discusses Retrieval-Augmented Generation (RAG) architectures, focusing on three alternatives to the most common **vector-based RAG** systems:

1.  **Structured Retrieval RAG (SQL/Tabular RAG):** Best for enterprise and regulated data, using precise queries against structured sources (like SQL tables) for deterministic, schema-aware results with low hallucination risk.
2.  **API-Augmented RAG:** Best for real-time and volatile information, retrieving live data by calling APIs during the reasoning process, avoiding the need for a static index.
3.  **Knowledge-Based RAG (Symbolic RAG):** Best for logic-heavy and compliance-focused tasks, using knowledge graphs or rule engines for precise, explainable, and rule-based retrieval.

The page notes that while vector-based RAG is flexible for unstructured data, it can suffer from semantic drift and lack of structure-awareness. The choice of architecture depends on data structure, timeliness requirements, and the need for precision/traceability.

**Regarding your specific query components:**

*   **Vector databases, embeddings (new efficient models), rerankers, RAG architectures:** The page discusses **Vector-Based RAG** as the common backbone, mentioning embeddings and similarity search, and contrasts it with other **RAG architectures**. It does not specifically detail new efficient embedding models or rerankers.
*   **RAG architectures, RAG alternatives:** The core of the article is dedicated to discussing **RAG architectures** and presenting three **RAG alternatives** (Structured, API-Augmented, Knowledge Base).
*   **Hybrid search:** The conclusion mentions that a **hybrid approach** combining multiple retrieval methods can often deliver the best results.
*   **Chunking strategies:** No answer found.",2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2512.20136,M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation,arXiv,2025-12-23,Recent multimodal RAG approach that retrieves and prunes multi-hop audio-visual knowledge from multimodal knowledge graphs for more grounded MLLM reasoning.,2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2512.16841,Radiology Report Generation with Layer-Wise Anatomical Attention,arXiv,2025-12-18,"Compact image-to-text radiology report generation method using anatomical attention (single-view CXR), relevant for multimodal report generation workflows.",2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2512.10888,PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction,arXiv,2025-12-11,"Introduces a large benchmark for full-page and multi-page table extraction plus the POTATR model, directly targeting table structure extraction from PDFs.",2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2512.02498,dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model,arXiv,2025-12-02,End-to-end VLM for layout parsing + text recognition + reading order across languages; introduces multilingual benchmark components (XDocParse).,2025-12-25
multimodal_and_generation,openai,https://github.com/rednote-hilab/dots.ocr,rednote-hilab/dots.ocr — Multilingual Document Layout Parsing in a Single Vision-Language Model,GitHub,2025-07-30,Official open-source repo for dots.ocr (model/code/prompts) to parse PDFs/images into structured outputs with multilingual support.,2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2511.18434,DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation,arXiv,2025-11-23,Benchmark focusing on real photographed documents (distortion/lighting) for robust parsing + translation—more realistic than scanned-PDF-only evals.,2025-12-25
multimodal_and_generation,openai,https://github.com/Topdu/DocPTBench,Topdu/DocPTBench,GitHub,2025-11-23,"Code/data entry point for DocPTBench, useful for evaluating photographed-document parsing and translation pipelines.",2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2511.10390,MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns,arXiv,2025-11-13,"Document parsing system targeting complex layouts (multi-level tables, formulas, cross-page structures) with RL-style verification for table parsing.",2025-12-25
multimodal_and_generation,openai,https://github.com/Yuliang-Liu/MonkeyOCR,Yuliang-Liu/MonkeyOCR,GitHub,2025-11-14,"Implementation and demos for MonkeyOCR (including v1.5), producing Markdown-style structured parses from PDFs/images.",2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2510.18234,DeepSeek-OCR: Contexts Optical Compression,arXiv,2025-10-21,Explores “vision-text compression” by mapping long text/docs into 2D images to reduce tokens for long-context document understanding/OCR.,2025-12-25
multimodal_and_generation,openai,https://github.com/deepseek-ai/DeepSeek-OCR,deepseek-ai/DeepSeek-OCR: Contexts Optical Compression,GitHub,2025-10-20,Official code/weights and inference instructions for DeepSeek-OCR (document-to-structured output with aggressive context compression).,2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2510.14528,PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model,arXiv,2025-10-16,Ultra-compact multilingual document parsing VLM (tables/formulas/charts) with strong efficiency—relevant for production OCR+layout extraction.,2025-12-25
multimodal_and_generation,openai,https://github.com/PaddlePaddle/PaddleOCR,PaddlePaddle/PaddleOCR,GitHub,2025-10-16,Major open-source OCR/doc-parsing toolkit; includes PaddleOCR-VL and pipelines that convert PDFs/images into Markdown/JSON for LLM workflows.,2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2509.22186,MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing,arXiv,2025-09-26,"Two-stage high-resolution document parsing VLM (layout then crop-based recognition) designed for efficient, accurate PDF-to-structured extraction.",2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2510.03663,UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG,arXiv,2025-10-04,Large-scale benchmark for document-centric multimodal RAG built from real PDFs; compares text-only vs image-only vs fusion paradigms.,2025-12-25
multimodal_and_generation,openai,https://huggingface.co/datasets/Salesforce/UniDoc-Bench,Salesforce/UniDoc-Bench (dataset),Hugging Face Datasets,2025-10-04,Downloadable UniDoc-Bench dataset (PDF-derived images + QA) for evaluating multimodal RAG and document VQA end-to-end.,2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2412.07626,OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations,arXiv,2024-12-10,"Foundational benchmark for diverse PDF parsing with detailed annotations/metrics across text, tables, formulas, and reading order.",2025-12-25
multimodal_and_generation,openai,https://github.com/opendatalab/OmniDocBench,opendatalab/OmniDocBench,GitHub,2025-09-25,Benchmark repo with evaluation code and v1.5 updates; a key reference point for modern PDF parsing model comparisons.,2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2509.11937,MMORE: Massive Multimodal Open RAG & Extraction,arXiv,2025-09-15,Open pipeline for large-scale multimodal ingestion/extraction across many file types to support multimodal RAG systems in practice.,2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2510.15253,Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding,arXiv,2025-10-17,"Survey/taxonomy of multimodal RAG for document understanding, summarizing retrieval modalities, benchmarks, and open challenges.",2025-12-25
multimodal_and_generation,openai,https://blog.google/technology/developers/gemini-3-pro-vision/,Gemini 3 Pro: the frontier of vision AI,Google blog,2025-12-05,Official Gemini 3 Pro vision-focused announcement highlighting document/screen/video understanding capabilities.,2025-12-25
multimodal_and_generation,openai,https://ai.google.dev/gemini-api/docs/document-processing,Document understanding | Gemini API,Google AI for Developers (docs),recent,"Technical documentation on PDF limits, tokenization, and document-processing behavior in the Gemini API (useful for multimodal doc pipelines).",2025-12-25
multimodal_and_generation,openai,https://docs.anthropic.com/en/docs/build-with-claude/pdf-support,PDF support,Anthropic documentation,recent,"Claude PDF ingestion guidance (text+image understanding, charts/tables) with constraints and citations-related behavior for document QA/RAG.",2025-12-25
multimodal_and_generation,openai,https://github.com/richard-peng-xia/MMed-RAG,richard-peng-xia/MMed-RAG,GitHub,2025-01-22,Multimodal RAG system for medical vision-language models (VQA + report generation) with code/data pointers—highly relevant to grounded report generation.,2025-12-25
multimodal_and_generation,openai,https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B,opendatalab/MinerU2.5-2509-1.2B,Hugging Face (model),2025-09-26,Model card + weights and usage examples for MinerU2.5; practical entry point for running high-res PDF parsing locally or via vLLM.,2025-12-25
multimodal_and_generation,openai,https://arxiv.org/abs/2510.10973,Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning,arXiv,2025-10-13,Targets chart understanding with verifiable rewards to improve robustness and explanation fidelity—useful for chart QA and chart-to-text generation.,2025-12-25
multimodal_and_generation,anthropic,https://github.com/deepseek-ai/Janus,Janus-Series: Unified Multimodal Understanding and Generation Models,GitHub - DeepSeek AI,January 2025,DeepSeek's Janus-Pro unified model for both multimodal understanding and generation with decoupled visual encoding architecture.,2025-12-25
multimodal_and_generation,anthropic,https://arxiv.org/abs/2501.17811,Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling,arXiv,January 2025,Technical paper on Janus-Pro 7B model achieving significant advances in both understanding and text-to-image generation.,2025-12-25
multimodal_and_generation,anthropic,https://huggingface.co/blog/manu/colpali,ColPali: Efficient Document Retrieval with Vision Language Models,Hugging Face Blog,2024,Detailed explanation of ColPali's visual document embeddings and late interaction mechanism for RAG applications.,2025-12-25
multimodal_and_generation,anthropic,https://github.com/QwenLM/Qwen3-VL,Qwen2.5-VL / Qwen3-VL: Multimodal Large Language Models,GitHub - Alibaba,January 2025,"Alibaba's Qwen2.5-VL with powerful document parsing, chart/table understanding, and ultra-long video comprehension capabilities.",2025-12-25
multimodal_and_generation,anthropic,https://qwenlm.github.io/blog/qwen2.5-vl/,Qwen2.5-VL: The New Flagship Vision-Language Model,Qwen Blog,January 2025,"Qwen2.5-VL achieves GPT-4o comparable performance with omnidocument parsing for handwriting, tables, charts, and music sheets.",2025-12-25
multimodal_and_generation,anthropic,https://github.com/OpenBMB/MiniCPM-V,MiniCPM-V 4.5: A GPT-4o Level MLLM on Your Phone,GitHub - OpenBMB,2024-2025,"8B parameter model outperforming GPT-4o-latest with strong OCR, document parsing, and efficient edge deployment.",2025-12-25
multimodal_and_generation,anthropic,https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models,Awesome-Multimodal-Large-Language-Models: Latest Advances,GitHub,ongoing,"Comprehensive curated list of MLLM papers, models, datasets, and benchmarks updated regularly.",2025-12-25
multimodal_and_generation,anthropic,https://aclanthology.org/2024.acl-long.775/,Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models,ACL 2024,August 2024,"6.4M scientific figure-caption dataset improving VLM comprehension of charts, diagrams, and academic figures.",2025-12-25
multimodal_and_generation,anthropic,https://arxiv.org/abs/2406.08394,VisionLLM v2: End-to-End Generalist Multimodal Large Language Model,arXiv,June 2024,Unified MLLM framework handling hundreds of vision-language tasks including localization and image generation.,2025-12-25
multimodal_and_generation,anthropic,https://academic.oup.com/nsr/article/11/12/nwae403/7896414,A Survey on Multimodal Large Language Models,National Science Review,November 2024,"Comprehensive academic survey covering MLLM architectures, training strategies, hallucination, and chain-of-thought reasoning.",2025-12-25
multimodal_and_generation,anthropic,https://arxiv.org/abs/2404.01322,A Review of Multi-Modal Large Language and Vision Models,arXiv,March 2024,"Review covering transformer evolution, MM-LLM architectures, and techniques for multimodal understanding and generation.",2025-12-25
multimodal_and_generation,anthropic,https://github.com/QwenLM/Qwen2.5-Omni,Qwen2.5-Omni: End-to-end Multimodal Model,GitHub - Alibaba,March 2025,Omni-modal model with TMRoPE for synchronized video-audio processing and real-time streaming speech generation.,2025-12-25
multimodal_and_generation,exa,https://medium.com/@shravankoninti/multimodal-rag-with-gpt-4-vision-and-langchain-60a6a13a92e4,Multimodal RAG with GPT-4-Vision and LangChain,Exa,2024-09-04,"The webpage describes a framework called **Multimodal RAG with GPT-4-Vision and LangChain**.

This framework combines:
1.  **Multimodal RAG (Retrieval-Augmented Generation):** The ability to process and generate multiple data types (like text and images) while grounding responses in retrieved information.
2.  **GPT-4-Vision (specifically using `gpt-4o-mini` in the example):** A multimodal model capable of processing both text and visual inputs (images).
3.  **LangChain:** A tool used to build applications around language models.

The practical implementation detailed on the page involves:
*   Using the `unstructured` library to **parse PDFs** to extract **text, tables, and images**.
*   Using **GPT-4o-mini** to generate summaries for these extracted text, table, and image elements.
*   Storing these elements and their summaries using a **MultiVectorRetriever** with a **Chroma** vector store.
*   Setting up a chain to answer questions based on the retrieved context, which can include information derived from text, tables, and images.

In summary, the page details how to build a system that can understand and answer questions based on documents containing text, tables, and images by leveraging multimodal models and RAG techniques within the LangChain ecosystem.",2025-12-25
multimodal_and_generation,exa,https://arxiv.org/html/2510.03663v2,UniDoc-Bench: A Unified Benchmark for Document-Centric Multimodal RAG,Exa,2023-02-01,"The webpage introduces **UniDoc-Bench**, a large-scale, realistic benchmark specifically designed for **Document-Centric Multimodal Retrieval-Augmented Generation (MM-RAG)**.

**Key aspects related to your query:**

*   **Multimodal RAG and Document Understanding:** The benchmark addresses the limitations of current evaluations by focusing on real-world PDF documents (70k pages across 8 domains) that contain interleaved text, tables, and figures. It aims to evaluate systems that need to retrieve and reason over information across these modalities.
*   **PDF Parsing and Chart/Table Extraction:** The data curation process involves parsing PDFs to extract text chunks, tables, and figures. Tables and figures are stored separately, and placeholders are inserted into the text to represent their location, facilitating multimodal grounding.
*   **Report Generation with LLMs (MM-RAG):** The core purpose is to evaluate MM-RAG pipelines. The benchmark includes 1,600 human-verified Question-Answer (QA) pairs covering factual retrieval, comparison, summarization, and logical reasoning, requiring evidence from text, tables, and images.
*   **Vision-Language Models (VLMs) and Multimodal Models:** The experiments compare various RAG paradigms, including text-only, image-only, multimodal joint retrieval (using models like GME), and multimodal text-image fusion. The results indicate that **text-image fusion RAG** (combining separate text and image retrievals) performs best, outperforming joint multimodal embedding approaches.
*   **GPT-4V, Claude vision, Gemini:** While the text mentions using **GPT-4** and **Gemini-Pro** in the data synthesis pipeline (for QA generation and verification), it does not specifically benchmark or detail the performance of dedicated vision models like GPT-4V, Claude vision, or Gemini as the final RAG generator/reasoner, though they are implicitly involved in the broader VLM landscape discussed in related works.
*   **Structured Document Output:** The evaluation metrics focus on answer **completeness** and **faithfulness**, which are crucial for ensuring the generated reports/answers are accurate and grounded in the retrieved multimodal evidence.

**In summary, UniDoc-Bench provides a unified platform to benchmark MM-RAG systems that handle complex document understanding tasks involving text, tables, and charts, finding that fusing separate text and image retrieval is currently superior to joint

The user query asks for a summary related to **multimodal and generation** topics, specifically mentioning: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.

The provided webpage text is an appendix section from a paper, likely titled ""UniDoc-Bench: A Unified Benchmark for Document-Centric Multimodal RAG,"" which details dataset creation, QA synthesizing prompts, human annotation guidelines, and experimental results comparing different RAG systems (Text-only, Image-only, Multimodal (MM), and Text+Image fusion (T++I)).

Here is a summary of how the text relates to the query:

*   **Multimodal RAG & Vision-Language Models:** The entire document focuses on **Multimodal RAG** (Retrieval-Augmented Generation) over documents. It references several RAG systems (MM, T++I) and compares retrieval performance based on different modalities required for the answer (Text-only, Img-only, Text+Img). It also compares multimodal embeddings (Voyage vs. GME).
*   **Document Understanding, PDF Parsing, Chart/Table Extraction:** Appendix B.3 explicitly mentions parsing PDFs into ""text chunks, images of figures, and images of tables"" using `unstructured`. Appendix B.4 provides detailed question templates for **Factual Retrieval, Comparison, Summarization, and Causal/Reasoning** questions specifically tailored for a finance domain, implying structured data extraction is a key component. Appendix F.1 discusses classifying images as ""content-rich"" (providing information not present in the text) or ""illustrative,"" which relates directly to extracting information from visual elements like charts/tables.
*   **Report Generation with LLMs:** Appendix A mentions using **LLMs** for ""polishing grammar and improving readability"" and ""assisting in the evaluation of RAG outputs"" and ""synthesizing the QA pairs."" While it doesn't detail *report generation* itself, it confirms the use of LLMs in the RAG pipeline for generation/assistance tasks.
*   **GPT-4V, Claude vision, Gemini:** The text mentions using **Gemini-2.5-pro** in Appendix F.1 to classify images. It does not explicitly mention GPT-4V or Claude vision, though the context implies",2025-12-25
multimodal_and_generation,exa,https://www.chitika.com/vision-models-pdf-parsing-rag/,Using Vision Models for Complex PDF Parsing in RAG Systems,Exa,2025-03-18,"Vision models are revolutionizing PDF parsing in RAG systems by analyzing both text and layout to understand document structure, which overcomes the limitations of traditional text-based tools that struggle with complex elements like tables, multi-column layouts, and diagrams.

Key aspects covered include:
*   **Vision Language Models (VLMs):** These models treat PDFs as both images and text sources, blending visual and textual cues to maintain structure, align tables, and keep annotations intact, leading to richer datasets for retrieval through multimodal embeddings.
*   **Multimodal Embeddings:** These integrate text, images, tables, and diagrams into a unified representation, preserving document structure and improving retrieval precision in RAG systems.
*   **Benefits:** Vision-powered parsing leads to cleaner tables, intact diagrams, reliable data retrieval, faster insights, and fewer errors, benefiting industries like healthcare (clinical trial reports) and finance (financial documents).
*   **Advanced Techniques:** Late interaction mechanisms are mentioned as a way to improve retrieval efficiency by delaying the fusion of text and image embeddings until query time.

The page focuses heavily on how vision models enhance PDF parsing within RAG workflows, particularly through structural preservation and multimodal data handling.",2025-12-25
multimodal_and_generation,exa,https://devblogs.microsoft.com/ise/multimodal-rag-with-vision/,Multimodal RAG with Vision: From Experimentation to Implementation - ISE Developer Blog,Exa,2024-10-11,"This page summarizes an experimentation process for optimizing a **multimodal RAG** pipeline that handles documents containing both text and images, utilizing **multimodal LLMs** like **GPT-4V** and **GPT-4o**.

Key aspects covered include:

*   **Multimodal RAG Pattern:** Transforming image content into text descriptions using an MLLM during ingestion so that both text and image descriptions can be stored and retrieved from a vector database.
*   **Ingestion Flow:** Detailed steps involving custom loading, image enrichment (using MLLMs to generate descriptions, sometimes aided by surrounding text context), and ingestion into Azure AI Search.
*   **Experimentation Learnings:**
    *   **Prompt Engineering:** Tailoring prompts for image enrichment and inference to improve description quality and structured output (JSON format).
    *   **Metadata Impact:** Including document-level metadata significantly improved source recall performance.
    *   **Chunking Strategy:** Using **separate chunks** for image annotations (instead of inline) resulted in better retrieval metrics for vision-related queries.
    *   **Classifier Use:** Using an image classifier (like Azure Computer Vision) to filter out irrelevant images (e.g., logos) improved ingestion efficiency without sacrificing recall.
    *   **Inference Model Choice:** **GPT-4o** was recommended for inference due to better quality, speed, and cost compared to GPT-4.
    *   **Enrichment Model Choice:** **GPT-4V** was found to perform better than GPT-4o for generating image summaries during the ingestion/enrichment phase.
*   **Evaluation:** The process relied on a curated Q&A dataset and specific retrieval and generative metrics to assess performance systematically.",2025-12-25
multimodal_and_generation,exa,https://www.youtube.com/watch?v=uLrReyH5cu0,Multimodal RAG: Chat with PDFs (Images \u0026 Tables) [2025],Exa,2024-11-12,"This page describes a tutorial video on building a **multimodal Retrieval-Augmented Generation (RAG)** pipeline using **LangChain** and the **Unstructured library**. This system is designed to query complex documents like **PDFs containing text, images, and tables** by leveraging the multimodal capabilities of advanced **LLMs (like GPT-4 with vision)**.

The tutorial covers:
*   Setting up the **Unstructured library** for parsing and pre-processing diverse document types (including images).
*   Creating a document retrieval system that integrates both **textual and visual data**.
*   Integrating this multimodal data into a **LangChain-powered RAG pipeline**.
*   Achieving **comprehensive understanding and accurate responses** using a multimodal LLM.

This directly relates to your query regarding **multimodal RAG**, **document understanding**, **PDF parsing**, and the use of models like **GPT-4V**.",2025-12-25
multimodal_and_generation,exa,https://github.com/zhengxuJosh/Awesome-RAG-Vision,GitHub - zhengxuJosh/Awesome-RAG-Vision: Awesome-RAG-Vision: a curated list of advanced retrieval augmented generation (RAG) for Computer Vision,Exa,2024-11-21,"The webpage is a curated list of state-of-the-art papers on **Retrieval-Augmented Generation (RAG) in Computer Vision**.

It covers various applications of RAG in vision tasks, including:
*   **Visual Understanding** (Image Understanding, Long Video Understanding, Visual Spatial Understanding)
*   **Multi-modal** RAG applications.
*   **Visual Generation** (Image/Video Generation, 3D Generation).
*   **Embodied AI**.

The page also lists resources, including tutorials on **Multimodal RAG** for images and video, and a section dedicated to **RAG for Document** processing, which mentions:
*   Multimodal RAG for PDFs with Text, Images, and Charts.
*   Multimodal RAG with Document Retrieval and Vision Language Models (VLMs).
*   Multi-Vector Retriever for RAG on tables, text, and images.

While the page extensively covers **Multimodal RAG**, **Vision-Language Models (VLMs)**, and **Document Understanding** (including PDF/chart/table aspects via the ""RAG for Document"" resources), it **does not explicitly list or detail** specific models like **GPT-4V, Claude vision, or Gemini**, nor does it focus on **report generation with LLMs** or **structured document output** as primary topics, although these concepts are implied within the broader RAG and document understanding context.

**Summary relative to the query:**

The page is highly relevant to **multimodal RAG**, **vision-language models**, **document understanding**, **PDF parsing**, and **chart/table extraction** as it curates resources and papers in these areas, particularly under the ""Multi-modal"" and ""RAG for Document"" sections. However, it does not provide specific information on **report generation with LLMs**, **GPT-4V, Claude vision, Gemini**, or **structured document output** beyond the general scope of multimodal document RAG.",2025-12-25
multimodal_and_generation,exa,https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/llm-parser,Use the LLM parser,Exa,2025-11-18,"The webpage describes how to use the **Vertex AI RAG Engine LLM parser** for document processing.

The LLM parser leverages Large Language Models (LLMs) to:
*   Understand and interpret semantic content across various formats.
*   Retrieve relevant document chunks and extract meaningful information.
*   Understand and interact with visuals, including extracting data from charts and diagrams, and describing images.
*   Provide contextually rich and accurate responses.

**Supported Models** include various versions of Gemini (e.g., Gemini 2.5 Pro, Gemini 2.0 Flash).
**Supported File Types** include PDFs and common image formats (PNG, JPEG, etc.).

The page also details **pricing**, provides **code examples** (REST and Python) for importing files with the LLM parser enabled, and offers guidance on **prompt engineering** for document parsing, including a template table and general best practices (be specific, detailed, constraining, clear, and structured).

Finally, it shows **parsing quality analysis** examples, demonstrating improvements in linking sections across slides, understanding tables and flowcharts, extracting data from graphs, and capturing heading-text relationships.

While the page discusses advanced document understanding capabilities like chart/table extraction and leveraging LLMs for parsing, it **does not explicitly mention** ""multimodal RAG,"" ""document understanding"" as a general term (though it implies it), ""PDF parsing"" (though it supports PDF files), ""report generation with LLMs,"" ""GPT-4V,"" ""Claude vision,"" or ""structured document output"" in the context of the user query's specific terms, other than the general capabilities of the LLM parser.",2025-12-25
multimodal_and_generation,exa,https://build.nvidia.com/nvidia/build-an-enterprise-rag-pipeline,Build an Enterprise RAG Pipeline Blueprint,Exa,unknown,"The NVIDIA AI Blueprint for Retrieval-Augmented Generation (RAG) supports **multimodal data ingestion**, including **multimodal PDF data extraction** for text, tables, charts, and infographics. It also features **Optional Vision Language Model (VLM) Support in answer generation** and **Opt-in image captioning with vision language models (VLMs)**.

While the blueprint explicitly mentions support for extracting data from **tables** and **charts** as part of its multimodal data ingestion capabilities, and uses models like **PaddleOCR** for potential text extraction from images/documents, it **does not explicitly mention** specific models like GPT-4V, Claude vision, or Gemini, nor does it detail a specific process for **report generation with LLMs** or **structured document output** beyond the extraction of elements like tables and charts.",2025-12-25
multimodal_and_generation,exa,https://pmc.ncbi.nlm.nih.gov/articles/PMC11441350/,GPT-4 Vision: Multi-Modal Evolution of ChatGPT and Potential Role in Radiology,Exa,2024-08-31,"The webpage discusses **GPT-4 Vision (GPT-4V)**, which represents the evolution of ChatGPT into a **Large Multimodal Model (LMM)** capable of generating text from images without specialized training.

Key points relevant to your query include:

*   **Vision-Language Models (LMMs):** GPT-4V is highlighted as a significant advancement in multimodal AI, capable of contextualizing and explaining visual inputs using an integrated understanding of images and text.
*   **Report Generation with LLMs:** The paper explores GPT-4V's potential for **radiologic image report generation**, though it notes current limitations in accuracy for medical tasks. Future iterations could provide preliminary structured reports and assist with decision support.
*   **GPT-4V:** The article focuses entirely on the capabilities, testing, strengths, and weaknesses of GPT-4V, particularly in the context of radiology.

The page **does not explicitly mention** ""multimodal RAG,"" ""document understanding,"" ""PDF parsing,"" ""chart/table extraction,"" ""Claude vision,"" or ""Gemini.""",2025-12-25
multimodal_and_generation,exa,https://www.promptlayer.com/research-papers/ai-tackles-multimodal-multi-document-qa,VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation,Exa,2024-12-14,"The webpage describes **VisDoMRAG**, a system designed for **Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation**.

It addresses the challenge of answering questions when information is scattered across multiple documents containing text, tables, charts, and images, which traditional AI struggles with by often focusing only on text.

**Key aspects related to your query:**

*   **Multimodal RAG:** VisDoMRAG is a multimodal RAG system that retrieves relevant information simultaneously from both text and images.
*   **Document Understanding/PDF Parsing/Chart/Table Extraction:** The system is specifically designed to handle documents containing diverse visual and textual content, implying capabilities for understanding and extracting information from these elements (though specific details on PDF parsing techniques are not elaborated).
*   **Report Generation with LLMs:** The goal is to synthesize findings from both modalities using a 'consistency check' to generate accurate answers, which implies the use of LLMs for final report/answer generation.
*   **Vision-Language Models:** The core functionality relies on integrating visual and textual cues for reasoning.

The research also introduces **VisDoMBench**, a new benchmark to test AI performance on this type of multimodal, multi-document QA.",2025-12-25
multimodal_and_generation,exa,https://arxiv.org/html/2509.11937v1,MMORE: Massive Multimodal Open RAG & Extraction,Exa,2025-05-26,"The web page describes **MMORE (Massive Multimodal Open RAG & Extraction)**, an open-source pipeline designed for scalable ingestion, transformation, and retrieval of knowledge from heterogeneous document formats for use with Large Language Models (LLMs).

**Key features and capabilities relevant to your query:**

*   **Multimodal Support:** MMORE supports over fifteen file types, including text, tables, images, emails, audio, and video, processing them into a unified format for downstream LLM applications.
*   **Document Understanding & Parsing:** It integrates extraction tools for tasks like **PDF parsing** (using tools like Surya) and handles various document formats (DOCX, PPTX, spreadsheets).
*   **RAG (Retrieval-Augmented Generation):** It features a robust RAG pipeline with hybrid dense-sparse retrieval, supporting both interactive APIs and batch endpoints. Evaluation on PubMedQA showed that MMORE-augmented LLMs improve biomedical QA accuracy with increasing retrieval depth.
*   **Scalability and Performance:** The architecture is modular and distributed (built on Dask), demonstrating a 3.8-fold speedup over single-node baselines in distributed mode and achieving 40% higher accuracy than Docling on scanned PDFs.
*   **Structured Output:** The processing module standardizes heterogeneous content into a unified JSON-based format called `MultimodalSample`, which interleaves text with modality placeholders (e.g., for images/charts) to preserve context linkage.

While the page discusses the general architecture for handling multimodal data and extraction, it does **not** specifically mention or benchmark proprietary models like **GPT-4V, Claude vision, or Gemini** for vision tasks, nor does it detail specific methods for **chart/table extraction** beyond general document understanding, although it supports spreadsheet formats and mentions layout accuracy. It focuses on the open-source pipeline infrastructure itself.",2025-12-25
multimodal_and_generation,exa,https://arxiv.org/html/2509.02123v1,CMRAG: Co-modality–based document retrieval and visual question answering,Exa,2025-01-01,"The webpage describes **CMRAG (Co-modality–based document retrieval and visual question answering)**, a novel Retrieval-Augmented Generation (RAG) framework designed to handle multimodal documents by simultaneously leveraging both **text and image** information.

Key aspects related to your query:

*   **Multimodal RAG/Vision-Language Models (VLMs):** CMRAG is a multimodal RAG approach that addresses the limitations of pure text-based RAG (which ignores images) and pure vision-based RAG (which ignores precise text semantics). It uses VLMs (like Qwen2.5-VL-7B-Instruct) for document parsing and final answer generation.
*   **Document Understanding/PDF Parsing/Chart/Table Extraction:** The framework performs structured parsing on documents to obtain co-modality representations, including the entire page image, parsed sub-images (capturing localized elements like figures/tables), and extracted text.
*   **Report Generation with LLMs (VLM):** The retrieved co-modality evidence (text and image) is fed into a VLM to generate the final answer, enabling cross-modal reasoning.
*   **Performance:** Experiments show that CMRAG significantly outperforms pure-vision-based RAG, particularly when integrating parsed text with entire images, which provides complementary grounding for accurate reasoning.

The paper focuses on improving **Visual Document Question Answering (VQA)** by unifying text and image modalities within the RAG pipeline.",2025-12-25
multimodal_and_generation,exa,https://www.xugj520.cn/en/archives/rag-anything-multimodal-document-processing.html,RAG-Anything: The Complete Guide to Unified Multimodal Document Processing,Exa,2025-06-18,"RAG-Anything is a revolutionary **multimodal RAG system** designed for unified document processing, capable of understanding and querying complex documents containing diverse content types like text, images, tables, and formulas simultaneously.

Key features relevant to your query include:

*   **Multimodal Document Processing:** It handles diverse formats (PDF, Office, Images) by decomposing content into text blocks, images, tables, and formulas.
*   **Specialized Processors:** It uses dedicated processors for visual content analysis, structured data interpretation (tables), and mathematical expression parsing.
*   **Knowledge Graph Construction:** It transforms multimodal content into a structured semantic network to enable retrieval based on deep semantic relationships.
*   **Modal-Aware Retrieval:** It employs a hybrid retrieval strategy (Vector-Graph Fusion) that considers content type relevance.

While the page details a system that *processes* multimodal content, it does not explicitly mention or compare specific large language models like **GPT-4V, Claude vision, or Gemini** for the generation or understanding tasks, nor does it focus specifically on **report generation with LLMs** beyond the general RAG output. It does support **structured document output** via its structured extraction engine and knowledge graph.",2025-12-25
multimodal_and_generation,exa,https://milvus.io/ai-quick-reference/how-is-multimodal-rag-used-in-document-understanding-systems,How is multimodal RAG used in document understanding systems?,Exa,2025-10-17,"Multimodal RAG (Retrieval-Augmented Generation) enhances document understanding systems by integrating multiple data types—such as text, images, tables, and diagrams—into a single framework, moving beyond traditional text-only retrieval.

It works by using encoders to convert different data formats into a shared embedding space, allowing the system to search and retrieve relevant information across modalities before synthesizing a response.

Practical applications include:
*   **Processing Scanned Invoices/Forms:** Extracting structured data (invoice numbers from text, payment terms from tables) and validating visual cues (logos, signatures).
*   **Academic Research Analysis:** Analyzing a paper's text, equations, and figures to answer questions about methodology by linking visual elements to textual descriptions.

Implementation typically involves combining separate encoders for each data type (e.g., BERT for text, ResNet for images), a fusion mechanism, and storing embeddings in a vector database (like FAISS) for retrieval, with a generator model (like GPT) producing the final answer. This requires aligning embeddings across modalities, often using vision-language models like CLIP.",2025-12-25
multimodal_and_generation,exa,https://www.aimodels.fyi/papers/arxiv/m3docrag-multi-modal-retrieval-is-what-you,M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding | AI Research Paper Details,Exa,2024-11-14,"The M3DocRAG framework is a new system designed for **multi-modal, multi-page, and multi-document understanding** that utilizes **retrieval-augmented generation (RAG)**. It excels at integrating information from various sources, including different **modalities** like text, images, and tables, across multiple documents to answer complex questions and perform reasoning tasks. The paper demonstrates that M3DocRAG achieves state-of-the-art performance on benchmarks for multi-document question answering and multi-modal reasoning, outperforming models that only consider a single document or modality.",2025-12-25
multimodal_and_generation,exa,https://m3docrag.github.io/,M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding,Exa,2017-07-11,"The webpage describes **M3DocRAG**, a novel **multi-modal Retrieval-Augmented Generation (RAG)** framework designed for understanding **multi-page and multi-document** contexts.

Key aspects relevant to your query:

*   **Multimodal RAG and Document Understanding:** M3DocRAG uses **multi-modal retrieval** and a **Multi-modal Language Model (MLM)** (like Qwen2-VL) to answer questions from documents, effectively handling visual information (charts, figures) that text-based RAG or single-page MLMs often ignore.
*   **Handling Long Documents/Multiple Pages:** It is specifically designed to overcome the limitations of MLMs that cannot handle many long documents by retrieving relevant pages first.
*   **Chart/Table Extraction & Report Generation:** While the focus is on VQA, the framework explicitly mentions preserving visual information, including **charts** and **tables**, as evidence modalities. The overall process leads to an answer generation component (using an MLM).
*   **Vision-Language Models (GPT-4V, Claude vision, Gemini):** The framework utilizes MLMs for the final question-answering step, citing **Qwen2-VL 7B** as a successful component, demonstrating the application of vision-language models in this RAG pipeline.
*   **PDF Parsing:** The framework operates on **PDF documents** and uses visual embedding extraction (with ColPali) to represent each page.

In summary, M3DocRAG is a multi-modal RAG system that integrates visual understanding (using MLMs) with efficient retrieval across many pages/documents to answer complex questions, addressing limitations in traditional document understanding pipelines.",2025-12-25
multimodal_and_generation,exa,https://docs.aws.amazon.com/nova/latest/userguide/rag-multimodal.html,Using Amazon Nova for Multimodal RAG,Exa,unknown,"Amazon Nova can be used to build **multimodal RAG systems** capable of searching documents like **PDFs, images, or videos**. This allows for RAG systems with **mixed data containing both text and images**.

You can create a multimodal RAG system using Amazon Nova in two main ways for creating the vector database:

1.  **Creating a vector database using multimodal embeddings:** This involves parsing documents into text, tables, and images, and then using a multimodal embeddings model (like Titan multimodal embeddings) on the parsed content.
2.  **Creating a vector database using text embeddings:** This involves using Amazon Nova via the Converse API to convert images into detailed text descriptions (using a specific narrator prompt) and then using a text embeddings model (like Titan Text Embeddings V2). This approach is suggested for documents like slides and infographics.

After setting up the database, inference involves querying the database, sending the retrieved content (ideally in its **original modality**, like returning images) back to Amazon Nova, and having Amazon Nova respond to the original user query using the retrieved context. This can be done via Amazon Bedrock Knowledge bases or a custom RAG system using the Converse API.",2025-12-25
multimodal_and_generation,exa,https://hal.science/hal-05322313/document,Untitled,Exa,2025-10-23,"The webpage provides a comprehensive survey on **Multimodal Agentic Retrieval-Augmented Generation (MMA-RAG)**.

This paradigm represents an advancement over traditional RAG by empowering Large Language Models (LLMs) to integrate and reason over diverse data types, including **text, images, audio, and structured data**, using autonomous agents.

Key aspects covered include:

*   **Evolution:** Tracing the path from text-only RAG to Multimodal RAG and Agentic RAG, culminating in MMA-RAG.
*   **Architectures:** Detailing core patterns like Hierarchical Multi-Agent Organization, Cross-Modal Retrieval and Fusion, and Agentic Capabilities (planning, tool use, iterative refinement).
*   **Applications:** Highlighting uses in **Document Understanding** (e.g., MDocAgent), **Healthcare** (e.g., radiology report generation, clinical VQA), **Sports Analytics**, **Scientific Exploration**, and **Embodied AI**.
*   **Challenges:** Discussing issues related to **Cross-Modal Alignment**, **Scalability/Efficiency**, and **Evaluation Fragmentation**.

While the text discusses document understanding, PDF parsing, chart/table extraction, and report generation with LLMs as key application areas and challenges within MMA-RAG, it **does not specifically mention or survey commercial models like GPT-4V, Claude vision, or Gemini** as primary subjects of analysis, although it discusses the general capabilities these models enable. The focus is on the *frameworks* and *architectures* (like HM-RAG, MDocAgent, CAL-RAG) that implement these multimodal and agentic capabilities.",2025-12-25
multimodal_and_generation,exa,https://arxiv.org/abs/2411.04952,Computer Science > Computer Vision and Pattern Recognition,Exa,2024-11-07,"The webpage describes **M3DocRAG**, a novel multi-modal Retrieval-Augmented Generation (RAG) framework designed for understanding multi-page and multi-document contexts.

Key aspects relevant to your query include:

*   **Multi-modal RAG:** It uses a multi-modal retriever and a Multi-modal Language Model (MLM) to handle various document contexts and question types (single-hop and multi-hop).
*   **Handling Visual Information:** It preserves visual information, addressing the limitation of text-based RAG methods that rely solely on OCR and ignore visual elements like figures, charts, and tables.
*   **Document Understanding:** It is designed to answer questions that require information across different pages or documents, which is difficult for standard MLMs that have limited context windows.
*   **Vision Models Mentioned:** The framework achieves superior performance using models like **ColPali** and **Qwen2-VL 7B** (a vision-language model).

While the page focuses on a specific RAG framework (M3DocRAG) and a new benchmark (M3DocVQA), it directly relates to **Vision-language models**, **multimodal RAG**, and **document understanding** (including handling charts/figures). It does not explicitly detail report generation with LLMs, PDF parsing tools, or mention GPT-4V, Claude vision, or Gemini by name, though the underlying concepts are related to the broader field you listed.",2025-12-25
multimodal_and_generation,exa,https://arxiv.org/abs/2406.18116,BADGE: BADminton report Generation and Evaluation with LLM,Exa,2024-06-26,"The webpage describes a framework called **BADGE** (BADminton report Generation and Evaluation with LLM), which uses a Large Language Model (LLM) to automate the generation and evaluation of badminton match reports.

The process involves two phases:
1.  **Report Generation:** An LLM processes badminton-related data (tested with CSV data type) to generate a detailed match report. GPT-4 was found to perform best using Chain of Thought prompting with CSV data.
2.  **Report Evaluation:** The LLM then evaluates and scores the generated reports. Comparisons showed that human judges tended to prefer reports generated by GPT-4.

While the user query lists several topics related to multimodal AI, RAG, document understanding, and specific models like GPT-4V, Claude vision, and Gemini, this specific paper focuses on **report generation with LLMs** (specifically GPT-4) for **badminton reports**, which is a form of structured text generation based on input data, but it does not explicitly detail vision-language models, multimodal RAG, PDF parsing, chart/table extraction, or the other models mentioned in the query.",2025-12-25
multimodal_and_generation,exa,https://arxiv.org/abs/2407.12176,GPT-4V Cannot Generate Radiology Reports Yet,Exa,2024-07-16,"The provided webpage is an abstract for a research paper titled ""GPT-4V Cannot Generate Radiology Reports Yet."" The paper systematically evaluates GPT-4V's ability to generate radiology reports from chest X-rays using two datasets (MIMIC-CXR and IU X-Ray). The findings indicate that GPT-4V performs poorly in both lexical and clinical metrics when directly generating reports. Further decomposition showed that its performance in the medical image reasoning step (predicting conditions from images) is consistently low, suggesting it doesn't interpret X-rays meaningfully. Even when provided with groundtruth conditions, its generated reports were less correct and natural than those from a fine-tuned LLaMA-2. The conclusion casts doubt on using GPT-4V for automating radiology workflows.

This directly relates to your query about **Vision-language models** (like GPT-4V) and **report generation with LLMs**, but it does not discuss multimodal RAG, document understanding, PDF parsing, chart/table extraction, Claude vision, or Gemini.",2025-12-25
multimodal_and_generation,exa,https://www.sciencedirect.com/science/article/pii/S2950162824000535,A systematic evaluation of GPT-4V's multimodal capability for chest X-ray image analysis,Exa,2024-12-01,"The web page provides a systematic evaluation of **GPT-4V's multimodal capability for chest X-ray image analysis**, focusing on three tasks: **radiology report generation**, **medical visual question answering (VQA)**, and **medical visual grounding**.

Key findings related to your query:

*   **Vision-Language Models (GPT-4V):** The study evaluates GPT-4V, a state-of-the-art Large Multimodal Model (LMM), on medical image tasks.
*   **Report Generation with LLMs:** GPT-4V generates **competitive radiology reports** with high human-assessed accuracy, often surpassing conventional metrics like BLEU and ROUGE, as it can describe visually evident details not present in the ground truth.
*   **Medical VQA:** GPT-4V generates **detailed answers** for medical VQA, which human experts often rate as more accurate than suggested by conventional metrics (like BLEU-4), indicating that standard metrics are insufficient for assessing its performance in this area.
*   **Visual Grounding:** GPT-4V **needs substantial improvement** in the medical visual grounding task (accurately locating specific elements in images via bounding box coordinates).
*   **Evaluation:** The evaluation employed quantitative analysis, human analysis by medical experts, and case studies, highlighting a **discrepancy between conventional metrics and human assessment** for GPT-4V's performance.

The page does not specifically discuss **multimodal RAG**, **document understanding**, **PDF parsing**, or **structured document output** beyond the context of generating radiology reports (which are unstructured text outputs).

The user query covers a broad range of topics related to **multimodal vision-language models and document understanding**, including: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.

The provided webpage text focuses specifically on a **systematic evaluation of GPT-4V's multimodal capability for chest X-ray image analysis**, covering:

*   **Report Generation:** Evaluating GPT-4V's generated radiology reports using human assessment (consistency, completeness, terminology) and comparing these ratings against NLP metrics (BLEU, CIDEr, F1). It notes that human evaluation suggests GPT-4V's report quality is often higher than traditional metrics indicate, and that a mixed-example prompt improves performance.
*   **Visual Question Answering (VQA):** Assessing GPT-4V's answers to open-end VQA questions, finding that human evaluation rates its accuracy higher than classification-based metrics, and noting instances where GPT-4V relies on general medical knowledge instead of image specifics.
*   **Visual Grounding:** Evaluating GPT-4V's ability to generate bounding boxes, concluding its performance is suboptimal, especially for medical images, and suggesting methods like Set-of-Mark (SoM) prompting could improve this.

**Summary relative to the query:**

The page directly addresses **Vision-language models (specifically GPT-4V)** and **report generation with LLMs** in a medical context. It also touches upon **Visual Question Answering (VQA)**, which is a core multimodal task.

However, the page **does not mention or discuss** the following specific topics from the query:
*   Multimodal RAG
*   Document understanding
*   PDF parsing
*   Chart/table extraction
*   Claude vision
*   Gemini
*   Structured document output

Since the page only covers a subset of the requested topics, the summary focuses on what is present:

**Summary:**
This page systematically evaluates the performance of the vision-language model **GPT-4V** in medical image analysis, focusing on **radiology report generation** and **visual question answering (VQA)**. For report generation, human evaluation suggests GPT-4V produces high-quality reports whose clinical efficacy aligns better with clinical scores (like F1) than",2025-12-25
multimodal_and_generation,exa,https://www.semanticscholar.org/paper/GPT-4-Technical-Report-Achiam-Adler/163b4d6a79a5b19af88b8585456363340d9efd04,GPT-4 Technical Report,Exa,2023-03-15,"The GPT-4 Technical Report describes **GPT-4** as a **large-scale, multimodal model** that can accept **image and text inputs and produce text outputs**.

While the user query covers several specific topics related to multimodal AI, RAG, document understanding, and specific models like Claude vision and Gemini, the provided text directly confirms GPT-4's **multimodal** capability (accepting image and text inputs).

The document mentions:
*   **GPT-4** is a **multimodal model** (accepts image and text inputs).
*   It exhibits human-level performance on various benchmarks.
*   Related papers mention **GPT-4V(ision)**, which specifically addresses vision capabilities.
*   Another related paper evaluates **GPT-4V and Gemini** in Visual Question Answering (VQA).
*   Another paper discusses **report generation** using foundation models (though not specifically GPT-4 for *all* listed report types).

However, the text **does not explicitly detail** information on:
*   Multimodal RAG
*   Document understanding
*   PDF parsing
*   Chart/table extraction
*   Claude vision
*   Gemini (beyond a comparative evaluation in a related paper)
*   Structured document output

**Summary based on direct content:** The GPT-4 Technical Report confirms that **GPT-4 is a multimodal model** capable of accepting image and text inputs. Information regarding multimodal RAG, PDF parsing, chart/table extraction, and Claude vision is not present in the main summary of the technical report.",2025-12-25
multimodal_and_generation,exa,https://pmc.ncbi.nlm.nih.gov/articles/PMC12021971/,"Large language models for structured reporting in radiology: past, present, and future",Exa,unknown,"The user query asks about several topics related to multimodal and generative AI, including Vision-Language Models (VLMs), multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, and specific models like GPT-4V, Claude vision, and Gemini.

The provided webpage text is a review titled ""Large language models for structured reporting in radiology: past, present, and future.""

The text focuses primarily on:
1.  The history and goals of **Structured Reporting (SR)** in radiology.
2.  The evolution of **Natural Language Processing (NLP)** from statistical models to **Large Language Models (LLMs)**.
3.  A review of current literature using **LLMs (specifically GPT-3.5 and GPT-4)** for transforming **free-text radiology reports into structured reports**.
4.  Limitations and future applications of LLMs in radiology report processing (documentation, translation, summarization, clinical evaluation, and data mining).

While the text mentions that ""visual language models"" (which process images alongside text) are related but beyond the scope of their review, and it discusses LLMs generating structured output (which relates to report generation), it **does not** specifically discuss:
*   Vision-Language Models (VLMs) in general.
*   Multimodal RAG.
*   Document understanding, PDF parsing, or chart/table extraction as general tasks (though report structuring implies some level of data extraction).
*   Specific models like **GPT-4V, Claude vision, or Gemini**.

Since the core components of the query (VLMs, multimodal RAG, specific vision models) are not addressed in this text focused on text-based LLMs for radiology report structuring, the page does not fully answer the user's question.

No answer found

The webpage discusses the use of Large Language Models (LLMs) in structured reporting in radiology and their broader potential applications.

Regarding your query on **multimodal and generation** topics:

*   **Vision-language models (GPT-4V, Claude vision, Gemini):** The text mentions that multimodal generative models like **GPT-4V** have shown the ability to identify pathologies in selected images, sometimes outperforming GPT-4 without images in certain radiological subspecialties. However, it notes that they have not yet matched the performance of specialized deep learning models for slice-by-slice analysis. Their potential for image analysis, informing about incidental findings, and improving image reconstruction is mentioned as a future prospect.
*   **Report generation with LLMs:** The core of the article focuses on LLMs for **report generation** (structured reporting, SR), mentioning the use of models like GPT-3.5 and GPT-4 for transforming free-text reports into structured formats.
*   **Multimodal RAG, document understanding, PDF parsing, chart/table extraction, structured document output:** While the text extensively covers structured reporting and data mining from text reports, it **does not explicitly mention** multimodal RAG, PDF parsing, chart/table extraction, or structured document output beyond the context of radiology reports.

**Summary relevant to your query:**

The page confirms the use of **multimodal generative models** like **GPT-4V** in radiology for image analysis, though their performance is still developing compared to specialized models. It heavily discusses **report generation** using LLMs (like GPT-4) for structured reporting in radiology. Other specific terms like multimodal RAG, PDF parsing, and chart/table extraction are **not addressed**.",2025-12-25
multimodal_and_generation,exa,https://www.researchgate.net/publication/383739523_GPT-4_Technical_Report,(PDF) GPT-4 Technical Report,Exa,2023-03-15,"GPT-4 is described as a large-scale, **multimodal model** which can accept **image and text inputs** and produce text outputs. The report details its performance on various professional and academic benchmarks, showing human-level performance in many areas.

Regarding the specific terms in your query:
*   **Vision-language models:** GPT-4 is explicitly stated to be a ""large-scale, **multimodal model which can accept image and text inputs** and produce text outputs.""
*   **GPT-4V, Claude vision, Gemini:** The report focuses on **GPT-4** and its visual capabilities. It does not mention Claude vision or Gemini.
*   **Document understanding, PDF parsing, chart/table extraction, report generation with LLMs, structured document output:** The text mentions that GPT-4 exhibits similar capabilities to its text-only performance over inputs including ""documents with text and photographs, **diagrams, or screenshots**,"" and provides an example of analyzing a multi-panel image. However, it does not specifically detail its capabilities in dedicated **PDF parsing, chart/table extraction, or structured document output** beyond general visual input processing.

**Summary relevant to your query:**
GPT-4 is a large-scale, **multimodal model** capable of processing **image and text inputs** to generate text outputs. The report confirms its visual input capability, which extends to processing inputs containing documents, diagrams, or screenshots. It does not provide specific details on PDF parsing, chart/table extraction, or structured document output, nor does it mention Claude vision or Gemini.

The user query is about **multimodal and generation capabilities**, specifically mentioning: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.

The provided webpage is the **(PDF) GPT-4 Technical Report**.

While the report details the development and evaluation of GPT-4, and includes a section on **Vision Corecontributors** and an appendix showing an example of **GPT-4 visual input capability (Table 14)** demonstrating chart reading and computation, it does not explicitly detail or summarize the following specific topics mentioned in the query:

*   Multimodal RAG
*   Document understanding (beyond chart/table extraction shown in the example)
*   PDF parsing
*   Report generation with LLMs (though GPT-4 is used for report generation in the context of the technical report itself, the document doesn't summarize this capability generally)
*   Claude vision or Gemini (as this is a GPT-4 technical report)
*   Structured document output (beyond the general capabilities implied by chart/table extraction)

The report confirms GPT-4 has **vision capabilities** (GPT-4V is implied by the vision section and the visual input example).

**Conclusion:** The document confirms GPT-4 has vision capabilities, but it does not provide a summary covering all the specific multimodal and generation topics listed in the user query.

**No answer found**",2025-12-25
multimodal_and_generation,exa,https://arxiv.org/html/2303.08774v4,GPT-4 Technical Report,Exa,2025-10-31,"The GPT-4 Technical Report describes GPT-4 as a **large, multimodal model which can accept image and text inputs and produce text outputs.**

Regarding the user query's specific topics:

*   **Vision-language models:** GPT-4 is explicitly stated to be a **multimodal model** that accepts **image and text inputs**.
*   **GPT-4V, Claude vision, Gemini:** The report focuses on **GPT-4** and its visual capabilities. It does not mention Claude vision or Gemini.
*   **Document understanding, PDF parsing, chart/table extraction, report generation with LLMs, structured document output:** While the model handles image and text inputs, and is evaluated on academic exams that may include visual elements (like charts/tables in the exam context), the report **does not specifically detail** its capabilities in general document understanding, PDF parsing, chart/table extraction, or structured report generation. It focuses more on general image-text understanding and performance on standardized tests.

**Summary relevant to the query:**

GPT-4 is a **large, multimodal model** developed by OpenAI that can process **image and text inputs** to produce text outputs. The report confirms its visual input capability, but does not provide specific details on document understanding, PDF parsing, chart/table extraction, or structured document output beyond its general multimodal function. It does not mention Claude vision or Gemini.

No answer found",2025-12-25
multimodal_and_generation,exa,https://cdn.openai.com/papers/gpt-4.pdf,Untitled,Exa,2023-03-27,"GPT-4 is a large **multimodal** model developed by OpenAI that can accept **image and text inputs** and produce text outputs. It exhibits human-level performance on various professional and academic benchmarks.

The report details its capabilities, including:
*   **Multimodality:** It can process both images and text inputs.
*   **Performance on Exams:** It scored around the top 10% on a simulated bar exam, significantly outperforming GPT-3.5.
*   **Benchmark Performance:** It outperforms existing language models on numerous NLP benchmarks like MMLU and HumanEval.
*   **Language Capabilities:** It shows strong performance across many languages when tested on translated benchmarks.

The document **does not specifically mention** ""multimodal RAG,"" ""**document understanding**,"" ""**PDF parsing**,"" ""**chart/table extraction**,"" ""**report generation with LLMs**,"" ""**Claude vision**,"" or ""**Gemini**."" It focuses on GPT-4's general multimodal input capability (handling images and text) and its text output generation. It also discusses structured output implicitly through its performance on exams requiring specific formats.

The provided text is a detailed list of contributors to a project (likely GPT-4, based on context clues like ""GPT-4"" and ""GPT-3.5"" mentions in the appendices) across various core areas like Pretraining, Data, Optimization, Vision, Reinforcement Learning & Alignment, Evaluation & Analysis, and Deployment.

The user query asks for a summary related to: **'multimodal\_and\_generation: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, structured document output'**

The webpage text mentions a ""Vision Core contributors"" section and discusses evaluations involving ""GPT-4 (with and without vision),"" indicating that multimodal capabilities were part of the work being documented. However, the text **does not contain any specific information or discussion** about:

*   Vision-language models (beyond listing contributors to the Vision team).
*   Multimodal RAG.
*   Document understanding, PDF parsing, chart/table extraction, or structured document output.
*   Specific mentions of GPT-4V, Claude vision, or Gemini (though GPT-4 vision evaluation is mentioned).
*   Report generation with LLMs.

Since the text is primarily a list of personnel and appendices detailing evaluation methodology (including vision evaluations), it does not summarize the specific topics requested in the user query.

**No answer found**",2025-12-25
multimodal_and_generation,exa,https://aclanthology.org/2025.findings-naacl.113.pdf,Untitled,Exa,2025-04-29,"The user query asks for a summary related to **multimodal and generation** topics, specifically mentioning: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.

The provided webpage text is a research paper titled ""GPT-4V Cannot Generate Radiology Reports Yet.""

Here is a summary of the webpage content relevant to the user's query:

The paper systematically evaluates **GPT-4V** (including GPT-4o and vision preview) for **radiology report generation** using chest X-ray benchmarks (MIMIC CXR, CheXpert Plus, and IU X-Ray).

**Key Findings Related to the Query:**

*   **Vision-Language Models (GPT-4V) and Report Generation:** The study concludes that **GPT-4V cannot generate satisfactory radiology reports yet**, performing significantly worse than specialized state-of-the-art models across both lexical and clinical efficacy metrics, even with various prompting strategies (zero-shot, CoT, few-shot).
*   **Medical Image Reasoning:** The research decomposes the task and finds that GPT-4V performs poorly in the **medical image reasoning step** (predicting medical condition labels from images). Statistical tests suggest the model is not interpreting chest X-rays meaningfully, as its label predictions do not vary significantly based on the ground truth conditions present in the image.
*   **Report Synthesis:** Even when the image reasoning bottleneck is bypassed by providing **groundtruth conditions**, GPT-4V still underperforms a finetuned Llama-2 baseline in generating clinically accurate and well-written reports. Human evaluation confirmed that GPT-4V reports lacked diagnostic accuracy and completeness compared to the finetuned Llama-2 and groundtruth reports.
*   **Comparison to Other Models:** The paper notes that open-sourced models like Llama-3.2 vision perform similarly poorly to GPT-4V. The limitations section explicitly states that comparisons with other general-domain multimodal LMMs, including **Gemini** and **Claude vision**, are reserved for future research.

**In summary, the page focuses heavily on evaluating GPT-4V's limitations in multimodal report generation (specifically radiology reports) due to poor image reasoning, but it does not discuss multimodal RAG, PDF parsing, chart/table extraction, or structured

The provided web page text is a collection of references, model implementation details, data licensing information, experiment results (including statistical tests and performance tables), evaluation prompts, and examples of generated radiology reports, primarily focusing on evaluating Large Language Models (LLMs) like GPT-4V, GPT-4o, and Llama-3.2-90B-Vision for **medical image analysis and report generation** (specifically chest X-rays).

The user query is: 'multimodal_and_generation: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, structured document output'

**Summary relevant to the query:**

The text details experiments involving **Vision-language models** (specifically **GPT-4V**, **GPT-4o**, and **Llama-3.2-90B-Vision-Instruct**) for **report generation** based on medical images (chest X-rays). It evaluates their performance using various prompting strategies (Basic, Indication enhancement, Chain-of-Thought, Few-shot) across different datasets (IU X-RAY, MIMIC-CXR, ChexPert Plus). The evaluation includes lexical metrics (BLEU, ROUGE) and clinical efficacy metrics (Positive F1 scores). There is also mention of **image reasoning** (medical condition labeling) using these models.

**Information missing or not explicitly covered:**

The text **does not** discuss:
*   Multimodal RAG (Retrieval-Augmented Generation).
*   General **document understanding** or **PDF parsing**.
*   **Chart/table extraction** from documents.
*   The performance of **Claude vision** or **Gemini** (though GPT-4V and GPT-4o are evaluated).
*   Generating **structured document output** beyond the standard FINDINGS/IMPRESSION report format.

Since the core topics of multimodal RAG, PDF parsing, chart/table extraction, and the specific models Claude vision and Gemini are not addressed in the provided text, the answer is:

No answer found",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21336v1,Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty,arXiv,2025-12-24,"Summary: Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21335v1,Autonomous Uncertainty Quantification for Computational Point-of-care Sensors,arXiv,2025-12-24,"Summary: Computational point-of-care (POC) sensors enable rapid, low-cost, and accessible diagnostics in emergency, remote and resource-limited areas that lack access to centralized medical facilities. These systems can utilize neural network-based algorithms to accurately infer a diagnosis from the signals generated by rapid diagnostic tests or sensors. However, neural network-based diagnostic models are subject to hallucinations and can produce erroneous predictions, posing a risk of misdiagnosis and inaccurate clinical decisions. To address this challenge, here we present an autonomous uncertainty quantification technique developed for POC diagnostics. As our testbed, we used a paper-based, computational vertical flow assay (xVFA) platform developed for rapid POC diagnosis of Lyme disease, the most prevalent tick-borne disease globally. The xVFA platform integrates a disposable paper-based assay, a handheld optical reader and a neural network-based inference algorithm, providing rapid and cost-effective Lyme disease diagnostics in under 20 min using only 20 uL of patient serum. By incorporating a Monte Carlo dropout (MCDO)-based uncertainty quantification approach into the diagnostics pipeline, we identified and excluded erroneous predictions with high uncertainty, significantly improving the sensitivity and reliability of the xVFA in an autonomous manner, without access to the ground truth diagnostic information of patients. Blinded testing using new patient samples demonstrated an increase in diagnostic sensitivity from 88.2% to 95.7%, indicating the effectiveness of MCDO-based uncertainty quantification in enhancing the robustness of neural network-driven computational POC sensing systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21332v1,C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling,arXiv,2025-12-24,"Summary: We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21329v1,Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks,arXiv,2025-12-24,"Summary: Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21326v1,Measuring all the noises of LLM Evals,arXiv,2025-12-24,"Summary: Separating signal from noise is central to experimental science. Applying well-established statistical method effectively to LLM evals requires consideration of their unique noise characteristics. We clearly define and measure three types of noise: prediction noise from generating different answers on a given question, data noise from sampling questions, and their combined total noise following the law of total variance. To emphasize relative comparisons and gain statistical power, we propose the all-pairs paired method, which applies the paired analysis to all pairs of LLMs and measures all the noise components based on millions of question-level predictions across many evals and settings. These measurements revealed clear patterns. First, each eval exhibits a characteristic and highly predictable total noise level across all model pairs. Second, paired prediction noise typically exceeds paired data noise, which means reducing prediction noise by averaging can significantly increase statistical power. These findings enable practitioners to assess significance without custom testing and to detect much smaller effects in controlled experiments.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21323v1,Parallel Token Prediction for Language Models,arXiv,2025-12-24,"Summary: We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21320v1,An Allele-Centric Pan-Graph-Matrix Representation for Scalable Pangenome Analysis,arXiv,2025-12-24,"Summary: Population-scale pangenome analysis increasingly requires representations that unify single-nucleotide and structural variation while remaining scalable across large cohorts. Existing formats are typically sequence-centric, path-centric, or sample-centric, and often obscure population structure or fail to exploit carrier sparsity. We introduce the H1 pan-graph-matrix, an allele-centric representation that encodes exact haplotype membership using adaptive per-allele compression. By treating alleles as first-class objects and selecting optimal encodings based on carrier distribution, H1 achieves near-optimal storage across both common and rare variants. We further introduce H2, a path-centric dual representation derived from the same underlying allele-haplotype incidence information that restores explicit haplotype ordering while remaining exactly equivalent in information content. Using real human genome data, we show that this representation yields substantial compression gains, particularly for structural variants, while remaining equivalent in information content to pangenome graphs. H1 provides a unified, population-aware foundation for scalable pangenome analysis and downstream applications such as rare-variant interpretation and drug discovery.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21319v1,Variationally correct operator learning: Reduced basis neural operator with a posteriori error estimation,arXiv,2025-12-24,"Summary: Minimizing PDE-residual losses is a common strategy to promote physical consistency in neural operators. However, standard formulations often lack variational correctness, meaning that small residuals do not guarantee small solution errors due to the use of non-compliant norms or ad hoc penalty terms for boundary conditions. This work develops a variationally correct operator learning framework by constructing first-order system least-squares (FOSLS) objectives whose values are provably equivalent to the solution error in PDE-induced norms. We demonstrate this framework on stationary diffusion and linear elasticity, incorporating mixed Dirichlet-Neumann boundary conditions via variational lifts to preserve norm equivalence without inconsistent penalties. To ensure the function space conformity required by the FOSLS loss, we propose a Reduced Basis Neural Operator (RBNO). The RBNO predicts coefficients for a pre-computed, conforming reduced basis, thereby ensuring variational stability by design while enabling efficient training. We provide a rigorous convergence analysis that bounds the total error by the sum of finite element discretization bias, reduced basis truncation error, neural network approximation error, and statistical estimation errors arising from finite sampling and optimization. Numerical benchmarks validate these theoretical bounds and demonstrate that the proposed approach achieves superior accuracy in PDE-compliant norms compared to standard baselines, while the residual loss serves as a reliable, computable a posteriori error estimator.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21316v1,"Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Consulting, Data Analyst, and Management Tasks",arXiv,2025-12-24,"Summary: This paper derives `Scaling Laws for Economic Impacts' -- empirical relationships between the training compute of Large Language Models (LLMs) and professional productivity. In a preregistered experiment, over 500 consultants, data analysts, and managers completed professional tasks using one of 13 LLMs. We find that each year of AI model progress reduced task time by 8%, with 56% of gains driven by increased compute and 44% by algorithmic progress. However, productivity gains were significantly larger for non-agentic analytical tasks compared to agentic workflows requiring tool use. These findings suggest continued model scaling could boost U.S. productivity by approximately 20% over the next decade.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21315v1,Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks,arXiv,2025-12-24,"Summary: The data processing inequality is an information-theoretic principle stating that the information content of a signal cannot be increased by processing the observations. In particular, it suggests that there is no benefit in enhancing the signal or encoding it before addressing a classification problem. This assertion can be proven to be true for the case of the optimal Bayes classifier. However, in practice, it is common to perform ""low-level"" tasks before ""high-level"" downstream tasks despite the overwhelming capabilities of modern deep neural networks. In this paper, we aim to understand when and why low-level processing can be beneficial for classification. We present a comprehensive theoretical study of a binary classification setup, where we consider a classifier that is tightly connected to the optimal Bayes classifier and converges to it as the number of training samples increases. We prove that for any finite number of training samples, there exists a pre-classification processing that improves the classification accuracy. We also explore the effect of class separation, training set size, and class balance on the relative gain from this procedure. We support our theory with an empirical investigation of the theoretical setup. Finally, we conduct an empirical study where we investigate the effect of denoising and encoding on the performance of practical deep classifiers on benchmark datasets. Specifically, we vary the size and class distribution of the training set, and the noise level, and demonstrate trends that are consistent with our theoretical results.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21311v1,Learning to Solve PDEs on Neural Shape Representations,arXiv,2025-12-24,"Summary: Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21309v1,A Plan Reuse Mechanism for LLM-Driven Agent,arXiv,2025-12-24,"Summary: Integrating large language models (LLMs) into personal assistants, like Xiao Ai and Blue Heart V, effectively enhances their ability to interact with humans, solve complex tasks, and manage IoT devices. Such assistants are also termed LLM-driven agents. Upon receiving user requests, the LLM-driven agent generates plans using an LLM, executes these plans through various tools, and then returns the response to the user. During this process, the latency for generating a plan with an LLM can reach tens of seconds, significantly degrading user experience. Real-world dataset analysis shows that about 30% of the requests received by LLM-driven agents are identical or similar, which allows the reuse of previously generated plans to reduce latency. However, it is difficult to accurately define the similarity between the request texts received by the LLM-driven agent through directly evaluating the original request texts. Moreover, the diverse expressions of natural language and the unstructured format of plan texts make implementing plan reuse challenging. To address these issues, we present and implement a plan reuse mechanism for LLM-driven agents called AgentReuse. AgentReuse leverages the similarities and differences among requests' semantics and uses intent classification to evaluate the similarities between requests and enable the reuse of plans. Experimental results based on a real-world dataset demonstrate that AgentReuse achieves a 93% effective plan reuse rate, an F1 score of 0.9718, and an accuracy of 0.9459 in evaluating request similarities, reducing latency by 93.12% compared with baselines without using the reuse mechanism.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21301v1,Transcriptome-Conditioned Personalized De Novo Drug Generation for AML Using Metaheuristic Assembly and Target-Driven Filtering,arXiv,2025-12-24,"Summary: Acute Myeloid Leukemia (AML) remains a clinical challenge due to its extreme molecular heterogeneity and high relapse rates. While precision medicine has introduced mutation-specific therapies, many patients still lack effective, personalized options. This paper presents a novel, end-to-end computational framework that bridges the gap between patient-specific transcriptomics and de novo drug discovery. By analyzing bulk RNA sequencing data from the TCGA-LAML cohort, the study utilized Weighted Gene Co-expression Network Analysis (WGCNA) to prioritize 20 high-value biomarkers, including metabolic transporters like HK3 and immune-modulatory receptors such as SIGLEC9. The physical structures of these targets were modeled using AlphaFold3, and druggable hotspots were quantitatively mapped via the DOGSiteScorer engine. Then developed a novel, reaction-first evolutionary metaheuristic algorithm as well as multi-objective optimization programming that assembles novel ligands from fragment libraries, guided by spatial alignment to these identified hotspots. The generative model produced structurally unique chemical entities with a strong bias toward drug-like space, as evidenced by QED scores peaking between 0.5 and 0.7. Validation through ADMET profiling and SwissDock molecular docking identified high-confidence candidates, such as Ligand L1, which achieved a binding free energy of -6.571 kcal/mol against the A08A96 biomarker. These results demonstrate that integrating systems biology with metaheuristic molecular assembly can produce pharmacologically viable, patient tailored leads, offering a scalable blueprint for precision oncology in AML and beyond",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21300v1,Closed-form empirical Bernstein confidence sequences for scalars and matrices,arXiv,2025-12-24,"Summary: We derive a new closed-form variance-adaptive confidence sequence (CS) for estimating the average conditional mean of a sequence of bounded random variables. Empirically, it yields the tightest closed-form CS we have found for tracking time-varying means, across sample sizes up to $\approx 10^6$. When the observations happen to have the same conditional mean, our CS is asymptotically tighter than the recent closed-form CS of Waudby-Smith and Ramdas [38]. It also has other desirable properties: it is centered at the unweighted sample mean and has limiting width (multiplied by $\sqrt{t/\log t}$) independent of the significance level. We extend our results to provide a CS with the same properties for random matrices with bounded eigenvalues.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21293v1,Quadrupped-Legged Robot Movement Plan Generation using Large Language Model,arXiv,2025-12-24,"Summary: Traditional control interfaces for quadruped robots often impose a high barrier to entry, requiring specialized technical knowledge for effective operation. To address this, this paper presents a novel control framework that integrates Large Language Models (LLMs) to enable intuitive, natural language-based navigation. We propose a distributed architecture where high-level instruction processing is offloaded to an external server to overcome the onboard computational constraints of the DeepRobotics Jueying Lite 3 platform. The system grounds LLM-generated plans into executable ROS navigation commands using real-time sensor fusion (LiDAR, IMU, and Odometry). Experimental validation was conducted in a structured indoor environment across four distinct scenarios, ranging from single-room tasks to complex cross-zone navigation. The results demonstrate the system's robustness, achieving an aggregate success rate of over 90\% across all scenarios, validating the feasibility of offloaded LLM-based planning for autonomous quadruped deployment in real-world settings.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21288v1,Model Merging via Multi-Teacher Knowledge Distillation,arXiv,2025-12-24,"Summary: Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics to approximate the optimal combination of parameters. This dependence is most critical in coefficient scaling, the weighting factors that modulate the magnitude of each fine-tuned model's contribution to the shared parameter. However, without a principled objective to guide their selection, these methods lead to brittle performance and are highly sensitive to scaling initialization. We address this gap by (i) establishing a novel flatness-aware PAC-Bayes generalization bound specifically for the model merging setting. This analysis introduces a ""cross-task heterogeneity"" term that formally captures the mismatch between diverse fine-tuned model priors and the target multi-task distributions. Guided by this theoretical insight, (ii) we frame model merging as multi-teacher knowledge distillation on scarce, unlabeled data. We formally demonstrate that minimizing the student-teacher Kullback-Leibler divergence directly tightens the upper bound on the merged model's excess risk. Guided by the flatness-aware bound derived, (iii) we operationalize this objective via SAMerging, a method that employs Sharpness-Aware Minimization (SAM) to find flat minima. Empirically, SAMerging establishes a new state of the art across vision and NLP benchmarks, achieving remarkable performance. The code is available at https://github.com/arshandalili/SAMerging.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21280v1,"SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance",arXiv,2025-12-24,"Summary: The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21257v1,ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling,arXiv,2025-12-24,"Summary: Industrial recommender systems face two fundamental limitations under the log-driven paradigm: (1) knowledge poverty in ID-based item representations that causes brittle interest modeling under data sparsity, and (2) systemic blindness to beyond-log user interests that constrains model performance within platform boundaries. These limitations stem from an over-reliance on shallow interaction statistics and close-looped feedback while neglecting the rich world knowledge about product semantics and cross-domain behavioral patterns that Large Language Models have learned from vast corpora.
  To address these challenges, we introduce ReaSeq, a reasoning-enhanced framework that leverages world knowledge in Large Language Models to address both limitations through explicit and implicit reasoning. Specifically, ReaSeq employs explicit Chain-of-Thought reasoning via multi-agent collaboration to distill structured product knowledge into semantically enriched item representations, and latent reasoning via Diffusion Large Language Models to infer plausible beyond-log behaviors. Deployed on Taobao's ranking system serving hundreds of millions of users, ReaSeq achieves substantial gains: >6.0% in IPV and CTR, >2.9% in Orders, and >2.5% in GMV, validating the effectiveness of world-knowledge-enhanced reasoning over purely log-driven approaches.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21250v1,CoTDeceptor:Adversarial Code Obfuscation Against CoT-Enhanced LLM Code Agents,arXiv,2025-12-24,"Summary: LLM-based code agents(e.g., ChatGPT Codex) are increasingly deployed as detector for code review and security auditing tasks. Although CoT-enhanced LLM vulnerability detectors are believed to provide improved robustness against obfuscated malicious code, we find that their reasoning chains and semantic abstraction processes exhibit exploitable systematic weaknesses.This allows attackers to covertly embed malicious logic, bypass code review, and propagate backdoored components throughout real-world software supply chains.To investigate this issue, we present CoTDeceptor, the first adversarial code obfuscation framework targeting CoT-enhanced LLM detectors. CoTDeceptor autonomously constructs evolving, hard-to-reverse multi-stage obfuscation strategy chains that effectively disrupt CoT-driven detection logic.We obtained malicious code provided by security enterprise, experimental results demonstrate that CoTDeceptor achieves stable and transferable evasion performance against state-of-the-art LLMs and vulnerability detection agents. CoTDeceptor bypasses 14 out of 15 vulnerability categories, compared to only 2 bypassed by prior methods. Our findings highlight potential risks in real-world software supply chains and underscore the need for more robust and interpretable LLM-powered security analysis systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21246v1,Learning Factors in AI-Augmented Education: A Comparative Study of Middle and High School Students,arXiv,2025-12-24,"Summary: The increasing integration of AI tools in education has led prior research to explore their impact on learning processes. Nevertheless, most existing studies focus on higher education and conventional instructional contexts, leaving open questions about how key learning factors are related in AI-mediated learning environments and how these relationships may vary across different age groups. Addressing these gaps, our work investigates whether four critical learning factors, experience, clarity, comfort, and motivation, maintain coherent interrelationships in AI-augmented educational settings, and how the structure of these relationships differs between middle and high school students. The study was conducted in authentic classroom contexts where students interacted with AI tools as part of programming learning activities to collect data on the four learning factors and students' perceptions. Using a multimethod quantitative analysis, which combined correlation analysis and text mining, we revealed markedly different dimensional structures between the two age groups. Middle school students exhibit strong positive correlations across all dimensions, indicating holistic evaluation patterns whereby positive perceptions in one dimension generalise to others. In contrast, high school students show weak or near-zero correlations between key dimensions, suggesting a more differentiated evaluation process in which dimensions are assessed independently. These findings reveal that perception dimensions actively mediate AI-augmented learning and that the developmental stage moderates their interdependencies. This work establishes a foundation for the development of AI integration strategies that respond to learners' developmental levels and account for age-specific dimensional structures in student-AI interactions.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21243v1,LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation,arXiv,2025-12-24,"Summary: Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21241v1,Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks,arXiv,2025-12-24,"Summary: In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum $\ell_2$-norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, which proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum. We provide a theoretical analysis of its convergence behavior, showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization. To further accelerate convergence, we incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance. The superiority of our approach is supported by theoretical guarantees under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that our method surpasses 13 state-of-the-art approaches in query efficiency.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21238v1,Assessing the Software Security Comprehension of Large Language Models,arXiv,2025-12-24,"Summary: Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21236v1,Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking,arXiv,2025-12-24,"Summary: Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21231v1,MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models,arXiv,2025-12-24,"Summary: Large Language Models can develop reasoning capabilities through online fine-tuning with rule-based rewards. However, recent studies reveal a critical constraint: reinforcement learning succeeds only when the base model already assigns non-negligible probability to correct answers -- a property we term 'latent solvability'. This work investigates the emergence of chemical reasoning capabilities and what these prerequisites mean for chemistry. We identify two necessary conditions for RL-based chemical reasoning: 1) Symbolic competence, and 2) Latent chemical knowledge. We propose mid-stage scientific training (MiST): a set of mid-stage training techniques to satisfy these, including data-mixing with SMILES/CIF-aware pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. These steps raise the latent-solvability score on 3B and 7B models by up to 1.8x, and enable RL to lift top-1 accuracy from 10.9 to 63.9% on organic reaction naming, and from 40.6 to 67.4% on inorganic material generation. Similar results are observed for other challenging chemical tasks, while producing interpretable reasoning traces. Our results define clear prerequisites for chemical reasoning training and highlight the broader role of mid-stage training in unlocking reasoning capabilities.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21227v1,PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation,arXiv,2025-12-24,"Summary: In this work, we introduce PhononBench, the first large-scale benchmark for dynamical stability in AI-generated crystals. Leveraging the recently developed MatterSim interatomic potential, which achieves DFT-level accuracy in phonon predictions across more than 10,000 materials, PhononBench enables efficient large-scale phonon calculations and dynamical-stability analysis for 108,843 crystal structures generated by six leading crystal generation models. PhononBench reveals a widespread limitation of current generative models in ensuring dynamical stability: the average dynamical-stability rate across all generated structures is only 25.83%, with the top-performing model, MatterGen, reaching just 41.0%. Further case studies show that in property-targeted generation-illustrated here by band-gap conditioning with MatterGen--the dynamical-stability rate remains as low as 23.5% even at the optimal band-gap condition of 0.5 eV. In space-group-controlled generation, higher-symmetry crystals exhibit better stability (e.g., cubic systems achieve rates up to 49.2%), yet the average stability across all controlled generations is still only 34.4%. An important additional outcome of this study is the identification of 28,119 crystal structures that are phonon-stable across the entire Brillouin zone, providing a substantial pool of reliable candidates for future materials exploration. By establishing the first large-scale dynamical-stability benchmark, this work systematically highlights the current limitations of crystal generation models and offers essential evaluation criteria and guidance for their future development toward the design and discovery of physically viable materials. All model-generated crystal structures, phonon calculation results, and the high-throughput evaluation workflows developed in PhononBench will be openly released at https://github.com/xqh19970407/PhononBench",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21221v1,Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval,arXiv,2025-12-24,"Summary: Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21220v1,RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic,arXiv,2025-12-24,"Summary: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21211v1,Causal-driven attribution (CDA): Estimating channel influence without user-level data,arXiv,2025-12-24,"Summary: Attribution modelling lies at the heart of marketing effectiveness, yet most existing approaches depend on user-level path data, which are increasingly inaccessible due to privacy regulations and platform restrictions. This paper introduces a Causal-Driven Attribution (CDA) framework that infers channel influence using only aggregated impression-level data, avoiding any reliance on user identifiers or click-path tracking. CDA integrates temporal causal discovery (using PCMCI) with causal effect estimation via a Structural Causal Model to recover directional channel relationships and quantify their contributions to conversions. Using large-scale synthetic data designed to replicate real marketing dynamics, we show that CDA achieves an average relative RMSE of 9.50% when given the true causal graph, and 24.23% when using the predicted graph, demonstrating strong accuracy under correct structure and meaningful signal recovery even under structural uncertainty. CDA captures cross-channel interdependencies while providing interpretable, privacy-preserving attribution insights, offering a scalable and future-proof alternative to traditional path-based models.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21208v1,Analytic and Variational Stability of Deep Learning Systems,arXiv,2025-12-24,"Summary: We propose a unified analytic and variational framework for studying stability in deep learning systems viewed as coupled representation-parameter dynamics. The central object is the Learning Stability Profile, which tracks the infinitesimal response of representations, parameters, and update mechanisms to perturbations along the learning trajectory. We prove a Fundamental Analytic Stability Theorem showing that uniform boundedness of these stability signatures is equivalent, up to norm equivalence, to the existence of a Lyapunov-type energy that dissipates along the learning flow. In smooth regimes, the framework yields explicit stability exponents linking spectral norms, activation regularity, step sizes, and learning rates to contractivity of the learning dynamics. Classical spectral stability results for feedforward networks, a discrete CFL-type condition for residual architectures, and parametric and temporal stability laws for stochastic gradient methods arise as direct consequences. The theory extends to non-smooth learning systems, including ReLU networks, proximal and projected updates, and stochastic subgradient flows, by replacing classical derivatives with Clarke generalized derivatives and smooth energies with variational Lyapunov functionals. The resulting framework provides a unified dynamical description of stability across architectures and optimization methods, clarifying how architectural and algorithmic choices jointly govern robustness and sensitivity to perturbations. It also provides a foundation for further extensions to continuous-time limits and geometric formulations of learning dynamics.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21204v1,SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation,arXiv,2025-12-24,"Summary: Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21201v1,Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation,arXiv,2025-12-24,"Summary: Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21170v1,A Unified Framework for EEG Seizure Detection Using Universum-Integrated Generalized Eigenvalues Proximal Support Vector Machine,arXiv,2025-12-24,"Summary: The paper presents novel Universum-enhanced classifiers: the Universum Generalized Eigenvalue Proximal Support Vector Machine (U-GEPSVM) and the Improved U-GEPSVM (IU-GEPSVM) for EEG signal classification. Using the computational efficiency of generalized eigenvalue decomposition and the generalization benefits of Universum learning, the proposed models address critical challenges in EEG analysis: non-stationarity, low signal-to-noise ratio, and limited labeled data. U-GEPSVM extends the GEPSVM framework by incorporating Universum constraints through a ratio-based objective function, while IU-GEPSVM enhances stability through a weighted difference-based formulation that provides independent control over class separation and Universum alignment. The models are evaluated on the Bonn University EEG dataset across two binary classification tasks: (O vs S)-healthy (eyes closed) vs seizure, and (Z vs S)-healthy (eyes open) vs seizure. IU-GEPSVM achieves peak accuracies of 85% (O vs S) and 80% (Z vs S), with mean accuracies of 81.29% and 77.57% respectively, outperforming baseline methods.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21166v1,A Community-Enhanced Graph Representation Model for Link Prediction,arXiv,2025-12-24,"Summary: Although Graph Neural Networks (GNNs) have become the dominant approach for graph representation learning, their performance on link prediction tasks does not always surpass that of traditional heuristic methods such as Common Neighbors and Jaccard Coefficient. This is mainly because existing GNNs tend to focus on learning local node representations, making it difficult to effectively capture structural relationships between node pairs. Furthermore, excessive reliance on local neighborhood information can lead to over-smoothing. Prior studies have shown that introducing global structural encoding can partially alleviate this issue. To address these limitations, we propose a Community-Enhanced Link Prediction (CELP) framework that incorporates community structure to jointly model local and global graph topology. Specifically, CELP enhances the graph via community-aware, confidence-guided edge completion and pruning, while integrating multi-scale structural features to achieve more accurate link prediction. Experimental results across multiple benchmark datasets demonstrate that CELP achieves superior performance, validating the crucial role of community structure in improving link prediction accuracy.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21165v1,BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft,arXiv,2025-12-24,"Summary: Randomized election timeouts are a simple and effective liveness heuristic for Raft, but they become brittle under long-tail latency, jitter, and partition recovery, where repeated split votes can inflate unavailability. This paper presents BALLAST, a lightweight online adaptation mechanism that replaces static timeout heuristics with contextual bandits. BALLAST selects from a discrete set of timeout ""arms"" using efficient linear contextual bandits (LinUCB variants), and augments learning with safe exploration to cap risk during unstable periods. We evaluate BALLAST on a reproducible discrete-event simulation with long-tail delay, loss, correlated bursts, node heterogeneity, and partition/recovery turbulence. Across challenging WAN regimes, BALLAST substantially reduces recovery time and unwritable time compared to standard randomized timeouts and common heuristics, while remaining competitive on stable LAN/WAN settings.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21153v1,ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update,arXiv,2025-12-24,"Summary: In this paper, we present ElfCore, a 28nm digital spiking neural network processor tailored for event-driven sensory signal processing. ElfCore is the first to efficiently integrate: (1) a local online self-supervised learning engine that enables multi-layer temporal learning without labeled inputs; (2) a dynamic structured sparse training engine that supports high-accuracy sparse-to-sparse learning; and (3) an activity-dependent sparse weight update mechanism that selectively updates weights based solely on input activity and network dynamics. Demonstrated on tasks including gesture recognition, speech, and biomedical signal processing, ElfCore outperforms state-of-the-art solutions with up to 16X lower power consumption, 3.8X reduced on-chip memory requirements, and 5.9X greater network capacity efficiency.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21152v1,MODE: Multi-Objective Adaptive Coreset Selection,arXiv,2025-12-24,"Summary: We present Mode(Multi-Objective adaptive Data Efficiency), a framework that dynamically combines coreset selection strategies based on their evolving contribution to model performance. Unlike static methods, \mode adapts selection criteria to training phases: emphasizing class balance early, diversity during representation learning, and uncertainty at convergence. We show that MODE achieves (1-1/e)-approximation with O(n \log n) complexity and demonstrates competitive accuracy while providing interpretable insights into data utility evolution. Experiments show \mode reduces memory requirements",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21149v1,Equilibrium investment under dynamic preference uncertainty,arXiv,2025-12-24,"Summary: We study a continuous-time portfolio choice problem for an investor whose state-dependent preferences are determined by an exogenous factor that evolves as an Itô diffusion process. Since risk attitudes at the end of the investment horizon are uncertain, terminal wealth is evaluated under a set of utility functions corresponding to all possible future preference states. These utilities are first converted into certainty equivalents at their respective levels of terminal risk aversion and then (nonlinearly) aggregated over the conditional distribution of future states, yielding an inherently time-inconsistent optimization criterion. We approach this problem by developing a general equilibrium framework for such state-dependent preferences and characterizing subgame-perfect equilibrium investment policies through an extended Hamilton-Jacobi-Bellman system. This system gives rise to a coupled nonlinear partial integro-differential equation for the value functions associated with each state. We then specialize the model to a tractable constant relative risk aversion specification in which the preference factor follows an arithmetic Brownian motion. In this setting, the equilibrium policy admits a semi-explicit representation that decomposes into a standard myopic demand and a novel preference-hedging component that captures incentives to hedge against anticipated changes in risk aversion. Numerical experiments illustrate how features of the preference dynamics -- most notably the drift of the preference process and the correlation between preference shocks and asset returns -- jointly determine the sign and magnitude of the hedging demand and the evolution of the equilibrium risky investment over time.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21137v1,Declarative distributed broadcast using three-valued modal logic and semitopologies,arXiv,2025-12-24,"Summary: We demonstrate how to formally specify distributed algorithms as declarative axiomatic theories in a modal logic. We exhibit the method on a simple voting protocol, a simple broadcast protocol, and a simple agreement protocol. The methods scale well and have been used to find errors in a proposed industrial protocol. The key novelty is to use modal logic to capture a declarative, high-level representation of essential system properties -- the logical essence of the algorithm -- while abstracting away from transitions of an abstract machine that implements it. It is like the difference between specifying code in a functional or logic programming language, versus specifying code in an imperative one.
  A logical axiomatisation in the style we propose provides a precise, compact, human-readable specification that abstractly captures essential system properties, while eliding low-level implementation details; it is more precise than a natural language description, yet more abstract than source code or a logical specification thereof. This creates new opportunities for reasoning about correctness, resilience, and failure, and could serve as a foundation for human- and machine verification efforts, design improvements, and even alternative protocol implementations.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21135v1,TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation,arXiv,2025-12-24,"Summary: Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21132v1,AutoBaxBuilder: Bootstrapping Code Security Benchmarking,arXiv,2025-12-24,"Summary: As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21129v1,Active inference and artificial reasoning,arXiv,2025-12-24,"Summary: This technical note considers the sampling of outcomes that provide the greatest amount of information about the structure of underlying world models. This generalisation furnishes a principled approach to structure learning under a plausible set of generative models or hypotheses. In active inference, policies - i.e., combinations of actions - are selected based on their expected free energy, which comprises expected information gain and value. Information gain corresponds to the KL divergence between predictive posteriors with, and without, the consequences of action. Posteriors over models can be evaluated quickly and efficiently using Bayesian Model Reduction, based upon accumulated posterior beliefs about model parameters. The ensuing information gain can then be used to select actions that disambiguate among alternative models, in the spirit of optimal experimental design. We illustrate this kind of active selection or reasoning using partially observed discrete models; namely, a 'three-ball' paradigm used previously to describe artificial insight and 'aha moments' via (synthetic) introspection or sleep. We focus on the sample efficiency afforded by seeking outcomes that resolve the greatest uncertainty about the world model, under which outcomes are generated.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21127v1,A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care,arXiv,2025-12-24,"Summary: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21126v1,MarineEval: Assessing the Marine Intelligence of Vision-Language Models,arXiv,2025-12-24,"Summary: We have witnessed promising progress led by large language models (LLMs) and further vision language models (VLMs) in handling various queries as a general-purpose assistant. VLMs, as a bridge to connect the visual world and language corpus, receive both visual content and various text-only user instructions to generate corresponding responses. Though great success has been achieved by VLMs in various fields, in this work, we ask whether the existing VLMs can act as domain experts, accurately answering marine questions, which require significant domain expertise and address special domain challenges/requirements. To comprehensively evaluate the effectiveness and explore the boundary of existing VLMs, we construct the first large-scale marine VLM dataset and benchmark called MarineEval, with 2,000 image-based question-answering pairs. During our dataset construction, we ensure the diversity and coverage of the constructed data: 7 task dimensions and 20 capacity dimensions. The domain requirements are specially integrated into the data construction and further verified by the corresponding marine domain experts. We comprehensively benchmark 17 existing VLMs on our MarineEval and also investigate the limitations of existing models in answering marine research questions. The experimental results reveal that existing VLMs cannot effectively answer the domain-specific questions, and there is still a large room for further performance improvements. We hope our new benchmark and observations will facilitate future research. Project Page: http://marineeval.hkustvgd.com/",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21120v1,ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models,arXiv,2025-12-24,"Summary: Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21118v1,STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting,arXiv,2025-12-24,"Summary: Precipitation nowcasting is a critical spatio-temporal prediction task for society to prevent severe damage owing to extreme weather events. Despite the advances in this field, the complex and stochastic nature of this task still poses challenges to existing approaches. Specifically, deterministic models tend to produce blurry predictions while generative models often struggle with poor accuracy. In this paper, we present a simple yet effective model architecture termed STLDM, a diffusion-based model that learns the latent representation from end to end alongside both the Variational Autoencoder and the conditioning network. STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model. Experimental results on multiple radar datasets demonstrate that STLDM achieves superior performance compared to the state of the art, while also improving inference efficiency. The code is available in https://github.com/sqfoo/stldm_official.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21115v1,Discrete-time asset price bubbles with short sales prohibitions under model uncertainty,arXiv,2025-12-24,"Summary: In this study, we investigate asset price bubbles in a discrete-time, discrete-state market under model uncertainty and short sales prohibitions. Building on a new fundamental theorem of asset pricing and a superhedging duality in this setting, we introduce a notion of bubble based on a novel definition of the fundamental price, and analyze their types and characterization. We show that two distinct types of bubbles arise, depending on the maturity structure of the asset. For assets with bounded maturity and no dividend payments, the $G$-supermartingale property of prices provides a necessary and sufficient condition for the existence of bubbles. In contrast, when maturity is unbounded, the infi-supermartingale property yields a necessary condition, while the $G$-supermartingale property remains sufficient. Moreover, there is no bubble under a strengthened no dominance condition. As applications, we examine price bubbles for several standard contingent claims. We show that put-call parity generally fails for fundamental prices, whereas it holds for market prices under no dominance assumption. Furthermore, we establish bounds for the fundamental and market prices of American call options in terms of the corresponding European call prices, adjusted by the associated bubble components.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21113v1,A Mechanistic Analysis of Transformers for Dynamical Systems,arXiv,2025-12-24,"Summary: Transformers are increasingly adopted for modeling and forecasting time-series, yet their internal mechanisms remain poorly understood from a dynamical systems perspective. In contrast to classical autoregressive and state-space models, which benefit from well-established theoretical foundations, Transformer architectures are typically treated as black boxes. This gap becomes particularly relevant as attention-based models are considered for general-purpose or zero-shot forecasting across diverse dynamical regimes. In this work, we do not propose a new forecasting model, but instead investigate the representational capabilities and limitations of single-layer Transformers when applied to dynamical data. Building on a dynamical systems perspective we interpret causal self-attention as a linear, history-dependent recurrence and analyze how it processes temporal information. Through a series of linear and nonlinear case studies, we identify distinct operational regimes. For linear systems, we show that the convexity constraint imposed by softmax attention fundamentally restricts the class of dynamics that can be represented, leading to oversmoothing in oscillatory settings. For nonlinear systems under partial observability, attention instead acts as an adaptive delay-embedding mechanism, enabling effective state reconstruction when sufficient temporal context and latent dimensionality are available. These results help bridge empirical observations with classical dynamical systems theory, providing insight into when and why Transformers succeed or fail as models of dynamical systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21111v1,Statistical and computational challenges in ranking,arXiv,2025-12-24,"Summary: We consider the problem of ranking $n$ experts according to their abilities, based on the correctness of their answers to $d$ questions. This is modeled by the so-called crowd-sourcing model, where the answer of expert $i$ on question $k$ is modeled by a random entry, parametrized by $M_{i,k}$ which is increasing linearly with the expected quality of the answer. To enable the unambiguous ranking of the experts by ability, several assumptions on $M$ are available in the literature. We consider here the general isotonic crowd-sourcing model, where $M$ is assumed to be isotonic up to an unknown permutation $π^*$ of the experts - namely, $M_{π^{*-1}(i),k} \geq M_{π^{*-1}(i+1),k}$ for any $i\in [n-1], k \in [d]$. Then, ranking experts amounts to constructing an estimator of $π^*$. In particular, we investigate here the existence of statistically optimal and computationally efficient procedures and we describe recent results that disprove the existence of computational-statistical gaps for this problem. To provide insights on the key ideas, we start by discussing simpler and yet related sub-problems, namely sub-matrix detection and estimation. This corresponds to specific instances of the ranking problem where the matrix $M$ is constrained to be of the form $λ\mathbf 1\{S\times T\}$ where $S\subset [n], T\subset [d]$. This model has been extensively studied. We provide an overview of the results and proof techniques for this problem with a particular emphasis on the computational lower bounds based on low-degree polynomial methods. Then, we build upon this instrumental sub-problem to discuss existing results and algorithmic ideas for the general ranking problem.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21110v1,Beyond Context: Large Language Models Failure to Grasp Users Intent,arXiv,2025-12-24,"Summary: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21107v1,Semi-Supervised Learning for Large Language Models Safety and Content Moderation,arXiv,2025-12-24,"Summary: Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21106v1,Semantic Refinement with LLMs for Graph Representations,arXiv,2025-12-24,"Summary: Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21105v1,Volatile Organic Compounds for Stress Detection: A Scoping Review and Exploratory Feasibility Study with Low-Cost Sensors,arXiv,2025-12-24,"Summary: Volatile organic compounds (VOCs) represent a novel but underexplored modality for emotion recognition. This paper presents a systematic evidence synthesis and exploratory investigation of VOC-based affective computing using low-cost sensors. Study 1, a systematic scoping review following PRISMA-ScR guidelines, analyzed 16 studies from 610 records across breath, sweat, skin, and urine biosources. Evidence indicates that stress and affective states are reflected in VOC signatures (aldehydes, ketones, fatty acids, sulfur compounds), though with considerable heterogeneity. Current research relies predominantly on laboratory-grade GC-MS or PTR-MS, while wearable sensors provide pattern-level outputs without compound-specific identification - a critical gap for practical systems. Study 2 (n=25) investigated whether low-cost TVOC sensors (BME688, ENS160) combined with physiological monitoring (HR, HRV, GSR) can detect laboratory-induced stress. Exploratory analysis revealed that high cardiovascular reactors exhibited elevated TVOC during arithmetic stress (d=1.38), though requiring replication in larger samples. Substantial interindividual variability emerged (CV>80%), with coupling patterns moderated by baseline emission levels and temporal lags of 30-80 seconds. Random Forest-based multimodal classification achieved 77.3% accuracy (5-fold CV). SHAP analysis indicated VOC sensors contributed 24.9% of model performance. Leave-one-subject-out validation yielded 65.3% accuracy, highlighting the need for individual calibration. This work provides three contributions: (1) comprehensive mapping of VOC biomarker evidence and technological gaps, (2) initial demonstration that low-cost sensors can capture stress-related VOC patterns in multimodal fusion, and (3) identification of key implementation challenges. Findings require replication in larger samples (n>=50).",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21102v1,Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends,arXiv,2025-12-24,"Summary: This study proposes a unified forecasting framework for high-dimensional multi-task time series to meet the prediction demands of cloud native backend systems operating under highly dynamic loads, coupled metrics, and parallel tasks. The method builds a shared encoding structure to represent diverse monitoring indicators in a unified manner and employs a state fusion mechanism to capture trend changes and local disturbances across different time scales. A cross-task structural propagation module is introduced to model potential dependencies among nodes, enabling the model to understand complex structural patterns formed by resource contention, link interactions, and changes in service topology. To enhance adaptability to non-stationary behaviors, the framework incorporates a dynamic adjustment mechanism that automatically regulates internal feature flows according to system state changes, ensuring stable predictions in the presence of sudden load shifts, topology drift, and resource jitter. The experimental evaluation compares multiple models across various metrics and verifies the effectiveness of the framework through analyses of hyperparameter sensitivity, environmental sensitivity, and data sensitivity. The results show that the proposed method achieves superior performance on several error metrics and provides more accurate representations of future states under different operating conditions. Overall, the unified forecasting framework offers reliable predictive capability for high-dimensional, multi-task, and strongly dynamic environments in cloud native systems and provides essential technical support for intelligent backend management.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21099v1,TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars,arXiv,2025-12-24,"Summary: Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields. While effective in constrained settings, both approaches often fail to generalize to unseen expressions and poses, particularly in extreme reenactment scenarios. Other methods constrain Gaussians to the global texel space of 3DMMs to reduce rendering complexity. However, these texel-based avatars tend to underutilize the underlying mesh structure. They apply minimal analytic deformation and rely heavily on neural regressors and heuristic regularization in UV space, which weakens geometric consistency and limits extrapolation to complex, out-of-distribution deformations. To address these limitations, we introduce TexAvatars, a hybrid avatar representation that combines the explicit geometric grounding of analytic rigging with the spatial continuity of texel space. Our approach predicts local geometric attributes in UV space via CNNs, but drives 3D deformation through mesh-aware Jacobians, enabling smooth and semantically meaningful transitions across triangle boundaries. This hybrid design separates semantic modeling from geometric control, resulting in improved generalization, interpretability, and stability. Furthermore, TexAvatars captures fine-grained expression effects, including muscle-induced wrinkles, glabellar lines, and realistic mouth cavity geometry, with high fidelity. Our method achieves state-of-the-art performance under extreme pose and expression variations, demonstrating strong generalization in challenging head reenactment settings.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21092v1,Portfolio Optimization for Index Tracking with Constraints on Downside Risk and Carbon Footprint,arXiv,2025-12-24,"Summary: Historically, financial risk management has mostly addressed risk factors that arise from the financial environment. Climate risks present a novel and significant challenge for companies and financial markets. Investors aiming for avoidance of firms with high carbon footprints require suitable risk measures and portfolio management strategies. This paper presents the construction of decarbonized indices for tracking the S \& P-500 index of the U.S. stock market, as well as the Indian index NIFTY-50, employing two distinct methodologies and study their performances. These decarbonized indices optimize the portfolio weights by minimizing the mean-VaR and mean-ES and seek to reduce the risk of significant financial losses while still pursuing decarbonization goals. Investors can thereby find a balance between financial performance and environmental responsibilities. Ensuring transparency in the development of these indices will encourage the excluded and under-weighted asset companies to lower their carbon footprints through appropriate action plans. For long-term passive investors, these indices may present a more favourable option than green stocks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21083v1,Hierarchical Modeling Approach to Fast and Accurate Table Recognition,arXiv,2025-12-24,"Summary: The extraction and use of diverse knowledge from numerous documents is a pressing challenge in intelligent information retrieval. Documents contain elements that require different recognition methods. Table recognition typically consists of three subtasks, namely table structure, cell position and cell content recognition. Recent models have achieved excellent recognition with a combination of multi-task learning, local attention, and mutual learning. However, their effectiveness has not been fully explained, and they require a long period of time for inference. This paper presents a novel multi-task model that utilizes non-causal attention to capture the entire table structure, and a parallel inference algorithm for faster cell content inference. The superiority is demonstrated both visually and statistically on two large public datasets.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21081v1,Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics,arXiv,2025-12-24,"Summary: Controlling systems with complex, nonlinear dynamics poses a significant challenge, particularly in achieving efficient and robust control. In this paper, we propose a Dyna-Style Reinforcement Learning control framework that integrates Sparse Identification of Nonlinear Dynamics (SINDy) with Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning. SINDy is used to identify a data-driven model of the system, capturing its key dynamics without requiring an explicit physical model. This identified model is used to generate synthetic rollouts that are periodically injected into the reinforcement learning replay buffer during training on the real environment, enabling efficient policy learning with limited data available. By leveraging this hybrid approach, we mitigate the sample inefficiency of traditional model-free reinforcement learning methods while ensuring accurate control of nonlinear systems. To demonstrate the effectiveness of this framework, we apply it to a bi-rotor system as a case study, evaluating its performance in stabilization and trajectory tracking. The results show that our SINDy-TD3 approach achieves superior accuracy and robustness compared to direct reinforcement learning techniques, highlighting the potential of combining data-driven modeling with reinforcement learning for complex dynamical systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21080v1,LLM Personas as a Substitute for Field Experiments in Method Benchmarking,arXiv,2025-12-24,"Summary: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21076v1,Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions,arXiv,2025-12-24,"Summary: Accurate book genre classification is fundamental to digital library organization, content discovery, and personalized recommendation. Existing approaches typically model genre prediction as a flat, single-label task, ignoring hierarchical genre structure and relying heavily on noisy, subjective user reviews, which often degrade classification reliability. We propose HiGeMine, a two-phase hierarchical genre mining framework that robustly integrates user reviews with authoritative book blurbs. In the first phase, HiGeMine employs a zero-shot semantic alignment strategy to filter reviews, retaining only those semantically consistent with the corresponding blurb, thereby mitigating noise, bias, and irrelevance. In the second phase, we introduce a dual-path, two-level graph-based classification architecture: a coarse-grained Level-1 binary classifier distinguishes fiction from non-fiction, followed by Level-2 multi-label classifiers for fine-grained genre prediction. Inter-genre dependencies are explicitly modeled using a label co-occurrence graph, while contextual representations are derived from pretrained language models applied to the filtered textual content. To facilitate systematic evaluation, we curate a new hierarchical book genre dataset. Extensive experiments demonstrate that HiGeMine consistently outperformed strong baselines across hierarchical genre classification tasks. The proposed framework offers a principled and effective solution for leveraging both structured and unstructured textual data in hierarchical book genre analysis.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21075v1,Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics,arXiv,2025-12-24,"Summary: The empirical success of deep learning is often attributed to scaling laws that predict consistent gains as model, data, and compute grow; however, large models can exhibit training instability and diminishing returns, suggesting that scaling laws describe what success looks like but not when and why scaling succeeds or fails. A central obstacle is the lack of a rigorous understanding of feature learning at large depth. While muP characterizes feature-learning dynamics in the infinite-width limit and enables hyperparameter transfer across width, its depth extension (depth-muP) breaks down for residual blocks with more than one internal layer. We derive Neural Feature Dynamics (NFD) for ResNets with single-layer residual blocks, characterizing feature learning via a coupled forward-backward stochastic system in the joint infinite-width and infinite-depth limit. In this regime, NFD identifies when scaling-law trends persist and explains diminishing returns. It also reveals a vanishing mechanism induced by the 1/sqrt(depth) residual scaling under which the gradient-independence assumption (GIA), known to fail during training at finite depth, becomes provably valid again at infinite depth, yielding an analytically tractable regime for end-to-end feature learning. Motivated by this insight, we study two-layer residual blocks and show that the same mechanism causes feature-learning collapse in the first internal layer at large depth, providing a structural explanation for the empirical failure of depth-muP. Based on this diagnosis, we propose a depth-aware learning-rate correction that counteracts the collapse and empirically restores depth-wise hyperparameter transfer, yielding stronger performance in deeper ResNets.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21066v1,Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation,arXiv,2025-12-24,"Summary: Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21055v1,Making AI Work: An Autoethnography of a Workaround in Higher Education,arXiv,2025-12-24,"Summary: Research on the implementation of Generative Artificial Intelligence (GenAI) in higher education often focuses on strategic goals, overlooking the hidden, and often politically charged, labour required to make it functional. This paper provides an insider's account of the sociotechnical friction that arises when an institutional goal of empowering non-technical staff conflicts with the technical limitations of enterprise Large Language Models (LLMs). Through analytic autoethnography, this study examines a GenAI project pushed to an impasse, focusing on a workaround developed to navigate not only technical constraints but also the combined challenge of organisational territoriality and assertions of positional power. Drawing upon Alter's (2014) theory of workarounds, the analysis interprets ""articulation work"" as a form of ""invisible labour"". By engaging with the Information Systems (IS) domains of user innovation and technology-in-practice, this study argues that such user-driven workarounds should be understood not as deviations, but as integral acts of sociotechnical integration. This integration, however, highlights the central paradoxes of modern GenAI where such workarounds for ""unfinished"" systems can simultaneously create unofficial ""shadow"" systems and obscure the crucial, yet invisible, sociotechnical labour involved. The findings suggest that the invisible labour required to integrate GenAI within complex organisational politics is an important, rather than peripheral, component of how it becomes functional in practice.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21054v1,DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors,arXiv,2025-12-24,"Summary: The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21048v1,zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy,arXiv,2025-12-24,"Summary: Healthcare AI needs large, diverse datasets, yet strict privacy and governance constraints prevent raw data sharing across institutions. Federated learning (FL) mitigates this by training where data reside and exchanging only model updates, but practical deployments still face two core risks: (1) privacy leakage via gradients or updates (membership inference, gradient inversion) and (2) trust in the aggregator, a single point of failure that can drop, alter, or inject contributions undetected. We present zkFL-Health, an architecture that combines FL with zero-knowledge proofs (ZKPs) and Trusted Execution Environments (TEEs) to deliver privacy-preserving, verifiably correct collaborative training for medical AI. Clients locally train and commit their updates; the aggregator operates within a TEE to compute the global update and produces a succinct ZK proof (via Halo2/Nova) that it used exactly the committed inputs and the correct aggregation rule, without revealing any client update to the host. Verifier nodes validate the proof and record cryptographic commitments on-chain, providing an immutable audit trail and removing the need to trust any single party. We outline system and threat models tailored to healthcare, the zkFL-Health protocol, security/privacy guarantees, and a performance evaluation plan spanning accuracy, privacy risk, latency, and cost. This framework enables multi-institutional medical AI with strong confidentiality, integrity, and auditability, key properties for clinical adoption and regulatory compliance.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21041v1,When LLMs fall short in Deductive Coding: Model Comparison and Human AI Collaboration Workflow Design,arXiv,2025-12-24,"Summary: With generative artificial intelligence driving the growth of dialogic data in education, automated coding is a promising direction for learning analytics to improve efficiency. This surge highlights the need to understand the nuances of student-AI interactions, especially those rare yet crucial. However, automated coding may struggle to capture these rare codes due to imbalanced data, while human coding remains time-consuming and labour-intensive. The current study examined the potential of large language models (LLMs) to approximate or replace humans in deductive, theory-driven coding, while also exploring how human-AI collaboration might support such coding tasks at scale. We compared the coding performance of small transformer classifiers (e.g., BERT) and LLMs in two datasets, with particular attention to imbalanced head-tail distributions in dialogue codes. Our results showed that LLMs did not outperform BERT-based models and exhibited systematic errors and biases in deductive coding tasks. We designed and evaluated a human-AI collaborative workflow that improved coding efficiency while maintaining coding reliability. Our findings reveal both the limitations of LLMs -- especially their difficulties with semantic similarity and theoretical interpretations and the indispensable role of human judgment -- while demonstrating the practical promise of human-AI collaborative workflows for coding.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21039v1,Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection,arXiv,2025-12-24,"Summary: The rapid proliferation of online misinformation poses significant risks to public trust, policy, and safety, necessitating reliable automated fake news detection. Existing methods often struggle with multimodal content, domain generalization, and explainability. We propose AMPEND-LS, an agentic multi-persona evidence-grounded framework with LLM-SLM synergy for multimodal fake news detection. AMPEND-LS integrates textual, visual, and contextual signals through a structured reasoning pipeline powered by LLMs, augmented with reverse image search, knowledge graph paths, and persuasion strategy analysis. To improve reliability, we introduce a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context, and a complementary SLM classifier to mitigate LLM uncertainty and hallucinations. Extensive experiments across three benchmark datasets demonstrate that AMPEND-LS consistently outperformed state-of-the-art baselines in accuracy, F1 score, and robustness. Qualitative case studies further highlight its transparent reasoning and resilience against evolving misinformation. This work advances the development of adaptive, explainable, and evidence-aware systems for safeguarding online information integrity.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21034v1,A Design Study Process Model for Medical Visualization,arXiv,2025-12-24,"Summary: We introduce a design study process model for medical visualization based on the analysis of existing medical visualization and visual analysis works, and our own interdisciplinary research experience. With a literature review of related works covering various data types and applications, we identify features of medical visualization and visual analysis research and formulate our model thereafter. Compared to previous design study process models, our new model emphasizes: distinguishing between different stakeholders and target users before initiating specific designs, distinguishing design stages according to analytic logic or cognitive habits, and classifying task types as inferential or descriptive, and further hypothesis-based or hypothesis-free based on whether they involve multiple subgroups. In addition, our model refines previous models according to the characteristics of medical problems and provides referable guidance for each step. These improvements make the visualization design targeted, generalizable, and operational, which can adapt to the complexity and diversity of medical problems. We apply this model to guide the design of a visual analysis method and reanalyze three medical visualization-related works. These examples suggest that the new process model can provide a systematic theoretical framework and practical guidance for interdisciplinary medical visualization research. We give recommendations that future researchers can refer to, report on reflections on the model, and delineate it from existing models.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21033v1,Quantum Homotopy Algorithm for Solving Nonlinear PDEs and Flow Problems,arXiv,2025-12-24,"Summary: Quantum algorithms to integrate nonlinear PDEs governing flow problems are challenging to discover but critical to enhancing the practical usefulness of quantum computing. We present here a near-optimal, robust, and end-to-end quantum algorithm to solve time-dependent, dissipative, and nonlinear PDEs. We embed the PDEs in a truncated, high dimensional linear space on the basis of quantum homotopy analysis. The linearized system is discretized and integrated using finite-difference methods that use a compact quantum algorithm. The present approach can adapt its input to the nature of nonlinearity and underlying physics. The complexity estimates improve existing approaches in terms of scaling of matrix operator norms, condition number, simulation time, and accuracy. We provide a general embedding strategy, bounds on stability criteria, accuracy, gate counts and query complexity. A physically motivated measure of nonlinearity is connected to a parameter that is similar to the flow Reynolds number $Re_{\textrm{H}}$, whose inverse marks the allowed integration window, for given accuracy and complexity. We illustrate the embedding scheme with numerical simulations of a one-dimensional Burgers problem. This work shows the potential of the hybrid quantum algorithm for simulating practical and nonlinear phenomena on near-term and fault-tolerant quantum devices.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21029v1,Critical Points of Degenerate Metrics on Algebraic Varieties: A Tale of Overparametrization,arXiv,2025-12-24,"Summary: We study the critical points over an algebraic variety of an optimization problem defined by a quadratic objective that is degenerate. This scenario arises in machine learning when the dataset size is small with respect to the model, and is typically referred to as overparametrization. Our main result relates the degenerate optimization problem to a nondegenerate one via a projection. In the highly-degenerate regime, we find that a central role is played by the ramification locus of the projection. Additionally, we provide tools for counting the number of critical points over projective varieties, and discuss specific cases arising from deep learning. Our work bridges tools from algebraic geometry with ideas from machine learning, and it extends the line of literature around the Euclidean distance degree to the degenerate setting.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21024v1,Policy-Conditioned Policies for Multi-Agent Task Solving,arXiv,2025-12-24,"Summary: In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21021v1,Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces,arXiv,2025-12-24,"Summary: Consumer-to-consumer (C2C) marketplaces pose distinct retrieval challenges: short, ambiguous queries; noisy, user-generated listings; and strict production constraints. This paper reports our experiment to build a domain-aware Japanese text-embedding approach to improve the quality of search at Mercari, Japan's largest C2C marketplace. We experimented with fine-tuning on purchase-driven query-title pairs, using role-specific prefixes to model query-item asymmetry. To meet production constraints, we apply Matryoshka Representation Learning to obtain compact, truncation-robust embeddings. Offline evaluation on historical search logs shows consistent gains over a strong generic encoder, with particularly large improvements when replacing PCA compression with Matryoshka truncation. A manual assessment further highlights better handling of proper nouns, marketplace-specific semantics, and term-importance alignment. Additionally, an initial online A/B test demonstrates statistically significant improvements in revenue per user and search-flow efficiency, with transaction frequency maintained. Results show that domain-aware embeddings improve relevance and efficiency at scale and form a practical foundation for richer LLM-era search experiences.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21020v1,Enhancing diffusion models with Gaussianization preprocessing,arXiv,2025-12-24,"Summary: Diffusion models are a class of generative models that have demonstrated remarkable success in tasks such as image generation. However, one of the bottlenecks of these models is slow sampling due to the delay before the onset of trajectory bifurcation, at which point substantial reconstruction begins. This issue degrades generation quality, especially in the early stages. Our primary objective is to mitigate bifurcation-related issues by preprocessing the training data to enhance reconstruction quality, particularly for small-scale network architectures. Specifically, we propose applying Gaussianization preprocessing to the training data to make the target distribution more closely resemble an independent Gaussian distribution, which serves as the initial density of the reconstruction process. This preprocessing step simplifies the model's task of learning the target distribution, thereby improving generation quality even in the early stages of reconstruction with small networks. The proposed method is, in principle, applicable to a broad range of generative tasks, enabling more stable and efficient sampling processes.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21017v1,Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy,arXiv,2025-12-24,"Summary: With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21010v1,LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics,arXiv,2025-12-24,"Summary: The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation ($N=100,000$ iterations) is used to approximate the statistically robust Expected Win Score ($E[S_m]$), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity ($T_k$), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21009v1,ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting,arXiv,2025-12-24,"Summary: Higher-order interactions beyond pairwise relationships in large complex networks are often modeled as hypergraphs. Analyzing hypergraph properties such as triad counts is essential, as hypergraphs can reveal intricate group interaction patterns that conventional graphs fail to capture. In real-world scenarios, these networks are often large and dynamic, introducing significant computational challenges. Due to the absence of specialized software packages and data structures, the analysis of large dynamic hypergraphs remains largely unexplored. Motivated by this gap, we propose ESCHER, a GPU-centric parallel data structure for Efficient and Scalable Hypergraph Evolution Representation, designed to manage large scale hypergraph dynamics efficiently. We also design a hypergraph triad-count update framework that minimizes redundant computation while fully leveraging the capabilities of ESCHER for dynamic operations. We validate the efficacy of our approach across multiple categories of hypergraph triad counting, including hyperedge-based, incident-vertex-based, and temporal triads. Empirical results on both large real-world and synthetic datasets demonstrate that our proposed method outperforms existing state-of-the-art methods, achieving speedups of up to 104.5x, 473.7x, and 112.5x for hyperedge-based, incident-vertex-based, and temporal triad types, respectively.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21005v1,Learning from Neighbors with PHIBP: Predicting Infectious Disease Dynamics in Data-Sparse Environments,arXiv,2025-12-24,"Summary: Modeling sparse count data, which arise across numerous scientific fields, presents significant statistical challenges. This chapter addresses these challenges in the context of infectious disease prediction, with a focus on predicting outbreaks in geographic regions that have historically reported zero cases. To this end, we present the detailed computational framework and experimental application of the Poisson Hierarchical Indian Buffet Process (PHIBP), with demonstrated success in handling sparse count data in microbiome and ecological studies. The PHIBP's architecture, grounded in the concept of absolute abundance, systematically borrows statistical strength from related regions and circumvents the known sensitivities of relative-rate methods to zero counts. Through a series of experiments on infectious disease data, we show that this principled approach provides a robust foundation for generating coherent predictive distributions and for the effective use of comparative measures such as alpha and beta diversity. The chapter's emphasis on algorithmic implementation and experimental results confirms that this unified framework delivers both accurate outbreak predictions and meaningful epidemiological insights in data-sparse settings.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21002v1,Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation,arXiv,2025-12-24,"Summary: Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx94\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21000v1,CoSeNet: A Novel Approach for Optimal Segmentation of Correlation Matrices,arXiv,2025-12-24,"Summary: In this paper, we propose a novel approach for the optimal identification of correlated segments in noisy correlation matrices. The proposed model is known as CoSeNet (Correlation Seg-mentation Network) and is based on a four-layer algorithmic architecture that includes several processing layers: input, formatting, re-scaling, and segmentation layer. The proposed model can effectively identify correlated segments in such matrices, better than previous approaches for similar problems. Internally, the proposed model utilizes an overlapping technique and uses pre-trained Machine Learning (ML) algorithms, which makes it robust and generalizable. CoSeNet approach also includes a method that optimizes the parameters of the re-scaling layer using a heuristic algorithm and fitness based on a Window Difference-based metric. The output of the model is a binary noise-free matrix representing optimal segmentation as well as its seg-mentation points and can be used in a variety of applications, obtaining compromise solutions between efficiency, memory, and speed of the proposed deployment model.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20996v1,TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control,arXiv,2025-12-24,"Summary: Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20991v1,FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning,arXiv,2025-12-24,"Summary: The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20985v1,A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines,arXiv,2025-12-24,"Summary: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20983v1,Automatic Replication of LLM Mistakes in Medical Conversations,arXiv,2025-12-24,"Summary: Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20978v1,GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model,arXiv,2025-12-24,"Summary: Language Model (LM)-based generative modeling has emerged as a promising direction for TSE, offering potential for improved generalization and high-fidelity speech. We present GenTSE, a two-stage decoder-only generative LM approach for TSE: Stage-1 predicts coarse semantic tokens, and Stage-2 generates fine acoustic tokens. Separating semantics and acoustics stabilizes decoding and yields more faithful, content-aligned target speech. Both stages use continuous SSL or codec embeddings, offering richer context than discretized-prompt methods. To reduce exposure bias, we employ a Frozen-LM Conditioning training strategy that conditions the LMs on predicted tokens from earlier checkpoints to reduce the gap between teacher-forcing training and autoregressive inference. We further employ DPO to better align outputs with human perceptual preferences. Experiments on Libri2Mix show that GenTSE surpasses previous LM-based systems in speech quality, intelligibility, and speaker consistency.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20974v1,Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions,arXiv,2025-12-24,"Summary: Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applicability in real-world problems. As a result, recent deep BRL methods have started to incorporate model learning, though the use of neural networks directly on the joint data and task parameters requires optimising the Evidence Lower Bound (ELBO). ELBOs are difficult to optimise and may result in indistinctive task parameters, hence compromised BRL policies. To this end, we introduce a novel deep BRL method, Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions (GLiBRL), that enables efficient and accurate learning of transition and reward models, with fully tractable marginal likelihood and Bayesian inference on task parameters and model noises. On challenging MetaWorld ML10/45 benchmarks, GLiBRL improves the success rate of one of the state-of-the-art deep BRL methods, VariBAD, by up to 2.7x. Comparing against representative or recent deep BRL / Meta-RL methods, such as MAML, RL2, SDVT, TrMRL and ECET, GLiBRL also demonstrates its low-variance and decent performance consistently.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20973v1,DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination,arXiv,2025-12-24,"Summary: Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20968v1,Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality,arXiv,2025-12-24,"Summary: Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.
  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20967v1,Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions,arXiv,2025-12-24,"Summary: As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\mathcal{O}(\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\%.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20963v1,Generalization of Diffusion Models Arises with a Balanced Representation Space,arXiv,2025-12-24,"Summary: Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized ""spiky"" representations, whereas (ii) generalization arises when the model captures local data statistics, producing ""balanced"" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20959v1,Can Agentic AI Match the Performance of Human Data Scientists?,arXiv,2025-12-24,"Summary: Data science plays a critical role in transforming complex data into actionable insights across numerous domains. Recent developments in large language models (LLMs) have significantly automated data science workflows, but a fundamental question persists: Can these agentic AI systems truly match the performance of human data scientists who routinely leverage domain-specific knowledge? We explore this question by designing a prediction task where a crucial latent variable is hidden in relevant image data instead of tabular features. As a result, agentic AI that generates generic codes for modeling tabular data cannot perform well, while human experts could identify the important hidden variable using domain knowledge. We demonstrate this idea with a synthetic dataset for property insurance. Our experiments show that agentic AI that relies on generic analytics workflow falls short of methods that use domain-specific insights. This highlights a key limitation of the current agentic AI for data science and underscores the need for future research to develop agentic AI systems that can better recognize and incorporate domain knowledge.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20958v1,ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design,arXiv,2025-12-24,"Summary: De novo drug design is a crucial component of modern drug development, yet navigating the vast chemical space to find synthetically accessible, high-affinity candidates remains a significant challenge. Reinforcement Learning (RL) enhances this process by enabling multi-objective optimization and exploration of novel chemical space - capabilities that traditional supervised learning methods lack. In this work, we introduce \textbf{ReACT-Drug}, a fully integrated, target-agnostic molecular design framework based on Reinforcement Learning. Unlike models requiring target-specific fine-tuning, ReACT-Drug utilizes a generalist approach by leveraging ESM-2 protein embeddings to identify similar proteins for a given target from a knowledge base such as Protein Data Base (PDB). Thereafter, the known drug ligands corresponding to such proteins are decomposed to initialize a fragment-based search space, biasing the agent towards biologically relevant subspaces. For each such fragment, the pipeline employs a Proximal Policy Optimization (PPO) agent guiding a ChemBERTa-encoded molecule through a dynamic action space of chemically valid, reaction-template-based transformations. This results in the generation of \textit{de novo} drug candidates with competitive binding affinities and high synthetic accessibility, while ensuring 100\% chemical validity and novelty as per MOSES benchmarking. This architecture highlights the potential of integrating structural biology, deep representation learning, and chemical synthesis rules to automate and accelerate rational drug design. The dataset and code are available at https://github.com/YadunandanRaman/ReACT-Drug/.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20957v1,One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents,arXiv,2025-12-24,"Summary: Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20956v1,Solving Functional PDEs with Gaussian Processes and Applications to Functional Renormalization Group Equations,arXiv,2025-12-24,"Summary: We present an operator learning framework for solving non-perturbative functional renormalization group equations, which are integro-differential equations defined on functionals. Our proposed approach uses Gaussian process operator learning to construct a flexible functional representation formulated directly on function space, making it independent of a particular equation or discretization. Our method is flexible, and can apply to a broad range of functional differential equations while still allowing for the incorporation of physical priors in either the prior mean or the kernel design. We demonstrate the performance of our method on several relevant equations, such as the Wetterich and Wilson--Polchinski equations, showing that it achieves equal or better performance than existing approximations such as the local-potential approximation, while being significantly more flexible. In particular, our method can handle non-constant fields, making it promising for the study of more complex field configurations, such as instantons.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20954v1,Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models,arXiv,2025-12-24,"Summary: Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary ""thinking tokens"" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20953v1,Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications,arXiv,2025-12-24,"Summary: The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D parallelism in such environments, addressing critical issues such as the need for symmetric tensor parallelism, efficient gradient synchronization in asymmetric pipeline parallelism, and the trade-offs between memory utilization and computational efficiency. Building upon these insights, we introduce AutoHet, a novel system that automatically identifies the optimal parallelism plan for distributed training on heterogeneous GPUs. AutoHet supports asymmetric 3D parallelism structures and facilitates fine-grained workload distribution. We propose a theoretical model that frames the device grouping and load balancing as an optimization problem to minimize per-iteration training time, thus effectively balancing computing power and memory usage across GPUs with diverse capabilities. To enable elastic training upon spot instance preemption, AutoHet presents an efficient recovery strategy that prioritizes to retrieve training states from local nodes, and only downloads the missing checkpoints from the cloud storage. Our extensive evaluation, conducted on three large-scale models and utilizing combinations of three different GPU types, demonstrates that AutoHet outperforms existing DNN training systems, achieving up to a 1.79$\times$ speedup in training throughput compared with Megatron-LM and Whale, and a 4.38$\times$ speedup of recovery speed compared to a spot instance baseline.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20951v1,From Human Bias to Robot Choice: How Occupational Contexts and Racial Priming Shape Robot Selection,arXiv,2025-12-24,"Summary: As artificial agents increasingly integrate into professional environments, fundamental questions have emerged about how societal biases influence human-robot selection decisions. We conducted two comprehensive experiments (N = 1,038) examining how occupational contexts and stereotype activation shape robotic agent choices across construction, healthcare, educational, and athletic domains. Participants made selections from artificial agents that varied systematically in skin tone and anthropomorphic characteristics. Our study revealed distinct context-dependent patterns. Healthcare and educational scenarios demonstrated strong favoritism toward lighter-skinned artificial agents, while construction and athletic contexts showed greater acceptance of darker-toned alternatives. Participant race was associated with systematic differences in selection patterns across professional domains. The second experiment demonstrated that exposure to human professionals from specific racial backgrounds systematically shifted later robotic agent preferences in stereotype-consistent directions. These findings show that occupational biases and color-based discrimination transfer directly from human-human to human-robot evaluation contexts. The results highlight mechanisms through which robotic deployment may unintentionally perpetuate existing social inequalities.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20950v1,MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment,arXiv,2025-12-24,"Summary: This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20949v1,Neural Probe-Based Hallucination Detection for Large Language Models,arXiv,2025-12-24,"Summary: Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20948v1,"Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study",arXiv,2025-12-24,"Summary: Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20943v1,AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences,arXiv,2025-12-24,"Summary: Free-viewpoint video (FVV) enables immersive viewing experiences by allowing users to view scenes from arbitrary perspectives. As a prominent reconstruction technique for FVV generation, 4D Gaussian Splatting (4DGS) models dynamic scenes with time-varying 3D Gaussian ellipsoids and achieves high-quality rendering via fast rasterization. However, existing 4DGS approaches suffer from quality degradation over long sequences and impose substantial bandwidth and storage overhead, limiting their applicability in real-time and wide-scale deployments. Therefore, we present AirGS, a streaming-optimized 4DGS framework that rearchitects the training and delivery pipeline to enable high-quality, low-latency FVV experiences. AirGS converts Gaussian video streams into multi-channel 2D formats and intelligently identifies keyframes to enhance frame reconstruction quality. It further combines temporal coherence with inflation loss to reduce training time and representation size. To support communication-efficient transmission, AirGS models 4DGS delivery as an integer linear programming problem and design a lightweight pruning level selection algorithm to adaptively prune the Gaussian updates to be transmitted, balancing reconstruction quality and bandwidth consumption. Extensive experiments demonstrate that AirGS reduces quality deviation in PSNR by more than 20% when scene changes, maintains frame-level PSNR consistently above 30, accelerates training by 6 times, reduces per-frame transmission size by nearly 50% compared to the SOTA 4DGS approaches.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20941v1,A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate,arXiv,2025-12-24,"Summary: Data-driven surrogate models are increasingly adopted to accelerate vehicle design. However, open-source multi-fidelity datasets and empirical guidelines linking dataset size to model performance remain limited. This study investigates the relationship between training data size and prediction accuracy for a graph neural network (GNN) based surrogate model for aerodynamic field prediction. We release an open-source, multi-fidelity aerodynamic dataset for double-delta wings, comprising 2448 flow snapshots across 272 geometries evaluated at angles of attack from 11 (degree) to 19 (degree) at Ma=0.3 using both Vortex Lattice Method (VLM) and Reynolds-Averaged Navier-Stokes (RANS) solvers. The geometries are generated using a nested Saltelli sampling scheme to support future dataset expansion and variance-based sensitivity analysis. Using this dataset, we conduct a preliminary empirical scaling study of the MF-VortexNet surrogate by constructing six training datasets with sizes ranging from 40 to 1280 snapshots and training models with 0.1 to 2.4 million parameters under a fixed training budget. We find that the test error decreases with data size with a power-law exponent of -0.6122, indicating efficient data utilization. Based on this scaling law, we estimate that the optimal sampling density is approximately eight samples per dimension in a d-dimensional design space. The results also suggest improved data utilization efficiency for larger surrogate models, implying a potential trade-off between dataset generation cost and model training budget.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20939v1,Stochastic well-structured transition systems,arXiv,2025-12-24,"Summary: Extending well-structured transition systems to incorporate a probabilistic scheduling rule, we define a new class of stochastic well-structured transition systems that includes population protocols, chemical reaction networks, and many common gossip models; as well as augmentations of these systems by an oracle that exposes a total order on agents as in population protocols in the comparison model or an equivalence relation as in population protocols with unordered data.
  We show that any implementation of a phase clock in these systems either stops or ticks too fast after polynomially many expected steps, and that any terminating computation in these systems finishes or fails in expected polynomial time. This latter property allows an exact characterization of the computational power of many stochastic well-structured transition systems augmented with a total order or equivalence relation on agents, showing that these compute exactly the languages in BPP, while the corresponding unaugmented systems compute just the symmetric languages in BPL.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20938v1,Pioneering Multimodal Emotion Recognition in the Era of Large Models: From Closed Sets to Open Vocabularies,arXiv,2025-12-24,"Summary: Recent advances in multimodal large language models (MLLMs) have demonstrated remarkable multi- and cross-modal integration capabilities. However, their potential for fine-grained emotion understanding remains systematically underexplored. While open-vocabulary multimodal emotion recognition (MER-OV) has emerged as a promising direction to overcome the limitations of closed emotion sets, no comprehensive evaluation of MLLMs in this context currently exists. To address this, our work presents the first large-scale benchmarking study of MER-OV on the OV-MERD dataset, evaluating 19 mainstream MLLMs, including general-purpose, modality-specialized, and reasoning-enhanced architectures. Through systematic analysis of model reasoning capacity, fusion strategies, contextual utilization, and prompt design, we provide key insights into the capabilities and limitations of current MLLMs for MER-OV. Our evaluation reveals that a two-stage, trimodal (audio, video, and text) fusion approach achieves optimal performance in MER-OV, with video emerging as the most critical modality. We further identify a surprisingly narrow gap between open- and closed-source LLMs. These findings establish essential benchmarks and offer practical guidelines for advancing open-vocabulary and fine-grained affective computing, paving the way for more nuanced and interpretable emotion AI systems. Associated code will be made publicly available upon acceptance.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20934v1,Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning,arXiv,2025-12-24,"Summary: Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20932v1,Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy,arXiv,2025-12-24,"Summary: This paper presents a marketing analytics framework that operationalizes subscription pricing as a dynamic, guardrailed decision system, uniting multivariate demand forecasting, segment-level price elasticity, and churn propensity to optimize revenue, margin, and retention. The approach blends seasonal time-series models with tree-based learners, runs Monte Carlo scenario tests to map risk envelopes, and solves a constrained optimization that enforces business guardrails on customer experience, margin floors, and allowable churn. Validated across heterogeneous SaaS portfolios, the method consistently outperforms static tiers and uniform uplifts by reallocating price moves toward segments with higher willingness-to-pay while protecting price-sensitive cohorts. The system is designed for real-time recalibration via modular APIs and includes model explainability for governance and compliance. Managerially, the framework functions as a strategy playbook that clarifies when to shift from flat to dynamic pricing, how to align pricing with CLV and MRR targets, and how to embed ethical guardrails, enabling durable growth without eroding customer trust.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20929v1,Decoding Predictive Inference in Visual Language Processing via Spatiotemporal Neural Coherence,arXiv,2025-12-24,"Summary: Human language processing relies on the brain's capacity for predictive inference. We present a machine learning framework for decoding neural (EEG) responses to dynamic visual language stimuli in Deaf signers. Using coherence between neural signals and optical flow-derived motion features, we construct spatiotemporal representations of predictive neural dynamics. Through entropy-based feature selection, we identify frequency-specific neural signatures that differentiate interpretable linguistic input from linguistically disrupted (time-reversed) stimuli. Our results reveal distributed left-hemispheric and frontal low-frequency coherence as key features in language comprehension, with experience-dependent neural signatures correlating with age. This work demonstrates a novel multimodal approach for probing experience-driven generative models of perception in the brain.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20924v1,Clever Hans in Chemistry: Chemist Style Signals Confound Activity Prediction on Public Benchmarks,arXiv,2025-12-24,"Summary: Can machine learning models identify which chemist made a molecule from structure alone? If so, models trained on literature data may exploit chemist intent rather than learning causal structure-activity relationships. We test this by linking CHEMBL assays to publication authors and training a 1,815-class classifier to predict authors from molecular fingerprints, achieving 60% top-5 accuracy under scaffold-based splitting. We then train an activity model that receives only a protein identifier and an author-probability vector derived from structure, with no direct access to molecular descriptors. This author-only model achieves predictive power comparable to a simple baseline that has access to structure. This reveals a ""Clever Hans"" failure mode: models can predict bioactivity largely by inferring chemist goals and favorite targets without requiring a lab-independent understanding of chemistry. We analyze the sources of this leakage, propose author-disjoint splits, and recommend dataset practices to decouple chemist intent from biological outcomes.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20920v1,RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks,arXiv,2025-12-24,"Summary: Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20916v1,MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model,arXiv,2025-12-24,"Summary: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant potential in recommendation systems. However, the effective application of MLLMs to multimodal sequential recommendation remains unexplored: A) Existing methods primarily leverage the multimodal semantic understanding capabilities of pre-trained MLLMs to generate item embeddings or semantic IDs, thereby enhancing traditional recommendation models. These approaches generate item representations that exhibit limited interpretability, and pose challenges when transferring to language model-based recommendation systems. B) Other approaches convert user behavior sequence into image-text pairs and perform recommendation through multiple MLLM inference, incurring prohibitive computational and time costs. C) Current MLLM-based recommendation systems generally neglect the integration of collaborative signals. To address these limitations while balancing recommendation performance, interpretability, and computational cost, this paper proposes MultiModal Summarization-and-Retrieval-Augmented Sequential Recommendation. Specifically, we first employ MLLM to summarize items into concise keywords and fine-tune the model using rewards that incorporate summary length, information loss, and reconstruction difficulty, thereby enabling adaptive adjustment of the summarization policy. Inspired by retrieval-augmented generation, we then transform collaborative signals into corresponding keywords and integrate them as supplementary context. Finally, we apply supervised fine-tuning with multi-task learning to align the MLLM with the multimodal sequential recommendation. Extensive evaluations on common recommendation datasets demonstrate the effectiveness of MMSRARec, showcasing its capability to efficiently and interpretably understand user behavior histories and item information for accurate recommendations.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20915v1,Towards a General Framework for Predicting and Explaining the Hardness of Graph-based Combinatorial Optimization Problems using Machine Learning and Association Rule Mining,arXiv,2025-12-24,"Summary: This study introduces GCO-HPIF, a general machine-learning-based framework to predict and explain the computational hardness of combinatorial optimization problems that can be represented on graphs. The framework consists of two stages. In the first stage, a dataset is created comprising problem-agnostic graph features and hardness classifications of problem instances. Machine-learning-based classification algorithms are trained to map graph features to hardness categories. In the second stage, the framework explains the predictions using an association rule mining algorithm. Additionally, machine-learning-based regression models are trained to predict algorithmic computation times. The GCO-HPIF framework was applied to a dataset of 3287 maximum clique problem instances compiled from the COLLAB, IMDB, and TWITTER graph datasets using five state-of-the-art algorithms, namely three exact branch-and-bound-based algorithms (Gurobi, CliSAT, and MOMC) and two graph-neural-network-based algorithms (EGN and HGS). The framework demonstrated excellent performance in predicting instance hardness, achieving a weighted F1 score of 0.9921, a minority-class F1 score of 0.878, and an ROC-AUC score of 0.9083 using only three graph features. The best association rule found by the FP-Growth algorithm for explaining the hardness predictions had a support of 0.8829 for hard instances and an overall accuracy of 87.64 percent, underscoring the framework's usefulness for both prediction and explanation. Furthermore, the best-performing regression model for predicting computation times achieved a percentage RMSE of 5.12 and an R2 value of 0.991.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20914v1,Invariant Feature Extraction Through Conditional Independence and the Optimal Transport Barycenter Problem: the Gaussian case,arXiv,2025-12-24,"Summary: A methodology is developed to extract $d$ invariant features $W=f(X)$ that predict a response variable $Y$ without being confounded by variables $Z$ that may influence both $X$ and $Y$.
  The methodology's main ingredient is the penalization of any statistical dependence between $W$ and $Z$ conditioned on $Y$, replaced by the more readily implementable plain independence between $W$ and the random variable $Z_Y = T(Z,Y)$ that solves the [Monge] Optimal Transport Barycenter Problem for $Z\mid Y$. In the Gaussian case considered in this article, the two statements are equivalent.
  When the true confounders $Z$ are unknown, other measurable contextual variables $S$ can be used as surrogates, a replacement that involves no relaxation in the Gaussian case if the covariance matrix $Σ_{ZS}$ has full range. The resulting linear feature extractor adopts a closed form in terms of the first $d$ eigenvectors of a known matrix. The procedure extends with little change to more general, non-Gaussian / non-linear cases.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20910v1,Econometric Modeling of Input-Driven Output Risk through a Versatile CES Production Function,arXiv,2025-12-24,"Summary: The conventional functional form of the Constant-Elasticity-of-Substitution (CES) production function is a general production function nesting a number of other forms of production functions. Examples of such functions include Leontief, Cobb-Douglas, and linear production functions. Nevertheless, the conventional form of the CES production specification is still restrictive in multiple aspects. One example is the fact that the marginal effect of increasing input use always has to be to increase the variability of output quantity by the conventional construction of this function. This paper proposes a generalized variant of the CES production function that allows for various input effects on the probability distribution of output. Failure to allow for this possible input-output risk structure is indeed one of the limitations of the conventional form of the CES production function. This limitation may result in false inferences about input-driven output risk. In light of this, the present paper proposes a solution to this problem. First, it is shown that the familiar CES formulation suffers from very restrictive structural assumptions regarding risk considerations, and that such restrictions may lead to biased and inefficient estimates of production quantity and production risk. Following the general theme of Just and Pope's approach, a CES-based production-function specification that overcomes this shortcoming of the original CES production function is introduced, and a three-stage Nonlinear Least-Squares (NLS) estimation procedure for the estimation of the proposed functional form is presented. To illustrate the proposed approaches in this paper, two empirical applications in irrigation and fertilizer response using the famous Hexem-Heady experimental dataset are provided. Finally, implications for modeling input-driven production risks are discussed.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20909v1,Price risk aversion vs payoff risk aversion: a gender comparison through a laboratory experiment,arXiv,2025-12-24,"Summary: Purpose: This paper explores gender differences in two distinct forms of risk aversion -- Payoff Risk Aversion (PaRA) and Price Risk Aversion (PrRA) -- in order to provide a more nuanced understanding of how men and women respond to different types of economic uncertainty.
  Design/methodology/approach: The study employs a laboratory experiment using Multiple-Choice-List (MCL) risk-elicitation tasks based on both Direct Utility Function (DUF) and Indirect Utility Function (IUF) frameworks. These tasks present stochastic payoffs and stochastic prices, respectively. The analysis uses statistical hypothesis testing to compare gender-specific responses across three experimental designs.
  Findings: The key results of the study indicate that women typically exhibit higher degrees of PaRA than men, which is a consistent finding with the mainstream literature. However, remarkably, the results from all the three indirect MCL designs show that women typically exhibit lower degrees of PrRA than men, and this result is robust across different MCL designs. The paper also introduces an 'irrationality gap' as the difference between PaRA and PrRA and explores the size of the irrationality gap within either gender group, finding it larger and statistically significant for men, while smaller and statistically insignificant for women.
  Originality/value: This study is the first to distinguish between PaRA and PrRA in a gender comparison, using experimentally validated methods. It provides new behavioral insights into the nature of gender-specific risk preferences and introduces the irrationality gap as a novel concept with implications for understanding financial decision-making and the design of gender-sensitive economic policies.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20908v1,Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation,arXiv,2025-12-24,"Summary: Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20905v1,DiEC: Diffusion Embedded Clustering,arXiv,2025-12-24,"Summary: Deep clustering hinges on learning representations that are inherently clusterable. However, using a single encoder to produce a fixed embedding ignores the representation trajectory formed by a pretrained diffusion model across network hierarchies and noise timesteps, where clusterability varies substantially. We propose DiEC (Diffusion Embedded Clustering), which performs unsupervised clustering by directly reading internal activations from a pretrained diffusion U-Net.
  DiEC formulates representation selection as a two-dimensional search over layer x timestep, and exploits a weak-coupling property to decompose it into two stages. Specifically, we first fix the U-Net bottleneck layer as the Clustering-friendly Middle Layer (CML), and then use Optimal Timestep Search (OTS) to identify the clustering-optimal timestep (t*). During training, we extract bottleneck features at the fixed t* and obtain clustering representations via a lightweight residual mapping. We optimize a DEC-style KL self-training objective, augmented with adaptive graph regularization and entropy regularization to strengthen cluster structures. In parallel, we introduce a denoising-consistency branch at random timesteps to stabilize the representations and preserve generative consistency. Experiments show that DiEC achieves competitive clustering performance on multiple standard benchmarks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20902v1,Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction,arXiv,2025-12-24,"Summary: Due to their inherent flexibility and autonomous operation, unmanned aerial vehicles (UAVs) have been widely used in Internet of Medical Things (IoMT) to provide real-time biomedical edge computing service for wireless body area network (WBAN) users. In this paper, considering the time-varying task criticality characteristics of diverse WBAN users and the dual mobility between WBAN users and UAV, we investigate the dynamic task offloading and UAV flight trajectory optimization problem to minimize the weighted average task completion time of all the WBAN users, under the constraint of UAV energy consumption. To tackle the problem, an embodied AI-enhanced IoMT edge computing framework is established. Specifically, we propose a novel hierarchical multi-scale Transformer-based user trajectory prediction model based on the users' historical trajectory traces captured by the embodied AI agent (i.e., UAV). Afterwards, a prediction-enhanced deep reinforcement learning (DRL) algorithm that integrates predicted users' mobility information is designed for intelligently optimizing UAV flight trajectory and task offloading decisions. Real-word movement traces and simulation results demonstrate the superiority of the proposed methods in comparison with the existing benchmarks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20900v1,When Experts Speak:Sequential LLM-Bayesian Learning for Startup Success Prediction,arXiv,2025-12-24,"Summary: Evaluating startups is inherently challenging in entrepreneurial finance, where investors confront severe information asymmetry and limited quantitative data. Leveraging a novel expert network call data, we develop an LLM-Bayesian model that analyzes these conversations at the question-answer turn level, extracting semantic and evaluative signals via large language models (LLMs) and aggregating them in a sequential Bayesian architecture. The model dynamically updates beliefs as additional expert calls occur and attenuates contradictory assessments, which are absent from existing text-based screening tools. Empirically, our model outperforms state-of-the-art benchmarks by 6.691% in F1-score and increases portfolio-level Return on Investment by 15.255%. Attention and ablation analyses reveal that conversational cues are particularly informative for technologically complex startups, young firms, diverse founding teams, and firms with low public visibility. By converting expert dialogue into continually updated probabilities, our model advances research in entrepreneurial finance and information systems and offers policy implications for improving funding outcomes for informationally disadvantaged startups.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20898v1,DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction,arXiv,2025-12-24,"Summary: Lung cancer continues to be the leading cause of cancer-related deaths globally. Early detection and diagnosis of pulmonary nodules are essential for improving patient survival rates. Although previous research has integrated multimodal and multi-temporal information, outperforming single modality and single time point, the fusion methods are limited to inefficient vector concatenation and simple mutual attention, highlighting the need for more effective multimodal information fusion. To address these challenges, we introduce a Dual-Graph Spatiotemporal Attention Network, which leverages temporal variations and multimodal data to enhance the accuracy of predictions. Our methodology involves developing a Global-Local Feature Encoder to better capture the local, global, and fused characteristics of pulmonary nodules. Additionally, a Dual-Graph Construction method organizes multimodal features into inter-modal and intra-modal graphs. Furthermore, a Hierarchical Cross-Modal Graph Fusion Module is introduced to refine feature integration. We also compiled a novel multimodal dataset named the NLST-cmst dataset as a comprehensive source of support for related research. Our extensive experiments, conducted on both the NLST-cmst and curated CSTL-derived datasets, demonstrate that our DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with exceptional computational efficiency.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20896v1,Accurate and Diverse Recommendations via Propensity-Weighted Linear Autoencoders,arXiv,2025-12-24,"Summary: In real-world recommender systems, user-item interactions are Missing Not At Random (MNAR), as interactions with popular items are more frequently observed than those with less popular ones. Missing observations shift recommendations toward frequently interacted items, which reduces the diversity of the recommendation list. To alleviate this problem, Inverse Propensity Scoring (IPS) is widely used and commonly models propensities based on a power-law function of item interaction frequency. However, we found that such power-law-based correction overly penalizes popular items and harms their recommendation performance. We address this issue by redefining the propensity score to allow broader item recommendation without excessively penalizing popular items. The proposed score is formulated by applying a sigmoid function to the logarithm of the item observation frequency, maintaining the simplicity of power-law scoring while allowing for more flexible adjustment. Furthermore, we incorporate the redefined propensity score into a linear autoencoder model, which tends to favor popular items, and evaluate its effectiveness. Experimental results revealed that our method substantially improves the diversity of items in the recommendation list without sacrificing recommendation accuracy.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20893v1,Time-Efficient Evaluation and Enhancement of Adversarial Robustness in Deep Neural Networks,arXiv,2025-12-24,"Summary: With deep neural networks (DNNs) increasingly embedded in modern society, ensuring their safety has become a critical and urgent issue. In response, substantial efforts have been dedicated to the red-blue adversarial framework, where the red team focuses on identifying vulnerabilities in DNNs and the blue team on mitigating them. However, existing approaches from both teams remain computationally intensive, constraining their applicability to large-scale models. To overcome this limitation, this thesis endeavours to provide time-efficient methods for the evaluation and enhancement of adversarial robustness in DNNs.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20885v1,From GNNs to Symbolic Surrogates via Kolmogorov-Arnold Networks for Delay Prediction,arXiv,2025-12-24,"Summary: Accurate prediction of flow delay is essential for optimizing and managing modern communication networks. We investigate three levels of modeling for this task. First, we implement a heterogeneous GNN with attention-based message passing, establishing a strong neural baseline. Second, we propose FlowKANet in which Kolmogorov-Arnold Networks replace standard MLP layers, reducing trainable parameters while maintaining competitive predictive performance. FlowKANet integrates KAMP-Attn (Kolmogorov-Arnold Message Passing with Attention), embedding KAN operators directly into message-passing and attention computation. Finally, we distill the model into symbolic surrogate models using block-wise regression, producing closed-form equations that eliminate trainable weights while preserving graph-structured dependencies. The results show that KAN layers provide a favorable trade-off between efficiency and accuracy and that symbolic surrogates emphasize the potential for lightweight deployment and enhanced transparency.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20884v1,The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents,arXiv,2025-12-24,"Summary: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20877v1,Architectural Trade-offs in Small Language Models Under Compute Constraints,arXiv,2025-12-24,"Summary: We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20872v1,Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification,arXiv,2025-12-24,"Summary: Function call graphs (FCGs) have emerged as a powerful abstraction for malware detection, capturing the behavioral structure of applications beyond surface-level signatures. Their utility in traditional program analysis has been well established, enabling effective classification and analysis of malicious software. In the mobile domain, especially in the Android ecosystem, FCG-based malware classification is particularly critical due to the platform's widespread adoption and the complex, component-based structure of Android apps. However, progress in this direction is hindered by the lack of large-scale, high-quality Android-specific FCG datasets. Existing datasets are often outdated, dominated by small or redundant graphs resulting from app repackaging, and fail to reflect the diversity of real-world malware. These limitations lead to overfitting and unreliable evaluation of graph-based classification methods. To address this gap, we introduce Better Call Graphs (BCG), a comprehensive dataset of large and unique FCGs extracted from recent Android application packages (APKs). BCG includes both benign and malicious samples spanning various families and types, along with graph-level features for each APK. Through extensive experiments using baseline classifiers, we demonstrate the necessity and value of BCG compared to existing datasets. BCG is publicly available at https://erdemub.github.io/BCG-dataset.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20866v1,Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images,arXiv,2025-12-24,"Summary: To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20865v1,Robustness Certificates for Neural Networks against Adversarial Attacks,arXiv,2025-12-24,"Summary: The increasing use of machine learning in safety-critical domains amplifies the risk of adversarial threats, especially data poisoning attacks that corrupt training data to degrade performance or induce unsafe behavior. Most existing defenses lack formal guarantees or rely on restrictive assumptions about the model class, attack type, extent of poisoning, or point-wise certification, limiting their practical reliability. This paper introduces a principled formal robustness certification framework that models gradient-based training as a discrete-time dynamical system (dt-DS) and formulates poisoning robustness as a formal safety verification problem. By adapting the concept of barrier certificates (BCs) from control theory, we introduce sufficient conditions to certify a robust radius ensuring that the terminal model remains safe under worst-case ${\ell}_p$-norm based poisoning. To make this practical, we parameterize BCs as neural networks trained on finite sets of poisoned trajectories. We further derive probably approximately correct (PAC) bounds by solving a scenario convex program (SCP), which yields a confidence lower bound on the certified robustness radius generalizing beyond the training set. Importantly, our framework also extends to certification against test-time attacks, making it the first unified framework to provide formal guarantees in both training and test-time attack settings. Experiments on MNIST, SVHN, and CIFAR-10 show that our approach certifies non-trivial perturbation budgets while being model-agnostic and requiring no prior knowledge of the attack or contamination level.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20864v1,(Im)possibility of Incentive Design for Challenge-based Blockchain Protocols,arXiv,2025-12-24,"Summary: Blockchains offer a decentralized and secure execution environment strong enough to host cryptocurrencies, but the state-replication model makes on-chain computation expensive. To avoid heavy on-chain workloads, systems like Truebit and optimistic rollups use challenge-based protocols, performing computations off-chain and invoking the chain only when challenged. This keeps normal-case costs low and, if at least one honest challenger exists, can catch fraud. What has been less clear is whether honest challengers are actually incentivized and a dishonest proposer is properly damaged under the worst case environment. We build a model with a colluding minority, heterogeneous costs, and three ordering modes. We then ask whether two goals can be met together: honest non-loss and fraud deterrence. Our results are clear: in single-winner designs, the incentive design is impossible or limited in scale. By contrast, in multi-winner designs, we obtain simple, explicit conditions under which both goals hold.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20861v1,Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs,arXiv,2025-12-24,"Summary: Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\times$ speedups and $3\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr .",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20856v1,NVIDIA Nemotron 3: Efficient and Open Intelligence,arXiv,2025-12-24,"Summary: We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20854v1,How important is Recall for Measuring Retrieval Quality?,arXiv,2025-12-24,"Summary: In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20850v1,Implicit Numerical Scheme for the Hamilton-Jacobi-Bellman Quasi-Variational Inequality in the Optimal Market-Making Problem with Alpha Signal,arXiv,2025-12-24,"Summary: We address the problem of combined stochastic and impulse control for a market maker operating in a limit order book. The problem is formulated as a Hamilton-Jacobi-Bellman quasi-variational inequality (HJBQVI). We propose an implicit time-discretization scheme coupled with a policy iteration algorithm. This approach removes time-step restrictions typical of explicit methods and ensures unconditional stability. Convergence to the unique viscosity solution is established by verifying monotonicity, stability, and consistency conditions and applying the comparison principle.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20848v1,"Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",arXiv,2025-12-23,"Summary: We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20847v1,YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion,arXiv,2025-12-23,"Summary: This paper introduces the YCB-Handovers dataset, capturing motion data of 2771 human-human handovers with varying object weights. The dataset aims to bridge a gap in human-robot collaboration research, providing insights into the impact of object weight in human handovers and readiness cues for intuitive robotic motion planning. The underlying dataset for object recognition and tracking is the YCB (Yale-CMU-Berkeley) dataset, which is an established standard dataset used in algorithms for robotic manipulation, including grasping and carrying objects. The YCB-Handovers dataset incorporates human motion patterns in handovers, making it applicable for data-driven, human-inspired models aimed at weight-sensitive motion planning and adaptive robotic behaviors. This dataset covers an extensive range of weights, allowing for a more robust study of handover behavior and weight variation. Some objects also require careful handovers, highlighting contrasts with standard handovers. We also provide a detailed analysis of the object's weight impact on the human reaching motion in these handovers.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20845v1,MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs,arXiv,2025-12-23,"Summary: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20833v1,CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images,arXiv,2025-12-23,"Summary: Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20831v1,Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions,arXiv,2025-12-23,"Summary: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20826v1,Optimal Algorithms for Nonlinear Estimation with Convex Models,arXiv,2025-12-23,"Summary: A linear functional of an object from a convex symmetric set can be optimally estimated, in a worst-case sense, by a linear functional of observations made on the object. This well-known fact is extended here to a nonlinear setting: other simple functionals of the object can be optimally estimated by functionals of the observations that share a similar simple structure. This is established for the maximum of several linear functionals and even for the $\ell$th largest among them. Proving the latter requires an unusual refinement of the analytical Hahn--Banach theorem. The existence results are accompanied by practical recipes relying on convex optimization to construct the desired functionals, thereby justifying the term of estimation algorithms.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20823v1,"NotSoTiny: A Large, Living Benchmark for RTL Code Generation",arXiv,2025-12-23,"Summary: LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20822v1,MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs,arXiv,2025-12-23,"Summary: Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20821v1,Defending against adversarial attacks using mixture of experts,arXiv,2025-12-23,"Summary: Machine learning is a powerful tool enabling full automation of a huge number of tasks without explicit programming. Despite recent progress of machine learning in different domains, these models have shown vulnerabilities when they are exposed to adversarial threats. Adversarial threats aim to hinder the machine learning models from satisfying their objectives. They can create adversarial perturbations, which are imperceptible to humans' eyes but have the ability to cause misclassification during inference. Moreover, they can poison the training data to harm the model's performance or they can query the model to steal its sensitive information. In this paper, we propose a defense system, which devises an adversarial training module within mixture-of-experts architecture to enhance its robustness against adversarial threats. In our proposed defense system, we use nine pre-trained experts with ResNet-18 as their backbone. During end-to-end training, the parameters of expert models and gating mechanism are jointly updated allowing further optimization of the experts. Our proposed defense system outperforms state-of-the-art defense systems and plain classifiers, which use a more complex architecture than our model's backbone.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20817v1,EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading,arXiv,2025-12-23,"Summary: Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20814v1,FedMPDD: Communication-Efficient Federated Learning with Privacy Preservation Attributes via Projected Directional Derivative,arXiv,2025-12-23,"Summary: This paper introduces \texttt{FedMPDD} (\textbf{Fed}erated Learning via \textbf{M}ulti-\textbf{P}rojected \textbf{D}irectional \textbf{D}erivatives), a novel algorithm that simultaneously optimizes bandwidth utilization and enhances privacy in Federated Learning. The core idea of \texttt{FedMPDD} is to encode each client's high-dimensional gradient by computing its directional derivatives along multiple random vectors. This compresses the gradient into a much smaller message, significantly reducing uplink communication costs from $\mathcal{O}(d)$ to $\mathcal{O}(m)$, where $m \ll d$. The server then decodes the aggregated information by projecting it back onto the same random vectors. Our key insight is that averaging multiple projections overcomes the dimension-dependent convergence limitations of a single projection. We provide a rigorous theoretical analysis, establishing that \texttt{FedMPDD} converges at a rate of $\mathcal{O}(1/\sqrt{K})$, matching the performance of FedSGD. Furthermore, we demonstrate that our method provides some inherent privacy against gradient inversion attacks due to the geometric properties of low-rank projections, offering a tunable privacy-utility trade-off controlled by the number of projections. Extensive experiments on benchmark datasets validate our theory and demonstrates our results.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20813v1,GraphFire-X: Physics-Informed Graph Attention Networks and Structural Gradient Boosting for Building-Scale Wildfire Preparedness at the Wildland-Urban Interface,arXiv,2025-12-23,"Summary: As wildfires increasingly evolve into urban conflagrations, traditional risk models that treat structures as isolated assets fail to capture the non-linear contagion dynamics characteristic of the wildland urban interface (WUI). This research bridges the gap between mechanistic physics and data driven learning by establishing a novel dual specialist ensemble framework that disentangles vulnerability into two distinct vectors, environmental contagion and structural fragility. The architecture integrates two specialized predictive streams, an environmental specialist, implemented as a graph neural network (GNN) that operationalizes the community as a directed contagion graph weighted by physics informed convection, radiation, and ember probabilities, and enriched with high dimensional Google AlphaEarth Foundation embeddings, and a Structural Specialist, implemented via XGBoost to isolate granular asset level resilience. Applied to the 2025 Eaton Fire, the framework reveals a critical dichotomy in risk drivers. The GNN demonstrates that neighborhood scale environmental pressure overwhelmingly dominates intrinsic structural features in defining propagation pathways, while the XGBoost model identifies eaves as the primary micro scale ingress vector. By synthesizing these divergent signals through logistic stacking, the ensemble achieves robust classification and generates a diagnostic risk topology. This capability empowers decision makers to move beyond binary loss prediction and precisely target mitigation prioritizing vegetation management for high connectivity clusters and structural hardening for architecturally vulnerable nodes thereby operationalizing a proactive, data driven approach to community resilience.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20812v1,Semantic Deception: When Reasoning Models Can't Compute an Addition,arXiv,2025-12-23,"Summary: Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20811v1,Weighted MCC: A Robust Measure of Multiclass Classifier Performance for Observations with Individual Weights,arXiv,2025-12-23,"Summary: Several performance measures are used to evaluate binary and multiclass classification tasks.
  But individual observations may often have distinct weights, and none of these measures are sensitive to such varying weights.
  We propose a new weighted Pearson-Matthews Correlation Coefficient (MCC) for binary classification as well as weighted versions of related multiclass measures. The weighted MCC varies between $-1$ and $1$. But crucially, the weighted MCC values are higher for classifiers that perform better on highly weighted observations, and hence is able to distinguish them from classifiers that have a similar overall performance and ones that perform better on the lowly weighted observations.
  Furthermore, we prove that the weighted measures are robust with respect to the choice of weights in a precise manner:
  if the weights are changed by at most $ε$, the value of the weighted measure changes at most by a factor of $ε$ in the binary case
  and by a factor of $ε^2$ in the multiclass case.
  Our computations demonstrate that the weighted measures clearly identify classifiers that perform better on higher weighted observations, while the unweighted measures remain completely indifferent to the choices of weights.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20806v1,Safety Alignment of LMs via Non-cooperative Games,arXiv,2025-12-23,"Summary: Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20798v1,A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents,arXiv,2025-12-23,"Summary: As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant ""deliberative misalignment"", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20797v1,Assessing Coronary Microvascular Dysfunction using Angiography-based Data-driven Methods,arXiv,2025-12-23,"Summary: Coronary microvascular dysfunction (CMD), characterized by impaired regulation of blood flow in the coronary microcirculation, plays a key role in the pathogenesis of ischemic heart disease and is increasingly recognized as a contributor to adverse cardiovascular outcomes. Despite its clinical importance, CMD remains underdiagnosed due to the reliance on invasive procedures such as pressure wire-based measurements of the index of microcirculatory resistance (IMR) and coronary flow reserve (CFR), which are costly, time-consuming, and carry procedural risks. To date, no study has sought to quantify CMD indices using data-driven approaches while leveraging the rich information contained in coronary angiograms. To address these limitations, this study proposes a novel data-driven framework for inference of CMD indices based on coronary angiography. A physiologically validated multi-physics model was used to generate synthetic datasets for data-driven model training, consisting of CMD indices and computational angiograms with corresponding contrast intensity profiles (CIPs). Two neural network architectures were developed: a single-input-channel encoder-MLP model for IMR prediction and a dual-input-channel encoder-MLP model for CFR prediction, both incorporating epistemic uncertainty estimation to quantify prediction confidence. Results demonstrate that the data-driven models achieve high predictive accuracy when evaluated against physics-based synthetic datasets, and that the uncertainty estimates are positively correlated with prediction errors. Furthermore, the utility of CIPs as informative surrogates for coronary physiology is demonstrated, underscoring the potential of the proposed framework to enable accurate, real-time, image-based CMD assessment using routine angiography without the need for more invasive approaches.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20796v1,Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?,arXiv,2025-12-23,"Summary: We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20795v1,RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale,arXiv,2025-12-23,"Summary: Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and tightly coupled, agent-driven control within a single execution campaign. These workflows impose heterogeneous and often conflicting requirements on runtime systems, spanning MPI executables, persistent AI services, fine-grained tasks, and low-latency AI-HPC coupling. Existing systems typically address only subsets of these requirements, limiting their ability to support emerging AI-HPC applications at scale. We present RHAPSODY, a multi-runtime middleware that enables concurrent execution of heterogeneous AI-HPC workloads through uniform abstractions for tasks, services, resources, and execution policies. Rather than replacing existing runtimes, RHAPSODY composes and coordinates them, allowing simulation codes, inference services, and agentic workflows to coexist within a single job allocation on leadership-class HPC platforms. We evaluate RHAPSODY with Dragon and vLLM on multiple HPC systems using representative heterogeneous, inference-at-scale, and tightly coupled AI-HPC workflows. Our results show that RHAPSODY introduces minimal runtime overhead, sustains increasing heterogeneity at scale, achieves near-linear scaling for high-throughput inference workloads, and data- and control-efficient coupling between AI and HPC tasks in agentic workflows.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21337v1,Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models,arXiv,2025-12-24,"Summary: We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21334v1,Streaming Video Instruction Tuning,arXiv,2025-12-24,"Summary: We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21333v1,Fast SAM2 with Text-Driven Token Pruning,arXiv,2025-12-24,"Summary: Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21331v1,TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning,arXiv,2025-12-24,"Summary: The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21321v1,Large time behavior of the solution to the Cauchy problem for the discrete p-Laplacian with density on infinite graphs,arXiv,2025-12-24,"Summary: We consider the Cauchy problem for the nonstationary discrete p-Laplacian with inhomogeneous density \r{ho}(x) on an infinite graph which supports the Sobolev inequality. For nonnegative solutions when p > 2, we prove the precise rate of stabilization in time, provided \r{ho}(x) is a non-power function. When p > 2 and \r{ho}(x) goes to zero fast enough, we prove the universal bound. Our technique relies on suitable energy inequalities and a new embedding result.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21264v1,AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI,arXiv,2025-12-24,"Summary: Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs robust anomaly detection and localization under arbitrary MRI modality availability. The framework integrates a dual-pathway DINOv2 encoder with a feature distribution alignment mechanism that statistically aligns incomplete-modality features with full-modality representations, enabling stable inference even with severe modality dropout. To further enhance semantic consistency, we introduce an Intrinsic Normal Prototypes (INPs) extractor and an INP-guided decoder that reconstruct only normal anatomical patterns while naturally amplifying abnormal deviations. Through randomized modality masking and indirect feature completion during training, the model learns to adapt to all modality configurations without re-training. Extensive experiments on BraTS2018, MU-Glioma-Post, and Pretreat-MetsToBrain-Masks demonstrate that our approach consistently surpasses state-of-the-art industrial and medical AD baselines across 7 modality combinations, achieving superior generalization. This study establishes a scalable paradigm for multimodal medical AD under real-world, imperfect modality conditions. Our source code is available at https://github.com/wuchangw/AnyAD.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21263v1,Observation of the Aharonov-Bohm Effect in Pilot-Wave Hydrodynamics,arXiv,2025-12-24,"Summary: We report the results of an experimental study of an analog of the Aharonov-Bohm (AB) effect achieved with the hydrodynamic pilot-wave system. A walking droplet is confined to an annular cavity that encircles a shielded vortex, but lies outside its range of direct influence. While there is no vortex-induced flow in the immediate vicinity of the droplets, the vortex modifies the droplet's spatially extended pilot-wave field that guides its motion, producing a vortex-dependent bias in the droplet's orbital speed. High-speed tracking and delay-embedding reconstructions yield Wigner-like phase-space distributions for this hydrodynamic system that exhibits a rigid, flux-dependent translation, providing a force-free, gauge-like realization of an AB-type phase.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21253v1,Neural Network-Assisted RIS Weight Optimization for Spatial Nulling in Distorted Reflector Antenna Systems,arXiv,2025-12-24,"Summary: Reconfigurable intelligent surfaces (RIS) have recently been proposed as an effective means for spatial interference suppression in large reflector antenna systems. Existing RIS weight optimization algorithms typically rely on accurate theoretical radiation models. However, in practice, distortions on the reflector antenna may cause mismatches between the theoretical and true antenna patterns, leading to degraded interference cancellation performance when these weights are directly applied. In this report, a residual learning network-assisted simulated annealing (ResNet-SA) framework is proposed to address this mismatch without requiring explicit knowledge of the distorted electric field. By learning the residual difference between the theoretical and true antenna gains, a neural network (NN) is embedded in a heuristic optimization algorithm to find the optimal weight vector. Simulation results demonstrate that the proposed approach achieves improved null depth in the true radiation pattern as compared with conventional methods that optimize weights based solely on the theoretical model, validating the effectiveness of the ResNet-SA algorithm for reflector antenna systems with approximate knowledge of the pattern.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21237v1,SegMo: Segment-aligned Text to 3D Human Motion Generation,arXiv,2025-12-24,"Summary: Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21232v1,Fragmentation of neutron-rich carbon isotopes on light targets at 27.5 MeV/nucleon,arXiv,2025-12-24,"Summary: Experimental and theoretical investigation of the fragmentation reaction in Fermi-energy domain is currently of particular importance for not only the nuclear physics but also some interdisciplinary fields. In the present work, neutron-rich $^{14}$C and $^{16}$C ion beams at 27.5 MeV/nucleon were used to bombard carbon and polyethylene (CD$_{2}$)$_{n}$ targets. Energy and angular distributions of the produced fragments were measured. Background events originating from the carbon content in (CD$_{2}$)$_{n}$ target were efficiently excluded using an extended $E-P$ plot method. Experimental results are systematically analyzed by using HIPSE-SIMON dynamic model. The comparison reveals that, for the carbon target, the HIPSE-SIMON calculation overestimates the yields of the beam-velocity component for fragments near the projectile and also the energy phase space for fragments far away from the projectile, suggesting fine tuning of the overall interaction profile adopted in the model. In contrast, for reactions with the deuteron target, the model calculation can reasonably reproduce the experimental data. The implication of the fragmentation mechanism to the validity of the invariant mass method, as frequently used to reconstruct the clustering resonant structures in light nuclei, is also discussed.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21230v1,Assessing systematic uncertainties from spectral re-analysis of Cyg X-1 with different coronal geometries,arXiv,2025-12-24,"Summary: In this work, we carry out a new spectral reanalysis of NuSTAR and Suzaku observations of the disk reflection spectra in the stellar-mass black hole X-ray binary Cyg~X-1. We compare three types of models: a broken power-law disk emissivity profile with no assumption about the coronal shape used in the previous work of the same observations, a compact lamppost corona, and an extended disk-like corona motivated by recent X-ray polarization results. Our goal is to measure the systematic uncertainties caused by the assumed geometry, with a focus on key parameters such as the black hole spin and the inclination of the inner accretion disk. We find that the disk-like corona gives a fit that is statistically similar to the broken power-law and lamppost models, but it leads to more physically reasonable results, such as a lower inclination angle of about $30^{\circ}$. By using a variable disk density model, we measure the disk density to be $n_{\rm e}\approx10^{20}$\,cm$^{-3}$, which is similar to earlier results. While the extended corona model infers a wider allowed parameter space for black hole spin and the inner radius of the disk-shaped coronal region, this reflects the additional physical freedom of the model. Even so, the disk-like corona remains a strong and physically well-motivated candidate for explaining the X-ray emission from Cyg~X-1.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21219v1,Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3,arXiv,2025-12-24,"Summary: Maintaining stability during the single-support phase is a fundamental challenge in humanoid robotics, particularly in dance robots that require complex maneuvers and high mechanical freedom. Traditional tethered sensor configurations often restrict joint movement and introduce mechanical noises. This study proposes a wireless embedded balance system designed to maintain stability on uneven surfaces. The system utilizes a custom-designed foot unit integrated with four load cells and an ESP32-C3 microcontroller to estimate the Center of Pressure (CoP) in real time. The CoP data were transmitted wirelessly to the main controller to minimize the wiring complexity of the 29-DoF VI-ROSE humanoid robot. A PID control strategy is implemented to adjust the torso, hip, and ankle roll joints based on CoP feedback. Experimental characterization demonstrated high sensor precision with an average measurement error of 14.8 g. Furthermore, the proposed control system achieved a 100% success rate in maintaining balance during single-leg lifting tasks at a 3-degree inclination with optimized PID parameters (Kp=0.10, Kd=0.005). These results validate the efficacy of wireless CoP feedback in enhancing the postural stability of humanoid robots, without compromising their mechanical flexibility.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21218v1,Latent Implicit Visual Reasoning,arXiv,2025-12-24,"Summary: While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what ""useful"" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21212v1,Study of laser-beam arrival time synchronization towards sub-picosecond stability level,arXiv,2025-12-24,"Summary: A precise synchronization between laser pulse and electron beam arrival time is essential for achieving sub-picosecond stability in modern accelerator facilities. In this work, a Low-Level RF system architecture combined with White Rabbit based timing system has been tested through a collaboration between KEK (Japan) and CNRS/IN2P3, IJClab (France). The setup combines a frequency standard generator, an IDROGEN carrier board with an embedded White Rabbit node, and SkyWorks synthesizers of different form factors to distribute phase-locked clock signals over telecommunication fiber. Phase noise power spectral density measurements were performed at several RF sub-harmonics to confirm synchronization performance. These results demonstrate the feasibility of implementing the White Rabbit-IDROGEN synchronization scheme for large-scale accelerators, including applications to laser-based diagnostics.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21209v1,Human Motion Estimation with Everyday Wearables,arXiv,2025-12-24,"Summary: While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21200v1,A Multimodal Human-Centered Framework for Assessing Pedestrian Well-Being in the Wild,arXiv,2025-12-24,"Summary: Pedestrian well-being is a critical yet rarely measured component of sustainable urban mobility and livable city design. Existing approaches to evaluating pedestrian environments often rely on static, infrastructure-based indices or retrospective surveys, which overlook the dynamic, subjective, and psychophysiological dimensions of everyday walking experience. This paper introduces a multimodal, human-centered framework for assessing pedestrian well-being in the wild by integrating three complementary data streams: continuous physiological sensing, geospatial tracking, and momentary self-reports collected using the Experience Sampling Method. The framework conceptualizes pedestrian experience as a triangulation enabling a holistic understanding of how urban environments influence well-being. The utility of our framework is then demonstrated through a naturalistic case study conducted in the Greater Philadelphia region, in which participants wore research-grade wearable sensors and carried GPS-enabled smartphones during their regular daily activities. Physiological indicators of autonomic nervous system activity, including heart rate variability and electrodermal activity, were synchronized with spatial trajectories and in situ self-reports of stress, affect, and perceived infrastructure conditions. Results illustrate substantial inter- and intra-individual variability in both subjective experience and physiological response, as well as context-dependent patterns associated with traffic exposure, pedestrian infrastructure quality, and environmental enclosure. The findings also suggest that commonly used walkability indices may not fully capture experiential dimensions of pedestrian well-being. By enabling real-world, multimodal measurement of pedestrian experience, the proposed framework offers a scalable and transferable approach for advancing human-centered urban analytics.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21194v1,VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs,arXiv,2025-12-24,"Summary: Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21186v1,Preliminary forecasting constraint on scalar charge with LISA in non-vacuum environments,arXiv,2025-12-24,"Summary: We compute the gravitational wave signal from eccentric extreme-mass-ratio inspirals (EMRIs) embedded within beyond-vacuum environments, where the secondary object carries a scalar charge and evolves in the presence of both an accretion disk and a dark matter halo. The waveform modification is derived by incorporating the scalar charge correcting the fluxes and orbital trajectories of the secondary. Our results indicate that, under suitable parameter configurations, the influence of the scalar charge on EMRIs waveform in such environments can be distinguished from that in vacuum spacetime. For the EMRIs signal modified by the astrophysical environments, the future space-borne detector can determine the relative error of scalar charge constrained by LISA at the level of $\sim0.1$, providing a preliminary prediction of detecting scalar charge in the beyond-vacuum spacetime.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21151v1,Acoustic gravitational waves from primordial curvature perturbations,arXiv,2025-12-24,"Summary: Standard perturbative calculations of scalar-induced gravitational waves (SIGWs) have neglected nonperturbative effects in the large-amplitude regime. We develop a hybrid numerical framework to signify nonperturbative effects on the stochastic gravitational wave (GW) background sourced by primordial curvature perturbations, focusing on the acoustic channel (fluid motions). Fully general-relativistic, spherically symmetric simulations are used to extract nonperturbative sound-shell profiles from isolated curvature peaks; these profiles are then embedded into three-dimensional lattice evolutions of relativistic hydrodynamics coupled to transverse-traceless metric perturbations to compute the acoustic GW spectra. The acoustic signal has a peak frequency determined by the comoving shell thickness, and its amplitude is extremely sensitive to the mean comoving separation of peaks, scaling approximately as $R_{*c}^{-7}$. We find a robust causal low-frequency tail $\propto k^{3}$, and the nonlinear hydrodynamic interactions can enhance the ultraviolet power. Comparing with SIGWs computed perturbatively from the same real-space configuration, we show that acoustic GWs can be amplified by an order of magnitude and display a peak shifted to a lower frequency in the large-amplitude regime. These results highlight the importance of nonperturbative effects for accurate predictions of stochastic GW signals induced from primordial curvature perturbations.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21144v1,Encrypted Traffic Detection in Resource Constrained IoT Networks: A Diffusion Model and LLM Integrated Framework,arXiv,2025-12-24,"Summary: The proliferation of Internet-of-things (IoT) infrastructures and the widespread adoption of traffic encryption present significant challenges, particularly in environments characterized by dynamic traffic patterns, constrained computational capabilities, and strict latency constraints. In this paper, we propose DMLITE, a diffusion model and large language model (LLM) integrated traffic embedding framework for network traffic detection within resource-limited IoT environments. The DMLITE overcomes these challenges through a tri-phase architecture including traffic visual preprocessing, diffusion-based multi-level feature extraction, and LLM-guided feature optimization. Specifically, the framework utilizes self-supervised diffusion models to capture both fine-grained and abstract patterns in encrypted traffic through multi-level feature fusion and contrastive learning with representative sample selection, thus enabling rapid adaptation to new traffic patterns with minimal labeled data. Furthermore, DMLITE incorporates LLMs to dynamically adjust particle swarm optimization parameters for intelligent feature selection by implementing a dual objective function that minimizes both classification error and variance across data distributions. Comprehensive experimental validation on benchmark datasets confirms the effectiveness of DMLITE, achieving classification accuracies of 98.87\%, 92.61\%, and 99.83\% on USTC-TFC, ISCX-VPN, and Edge-IIoTset datasets, respectively. This improves classification accuracy by an average of 3.7\% and reduces training time by an average of 41.9\% compared to the representative deep learning model.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21136v1,Modeling gap acceptance behavior allowing for perceptual distortions and exogenous influences,arXiv,2025-12-24,"Summary: This work on gap acceptance is based on the premise that the decision to accept/reject a gap happens in a person's mind and therefore must be based on the perceived gap and not the measured gap. The critical gap must also exist in a person's mind and hence, together with the perceived gap, is a latent variable. Finally, it is also proposed that the critical gap is influenced by various exogenous variables such as subject and opposing vehicle types, and perceived waiting time. Mathematical models that (i) incorporate systematic and random distortions during the perception process and (ii) account for the effect of the various influencing variables are developed. The parameters of these models are estimated for two different gap acceptance data sets using the maximum likelihood technique. The data is collected as part of this study. The estimated parameters throw valuable insights into how these influencing variables affect the critical gap. The results corroborate the initial predictions on the nature of influence these variables must exert and give strength to the gap acceptance decision-making construct proposed here. This work also proposes a methodology to estimate a measurable/observable world emulator of the latent variable critical gap. The use of the emulator critical gap provides improved estimates of derived quantities like the average waiting time of subject vehicles. Finally, studies are also conducted to show that the number of rejected gaps can work as a reasonable surrogate for the influencing variable, waiting time.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21133v1,SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation,arXiv,2025-12-24,"Summary: Multi-agent trajectory generation is a core problem for autonomous driving and intelligent transportation systems. However, efficiently modeling the dynamic interactions between numerous road users and infrastructures in complex scenes remains an open problem. Existing methods typically employ distance-based or fully connected dense graph structures to capture interaction information, which not only introduces a large number of redundant edges but also requires complex and heavily parameterized networks for encoding, thereby resulting in low training and inference efficiency, limiting scalability to large and complex traffic scenes. To overcome the limitations of existing methods, we propose SparScene, a sparse graph learning framework designed for efficient and scalable traffic scene representation. Instead of relying on distance thresholds, SparScene leverages the lane graph topology to construct structure-aware sparse connections between agents and lanes, enabling efficient yet informative scene graph representation. SparScene adopts a lightweight graph encoder that efficiently aggregates agent-map and agent-agent interactions, yielding compact scene representations with substantially improved efficiency and scalability. On the motion prediction benchmark of the Waymo Open Motion Dataset (WOMD), SparScene achieves competitive performance with remarkable efficiency. It generates trajectories for more than 200 agents in a scene within 5 ms and scales to more than 5,000 agents and 17,000 lanes with merely 54 ms of inference time with a GPU memory of 2.9 GB, highlighting its superior scalability for large-scale traffic scenes.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21095v1,UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters,arXiv,2025-12-24,"Summary: Text and formulas constitute the core informational components of many documents. Accurately and efficiently recognizing both is crucial for developing robust and generalizable document parsing systems. Recently, vision-language models (VLMs) have achieved impressive unified recognition of text and formulas. However, they are large-sized and computationally demanding, restricting their usage in many applications. In this paper, we propose UniRec-0.1B, a unified recognition model with only 0.1B parameters. It is capable of performing text and formula recognition at multiple levels, including characters, words, lines, paragraphs, and documents. To implement this task, we first establish UniRec40M, a large-scale dataset comprises 40 million text, formula and their mix samples, enabling the training of a powerful yet lightweight model. Secondly, we identify two challenges when building such a lightweight but unified expert model. They are: structural variability across hierarchies and semantic entanglement between textual and formulaic content. To tackle these, we introduce a hierarchical supervision training that explicitly guides structural comprehension, and a semantic-decoupled tokenizer that separates text and formula representations. Finally, we develop a comprehensive evaluation benchmark covering Chinese and English documents from multiple domains and with multiple levels. Experimental results on this and public benchmarks demonstrate that UniRec-0.1B outperforms both general-purpose VLMs and leading document parsing expert models, while achieving a 2-9$\times$ speedup, validating its effectiveness and efficiency. Codebase and Dataset: https://github.com/Topdu/OpenOCR.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21082v1,Electron spectral shape of the third-forbidden $β$-decay of $^{87}$Rb measured using a Rb$_2$ZrCl$_6$ crystal scintillator,arXiv,2025-12-24,"Summary: In recent years, interest in experimental studies of $β$-decay electron spectra -- often referred to as $β$ spectra -- has been growing. This is particularly true for $β$ transitions where the electron spectra are sensitive to the effective value of the weak axial coupling, $g_{\rm A}$. Such measurements serve as important benchmarks for nuclear physics calculations and can also be used to characterize background in astroparticle physics experiments. In this work, a dedicated experiment has been carried out to investigate the spectral shape of the third-forbidden $^{87}$Rb $β$-decays, with the goal of estimating the effective $g_{\rm A}$ value for this transition and of deriving the T$_{1/2}$ value. This was done by comparing the experimental spectral shape with the estimates from various phenomenological models. The $^{87}$Rb source was embedded directly within the detector material of a new Rb$_2$ZrCl$_6$ crystal scintillator; the data taking was performed deep underground at Gran Sasso National Laboratory. The obtained experimental half-life value for the studied process is T$_{1/2} = 5.08(13) \times$ 10$^{10}$ yr; while a $g_{\rm A}$ value in the range 0.4 to 0.6 is obtained when accounting for uncertainties and depending on the model adopted as discussed in detail in the text.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21078v1,UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer,arXiv,2025-12-24,"Summary: Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21065v1,Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation,arXiv,2025-12-24,"Summary: Grasping is one of the most fundamental challenging capabilities in robotic manipulation, especially in unstructured, cluttered, and semantically diverse environments. Recent researches have increasingly explored language-guided manipulation, where robots not only perceive the scene but also interpret task-relevant natural language instructions. However, existing language-conditioned grasping methods typically rely on shallow fusion strategies, leading to limited semantic grounding and weak alignment between linguistic intent and visual grasp reasoning.In this work, we propose Language-Guided Grasp Detection (LGGD) with a coarse-to-fine learning paradigm for robotic manipulation. LGGD leverages CLIP-based visual and textual embeddings within a hierarchical cross-modal fusion pipeline, progressively injecting linguistic cues into the visual feature reconstruction process. This design enables fine-grained visual-semantic alignment and improves the feasibility of the predicted grasps with respect to task instructions. In addition, we introduce a language-conditioned dynamic convolution head (LDCH) that mixes multiple convolution experts based on sentence-level features, enabling instruction-adaptive coarse mask and grasp predictions. A final refinement module further enhances grasp consistency and robustness in complex scenes.Experiments on the OCID-VLG and Grasp-Anything++ datasets show that LGGD surpasses existing language-guided grasping methods, exhibiting strong generalization to unseen objects and diverse language queries. Moreover, deployment on a real robotic platform demonstrates the practical effectiveness of our approach in executing accurate, instruction-conditioned grasp actions. The code will be released publicly upon acceptance.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21064v1,Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition,arXiv,2025-12-24,"Summary: Multimodal human action understanding is a significant problem in computer vision, with the central challenge being the effective utilization of the complementarity among diverse modalities while maintaining model efficiency. However, most existing methods rely on simple late fusion to enhance performance, which results in substantial computational overhead. Although early fusion with a shared backbone for all modalities is efficient, it struggles to achieve excellent performance. To address the dilemma of balancing efficiency and effectiveness, we introduce a self-supervised multimodal skeleton-based action representation learning framework, named Decomposition and Composition. The Decomposition strategy meticulously decomposes the fused multimodal features into distinct unimodal features, subsequently aligning them with their respective ground truth unimodal counterparts. On the other hand, the Composition strategy integrates multiple unimodal features, leveraging them as self-supervised guidance to enhance the learning of multimodal representations. Extensive experiments on the NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets demonstrate that the proposed method strikes an excellent balance between computational cost and model performance.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21016v1,On the Euclidean Distance Degree of Quadratic Two-Neuron Neural Networks,arXiv,2025-12-24,"Summary: We study the Euclidean Distance degree of algebraic neural network models from the perspective of algebraic geometry. Focusing on shallow networks with two neurons, quadratic activation, and scalar output, we identify the associated neurovariety with the second secant variety of a quadratic Veronese embedding. We introduce and analyze the virtual Euclidean Distance degree, a projective invariant defined as the sum of the polar degrees of the variety, which coincides with the usual Euclidean Distance degree for a generic choice of scalar product. Using intersection theory, Chern-Mather classes, and the Nash blow-up provided by Kempf's resolution, we reduce the computation of the virtual Euclidean Distance degree to explicit intersection numbers on a Grassmannian. Applying equivariant localization, we prove that this invariant depends stably polynomially on the input dimension. Numerical experiments based on homotopy continuation illustrate the dependence of the Euclidean Distance degree on the chosen metric and highlight the distinction between the generic and nongeneric cases, such as the Bombieri-Weyl metric.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21014v1,Black Hole Evaporation Driven by Non-Thermal Squeezing Through SNS and CSNS Dynamics,arXiv,2025-12-24,"Summary: In this work, we present a comprehensive semiclassical analysis of black hole radiation in a spatially flat FRW Universe for two fundamental nonclassical states: the Squeezed Number State (SNS) and the Coherent Squeezed Number State (CSNS). Unlike thermally modified earlier studies, SNS and CSNS constitute fully non-thermal, number-state-dependent quantum configurations. By embedding these states within the framework of semiclassical theory of gravity, we derive state-resolved expressions for the Hawking temperature, entropy variation, and corresponding mass loss of an evaporating black hole. The influence of the squeezing parameter $ρ$ and number state parameter $n$ on Hawking emission is examined through a series of analytical results supported by twelve detailed plots. The analysis reveals that the Hawking temperature exhibits monotonic growth with increasing $ρ$ and $n$, thereby elevating the effective temperature experienced at the black hole horizon. The entropy variations $Δ\mathbb{S}_{\mathrm{SNS}}$ and $Δ\mathbb{S}_{\mathrm{CSNS}}$ show strong nonlinear enhancement, especially at moderate and large squeezing values. Overall, the study extends earlier thermal squeezed-state approaches to a fully number-state-resolved framework, highlighting the sensitivity of Hawking emission to nonclassical quantum configurations. These findings contribute a new perspective on gravitational particle creation in cosmological settings.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21008v1,GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs,arXiv,2025-12-24,"Summary: Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness.
  In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.21003v1,MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds,arXiv,2025-12-24,"Summary: Multi-view inverse rendering aims to recover geometry, materials, and illumination consistently across multiple viewpoints. When applied to multi-view images, existing single-view approaches often ignore cross-view relationships, leading to inconsistent results. In contrast, multi-view optimization methods rely on slow differentiable rendering and per-scene refinement, making them computationally expensive and hard to scale. To address these limitations, we introduce a feed-forward multi-view inverse rendering framework that directly predicts spatially varying albedo, metallic, roughness, diffuse shading, and surface normals from sequences of RGB images. By alternating attention across views, our model captures both intra-view long-range lighting interactions and inter-view material consistency, enabling coherent scene-level reasoning within a single forward pass. Due to the scarcity of real-world training data, models trained on existing synthetic datasets often struggle to generalize to real-world scenes. To overcome this limitation, we propose a consistency-based finetuning strategy that leverages unlabeled real-world videos to enhance both multi-view coherence and robustness under in-the-wild conditions. Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of multi-view consistency, material and normal estimation quality, and generalization to real-world imagery.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20997v1,LLM-Empowered Agentic AI for QoE-Aware Network Slicing Management in Industrial IoT,arXiv,2025-12-24,"Summary: The Industrial Internet of Things (IIoT) requires networks that deliver ultra-low latency, high reliability, and cost efficiency, which traditional optimization methods and deep reinforcement learning (DRL)-based approaches struggle to provide under dynamic and heterogeneous workloads. To address this gap, large language model (LLM)-empowered agentic AI has emerged as a promising paradigm, integrating reasoning, planning, and adaptation to enable QoE-aware network management. In this paper, we explore the integration of agentic AI into QoE-aware network slicing for IIoT. We first review the network slicing management architecture, QoE metrics for IIoT applications, and the challenges of dynamically managing heterogeneous network slices, while highlighting the motivations and advantages of adopting agentic AI. We then present the workflow of agentic AI-based slicing management, illustrating the full lifecycle of AI agents from processing slice requests to constructing slice instances and performing dynamic adjustments. Furthermore, we propose an LLM-empowered agentic AI approach for slicing management, which integrates a retrieval-augmented generation (RAG) module for semantic intent inference, a DRL-based orchestrator for slicing configuration, and an incremental memory mechanism for continual learning and adaptation. Through a case study on heterogeneous slice management, we demonstrate that the proposed approach significantly outperforms other baselines in balancing latency, reliability, and cost, and achieves up to a 19% improvement in slice availability ratio.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20992v1,Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection in Physiotherapy Palpation,arXiv,2025-12-24,"Summary: Robotic palpation relies on force sensing, but force signals in soft-tissue environments are variable and cannot reliably reveal subtle subsurface features. We present a compact multimodal sensor that integrates high-resolution vision-based tactile imaging with a 6-axis force-torque sensor. In experiments on silicone phantoms with diverse subsurface tendon geometries, force signals alone frequently produce ambiguous responses, while tactile images reveal clear structural differences in presence, diameter, depth, crossings, and multiplicity. Yet accurate force tracking remains essential for maintaining safe, consistent contact during physiotherapeutic interaction. Preliminary results show that combining tactile and force modalities enables robust subsurface feature detection and controlled robotic palpation.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20986v1,AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs,arXiv,2025-12-24,"Summary: The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs. Traditional defenses, based on static filters and rigid rules, are insufficient to address the semantic complexity of these new attacks. We argue that a paradigm shift is needed -- from passive filtering to active protection and autonomous reasoning. We introduce AegisAgent, an autonomous agent system designed to ensure the security of LLM-driven HAR systems. Instead of merely blocking threats, AegisAgent functions as a cognitive guardian. It autonomously perceives potential semantic inconsistencies, reasons about the user's true intent by consulting a dynamic memory of past interactions, and acts by generating and executing a multi-step verification and repair plan. We implement AegisAgent as a lightweight, full-stack prototype and conduct a systematic evaluation on 15 common attacks with five state-of-the-art LLM-based HAR systems on three public datasets. Results show it reduces attack success rate by 30\% on average while incurring only 78.6 ms of latency overhead on a GPU workstation. Our work makes the first step towards building secure and trustworthy LLM-driven HAR systems.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20970v1,Universal Transient Stability Analysis: A Large Language Model-Enabled Dynamics Prediction Framework,arXiv,2025-12-24,"Summary: Existing dynamics prediction frameworks for transient stability analysis (TSA) fail to achieve multi-scenario ""universality""--the inherent ability of a single, pre-trained architecture to generalize across diverse operating conditions, unseen faults, and heterogeneous systems. To address this, this paper proposes TSA-LLM, a large language model (LLM)-based universal framework that models multi-variate transient dynamics prediction as a univariate generative task with three key innovations: First, a novel data processing pipeline featuring channel independence decomposition to resolve dimensional heterogeneity, sample-wise normalization to eliminate separate stable or unstable pipelines, and temporal patching for efficient long-sequence modeling; Second, a parameter-efficient freeze-and-finetune strategy that augments the LLM's architecture with dedicated input embedding and output projection layers while freezing core transformer blocks to preserve generic feature extraction capabilities; Third, a two-stage fine-tuning scheme that combines teacher forcing, which feeds the model ground-truth data during initial training, with scheduled sampling, which gradually shifts to leveraging model-generated predictions, to mitigate cumulative errors in long-horizon iterative prediction. Comprehensive testing demonstrates the framework's universality, as TSA-LLM trained solely on the New England 39-bus system achieves zero-shot generalization to mixed stability conditions and unseen faults, and matches expert performance on the larger Iceland 189-bus system with only 5% fine-tuning data. This multi-scenario versatility validates a universal framework that eliminates scenario-specific retraining and achieves scalability via large-scale parameters and cross-scenario training data.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20940v1,ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments,arXiv,2025-12-24,"Summary: Vision-Language Navigation in Continuous Environments (VLN-CE) requires an embodied agent to navigate towards target in continuous environments, following natural language instructions. While current graph-based methods offer an efficient, structured approach by abstracting the environment into a topological map and simplifying the action space to waypoint selection, they lag behind methods based on Large Vision-Language Models (LVLMs) in leveraging large-scale data and advanced training paradigms. In this paper, we try to bridge this gap by introducing ETP-R1, a framework that applies the paradigm of scaling up data and Reinforcement Fine-Tuning (RFT) to a graph-based VLN-CE model. To build a strong foundation, we first construct a high-quality, large-scale pretraining dataset using the Gemini API. This dataset consists of diverse, low-hallucination instructions for topological trajectories, providing rich supervision for our graph-based policy to map language to topological paths. This foundation is further strengthened by unifying data from both R2R and RxR tasks for joint pretraining. Building on this, we introduce a three-stage training paradigm, which culminates in the first application of closed-loop, online RFT to a graph-based VLN-CE model, powered by the Group Relative Policy Optimization (GRPO) algorithm. Extensive experiments demonstrate that our approach is highly effective, establishing new state-of-the-art performance across all major metrics on both the R2R-CE and RxR-CE benchmarks. Our code is available at https://github.com/Cepillar/ETP-R1.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20936v1,Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation,arXiv,2025-12-24,"Summary: Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20927v1,Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting,arXiv,2025-12-24,"Summary: Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20926v1,"Uncovering Hierarchical Structure in LLM Embeddings with $δ$-Hyperbolicity, Ultrametricity, and Neighbor Joining",arXiv,2025-12-24,"Summary: The rapid advancement of large language models (LLMs) has enabled significant strides in various fields. This paper introduces a novel approach to evaluate the effectiveness of LLM embeddings in the context of inherent geometric properties. We investigate the structural properties of these embeddings through three complementary metrics $δ$-hyperbolicity, Ultrametricity, and Neighbor Joining. $δ$-hyperbolicity, a measure derived from geometric group theory, quantifies how much a metric space deviates from being a tree-like structure. In contrast, ultrametricity characterizes strictly hierarchical structures where distances obey a strong triangle inequality. While Neighbor Joining quantifies how tree-like the distance relationships are, it does so specifically with respect to the tree reconstructed by the Neighbor Joining algorithm. By analyzing the embeddings generated by LLMs using these metrics, we uncover to what extent the embedding space reflects an underlying hierarchical or tree-like organization. Our findings reveal that LLM embeddings exhibit varying degrees of hyperbolicity and ultrametricity, which correlate with their performance in the underlying machine learning tasks.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20907v1,PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding,arXiv,2025-12-24,"Summary: 3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20901v1,Benchmarking and Enhancing VLM for Compressed Image Understanding,arXiv,2025-12-24,"Summary: With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20876v1,Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task,arXiv,2025-12-24,"Summary: From the perspective of future developments in robotics, it is crucial to verify whether foundation models trained exclusively on offline data, such as images and language, can understand the robot motion. In particular, since Vision Language Models (VLMs) do not include low-level motion information from robots in their training datasets, video understanding including trajectory information remains a significant challenge. In this study, we assess two capabilities of VLMs through a video captioning task with low-level robot motion information: (1) automatic captioning of robot tasks and (2) segmentation of a series of tasks. Both capabilities are expected to enhance the efficiency of robot imitation learning by linking language and motion and serve as a measure of the foundation model's performance. The proposed method generates multiple ""scene"" captions using image captions and trajectory data from robot tasks. The full task caption is then generated by summarizing these individual captions. Additionally, the method performs subtask segmentation by comparing the similarity between text embeddings of image captions. In both captioning tasks, the proposed method aims to improve performance by providing the robot's motion data - joint and end-effector states - as input to the VLM. Simulator experiments were conducted to validate the effectiveness of the proposed method.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20873v1,Systemization of Knowledge: Resilience and Fault Tolerance in Cyber-Physical Systems,arXiv,2025-12-24,"Summary: Cyber-Physical Systems (CPS) now support critical infrastructure spanning transportation, energy, manufacturing, medical devices, and autonomous robotics. Their defining characteristic is the tight coupling between digital computation and continuous physical dynamics which enables sophisticated autonomy but also creates highly non-linear failure modes. Small disturbances at sensors, firmware, networks, or physical interfaces can propagate through estimation and control pipelines, producing cascading instabilities that defy traditional single-layer reasoning. This Systematization of Knowledge (SoK) unifies nearly two decades of CPS resilience research into a structured Origin-Layer-Effect (OLE) taxonomy. This taxonomy provides a cross-layer lens for understanding how faults arise, how they propagate, and why unrelated CPS failures often share deep structural similarities. By mapping representative systems including RockDrone, MAYDAY, M2MON, HACMS, Byzantine fault-tolerant control, and learning-based recovery mechanisms onto the taxonomy, we reveal patterns of coverage, persistent blind spots, and recurring pathways of fault amplification. Our analysis identifies four structural gaps that span multiple CPS domains: (1) physical-model manipulation, (2) ML-enabled control without stability guarantees, (3) semantic inconsistencies between formal models and firmware, and (4) inadequate forensic visibility across cyber and physical layers. These insights motivate new directions for resilient CPS design, integrating robust control, runtime monitoring, formal assurance, and system-level visibility.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20858v1,ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction,arXiv,2025-12-24,"Summary: Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.
  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.
  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.
  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20839v1,Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference,arXiv,2025-12-23,"Summary: Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20835v1,QoS- and Physics-Aware Routing in Optical LEO Satellite Networks via Deep Reinforcement Learning,arXiv,2025-12-23,"Summary: Optical inter-satellite links (ISLs) are becoming the principal communication backbone in modern large-scale LEO constellations, offering multi-Gb/s capacity and near speed-of-light latency. However, the extreme sensitivity of optical beams to relative satellite motion, pointing jitter, and rapidly evolving geometry makes routing fundamentally more challenging than in RF-based systems. In particular, intra-plane and inter-plane ISLs exhibit markedly different stability and feasible range profiles, producing a dynamic, partially constrained connectivity structure that must be respected by any physically consistent routing strategy. This paper presents a lightweight geometry- and QoS-aware routing framework for optical LEO networks that incorporates class-dependent feasibility constraints derived from a jitter-aware Gaussian-beam model. These analytically computed thresholds are embedded directly into the time-varying ISL graph and enforced via feasible-action masking in a deep reinforcement learning (DRL) agent. The proposed method leverages local geometric progress, feasible-neighbor structure, and congestion indicators to select next-hop relays without requiring global recomputation. Simulation results on a Starlink-like constellation show that the learned paths are physically consistent, exploit intra-plane stability, adapt to jitter-limited inter-plane connectivity, and maintain robust end-to-end latency under dynamic topology evolution.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20789v1,X-GridAgent: An LLM-Powered Agentic AI System for Assisting Power Grid Analysis,arXiv,2025-12-23,"Summary: The growing complexity of power system operations has created an urgent need for intelligent, automated tools to support reliable and efficient grid management. Conventional analysis tools often require significant domain expertise and manual effort, which limits their accessibility and adaptability. To address these challenges, this paper presents X-GridAgent, a novel large language model (LLM)-powered agentic AI system designed to automate complex power system analysis through natural language queries. The system integrates domain-specific tools and specialized databases under a three-layer hierarchical architecture comprising planning, coordination, and action layers. This architecture offers high flexibility and adaptability to previously unseen tasks, while providing a modular and extensible framework that can be readily expanded to incorporate new tools, data sources, or analytical capabilities. To further enhance performance, we introduce two novel algorithms: (1) LLM-driven prompt refinement with human feedback, and (2) schema-adaptive hybrid retrieval-augmented generation (RAG) for accurate information retrieval from large-scale structured grid datasets. Experimental evaluations across a variety of user queries and power grid cases demonstrate the effectiveness and reliability of X-GridAgent in automating interpretable and rigorous power system analysis.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20783v1,NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts,arXiv,2025-12-23,"Summary: Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20781v1,Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints,arXiv,2025-12-23,"Summary: Composed Image Retrieval (CIR) aims to find a target image that aligns with user intent, expressed through a reference image and a modification text. While Zero-shot CIR (ZS-CIR) methods sidestep the need for labeled training data by leveraging pretrained vision-language models, they often rely on a single fused query that merges all descriptive cues of what the user wants, tending to dilute key information and failing to account for what they wish to avoid. Moreover, current CIR benchmarks assume a single correct target per query, overlooking the ambiguity in modification texts. To address these challenges, we propose Soft Filtering with Textual constraints (SoFT), a training-free, plug-and-play filtering module for ZS-CIR. SoFT leverages multimodal large language models (LLMs) to extract two complementary constraints from the reference-modification pair: prescriptive (must-have) and proscriptive (must-avoid) constraints. These serve as semantic filters that reward or penalize candidate images to re-rank results, without modifying the base retrieval model or adding supervision. In addition, we construct a two-stage dataset pipeline that refines CIR benchmarks. We first identify multiple plausible targets per query to construct multi-target triplets, capturing the open-ended nature of user intent. Then guide multimodal LLMs to rewrite the modification text to focus on one target, while referencing contrastive distractors to ensure precision. This enables more comprehensive and reliable evaluation under varying ambiguity levels. Applied on top of CIReVL, a ZS-CIR retriever, SoFT raises R@5 to 65.25 on CIRR (+12.94), mAP@50 to 27.93 on CIRCO (+6.13), and R@50 to 58.44 on FashionIQ (+4.59), demonstrating broad effectiveness.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20778v1,Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication,arXiv,2025-12-23,"Summary: Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20769v1,A General Purpose Method for Robotic Interception of Non-Cooperative Dynamic Targets,arXiv,2025-12-23,"Summary: This paper presents a general purpose framework for autonomous, vision-based interception of dynamic, non-cooperative targets, validated across three distinct mobility platforms: an unmanned aerial vehicle (UAV), a four-wheeled ground rover, and an air-thruster spacecraft testbed. The approach relies solely on a monocular camera with fiducials for target tracking and operates entirely in the local observer frame without the need for global information. The core contribution of this work is a streamlined and general approach to autonomous interception that can be adapted across robots with varying dynamics, as well as our comprehensive study of the robot interception problem across heterogenous mobility systems under limited observability and no global localization. Our method integrates (1) an Extended Kalman Filter for relative pose estimation amid intermittent measurements, (2) a history-conditioned motion predictor for dynamic target trajectory propagation, and (3) a receding-horizon planner solving a constrained convex program in real time to ensure time-efficient and kinematically feasible interception paths. Our operating regime assumes that observability is restricted by partial fields of view, sensor dropouts, and target occlusions. Experiments are performed in these conditions and include autonomous UAV landing on dynamic targets, rover rendezvous and leader-follower tasks, and spacecraft proximity operations. Results from simulated and physical experiments demonstrate robust performance with low interception errors (both during station-keeping and upon scenario completion), high success rates under deterministic and stochastic target motion profiles, and real-time execution on embedded processors such as the Jetson Orin, VOXL2, and Raspberry Pi 5. These results highlight the framework's generalizability, robustness, and computational efficiency.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20766v1,Watt-class injection-locked diode laser system at 399 nm for atomic physics,arXiv,2025-12-23,"Summary: We demonstrate an injection-locked 399 nm laser system with up to 1 W output power and a locked power fraction of 0.57. The system consists of a high power, multimode diode laser that is seeded by 5 mW from a single-mode external cavity diode laser. The locked high-power laser inherits the frequency agility and linewidth of the seed laser with 3.9 kHz broadening. With active stabilization, the injection lock can be maintained for more than a day. We verify the utility of this system for atomic physics by performing spectroscopy of an ytterbium atomic beam.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20763v1,Streamfunction-vorticity formulation for incompressible viscid and inviscid flows on general surfaces,arXiv,2025-12-23,"Summary: This paper presents a streamfunction-vorticity formulation for the Navier--Stokes and Euler equations on general surfaces. Notably, this includes non-simply connected surfaces, on which the harmonic components of the velocity field play a fundamental role in the dynamics. By relying only on scalar and finite-dimensional quantities, our formulation ensures that the resulting methods give exactly tangential and incompressible velocity fields, while also being pressure robust. Compared to traditional methods based on velocity-pressure formulations, where one can only guarantee these structural properties by increasing the computational costs, this is a key advantage. We rigorously validate our formulation by proving its equivalence to the well understood velocity-pressure formulation under reasonable regularity assumptions. Furthermore, we demonstrate the applicability of the approach with numerical examples.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20760v1,Generalization of RLVR Using Causal Reasoning as a Testbed,arXiv,2025-12-23,"Summary: Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20749v1,Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies,arXiv,2025-12-23,"Summary: In recent years, the development of multimodal autoencoders has gained significant attention due to their potential to handle multimodal complex data types and improve model performance. Understanding the stability and robustness of these models is crucial for optimizing their training, architecture, and real-world applicability. This paper presents an analysis of Lipschitz properties in multimodal autoencoders, combining both theoretical insights and empirical validation to enhance the training stability of these models. We begin by deriving the theoretical Lipschitz constants for aggregation methods within the multimodal autoencoder framework. We then introduce a regularized attention-based fusion method, developed based on our theoretical analysis, which demonstrates improved stability and performance during training. Through a series of experiments, we empirically validate our theoretical findings by estimating the Lipschitz constants across multiple trials and fusion strategies. Our results demonstrate that our proposed fusion function not only aligns with theoretical predictions but also outperforms existing strategies in terms of consistency, convergence speed, and accuracy. This work provides a solid theoretical foundation for understanding fusion in multimodal autoencoders and contributes a solution for enhancing their performance.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20745v1,AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent,arXiv,2025-12-23,"Summary: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20740v1,NP-completeness of the $\ell_1$-embedding problem for simple graphs as sphere-of-influence graphs,arXiv,2025-12-23,"Summary: In graph theory an interesting question is whether for a fixed choice of $p\in [0,\infty]$, all simple graphs appear as sphere-of-influence graphs in some Euclidean space with respect to the $\ell_p$ metric. The answer is affirmative for $p=\infty$, negative for any $p\in (0,\infty)$, and unknown for $p=1$. The result of this work shows that for the case of $p=1$, this embeddability question is a (Promise) NP-Complete problem.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20735v1,VL4Gaze: Unleashing Vision-Language Models for Gaze Following,arXiv,2025-12-23,"Summary: Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20732v1,FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs,arXiv,2025-12-23,"Summary: As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20729v1,Shifted Partial Derivative Polynomial Rank and Codimension,arXiv,2025-12-23,"Summary: Shifted partial derivative (SPD) methods are a central algebraic tool for circuit lower bounds, measuring the dimension of spaces of shifted derivatives of a polynomial. We develop the Shifted Partial Derivative Polynomial (SPDP) framework, packaging SPD into an explicit coefficient-matrix formalism. This turns shifted-derivative spans into concrete linear-algebraic objects and yields two dual measures: SPDP rank and SPDP codimension.
  We define the SPDP generating family, its span, and the SPDP matrix M_{k,l}(p) inside a fixed ambient coefficient space determined by the (k,l) regime, so rank is canonical and codimension is a deficit from ambient fullness. We prove structural properties needed for reuse: monotonicity in the shift/derivative parameters (with careful scoping for |S|=k versus |S|<=k conventions), invariance under admissible variable symmetries and basis changes, and robustness across standard Boolean/multilinear embeddings. We then give generic width-to-rank upper-bound templates for local circuit models via profile counting, separating the model-agnostic SPDP toolkit from additional compiled refinements used elsewhere. We illustrate the codimension viewpoint on representative examples.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20723v1,From artificial to organic: Rethinking the roots of intelligence for digital health,arXiv,2025-12-23,"Summary: The term artificial implies an inherent dichotomy from the natural or organic. However, AI, as we know it, is a product of organic ingenuity: designed, implemented, and iteratively improved by human cognition. The very principles that underpin AI systems, from neural networks to decision-making algorithms, are inspired by the organic intelligence embedded in human neurobiology and evolutionary processes. The path from organic to artificial intelligence in digital health is neither mystical nor merely a matter of parameter count, it is fundamentally about organization and adaption. Thus, the boundaries between artificial and organic are far less distinct than the nomenclature suggests.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20722v1,Learning-Enabled Elastic Network Topology for Distributed ISAC Service Provisioning,arXiv,2025-12-23,"Summary: Conventional mobile networks, including both localized cell-centric and cooperative cell-free networks (CCN/CFN), are built upon rigid network topologies. However, neither architecture is adequate to flexibly support distributed integrated sensing and communication (ISAC) services, due to the increasing difficulty of aligning spatiotemporally distributed heterogeneous service demands with available radio resources. In this paper, we propose an elastic network topology (ENT) for distributed ISAC service provisioning, where multiple co-existing localized CCNs can be dynamically aggregated into CFNs with expanded boundaries for federated network operation. This topology elastically orchestrates localized CCN and federated CFN boundaries to balance signaling overhead and distributed resource utilization, thereby enabling efficient ISAC service provisioning. A two-phase operation protocol is then developed. In Phase I, each CCN autonomously classifies ISAC services as either local or federated and partitions its resources into dedicated and shared segments. In Phase II, each CCN employs its dedicated resources for local ISAC services, while the aggregated CFN consolidates shared resources from its constituent CCNs to cooperatively deliver federated services. Furthermore, we design a utility-to-signaling ratio (USR) to quantify the tradeoff between sensing/communication utility and signaling overhead. Consequently, a USR maximization problem is formulated by jointly optimizing the network topology (i.e., service classification and CCN aggregation) and the allocation of dedicated and shared resources. However, this problem is challenging due to its distributed optimization nature and the absence of complete channel state information. To address this problem efficiently, we propose a multi-agent deep reinforcement learning (MADRL) framework with centralized training and decentralized execution.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20714v1,From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education,arXiv,2025-12-23,"Summary: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.",2025-12-25
arxiv,arxiv,https://arxiv.org/abs/2512.20699v1,Comparing next-generation detector configurations for high-redshift gravitational wave sources with neural posterior estimation,arXiv,2025-12-23,"Summary: The coming decade will be crucial for determining the final design and configuration of a global network of next-generation (XG) gravitational-wave (GW) detectors, including the Einstein Telescope (ET) and Cosmic Explorer (CE). In this study and for the first time, we assess the performance of various network configurations using neural posterior estimation (NPE) implemented in Dingo-IS-a method based on normalizing flows and importance sampling that enables fast and accurate inference. We focus on a specific science case involving short-duration, massive and high-redshift binary black hole (BBH) mergers with detector-frame chirp masses $M_{\mathrm{d}} > 100$ M$_\odot$. These systems encompass early-Universe stellar and primordial black holes, as well as intermediate-mass black-hole binaries, for which XG observatories are expected to deliver major discoveries. Validation against standard Bayesian inference demonstrates that NPE robustly reproduces complex and disconnected posterior structures across all network configurations. For a network of two misaligned L-shaped ET detectors (2L MisA), the posterior distributions on luminosity distance can become multimodal and degenerate with the sky position, leading to less precise distance estimates compared to the triangular ET configuration. However, the number of sky-location multimodalities is substantially lower than the eight expected with the triangular ET, resulting in improved sky and volume localization. Adding CE to the network further reduces sky-position degeneracies, and the better performance of the 2L MisA configuration over the triangle remains evident.",2025-12-25
