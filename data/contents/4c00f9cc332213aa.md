# Computer Science > Computation and Language

**URL:** https://arxiv.org/abs/2405.07001v1
**Published:** 2024-05-11T00:00:00.000Z

---

## Summary

The webpage summarizes a research paper titled "Evaluating Task-based Effectiveness of MLLMs on Charts." The study focuses on evaluating **Multimodal Large Language Models (MLLMs)**, specifically **GPT-4V**, on low-level data analysis tasks performed on charts.

Key points relevant to your query include:
*   **Vision-language models (MLLMs):** The paper evaluates 18 advanced MLLMs, including closed-source models like **GPT-4V**.
*   **Chart Analysis:** The core focus is on data analysis tasks on charts, which falls under **document understanding** and **chart/table extraction** aspects of multimodal processing.
*   **Effectiveness:** It reports on the capabilities and limitations of these models in this domain, finding that GPT-4V achieved the highest accuracy (56.13% initially).
*   **Prompt Strategies:** The research proposes and tests prompt strategies ("Chain-of-Charts" and visual prompting) to improve performance, boosting accuracy up to 83.83%.

While the page discusses MLLMs and chart analysis (a form of structured data extraction), it does not explicitly mention **multimodal RAG**, **PDF parsing**, **report generation with LLMs**, **Claude vision**, **Gemini**, or **structured document output** beyond chart data extraction.

---

## Full Content

[2405.07001v1] Evaluating Task-based Effectiveness of MLLMs on Charts
[Skip to main content](#content)
[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)
We gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)
[](https://arxiv.org/IgnoreMe)
[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2405.07001v1
[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)
All fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text
Search
[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)
[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)
open search
GO
open navigation menu
# Computer Science \> Computation and Language
**arXiv:2405.07001v1**(cs)
A newer version of this paper has been withdrawn by Lutao Yan
[Submitted on 11 May 2024 (this version),*latest version 6 Nov 2024*([v4](https://arxiv.org/abs/2405.07001v4))]
# Title:Evaluating Task-based Effectiveness of MLLMs on Charts
Authors:[Yifan Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Y),[Lutao Yan](https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+L),[Yuyu Luo](https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+Y),[Yunhai Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y),[Nan Tang](https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+N)
View a PDF of the paper titled Evaluating Task-based Effectiveness of MLLMs on Charts, by Yifan Wu and 4 other authors
[View PDF](https://arxiv.org/pdf/2405.07001v1)[HTML (experimental)](https://arxiv.org/html/2405.07001v1)> > Abstract:
> In this paper, we explore a forward-thinking question: Is GPT-4V effective at low-level data analysis tasks on charts? To this end, we first curate a large-scale dataset, named ChartInsights, consisting of 89,388 quartets (chart, task, question, answer) and covering 10 widely-used low-level data analysis tasks on 7 chart types. Firstly, we conduct systematic evaluations to understand the capabilities and limitations of 18 advanced MLLMs, which include 12 open-source models and 6 closed-source models. Starting with a standard textual prompt approach, the average accuracy rate across the 18 MLLMs is 36.17%. Among all the models, GPT-4V achieves the highest accuracy, reaching 56.13%. To understand the limitations of multimodal large models in low-level data analysis tasks, we have designed various experiments to conduct an in-depth test of capabilities of GPT-4V. We further investigate how visual modifications to charts, such as altering visual elements (e.g. changing color schemes) and introducing perturbations (e.g. adding image noise), affect performance of GPT-4V. Secondly, we present 12 experimental findings. These findings suggest potential of GPT-4V to revolutionize interaction with charts and uncover the gap between human analytic needs and capabilities of GPT-4V. Thirdly, we propose a novel textual prompt strategy, named Chain-of-Charts, tailored for low-level analysis tasks, which boosts model performance by 24.36%, resulting in an accuracy of 80.49%. Furthermore, by incorporating a visual prompt strategy that directs attention of GPT-4V to question-relevant visual elements, we further improve accuracy to 83.83%. Our study not only sheds light on the capabilities and limitations of GPT-4V in low-level data analysis tasks but also offers valuable insights for future research. Subjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)|
Cite as:|[arXiv:2405.07001](https://arxiv.org/abs/2405.07001)[cs.CL]|
|(or[arXiv:2405.07001v1](https://arxiv.org/abs/2405.07001v1)[cs.CL]for this version)|
|[https://doi.org/10.48550/arXiv.2405.07001](https://doi.org/10.48550/arXiv.2405.07001)
Focus to learn more
arXiv-issued DOI via DataCite
|
## Submission history
From: Lutao Yan [[view email](https://arxiv.org/show-email/ab88c6ad/2405.07001)]
**[v1]**Sat, 11 May 2024 12:33:46 UTC (26,853 KB)
**[[v2]](https://arxiv.org/abs/2405.07001v2)**Mon, 17 Jun 2024 15:44:33 UTC (1 KB)*(withdrawn)*
**[[v3]](https://arxiv.org/abs/2405.07001v3)**Wed, 2 Oct 2024 00:46:22 UTC (38,547 KB)
**[[v4]](https://arxiv.org/abs/2405.07001v4)**Wed, 6 Nov 2024 13:56:28 UTC (38,543 KB)
Full-text links:## Access Paper:
View a PDF of the paper titled Evaluating Task-based Effectiveness of MLLMs on Charts, by Yifan Wu and 4 other authors
* [View PDF](https://arxiv.org/pdf/2405.07001v1)
* [HTML (experimental)](https://arxiv.org/html/2405.07001v1)
* [TeX Source](https://arxiv.org/src/2405.07001v1)
[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)
Current browse context:
cs.CL
[&lt;&lt;prev](https://arxiv.org/prevnext?id=2405.07001&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2405.07001&amp;function=next&amp;context=cs.CL)
[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2024-05](https://arxiv.org/list/cs.CL/2024-05)
Change to browse by:
[cs](https://arxiv.org/abs/2405.07001?context=cs)
[cs.AI](https://arxiv.org/abs/2405.07001?context=cs.AI)
[cs.CV](https://arxiv.org/abs/2405.07001?context=cs.CV)
### References &amp; Citations
* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.07001)
* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.07001)
* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.07001)
export BibTeX citationLoading...
## BibTeX formatted citation
&times;
loading...
Data provided by:
### Bookmark
[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()
Bibliographic Tools
# Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*
Connected Papers Toggle
Connected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*
Litmaps Toggle
Litmaps*([What is Litmaps?](https://www.litmaps.co/))*
scite.ai Toggle
scite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*
Code, Data, Media
# Code, Data and Media Associated with this Article
alphaXiv Toggle
alphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*
Links to Code Toggle
CatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*
DagsHub Toggle
DagsHub*([What is DagsHub?](https://dagshub.com/))*
GotitPub Toggle
Gotit.pub*([What is GotitPub?](http://gotit.pub/faq))*
Huggingface Toggle
Hugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*
Links to Code Toggle
Papers with Code*([What is Papers with Code?](https://paperswithcode.com/))*
ScienceCast Toggle
ScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*
Demos
# Demos
Replicate Toggle
Replicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*
Spaces Toggle
Hugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*
Spaces Toggle
TXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*
Related Papers
# Recommenders and Search Tools
Link to Influence Flower
Influence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*
Core recommender toggle
CORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*
* Author
* Venue
* Institution
* Topic
About arXivLabs
# arXivLabs: experimental projects with community collaborators
arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
Have an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).
[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2405.07001)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))
