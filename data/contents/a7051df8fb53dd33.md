# Multimodal RAG with GPT-4-Vision and LangChain

**URL:** https://medium.com/@shravankoninti/multimodal-rag-with-gpt-4-vision-and-langchain-60a6a13a92e4
**Published:** 2024-09-04T16:36:24.000Z

---

## Summary

The webpage describes a framework called **Multimodal RAG with GPT-4-Vision and LangChain**.

This framework combines:
1.  **Multimodal RAG (Retrieval-Augmented Generation):** The ability to process and generate multiple data types (like text and images) while grounding responses in retrieved information.
2.  **GPT-4-Vision (specifically using `gpt-4o-mini` in the example):** A multimodal model capable of processing both text and visual inputs (images).
3.  **LangChain:** A tool used to build applications around language models.

The practical implementation detailed on the page involves:
*   Using the `unstructured` library to **parse PDFs** to extract **text, tables, and images**.
*   Using **GPT-4o-mini** to generate summaries for these extracted text, table, and image elements.
*   Storing these elements and their summaries using a **MultiVectorRetriever** with a **Chroma** vector store.
*   Setting up a chain to answer questions based on the retrieved context, which can include information derived from text, tables, and images.

In summary, the page details how to build a system that can understand and answer questions based on documents containing text, tables, and images by leveraging multimodal models and RAG techniques within the LangChain ecosystem.

---

## Full Content

Multimodal RAG with GPT-4-Vision and LangChain | by Shravan Kumar | Medium
[Sitemap](https://medium.com/sitemap/sitemap.xml)
[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)
Sign up
[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@shravankoninti/multimodal-rag-with-gpt-4-vision-and-langchain-60a6a13a92e4&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)
[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)
[
Write
](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)
[
Search
](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)
Sign up
[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/@shravankoninti/multimodal-rag-with-gpt-4-vision-and-langchain-60a6a13a92e4&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)
![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)
# Multimodal RAG with GPT-4-Vision and LangChain
[
![Shravan Kumar](https://miro.medium.com/v2/resize:fill:64:64/1*bUTiYqYPpDticWBePxbPcA.jpeg)
](https://medium.com/@shravankoninti?source=post_page---byline--60a6a13a92e4---------------------------------------)
[Shravan Kumar](https://medium.com/@shravankoninti?source=post_page---byline--60a6a13a92e4---------------------------------------)
5 min read
·Sep 4, 2024
[
](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/p/60a6a13a92e4&amp;operation=register&amp;redirect=https://medium.com/@shravankoninti/multimodal-rag-with-gpt-4-vision-and-langchain-60a6a13a92e4&amp;user=Shravan+Kumar&amp;userId=ccc45b3ef025&amp;source=---header_actions--60a6a13a92e4---------------------clap_footer------------------)
--
1
[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/60a6a13a92e4&amp;operation=register&amp;redirect=https://medium.com/@shravankoninti/multimodal-rag-with-gpt-4-vision-and-langchain-60a6a13a92e4&amp;source=---header_actions--60a6a13a92e4---------------------bookmark_footer------------------)
Listen
Share
**Multimodal RAG with GPT-4-Vision and LangChain**refers to a framework that combines the capabilities of GPT-4-Vision (a multimodal version of OpenAI’s GPT-4 that can process and generate text, images, and possibly other data types) with LangChain, a tool designed to facilitate the building of applications that use language models. Here’s a breakdown of the key concepts:
1. **Multimodal RAG (Retrieval-Augmented Generation):**
* **Multimodal:**This term refers to the ability to process and generate multiple types of data, such as text, images, audio, and more. GPT-4-Vision is an example of a multimodal model capable of handling both text and visual inputs.
* **Retrieval-Augmented Generation (RAG):**RAG is a technique that combines the strengths of retrieval-based models (which fetch relevant information from a database or knowledge base) with generation-based models (which create content). In the context of GPT-4-Vision, RAG can be used to generate rich, informed responses that are grounded in both text and visual data.
2.**GPT-4-Vision:**
* GPT-4-Vision is a version of GPT-4 that can process both text and images, allowing it to answer questions, generate descriptions, and perform tasks that require an understanding of visual content. Here we can use**gpt-4o-mini**for both text/tables/images extraction.
* This capability is especially useful for tasks where visual context is important, such as analyzing images, creating visual descriptions, or combining text and image data to provide richer outputs.
Press enter or click to view image in full size
![]()
Here the code below demonstrate the option 3. Let us look at how this concept can be used practically for some applications where we will see text/tables/images are used. Here is the code with a detailed explanation for each part:
```
!pip install langchain unstructured[all-docs] pydantic lxml openai chromadb tiktoken pytesseract
```
Please install the above packages in your virtual environment. In addition to the above pip packages, you will also need**poppler**([installation instructions](https://pdf2image.readthedocs.io/en/latest/installation.html)) and**tesseract**([installation instructions](https://tesseract-ocr.github.io/tessdoc/Installation.html)) in your system.
```
from typing import Any
import os
from unstructured.partition.pdf import partition\_pdf
import pytesseract
import uuid
from langchain.embeddings import OpenAIEmbeddings
from langchain.retrievers.multi\_vector import MultiVectorRetriever
from langchain.schema.document import Document
from langchain.storage import InMemoryStore
from langchain.vectorstores import Chroma
import base64
from langchain.chat\_models import ChatOpenAI
from langchain.schema.messages import HumanMessage, AIMessage
from dotenv import load\_dotenv
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output\_parser import StrOutputParser
# Load environment variables from .env
load\_dotenv()
```
This section imports necessary libraries and loads environment variables from a**.env**file using**dotenv**. This file might contain sensitive information like API keys, which should be kept private.
```
pytesseract.pytesseract.tesseract\_cmd = r&#x27;&#x27;C:\\Program Files\\Tesseract-OCR\\tesseract.exe&#x27;&#x27;
```
Sets the path to the Tesseract OCR executable. This allows the program to use OCR for extracting text from images embedded in PDF documents.
```
input\_path = os.getcwd()
output\_path = os.path.join(os.getcwd(), &quot;&quot;figures&quot;&quot;)
# Get elements
raw\_pdf\_elements = partition\_pdf(
filename=os.path.join(input\_path, &quot;&quot;test.pdf&quot;&quot;),
extract\_images\_in\_pdf=True,
infer\_table\_structure=True,
chunking\_strategy=&quot;&quot;by\_title&quot;&quot;,
max\_characters=4000,
new\_after\_n\_chars=3800,
combine\_text\_under\_n\_chars=2000,
image\_output\_dir\_path=output\_path,
)
```
Defines the input and output paths.**input\_path**is the current working directory, and**output\_path**is where extracted images from PDFs will be saved.Extracts elements from a PDF file, including text, tables, and images. The parameters control how the document is chunked and where images are stored.
```
text\_elements = []
table\_elements = []
image\_elements = []
# Function to encode images
def encode\_image(image\_path):
with open(image\_path, &quot;&quot;rb&quot;&quot;) as image\_file:
return base64.b64encode(image\_file.read()).decode(&#x27;&#x27;utf-8&#x27;&#x27;)
for element in raw\_pdf\_elements:
if &#x27;CompositeElement&#x27; in str(type(element)):
text\_elements.append(element)
elif &#x27;Table&#x27; in str(type(element)):
table\_elements.append(element)
table\_elements = [i.text for i in table\_elements]
text\_elements = [i.text for i in text\_elements]
# Tables
print(&quot;&quot;The length of table elements are :&quot;&quot;, len(table\_elements))
# Text
print(&quot;&quot;The length of text elements are :&quot;&quot;, len(text\_elements))
for image\_file in os.listdir(output\_path):
if image\_file.endswith((&#x27;&#x27;.png&#x27;&#x27;, &#x27;&#x27;.jpg&#x27;&#x27;, &#x27;&#x27;.jpeg&#x27;&#x27;)):
image\_path = os.path.join(output\_path, image\_file)
encoded\_image = encode\_image(image\_path)
image\_elements.append(encoded\_image)
# image
print(&quot;&quot;The length of image elements are :&quot;&quot;,len(image\_elements))
```
Initializes lists to store different types of elements extracted from the PDF: text, tables, and images.Defines a function to encode images in base64 format, which allows them to be easily embedded in prompts or stored.Loops through the extracted elements, categorizing them into text and tables. The text of each element is stored in the corresponding list.Loops through the extracted images in the output directory, encodes them in base64, and stores them in the image\_elements list. Prints out the number of elements in each category (text, tables, images) that have been extracted and processed.
```
chain\_gpt= ChatOpenAI(model=&quot;&quot;gpt-4o-mini&quot;&quot;, max\_tokens=1024)
```
Initializes instance of**ChatOpenAI**model, which are used to generate summaries for text, tables, and images.
```
# Function for text summaries
def summarize\_text(text\_element):
prompt = f&quot;&quot;Summarize the following text:\\n\\n{text\_element}\\n\\nSummary:&quot;&quot;
response = chain\_gpt.invoke([HumanMessage(content=prompt)])
return response.content
# Function for table summaries
def summarize\_table(table\_element):
prompt = f&quot;&quot;Summarize the following table:\\n\\n{table\_element}\\n\\nSummary:&quot;&quot;
response = chain\_gpt.invoke([HumanMessage(content=prompt)])
return response.content
# Function for image summaries
def summarize\_image(encoded\_image):
prompt = [
AIMessage(content=&quot;You are a bot that is good at analyzing images.&quot;),
HumanMessage(content=[
{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe the contents of this image.&quot;},
{
&quot;&quot;type&quot;&quot;: &quot;&quot;image\_url&quot;&quot;,
&quot;&quot;image\_url&quot;&quot;: {
&quot;&quot;url&quot;&quot;: f&quot;&quot;data:image/jpeg;base64,{encoded\_image}&quot;&quot;
},
},
])
]
response = chain\_gpt.invoke(prompt)
return response.content
```
Defines functions to generate summaries for text, tables, and images using the initialized**ChatOpenAI**models. The image summary function includes a prompt for describing the contents of an image.
```
# Initialize the vector store and storage layer
vectorstore = Chroma(collection\_name=&quot;&quot;summaries&quot;&quot;, embedding\_function=OpenAIEmbeddings())
store = InMemoryStore()
id\_key = &quot;&quot;doc\_id&quot;&quot;
# Initialize the retriever
retriever = MultiVectorRetriever(vectorstore=vectorstore, docstore=store, id\_key=id\_key)
```
Initializes a vector store (**Chroma**) to store embeddings and an in-memory store to keep the original documents. The**id\_key**is used to uniquely identify documents.Initializes a**MultiVectorRetriever**, which will be used to retrieve relevant documents based on queries.
![]()
```
# Function to add documents to the retriever
def add\_documents\_to\_retriever(summaries, original\_contents):
doc\_ids = [str(uuid.uuid4()) for \_ in summaries]
summary\_docs = [
Document(page\_content=s, metadata={id\_key: doc\_ids[i]})
for i, s in enumerate(summaries)
]
retriever.vectorstore.add\_documents(summary\_docs)
retriever.docstore.mset(list(zip(doc\_ids, original\_contents)))
# Add text summaries
add\_documents\_to\_retriever(text\_summaries, text\_elements)
# Add table summaries
add\_documents\_to\_retriever(table\_summaries, table\_elements)
# Add image summaries
add\_documents\_to\_retriever(image\_summaries, image\_elements) # hopefully real images soon
```
Let us test how the LLM answers if we had given a question where context involves all the information from texts, images and tables.
```
template = &quot;&quot;&quot;Answer the question based only on the following context, which can include text, images and tables:
{context}
Question: {question}
&quot;&quot;&quot;
prompt = ChatPromptTemplate.from\_template(template)
model = ChatOpenAI(temperature=0, model=&quot;gpt-4o-mini&quot;)
chain = (
{&quot;context&quot;: retriever, &quot;question&quot;: RunnablePassthrough()}
| prompt
| model
| StrOutputParser()
)
print(chain.invoke(
&quot;What do you see on the images in the database? display A graph with unique noun phrases and frequency &quot;))
```
Multimodal RAG with GPT-4-Vision and LangChain represents a powerful combination for building advanced AI applications. By leveraging the multimodal capabilities of GPT-4-Vision and the flexible tooling provided by LangChain, developers can create systems that process and generate both text and visual content, leading to more sophisticated and contextually aware AI solutions. Here are the key things we have covered in this.
* We used[Unstructured](https://unstructured.io/)to parse images, text, and tables from documents (PDFs).
* We used the[multi-vector retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector)with[Chroma](https://www.trychroma.com/)to store raw text and images along with their summaries for retrieval.
* We used GPT-4V (GPT-4o-mini) for both image summarization (for retrieval) as well as final answer synthesis from join review of images and texts (or tables).
* Retrieval is performed based upon similarity to image summaries as well as text chunks.## **Reference:**
* Langchain-Cookbook
* [https://blog.langchain.dev/semi-structured-multi-modal-rag/](https://blog.langchain.dev/semi-structured-multi-modal-rag/)
[
Large Language Models
](https://medium.com/tag/large-language-models?source=post_page-----60a6a13a92e4---------------------------------------)
[
ChatGPT
](https://medium.com/tag/chatgpt?source=post_page-----60a6a13a92e4---------------------------------------)
[
Gpt 4
](https://medium.com/tag/gpt-4?source=post_page-----60a6a13a92e4---------------------------------------)
[
OpenAI
](https://medium.com/tag/openai?source=post_page-----60a6a13a92e4---------------------------------------)
[
Langchain
](https://medium.com/tag/langchain?source=post_page-----60a6a13a92e4---------------------------------------)
[
![Shravan Kumar](https://miro.medium.com/v2/resize:fill:96:96/1*bUTiYqYPpDticWBePxbPcA.jpeg)
](https://medium.com/@shravankoninti?source=post_page---post_author_info--60a6a13a92e4---------------------------------------)
[
![Shravan Kumar](https://miro.medium.com/v2/resize:fill:128:128/1*bUTiYqYPpDticWBePxbPcA.jpeg)
](https://medium.com/@shravankoninti?source=post_page---post_author_info--60a6a13a92e4---------------------------------------)
[## Written byShravan Kumar
](https://medium.com/@shravankoninti?source=post_page---post_author_info--60a6a13a92e4---------------------------------------)
[1.8K followers](https://medium.com/@shravankoninti/followers?source=post_page---post_author_info--60a6a13a92e4---------------------------------------)
·[32 following](https://medium.com/@shravankoninti/following?source=post_page---post_author_info--60a6a13a92e4---------------------------------------)
AI Leader | AI Speaker | Associate Director - DSAI @ Novartis | Alumnus, IIT Madras &amp; IIM Bangalore Follow me for more on AI, Data Science
## Responses (1)
[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--60a6a13a92e4---------------------------------------)
See all responses
[
Help
](https://help.medium.com/hc/en-us?source=post_page-----60a6a13a92e4---------------------------------------)
[
Status
](https://status.medium.com/?source=post_page-----60a6a13a92e4---------------------------------------)
[
About
](https://medium.com/about?autoplay=1&amp;source=post_page-----60a6a13a92e4---------------------------------------)
[
Careers
](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----60a6a13a92e4---------------------------------------)
[
Press
](mailto:pressinquiries@medium.com)
[
Blog
](https://blog.medium.com/?source=post_page-----60a6a13a92e4---------------------------------------)
[
Privacy
](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----60a6a13a92e4---------------------------------------)
[
Rules
](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----60a6a13a92e4---------------------------------------)
[
Terms
](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----60a6a13a92e4---------------------------------------)
[
Text to speech
](https://speechify.com/medium?source=post_page-----60a6a13a92e4---------------------------------------)
