# How to Integrate LangChain with MCP Servers Using langchain-mcp-adapters

**URL:** https://apidog.com/blog/langchain-mcp-adapters/
**Published:** 2025-04-14T05:07:49.000Z

---

## Summary

The webpage details how to integrate tools exposed via the **Model Context Protocol (MCP)** servers into **LangChain** applications using the `langchain-mcp-adapters` library.

This library acts as a bridge, converting MCP tools into LangChain-compatible `BaseTool` objects. Key components include:

*   **MCP Server:** Exposes functions (tools) using decorators (`@()`) and can run via `stdio` (subprocess) or `sse` (web service).
*   **`langchain-mcp-adapters`:** Provides the `MultiServerMCPClient` to connect to one or more MCP servers simultaneously, handling transport details (`stdio` or `sse`).
*   **Tool Conversion:** Functions like `load_mcp_tools` fetch tool definitions and wrap them into LangChain `StructuredTool` objects.
*   **Agent Integration:** These converted tools can then be passed directly to **LangGraph** agents (e.g., via `create_react_agent`).

The article provides step-by-step examples for setting up a single math server (`stdio`) and connecting to multiple servers (math via `stdio` and a weather server via `sse`) using the `MultiServerMCPClient`. It also shows how to integrate this setup into a persistent **LangGraph API Server** using an async context manager for client lifecycle management.

---

## Full Content

How to Integrate LangChain with MCP Servers Using langchain-mcp-adapters
[
Blog
](https://apidog.com/blog/)
# How to Integrate LangChain with MCP Servers Using langchain-mcp-adapters
This library provides a lightweight wrapper, enabling developers to leverage the growing ecosystem of MCP tools within their LangChain applications. Let&#x27;s learn how to use it.
![Mark Ponomarev](https://assets.apidog.com/blog-next/2025/06/oLqctXyKtOqakfT1.png)
Mark Ponomarev
29 October 2025
![How to Integrate LangChain with MCP Servers Using langchain-mcp-adapters](https://assets.apidog.com/blog-next/2025/04/01-7.webp)
The Model Context Protocol (MCP) aims to standardize how AI models interact with external tools and services. It defines a common interface, allowing different models and tool providers to communicate effectively. However, integrating these MCP-compliant tools directly into existing AI frameworks like LangChain requires adaptation.
This is where the`langchain-mcp-adapters`library comes in. It acts as a crucial bridge, seamlessly translating MCP tools into a format that LangChain and its powerful agent framework, LangGraph, can understand and utilize. This library provides a lightweight wrapper, enabling developers to leverage the growing ecosystem of MCP tools within their LangChain applications.
Key features include:
* **MCP Tool Conversion:**Automatically converts MCP tools into LangChain-compatible`BaseTool`objects.
* **Multi-Server Client:**Provides a robust client (`MultiServerMCPClient`) capable of connecting to multiple MCP servers simultaneously, aggregating tools from various sources.
* **Transport Flexibility:**Supports common MCP communication transports like standard input/output (`stdio`) and Server-Sent Events (`sse`).
This tutorial will guide you through setting up MCP servers, connecting to them using the adapter library, and integrating the loaded tools into a LangGraph agent.
ðŸ’¡Want a great API Testing tool that generates[beautiful API Documentation](https://apidog.com/api-doc/)?
Want an integrated, All-in-One platform for your Developer Team to work together with[maximum productivity](https://apidog.com/api-testing/)?
Apidog delivers all your demands, and[replaces Postman at a much more affordable price](https://apidog.com/compare/apidog-vs-postman/)!
button[![](https://assets.apidog.com/blog-next/2025/03/image-339.png)](https://apidog.com/?utm_campaign=blog)### What is MCP Server? How Does It Work?
Understanding a few core concepts is essential before diving into the examples:
**MCP Server:**
* An MCP server exposes tools (functions) that an AI model can call.
* The`mcp`library (a dependency of`langchain-mcp-adapters`) provides tools like`FastMCP`to easily create these servers in Python.
* Tools are defined using the`@mcp.tool()`decorator, which automatically infers the input schema from type hints and docstrings.
* Servers can also define prompts using`@mcp.prompt()`, providing structured conversational starters or instructions.
* Servers are run specifying a transport mechanism (e.g.,`mcp.run(transport="stdio")`or`mcp.run(transport="sse")`).`stdio`runs the server as a subprocess communicating via standard input/output, while`sse`typically runs a simple web server for communication.
**MCP Client (`langchain-mcp-adapters`):**
* The client's role is to connect to one or more MCP servers.
* It handles the communication protocol details (stdio, sse).
* It fetches the list of available tools and their definitions (name, description, input schema) from the server(s).
* The`MultiServerMCPClient`class is the primary way to manage connections, especially when dealing with multiple tool servers.
**Tool Conversion:**
* MCP tools have their own definition format. LangChain uses its`BaseTool`class structure.
* The`langchain-mcp-adapters`library provides functions like`load\_mcp\_tools`(found in`langchain\_mcp\_adapters.tools`) which connect to a server via an active`ClientSession`, list the MCP tools, and wrap each one into a LangChain`StructuredTool`.
* This wrapper handles invoking the actual MCP tool call (`session.call\_tool`) when the LangChain agent decides to use the tool and correctly formats the response.
**Prompt Conversion:**
* Similar to tools, MCP prompts can be fetched using`load\_mcp\_prompt`(from`langchain\_mcp\_adapters.prompts`).
* This function retrieves the prompt structure from the MCP server and converts it into a list of LangChain`HumanMessage`or`AIMessage`objects, suitable for initializing or guiding a conversation.### Install Langchain-mcp-adapter
First, install the necessary packages:
```
`pip install langchain-mcp-adapters langgraph langchain-openai # Or your preferred LangChain LLM integration`
```
You'll also need to configure API keys for your chosen language model provider, typically by setting environment variables:
```
`export OPENAI\_API\_KEY=&lt;&lt;your\_openai\_api\_key&gt;&gt;
# or export ANTHROPIC\_API\_KEY=&lt;&lt;...&gt;&gt; etc.`
```
### Build a Quick Single MCP Server with langchain-mcp-adapters
Let's build a simple example: an MCP server providing math functions and a LangGraph agent using those functions.
**Step 1: Create the MCP Server (`math\_server.py`)**
```
`# math\_server.py
from mcp.server.fastmcp import FastMCP
# Initialize the MCP server with a name
mcp = FastMCP("Math")
@mcp.tool()
def add(a: int, b: int) -&gt;&gt; int:
"""Add two numbers"""
print(f"Executing add({a}, {b})") # Server-side log
return a + b
@mcp.tool()
def multiply(a: int, b: int) -&gt;&gt; int:
"""Multiply two numbers"""
print(f"Executing multiply({a}, {b})") # Server-side log
return a \* b
# Example prompt definition
@mcp.prompt()
def configure\_assistant(skills: str) -&gt;&gt; list[dict]:
"""Configures the assistant with specified skills."""
return [
{
"role": "assistant", # Corresponds to AIMessage
"content": f"You are a helpful assistant. You have the following skills: {skills}. Always use only one tool at a time.",
}
]
if \_\_name\_\_ == "\_\_main\_\_":
# Run the server using stdio transport
print("Starting Math MCP server via stdio...")
mcp.run(transport="stdio")`
```
*Save this code as`math\_server.py`.*
**Step 2: Create the Client and Agent (`client\_app.py`)**
```
`import asyncio
import os
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio\_client
from langchain\_mcp\_adapters.tools import load\_mcp\_tools
from langgraph.prebuilt import create\_react\_agent
from langchain\_openai import ChatOpenAI
# --- IMPORTANT: Update this path ---
# Get the absolute path to the math\_server.py file
current\_dir = os.path.dirname(os.path.abspath(\_\_file\_\_))
math\_server\_script\_path = os.path.join(current\_dir, "math\_server.py")
# ---
async def main():
model = ChatOpenAI(model="gpt-4o") # Or your preferred model
# Configure parameters to run the math\_server.py script
server\_params = StdioServerParameters(
command="python", # The command to execute
args=[math\_server\_script\_path], # Arguments (the script path)
# cwd=..., env=... # Optional working dir and environment vars
)
print("Connecting to MCP server...")
# Establish connection using stdio\_client context manager
async with stdio\_client(server\_params) as (read, write):
# Create a ClientSession using the read/write streams
async with ClientSession(read, write) as session:
print("Initializing session...")
# Handshake with the server
await session.initialize()
print("Session initialized.")
print("Loading MCP tools...")
# Fetch MCP tools and convert them to LangChain tools
tools = await load\_mcp\_tools(session)
print(f"Loaded tools: {[tool.name for tool in tools]}")
# Create a LangGraph ReAct agent using the model and loaded tools
agent = create\_react\_agent(model, tools)
print("Invoking agent...")
# Run the agent
inputs = {"messages": [("human", "what's (3 + 5) \* 12?")]}
async for event in agent.astream\_events(inputs, version="v1"):
print(event) # Stream events for observability
# Or get final response directly
# final\_response = await agent.ainvoke(inputs)
# print("Agent response:", final\_response['messages'][-1].content)
if \_\_name\_\_ == "\_\_main\_\_":
asyncio.run(main())`
```
*Save this as`client\_app.py`in the same directory as`math\_server.py`.*
**To Run:**
Execute the client script:
```
`python client\_app.py`
```
The client script will automatically start`math\_server.py`as a subprocess, connect to it, load the`add`and`multiply`tools, and use the LangGraph agent to solve the math problem by calling those tools via the MCP server. You'll see logs from both the client and the server.
### Connecting to Multiple MCP Servers
Often, you'll want to combine tools from different specialized servers.`MultiServerMCPClient`makes this straightforward.
**Step 1: Create Another Server (`weather\_server.py`)**
Let's create a weather server that runs using SSE transport.
```
`# weather\_server.py
from mcp.server.fastmcp import FastMCP
import uvicorn # Needs: pip install uvicorn
mcp = FastMCP("Weather")
@mcp.tool()
async def get\_weather(location: str) -&gt;&gt; str:
"""Get weather for location."""
print(f"Executing get\_weather({location})")
# In a real scenario, this would call a weather API
return f"It's always sunny in {location}"
if \_\_name\_\_ == "\_\_main\_\_":
# Run the server using SSE transport (requires an ASGI server like uvicorn)
# The mcp library implicitly creates a FastAPI app for SSE.
# By default, it runs on port 8000 at the /sse endpoint.
print("Starting Weather MCP server via SSE on port 8000...")
# uvicorn.run(mcp.app, host="0.0.0.0", port=8000) # You can run manually
mcp.run(transport="sse", host="0.0.0.0", port=8000) # Or use mcp.run convenience`
```
*Save this as`weather\_server.py`.*
**Step 2: Update Client to Use`MultiServerMCPClient`(`multi\_client\_app.py`)**
```
`import asyncio
import os
from langchain\_mcp\_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create\_react\_agent
from langchain\_openai import ChatOpenAI
# --- IMPORTANT: Update paths ---
current\_dir = os.path.dirname(os.path.abspath(\_\_file\_\_))
math\_server\_script\_path = os.path.join(current\_dir, "math\_server.py")
# Weather server runs separately, connect via URL
# ---
async def main():
model = ChatOpenAI(model="gpt-4o")
# Define connections for multiple servers
server\_connections = {
"math\_service": { # Unique name for this connection
"transport": "stdio",
"command": "python",
"args": [math\_server\_script\_path],
# Add other StdioConnection params if needed (env, cwd, etc.)
},
"weather\_service": { # Unique name for this connection
"transport": "sse",
"url": "http://localhost:8000/sse", # URL where weather\_server is running
# Add other SSEConnection params if needed (headers, timeout, etc.)
}
}
print("Connecting to multiple MCP servers...")
# Use MultiServerMCPClient context manager
async with MultiServerMCPClient(server\_connections) as client:
print("Connections established.")
# Get \*all\* tools from \*all\* connected servers
all\_tools = client.get\_tools()
print(f"Loaded tools: {[tool.name for tool in all\_tools]}")
# Create agent with the combined tool list
agent = create\_react\_agent(model, all\_tools)
# --- Interact with the agent ---
print("\\nInvoking agent for math query...")
math\_inputs = {"messages": [("human", "what's (3 + 5) \* 12?")]}
math\_response = await agent.ainvoke(math\_inputs)
print("Math Response:", math\_response['messages'][-1].content)
print("\\nInvoking agent for weather query...")
weather\_inputs = {"messages": [("human", "what is the weather in nyc?")]}
weather\_response = await agent.ainvoke(weather\_inputs)
print("Weather Response:", weather\_response['messages'][-1].content)
# --- Example: Getting a prompt ---
# print("\\nGetting math server prompt...")
# prompt\_messages = await client.get\_prompt(
# server\_name="math\_service", # Use the name defined in connections
# prompt\_name="configure\_assistant",
# arguments={"skills": "basic arithmetic"}
# )
# print("Prompt:", prompt\_messages)
if \_\_name\_\_ == "\_\_main\_\_":
# Start the weather server first in a separate terminal:
# python weather\_server.py
# Then run this client script:
asyncio.run(main())`
```
*Save this as`multi\_client\_app.py`.*
**To Run:**
1. Start the weather server in one terminal:`python weather\_server.py`
2. Run the multi-client app in another terminal:`python multi\_client\_app.py`
The`MultiServerMCPClient`will start the`math\_server.py`subprocess (stdio) and connect to the running`weather\_server.py`(sse). It aggregates tools (`add`,`multiply`,`get\_weather`) which are then available to the LangGraph agent.
### Integration with LangGraph API Server
You can deploy a LangGraph agent using MCP tools as a persistent API service using`langgraph deploy`. The key is to manage the`MultiServerMCPClient`lifecycle correctly within the LangGraph application context.
Create a`graph.py`file:
```
`# graph.py
from contextlib import asynccontextmanager
import os
from langchain\_mcp\_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create\_react\_agent
from langchain\_openai import ChatOpenAI # Or Anthropic, etc.
# --- IMPORTANT: Update paths ---
# Assuming servers are relative to where the LangGraph server runs
math\_server\_script\_path = os.path.abspath("math\_server.py")
# ---
# Define connections (ensure paths/URLs are correct for the server environment)
server\_connections = {
"math\_service": {
"transport": "stdio",
"command": "python",
"args": [math\_server\_script\_path],
},
"weather\_service": {
"transport": "sse",
"url": "http://localhost:8000/sse", # Weather server must be running independently
}
}
model = ChatOpenAI(model="gpt-4o")
# Use an async context manager to handle client setup/teardown
@asynccontextmanager
async def lifespan(\_app): # LangGraph expects this structure for lifespan management
async with MultiServerMCPClient(server\_connections) as client:
print("MCP Client initialized within lifespan.")
# Create the agent \*inside\* the context where the client is active
agent = create\_react\_agent(model, client.get\_tools())
yield {"agent": agent} # Make the agent available
# No need for a separate main graph definition if lifespan yields it`
```
Configure your`langgraph.json`(or`pyproject.toml`under`[tool.langgraph]`) to use this graph definition with the lifespan manager:
```
`// langgraph.json (example)
{
"dependencies": ["."], // Or specify required packages
"graphs": {
"my\_mcp\_agent": {
"entrypoint": "graph:agent", // Refers to the key yielded by lifespan
"lifespan": "graph:lifespan"
}
}
}`
```
Now, when you run`langgraph up`, the`lifespan`function will execute, starting the`MultiServerMCPClient`(and the stdio math server). The agent created within this context will be served by LangGraph. Remember the SSE weather server still needs to be run separately.
### Server Transports (stdio vs. SSE)
**`stdio`**:
* Communication: Via the server process's standard input and output streams.
* Pros: Simple setup for local development; the client manages the server lifecycle. No networking involved.
* Cons: Tightly coupled; less suitable for distributed systems or non-Python servers. Requires`command`and`args`configuration.
**`sse`(Server-Sent Events):**
* Communication: Over HTTP using the SSE protocol. The server runs as a web service (often using FastAPI/Uvicorn implicitly).
* Pros: Standard web protocol; suitable for networked/remote servers, potentially implemented in different languages. Server runs independently.
* Cons: Requires the server to be running separately. Needs`url`configuration.
Choose the transport based on your deployment needs.
### Advanced Client Configuration for the langchain-mcp-adapters Setup
The`StdioConnection`and`SSEConnection`dictionaries within`MultiServerMCPClient`accept additional optional parameters for finer control:
* **Stdio:**`env`(custom environment variables for the subprocess),`cwd`(working directory),`encoding`,`encoding\_error\_handler`,`session\_kwargs`(passed to`mcp.ClientSession`).
* **SSE:**`headers`(custom HTTP headers),`timeout`(HTTP connection timeout),`sse\_read\_timeout`,`session\_kwargs`.
Refer to the`MultiServerMCPClient`definition in`langchain\_mcp\_adapters/client.py`for details.
### Conclusion (100 words)
The`langchain-mcp-adapters`library effectively bridges the gap between the standardized Model Context Protocol and the flexible LangChain ecosystem. By providing the`MultiServerMCPClient`and automatic tool conversion, it allows developers to easily incorporate diverse, MCP-compliant tools into their LangChain agents and LangGraph applications.
The core workflow involves:
1. Defining tools (and optionally prompts) in an MCP server using`@mcp.tool()`.
2. Configuring the`MultiServerMCPClient`with connection details (stdio or sse) for each server.
3. Using the client context manager (`async with ...`) to connect and fetch tools via`client.get\_tools()`.
4. Passing the retrieved LangChain-compatible tools to your agent (`create\_react\_agent`or custom agents).
This enables building powerful, modular AI applications that leverage specialized, external tools through a standardized protocol. Explore the examples and tests within the repository for further insights.
In this article
[What is MCP Server? How Does It Work?](#what-is-mcp-server-how-does-it-work)[Install Langchain-mcp-adapter](#install-langchain-mcp-adapter)[Build a Quick Single MCP Server with langchain-mcp-adapters](#build-a-quick-single-mcp-server-with-langchain-mcp-adapters)[Connecting to Multiple MCP Servers](#connecting-to-multiple-mcp-servers)[Integration with LangGraph API Server](#integration-with-langgraph-api-server)[Server Transports (stdio vs. SSE)](#server-transports-stdio-vs-sse)[Advanced Client Configuration for the langchain-mcp-adapters Setup](#advanced-client-configuration-for-the-langchain-mcp-adapters-setup)[Conclusion (100 words)](#conclusion-100-words)
Apidog: A Real Design-first API Development Platform
API Design
API Documentation
API Debugging
Automated Testing
API Mocking
More
[Get started for free](https://app.apidog.com)[Request demo](https://zcal.co/apidog/demo/)
Practice API Design-first in Apidog
Discover an easier way to build and use APIs
[Sign up for free](https://app.apidog.com/user/login)Download
