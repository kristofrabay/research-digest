# GitHub - zhengxuJosh/Awesome-RAG-Vision: Awesome-RAG-Vision: a curated list of advanced retrieval augmented generation (RAG) for Computer Vision

**URL:** https://github.com/zhengxuJosh/Awesome-RAG-Vision
**Published:** 2024-11-21T07:35:07.000Z

---

## Summary

The webpage is a curated list of state-of-the-art papers on **Retrieval-Augmented Generation (RAG) in Computer Vision**.

It covers various applications of RAG in vision tasks, including:
*   **Visual Understanding** (Image Understanding, Long Video Understanding, Visual Spatial Understanding)
*   **Multi-modal** RAG applications.
*   **Visual Generation** (Image/Video Generation, 3D Generation).
*   **Embodied AI**.

The page also lists resources, including tutorials on **Multimodal RAG** for images and video, and a section dedicated to **RAG for Document** processing, which mentions:
*   Multimodal RAG for PDFs with Text, Images, and Charts.
*   Multimodal RAG with Document Retrieval and Vision Language Models (VLMs).
*   Multi-Vector Retriever for RAG on tables, text, and images.

While the page extensively covers **Multimodal RAG**, **Vision-Language Models (VLMs)**, and **Document Understanding** (including PDF/chart/table aspects via the "RAG for Document" resources), it **does not explicitly list or detail** specific models like **GPT-4V, Claude vision, or Gemini**, nor does it focus on **report generation with LLMs** or **structured document output** as primary topics, although these concepts are implied within the broader RAG and document understanding context.

**Summary relative to the query:**

The page is highly relevant to **multimodal RAG**, **vision-language models**, **document understanding**, **PDF parsing**, and **chart/table extraction** as it curates resources and papers in these areas, particularly under the "Multi-modal" and "RAG for Document" sections. However, it does not provide specific information on **report generation with LLMs**, **GPT-4V, Claude vision, Gemini**, or **structured document output** beyond the general scope of multimodal document RAG.

---

## Full Content

[Skip to content](https://github.com/github.com#start-of-content)

You signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert

[zhengxuJosh](https://github.com/zhengxuJosh)/ **[Awesome-RAG-Vision](https://github.com/zhengxuJosh/Awesome-RAG-Vision)** Public

- [Notifications](https://github.com/login?return_to=%2FzhengxuJosh%2FAwesome-RAG-Vision) You must be signed in to change notification settings
- [Fork\
7](https://github.com/login?return_to=%2FzhengxuJosh%2FAwesome-RAG-Vision)
- [Star\
245](https://github.com/login?return_to=%2FzhengxuJosh%2FAwesome-RAG-Vision)


Awesome-RAG-Vision: a curated list of advanced retrieval augmented generation (RAG) for Computer Vision

[245\
stars](https://github.com/zhengxuJosh/Awesome-RAG-Vision/stargazers) [7\
forks](https://github.com/zhengxuJosh/Awesome-RAG-Vision/forks) [Branches](https://github.com/zhengxuJosh/Awesome-RAG-Vision/branches) [Tags](https://github.com/zhengxuJosh/Awesome-RAG-Vision/tags) [Activity](https://github.com/zhengxuJosh/Awesome-RAG-Vision/activity)

[Star](https://github.com/login?return_to=%2FzhengxuJosh%2FAwesome-RAG-Vision)

[Notifications](https://github.com/login?return_to=%2FzhengxuJosh%2FAwesome-RAG-Vision) You must be signed in to change notification settings

# zhengxuJosh/Awesome-RAG-Vision

main

[Branches](https://github.com/zhengxuJosh/Awesome-RAG-Vision/branches) [Tags](https://github.com/zhengxuJosh/Awesome-RAG-Vision/tags)

Go to file

Code

Open more actions menu

## Folders and files

| Name | Name | Last commit message | Last commit date |
| --- | --- | --- | --- |
| ## Latest commit ## History [62 Commits](https://github.com/zhengxuJosh/Awesome-RAG-Vision/commits/main/) |
| [README.md](https://github.com/zhengxuJosh/Awesome-RAG-Vision/blob/main/README.md) | [README.md](https://github.com/zhengxuJosh/Awesome-RAG-Vision/blob/main/README.md) |
| View all files |

## Repository files navigation

# Awesome RAG in Computer Vision

This repository aims to collect and organize **state-of-the-art papers on Retrieval-Augmented Generation (RAG) in Computer Vision**. RAG has gained significant traction in vision tasks like image understanding, video comprehension, visual generation, and more. By incorporating external retrieval, these approaches can enrich models with additional context, leading to better performance and interpretability.

We encourage researchers who want to showcase their work on **RAG for Vision** to open a Pull Request and add their paper!

## Table of Contents

- [Introduction](https://github.com/github.com#introduction)
- [Resources](https://github.com/github.com#resources)
 - [Workshops and Tutorials](https://github.com/github.com#workshops-and-tutorials)
- [Papers](https://github.com/github.com#papers)
 - [Survey and Benchmark](https://github.com/github.com#survey-and-benchmark)
 - [RAG for Vision](https://github.com/github.com#rag-for-vision)
 - [Visual Understanding](https://github.com/github.com#1-visual-understanding)
 - [Image Understanding](https://github.com/github.com#11-image-understanding)
 - [Long Video Understanding](https://github.com/github.com#12-long-video-understanding)
 - [Visual Spatial Understanding](https://github.com/github.com#13-visual-spacial-understanding)
 - [Multi-modal](https://github.com/github.com#14-multi-modal)
 - [Medical Vision](https://github.com/github.com#15-medical-vision)
 - [Visual Generation](https://github.com/github.com#2-visual-generation)
 - [Image (Video) Generation](https://github.com/github.com#21-image-video-generation)
 - [3D Generation](https://github.com/github.com#22-3d-generation)
 - [Embodied AI](https://github.com/github.com#3-embodied-ai)

## Introduction

Retrieval-Augmented Generation (RAG) integrates retrieval modules into generative models, allowing them to query external knowledge bases (or memory banks) during inference. In **Computer Vision**, RAG has powered:

- Image captioning and object detection with external knowledge.
- Video QA/comprehension by retrieving context from long transcripts or external references.
- Visual generation with retrieval of reference images, design templates, or domain-specific data.

## Resources

### Workshops and Tutorials

#### **RAG for Image**

- [Multimodal RAG using Langchain Expression Language And GPT4-Vision](https://medium.aiplanet.com/multimodal-rag-using-langchain-expression-language-and-gpt4-vision-8a94c8b02d21)
- [A Comprehensive Guide to Building Multimodal RAG Systems](https://www.analyticsvidhya.com/blog/2024/09/guide-to-building-multimodal-rag-systems/)
- [Guide to Multimodal RAG for Images and Text (in 2025)](https://medium.com/kx-systems/guide-to-multimodal-rag-for-images-and-text-10dab36e3117)
- [Building an Image Search RAG App with Llama 3.2 Vision](https://blog.stackademic.com/building-an-image-search-rag-app-with-llama-3-2-vision-a-step-by-step-implementation-guide-d2f79c1c4c15)
- [Improve Your Stable Diffusion Prompts with Retrieval-Augmented Generation](https://aws.amazon.com/cn/blogs/machine-learning/improve-your-stable-diffusion-prompts-with-retrieval-augmented-generation/)

#### **RAG for video**

- [Building Multimodal RAG Application for Video Preprocessing](https://pub.towardsai.net/building-multimodal-rag-application-4-video-preprocessing-multimodal-rag-abf086f81221)
- [Multimodal RAG Chat with Videos and the Future of AI Interaction](https://ai.plainenglish.io/multimodal-rag-chat-with-videos-and-the-future-of-ai-interaction-e427b755689c)
- [Multimodal RAG for Advanced Video Processing with LlamaIndex and LanceDB](https://www.llamaindex.ai/blog/multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e)
- [Multimodal RAG for processing videos using OpenAI GPT4V and LanceDB vectorstore](https://docs.llamaindex.ai/en/stable/examples/multi_modal/multi_modal_video_RAG/)
- [RAG (Q/A) of Videos with LLM](https://www.kaggle.com/code/gabrielvinicius/rag-q-a-of-videos-with-llm)
- [An Easy Introduction to Multimodal Retrieval-Augmented Generation for Video and Audio](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation-for-video-and-audio/)

#### **RAG for Document**

- [Multimodal RAG for PDFs with Text, Images, and Charts](https://pathway.com/developers/templates/multimodal-rag/)
- [Multimodal Retrieval-Augmented Generation (RAG) with Document Retrieval (ColPali) and Vision Language Models (VLMs)](https://huggingface.co/learn/cookbook/multimodal_rag_using_document_retrieval_and_vlms)
- [Multi-Vector Retriever for RAG on tables, text, and images](https://blog.langchain.dev/semi-structured-multi-modal-rag/)
- [Build an AI-powered multimodal RAG system with Docling and Granite](https://www.ibm.com/think/tutorials/build-multimodal-rag-langchain-with-docling-granite)

#### **Other Related Resources**

- [Multimodal Retrieval Augmented Generation(RAG)](https://weaviate.io/blog/multimodal-rag)
- [Tutorial \| Build a multimodal knowledge bank for a RAG project](https://knowledge.dataiku.com/latest/gen-ai/rag/tutorial-multimodal-embedding.html)

## Papers

### Survey and Benchmark

| Year | Paper | Focused Areas | Main Context | GitHub |
| --- | --- | --- | --- | --- |
| 2023 | Gao _et al._ | LLMs / NLP | RAG paradigms and components | - |
| 2024 | Fan _et al._ | LLMs / NLP | RA-LLMs' architectures, training, and applications | [link](https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/) |
| 2024 | Hu _et al._ | LLMs / NLP | RA-LMs' components, evaluation, and limitations | [link](https://github.com/2471023025/RALM_Survey) |
| 2024 | Zhao _et al._ | LLMs / NLP | Challenges in data-augmented LLMs | - |
| 2024 | Gupta _et al._ | LLMs / NLP | Advancements and downstream tasks of RAG | - |
| 2024 | Zhao _et al._ | RAG in AIGC | RAG applications across modalities | [link](https://github.com/PKU-DAIR/RAG-Survey) |
| 2024 | Yu _et al._ | LLMs / NLP | Unified evaluation process of RAG | [link](https://github.com/YHPeter/Awesome-RAG-Evaluation) |
| 2024 | Procko _et al._ | Graph Learning | Knowledge graphs with LLM RAG | - |
| 2024 | Zhou _et al._ | Trustworthiness AI | Six dimensions and benchmarks about Trustworthy RAG | [link](https://github.com/smallporridge/TrustworthyRAG) |
| 2025 | Singh _et al._ | AI Agent | Participles and evaluation | [link](https://github.com/asinghcsu/AgenticRAG-Survey) |
| 2025 | Ni _et al._ | Trustworthiness AI | Road-map and discussion | [link](https://github.com/Arstanley/Awesome-Trustworthy-Retrieval-Augmented-Generation) |
| 2025 | **_Ours_** | **_Computer Vision_** | **_RAG for visual understanding and generation_** | [link](https://github.com/zhengxuJosh/Awesome-RAG-Vision) |

## RAG for Vision

### 1 Visual Understanding

#### 1.1 Image Understanding

| Title | Authors | Venue/Date | Paper Link |
| --- | --- | --- | --- |
| Multimodal RAG Enhanced Visual Description | Jaiswal _et al._ | Arxiv 2025 (Aug) | [paper](https://arxiv.org/pdf/2508.09170) |
| DIR: Retrieval-Augmented Image Captioning with Comprehensive Understanding | Wu _et al._ | Arxiv 2024 (Dec) | [paper](https://arxiv.org/pdf/2412.01115) |
| Retrieval-Augmented Open-Vocabulary Object Detection | Kim _et al._ | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Retrieval-Augmented_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf) |
| Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning | Li _et al._ | Arxiv 2024 (Aug) | [paper](https://arxiv.org/pdf/2406.02265) |
| Retrieval-Augmented Classification for Long-Tail Visual Recognition | Long _et al._ | CVPR 2022 | [paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Learning_Customized_Visual_Models_With_Retrieval-Augmented_Knowledge_CVPR_2023_paper.pdf) |
| Learning Customized Visual Models with Retrieval-Augmented Knowledge | Liu _et al._ | CVPR 2023 | [paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Learning_Customized_Visual_Models_With_Retrieval-Augmented_Knowledge_CVPR_2023_paper.pdf) |

#### 1.2 (Long) Video Understanding

| Title | Authors | Venue/Date | Paper Link |
| --- | --- | --- | --- |
| Generative Frame Sampler for Long Video Understanding | Yao _et al._ | ACL 2024 | [paper](https://arxiv.org/abs/2503.09146) |
| Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding | Mao _et al._ | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2505.23990) |
| VRAG: Retrieval-Augmented Video Question Answering for Long-Form Videos | Gia _et al._ | CVPRW 2025 | [paper](https://openaccess.thecvf.com/content/CVPR2025W/IViSE/papers/Gia_VRAG_Retrieval-Augmented_Video_Question_Answering_for_Long-Form_Videos_CVPRW_2025_paper.pdf) |
| Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge | Xiong _et al._ | ICLR 2025 | [paper](https://arxiv.org/abs/2501.13468) |
| Temporal Preference Optimization for Long-Form Video Understanding | Li _et al._ | Arxiv 2025 (Jan) | [paper](https://arxiv.org/abs/2501.13919) |
| StreamingRAG: Real-time Contextual Retrieval and Generation Framework | Sankaradas _et al._ | Arxiv 2025 (Jan) | [paper](https://arxiv.org/abs/2501.14101) |
| VideoAuteur: Towards Long Narrative Video Generation | Xiao _et al._ | Arxiv 2025 (Jan) | [paper](https://arxiv.org/abs/2501.06173) |
| FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models | Fu _et al._ | Arxiv 2024 (Dec) | [paper](https://arxiv.org/abs/2501.01986) |
| Vinci: A Real-time Embodied Smart Assistant based on Egocentric Vision-Language Model | Huang _et al._ | Arxiv 2024 (Dec) | [paper](https://arxiv.org/abs/2412.21080) |
| Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models | Yi _et al._ | Arxiv 2024 (Dec) | [paper](https://arxiv.org/abs/2412.18609) |
| Goldfish: Vision-Language Understanding of Arbitrarily Long Videos | Ataallah _et al._ | Arxiv 2024 (Jul) | [paper](https://arxiv.org/abs/2407.12679) |
| Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension | Luo _et al._ | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.13093) |
| ViTA: An Efficient Video-to-Text Algorithm using VLM for RAG-based Video Analysis System | Arefeen _et al._ | CVPRW 2024 | [paper](https://aclanthology.org/2024.emnlp-main.62.pdf) |
| iRAG: Advancing RAG for Videos with an Incremental Approach | Arefeen _et al._ | CIKM 2024 | [paper](https://dl.acm.org/doi/pdf/10.1145/3627673.3680088?casa_token=CDXIXZP0y9QAAAAA:obaFKtQODdGsI3pB22GWuGH2dODwF7N0dj1dl58WfSwavmvrp_1eeaHXj6c2XCQyt-9vF1r1QrUd) |

#### 1.3 Visual Spacial Understanding

| Title | Authors | Venue/Date | Paper Link |
| --- | --- | --- | --- |
| RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector | Yu _et al._ | ACM MM 2024 | [paper](https://dl.acm.org/doi/abs/10.1145/3664647.3688990?casa_token=SlLR5jgRRkgAAAAA:DzC124tFMWQSMYkKRGkPTwU-aaT7TSv_iVjE-dsZtbna9j3zCYX1A6qcfgmpEKTms8DoZDgplc5u8g) |

#### 1.4 Multi-modal

| Title | Authors | Venue/Date | Paper Link |
| --- | --- | --- | --- |
| UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG | Peng _et al._ | Arxiv 2025 (Oct) | [paper](https://arxiv.org/pdf/2510.03663) |
| Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems | Zhang _et al._ | Arxiv 2025 (Sep) | [paper](https://arxiv.org/pdf/2509.20769) |
| Multimodal Iterative RAG for Knowledge Visual Question Answering | choi _et al._ | Arxiv 2025 (Sep) | [paper](https://arxiv.org/pdf/2509.00798) |
| VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples | sun _et al._ | Arxiv 2025 (Sep) | [paper](https://arxiv.org/pdf/2509.04502) |
| CMRAG: Co-modality-based document retrieval and visual question answering | chen _et al._ | Arxiv 2025 (Sep) | [paper](https://arxiv.org/pdf/2509.02123) |
| Beyond the Textual: Generating Coherent Visual Options for MCQs | Wang _et al._ | Arxiv 2025 (Aug) | [paper](https://arxiv.org/pdf/2508.18772) |
| mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering | Yuan _et al._ | Arxiv 2025 (Aug) | [paper](https://arxiv.org/pdf/2508.05318) |
| Cross-Modal Augmentation for Low-Resource Language Understanding and Generation | Li _et al._ | MAGMaR 2025 (Aug) | [paper](https://aclanthology.org/2025.magmar-1.9.pdf) |
| Multimodal Retrieval-Augmented Generation: Unified Information Processing Across Text, Image, Table, and Video Modalities | Drushchak _et al._ | MAGMaR 2025 (Aug) | [paper](https://aclanthology.org/2025.magmar-1.5.pdf) |
| VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation | Park _et al._ | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2506.21556) |
| CoRe-MMRAG: Cross-Source Knowledge Reconciliation |
| for Multimodal RAG | Tian _et al._ | ACL 2025 (Jun) | [paper](https://aclanthology.org/2025.acl-long.1583.pdf) |
| Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding | Mannam _et al._ | KDDW 2025 (Jun) | [paper](https://www.arxiv.org/pdf/2506.21604) |
| Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger | Yang _et al._ | Arxiv 2025 (Jun) | [paper](https://openreview.net/pdf?id=DJcEoC9JpQ) |
| FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation | Zhang _et al._ | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2506.12494) |
| MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering | Gondhalekar _et al._ | Arxiv 2025 (Jun) | [paper](https://arxiv.org/pdf/2506.20821) |
| DocReRank: Single-Page Hard Negative Query Generation for Training Multi-Modal RAG Rerankers | Wasserman _et al._ | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.22584) |
| A Multi-Granularity Retrieval Framework for Visually-Rich Documents | Xu _et al._ | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.01457) |
| Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation | Zhu _et al._ | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.21956) |
| Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models | Jia _et al._ | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.19509) |
| FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain | Zhao _et al._ | Arxiv 2025 (May) | [paper](https://arxiv.org/pdf/2505.17471) |
| RS-RAG: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented Generation Model | Wen _et al._ | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.04988) |
| MMKB-RAG: A Multi-Modal Knowledge-Based Retrieval-Augmented Generation Framework | Ling _et al._ | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.10074) |
| HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation | Liu _et al._ | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2504.12330) |
| MRAMG-Bench: A Comprehensive Benchmark for Advancing Multimodal Retrieval-Augmented Multimodal Generation | Yu _et al._ | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2502.04176) |
| AutoStyle-TTS: Retrieval-Augmented Generation based Automatic Style Matching Text-to-Speech Synthesis | Luo _et al._ | ICME 2025 | [paper](https://arxiv.org/pdf/2504.10309) |
| RS-RAG: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented Generation Model | Wen _et al._ | Arxiv 2025 (Apr) | [paper](https://arxiv.org/pdf/2503.13861) |
| RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving | Wang _et al._ | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.13861) |
| SuperRAG: Beyond RAG with Layout-Aware Graph Modeling | Yang _et al._ | NACCL 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.04790) |
| MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding | Han _et al._ | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.13964) |
| SiQA: A Large Multi-Modal Question Answering Model for Structured Images Based on RAG | Liu _et al._ | ICASSP 2025 (Mar) | [paper](https://ieeexplore.ieee.org/abstract/document/10888359) |
| CommGPT: A Graph and Retrieval-Augmented Multimodal Communication Foundation Model | Jiang _et al._ | Arxiv 2025 (Feb) | [paper](https://arxiv.org/pdf/2502.18763) |
| Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework | Yang _et al._ | Arxiv 2025 (Feb) | [paper](https://arxiv.org/pdf/2502.14864) |
| ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents | Wang _et al._ | Arxiv 2025 (Feb) | [paper](https://arxiv.org/pdf/2502.18017) |
| Retrieval-Augmented Visual Question Answering via Built-in Autoregressive Search Engines | Long _et al._ | Arxiv 2025 (Feb) | [paper](https://arxiv.org/pdf/2502.16641) |
| WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models | Chen _et al._ | Arxiv 2025 (Feb) | [paper](https://arxiv.org/pdf/2502.14727) |
| FilterRAG: Zero-Shot Informed Retrieval-Augmented Generation to Mitigate Hallucinations in VQA | S M Sarwar | Arxiv 2025 (Feb) | [paper](https://arxiv.org/pdf/2502.18536) |
| A General Retrieval-Augmented Generation Framework for Multimodal Case-Based Reasoning Applications | Ofir Marom | Arxiv 2025 (Jan) | [paper](https://arxiv.org/pdf/2501.05030) |
| Visual RAG: Expanding MLLM visual knowledge without fine-tuning | Bonomo _et al._ | Arxiv 2025 (Jan) | [paper](https://arxiv.org/pdf/2501.10834) |
| Re-ranking the Context for Multimodal Retrieval Augmented Generation | Mortaheb _et al._ | Arxiv 2025 (Jan) | [paper](https://arxiv.org/pdf/2501.04695) |
| MuKA: Multimodal Knowledge Augmented Visual Information-Seeking | Deng _et al._ | Coling 2025 (Jan) | [paper](https://aclanthology.org/2025.coling-main.647.pdf) |
| mR2AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA | Zhang _et al._ | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.15041) |
| Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs | Caffagni _et al._ | CVPRW 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024W/MMFM/papers/Caffagni_Wiki-LLaVA_Hierarchical_Retrieval-Augmented_Generation_for_Multimodal_LLMs_CVPRW_2024_paper.pdf) |
| M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding | Cho _et al._ | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2410.21943) |
| UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models | Sharifymoghaddam _et al._ | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2405.10311) |
| MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models | Hu _et al._ | ICLR 2025 (Oct) | [paper](https://arxiv.org/pdf/2410.08182) |
| VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents | Yu _et al._ | ICLR 2025 (Oct) | [paper](https://arxiv.org/pdf/2410.10594) |
| RoRA-VLM: Robust Retrieval Augmentation for Vision Language Models | Qi _et al._ | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.08876) |
| Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications | Riedler _et al._ | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.21943) |
| SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information | Sun _et al._ | EMNLP 2024 (Sep) | [paper](https://aclanthology.org/2024.emnlp-main.434.pdf) |
| ColPali: Efficient Document Retrieval with Vision Language Models | Faysse _et al._ | ICLR 2025 (Jul) | [paper](https://openreview.net/pdf?id=ogjBpZ8uSi) |
| MLLM Is a Strong Reranker | Chen _et al._ | Arxiv 2024 (Jul) | [paper](https://arxiv.org/pdf/2409.14083) |
| RAVEN: Multitask Retrieval Augmented Vision-Language Learning | Rao _et al._ | COLM 2024 (Jun) | [paper](https://openreview.net/pdf?id=GMalvQu0XL) |
| SearchLVLMs | Li _et al._ | NIPS 2024 (May) | [paper](https://papers.nips.cc/paper_files/paper/2024/file/76954b4a44e158e738b4c64494977c6a-Paper-Conference.pdf) |
| UDKAG | Li _et al._ | CoRR 2024 (May) | [paper](https://arxiv.org/abs/2405.14554v1) |
| Retrieval Meets Reasoning | Tan _et al._ | Arxiv 2024 (Apr) | [paper](https://arxiv.org/pdf/2405.20834) |
| RAR | Liu _et al._ | Arxiv 2024 (Mar) | [paper](https://arxiv.org/pdf/2403.13805) |
| MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning | Cui _et al._ | ACL 2024 (Feb) | [paper](https://aclanthology.org/2024.findings-acl.69.pdf) |
| Fine-grained Late-interaction Multi-modal Retrieval for RAG-VQA | Lin _et al._ | NIPS 2023 (Oct) | [paper](https://papers.nips.cc/paper_files/paper/2023/file/47393e8594c82ce8fd83adc672cf9872-Paper-Conference.pdf) |
| Retrieval-based Knowledge Augmented Vision Language Pre-training | Rao _et al._ | ACMMM 2023 (Apr) | [paper](https://arxiv.org/pdf/2304.13923) |
| ReVeaL | Hu _et al._ | CVPR 2023 (Apr) | [paper](https://arxiv.org/pdf/2212.05221) |
| Murag | Chen _et al._ | EMNLP 2022 (Oct) | [paper](https://aclanthology.org/2022.emnlp-main.375.pdf) |

#### 1.5 Medical Vision

| Title | Authors | Venue/Date |
| --- | --- | --- |
| How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG System | Zuo _et al._ | Arxiv 2025 (Aug) [paper](https://arxiv.org/pdf/2508.17215) |
| AlzheimerRAG: Multimodal Retrieval Augmented Generation for Clinical Use Cases using PubMed articles | Lahiri _et al._ | Arxiv 2025 (Aug) [paper](https://arxiv.org/pdf/2412.16701) |
| HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks | Chen _et al._ | Arxiv 2025 (Aug) [paper](https://arxiv.org/pdf/2508.12778) |
| REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models | Zhu _et al._ | Arxiv 2025 (Feb) [paper](https://arxiv.org/pdf/2402.07016) |
| Mmed-rag: Versatile multimodal rag system for medical vision language models | Xia _et al._ | Arxiv 2024 (Oct) [paper](https://arxiv.org/pdf/2410.13085) |
| Rule: Reliable multimodal rag for factuality in medical vision language models | Xia _et al._ | EMNLP 2024 [paper](https://aclanthology.org/2024.emnlp-main.62.pdf) |

### 2 Visual Generation

#### 2.1 Image (Video) Generation

| Title | Authors | Venue/Date | Paper Link |
| --- | --- | --- | --- |
| FairRAG: Fair Human Generation via Fair Retrieval Augmentation | Shrestha _et al._ | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Shrestha_FairRAG_Fair_Human_Generation_via_Fair_Retrieval_Augmentation_CVPR_2024_paper.pdf) |
| GarmentAligner (ECCV) | Zhang _et al._ | ECCV 2025 | [paper](https://link.springer.com/chapter/10.1007/978-3-031-72698-9_9) |
| Retrieval-Augmented Diffusion Models | Blattmann _et al._ | NIPS 2022 | [paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/62868cc2fc1eb5cdf321d05b4b88510c-Paper-Conference.pdf) |
| Label-Retrieval-Augmented Diffusion Models | Chen _et al._ | NIPS 2023 | [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/d191ba4c8923ed8fd8935b7c98658b5f-Paper-Conference.pdf) |
| CPR: Retrieval Augmented Generation for Copyright Protection | Golatkar _et al._ | CVPR 2023 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Golatkar_CPR_Retrieval_Augmented_Generation_for_Copyright_Protection_CVPR_2024_paper.pdf) |
| BrainRAM | Xie _et al._ | MM 2024 | [paper](https://dl.acm.org/doi/pdf/10.1145/3664647.3681296) |
| Animate-A-Story | He _et al._ | Arxiv 2023 | [paper](https://arxiv.org/pdf/2307.06940) |
| RealGen | Ding _et al._ | ECCV 2024 | [paper](https://arxiv.org/pdf/2312.13303) |
| Grounding Language Models for Visual Entity Recognition | Xiao _et al._ | ECCV 2024 | [paper](https://arxiv.org/pdf/2402.18695) |
| GarmentAligner (Arxiv) | Zhang _et al._ | ECCV 2024 | [paper](https://arxiv.org/pdf/2408.12352) |
| Retrieval-Augmented Layout Transformer | Horita _et al._ | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Horita_Retrieval-Augmented_Layout_Transformer_for_Content-Aware_Layout_Generation_CVPR_2024_paper.pdf) |
| The Neglected Tails in Vision-Language Models | Parashar _et al._ | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Horita_Retrieval-Augmented_Layout_Transformer_for_Content-Aware_Layout_Generation_CVPR_2024_paper.pdf) |
| Prompt Expansion for Adaptive Text-to-Image Generation | Datta _et al._ | ACL 2024 | [paper](https://arxiv.org/pdf/2312.16720) |
| Factuality Tax of Diversity-Intervened Generation | Wan _et al._ | EMNLP 2024 | [paper](https://arxiv.org/pdf/2407.00377) |
| Diffusion-Based Augmentation for Captioning and Retrieval | Cioni _et al._ | ICCV 2023 | [paper](https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/papers/Cioni_Diffusion_Based_Augmentation_for_Captioning_and_Retrieval_in_Cultural_Heritage_ICCVW_2023_paper.pdf) |
| ReMoDiffuse | Zhang _et al._ | ICCV 2023 | [paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_ReMoDiffuse_Retrieval-Augmented_Motion_Diffusion_Model_ICCV_2023_paper.pdf) |
| Re-imagen | Chen _et al._ | Arxiv 2022 | [paper](https://arxiv.org/pdf/2209.14491) |
| Instruct-Imagen | Hu _et al._ | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_Instruct-Imagen_Image_Generation_with_Multi-modal_Instruction_CVPR_2024_paper.pdf) |
| ImageRAG | Shalev-Arkushin _et al._ | Arxiv 2025 | [paper](https://arxiv.org/pdf/2502.09411) |
| FineRAG | Yuan _et al._ | COLING 2025 | [paper](https://aclanthology.org/2025.coling-main.741.pdf) |
| RealRAG | Lyu _et al._ | ICML 2025 | [paper](https://arxiv.org/pdf/2502.00848) |

#### 2.2 3D Generation

| Title | Authors | Venue/Date | Paper Link |
| --- | --- | --- | --- |
| MV-RAG: Retrieval Augmented Multiview Diffusion | Dayani _et al._ | Arxiv 2025 (Aug) | [paper](https://arxiv.org/pdf/2508.16577) |
| VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models | Xu _et al._ | Arxiv 2025 (Aug) | [paper](https://arxiv.org/pdf/2508.12081) |
| Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions | Wang _et al._ | Arxiv 2024 (Sep) | [paper](https://arxiv.org/pdf/2409.11406) |
| Retrieval-Augmented Score Distillation for Text-to-3D Generation | Seo _et al._ | ICML 2024 | [paper](https://arxiv.org/pdf/2402.02972) |
| Diorama: Unleashing Zero-shot Single-view 3D Scene Modeling | Wu _et al._ | Arxiv 2024 (Nov) | [paper](https://arxiv.org/pdf/2411.19492) |
| Interaction-based Retrieval-augmented Diffusion for Protein 3D Generation | Huang _et al._ | ICML 2024 | [paper](https://openreview.net/pdf?id=eejhD9FCP3) |
| ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model | Zhang _et al._ | ICCV 2023 | [paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_ReMoDiffuse_Retrieval-Augmented_Motion_Diffusion_Model_ICCV_2023_paper.pdf) |

### 3\. Embodied AI

| Title | Authors | Venue/Date | Paper Link |
| --- | --- | --- | --- |
| SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based Retrieval-Augmented Generation | Ye _et al._ | Arxiv 2025 (Jul) | [paper](https://arxiv.org/pdf/2507.21585) |
| RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base | Wang _et al._ | IROS 2025 July | [paper](https://arxiv.org/pdf/2506.18856) |
| RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving | Wang _et al._ | Arxiv 2025 (Mar) | [paper](https://arxiv.org/pdf/2503.13861) |
| RANa: Retrieval-Augmented Navigation | Monaci _et al._ | Arxiv 2025 (Apr) | paper |
| P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task | Xu _et al._ | ACM MM 2024 | [paper](https://dl.acm.org/doi/pdf/10.1145/3664647.3680661) |
| Realgen: Retrieval Augmented Generation for Controllable Traffic Scenarios | Ding _et al._ | ECCV 2024 | [paper](https://arxiv.org/pdf/2312.13303) |
| Retrieval-Augmented Embodied Agents | Zhu _et al._ | CVPR 2024 | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Retrieval-Augmented_Embodied_Agents_CVPR_2024_paper.pdf) |
| ENWAR: A RAG-empowered Multi-Modal LLM Framework for Wireless Environment Perception | Nazar _et al._ | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2410.18104) |
| Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation | Xie _et al._ | Arxiv 2024 (Oct) | [paper](https://arxiv.org/pdf/2409.18313) |
| RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning | Yuan _et al._ | Arxiv 2024 (May) | [paper](https://arxiv.org/abs/2402.10828) |

## About

Awesome-RAG-Vision: a curated list of advanced retrieval augmented generation (RAG) for Computer Vision

### Resources

[Readme](https://github.com/github.com#readme-ov-file)

### Uh oh!

There was an error while loading. Please reload this page.

[Activity](https://github.com/zhengxuJosh/Awesome-RAG-Vision/activity)

### Stars

[**245**\
stars](https://github.com/zhengxuJosh/Awesome-RAG-Vision/stargazers)

### Watchers

[**5**\
watching](https://github.com/zhengxuJosh/Awesome-RAG-Vision/watchers)

### Forks

[**7**\
forks](https://github.com/zhengxuJosh/Awesome-RAG-Vision/forks)

[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FzhengxuJosh%2FAwesome-RAG-Vision&report=zhengxuJosh+%28user%29)

## [Releases](https://github.com/zhengxuJosh/Awesome-RAG-Vision/releases)

No releases published

## [Packages\ 0](https://github.com/users/zhengxuJosh/packages?repo_name=Awesome-RAG-Vision)

No packages published

## [Contributors\ 5](https://github.com/zhengxuJosh/Awesome-RAG-Vision/graphs/contributors)

### Uh oh!

There was an error while loading. Please reload this page.

You canâ€™t perform that action at this time.
