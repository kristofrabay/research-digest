# Improved Long & Short-Term Memory for LlamaIndex Agents

**URL:** https://www.llamaindex.ai/blog/improved-long-and-short-term-memory-for-llamaindex-agents
**Published:** 2025-05-13T00:00:00.000Z

---

## Summary

The provided web page focuses on **Improved Long & Short-Term Memory for LlamaIndex Agents**.

It details the introduction of a new `Memory` component in LlamaIndex, which is crucial for agentic applications that need to retain information about past conversations or users.

Key aspects covered include:

*   **When to Use Memory:** Necessary for applications with chat interfaces or those relying on persistent user information.
*   **Basic Usage (Short-Term Memory):** Storing chat message history up to a token limit, often defaulting to an in-memory SQLite database.
*   **Long-Term Memory Blocks:** Advanced implementations that handle data overflow from short-term memory:
    *   **Static Memory Block:** Stores static, non-changing information (e.g., user's name).
    *   **Fact Extraction Memory Block:** Uses an LLM to extract and store facts from the conversation.
    *   **Vector Memory Block:** Uses a vector store (like Weaviate or Qdrant) to store chat messages for retrieval via vector search.
*   **Customizing Memory:** The ability to create custom memory blocks by extending the `BaseMemoryBlock` class.
*   **Future Improvements:** Plans include allowing memory to be used as a tool for tool-calling agents, adding support for NoSQL databases (like MongoDB, Redis), and enabling structured outputs for the Fact Extraction Memory Block.

The page **does not** specifically discuss **MCP servers**, **tool use** (beyond mentioning future plans for memory as a tool), **agent frameworks** other than LlamaIndex, **OpenAI Agents SDK**, **Anthropic Agents SDK**, **Google SDK**, **function calling**, or **agent orchestration**.

---

## Full Content

Improved Long &amp; Short-Term Memory for LlamaIndex Agents[Get 10k free credits when you sign up for LlamaCloud!](https://www.llamaindex.ai/signup)
[](https://www.llamaindex.ai/)
* Product
[
LlamaCloud
Industry-leading document processing
](https://www.llamaindex.ai/llamacloud)
* [Parse](https://www.llamaindex.ai/llamaparse)
* [Extract](https://www.llamaindex.ai/llamaextract)
* [Index](https://www.llamaindex.ai/llamacloud-index)[
LlamaIndex
Core building blocks for AI agents
](https://www.llamaindex.ai/llamaindex)[
Workflows
Control flow for your GenAI apps
](https://www.llamaindex.ai/workflows)
* Solutions
Persona
* [
Engineering &amp; R&amp;DAccelerate product development
](https://www.llamaindex.ai/solutions/engineering)
* [
Administrative OperationsStreamline business processes
](https://www.llamaindex.ai/solutions/administrative-operations)
* [
Financial AnalystsBuild AI-powered financial models
](https://www.llamaindex.ai/solutions/finance)
Industry
* [
InsuranceAutomate claims and underwriting
](https://www.llamaindex.ai/industry/insurance)
* [
FinancePower financial research
](https://www.llamaindex.ai/industry/finance)
* [
ManufacturingOptimize system uptime
](https://www.llamaindex.ai/industry/manufacturing)
* [
Healthcare &amp; PharmaAccelerate clinical research
](https://www.llamaindex.ai/industry/healthcare-pharma)
Use cases
* [
Financial Due DiligenceSpeed up compliance reviews
](https://www.llamaindex.ai/use-cases/financial-due-diligence)
* [
Invoice ProcessingAutomate manual review
](https://www.llamaindex.ai/use-cases/invoice-processing)
* [
Technical Document SearchFind answers in complex docs
](https://www.llamaindex.ai/use-cases/technical-document-search)
* [
Customer SupportInstant, accurate responses
](https://www.llamaindex.ai/use-cases/customer-support)
* [Developers](https://developers.llamaindex.ai/)
* Resources
* [
Customer storiesSee real-world success stories
](https://www.llamaindex.ai/customers)
[How Jeppesen (a Boeing Company) Saves \~2,000 Engineering Hours with Unified Chat Framework](https://www.llamaindex.ai/customers/jeppesen-a-boeing-company-saves-2-000-engineering-hours-with-unified-chat-framework)
* Company
* [
About usOur mission and story
](https://www.llamaindex.ai/about)
* [
CareersJoin our growing team
](https://www.llamaindex.ai/careers)
* [
BrandLogos and brand guidelines
](https://www.llamaindex.ai/brand)[![](https://www.llamaindex.ai/_astro/header-careers.q9Hk8rsS_1RSkuD.webp)View open roles at LlamaIndex](https://www.llamaindex.ai/careers)
* [Blog](https://www.llamaindex.ai/blog)
* [Pricing](https://www.llamaindex.ai/pricing)
[← Back](https://www.llamaindex.ai/blog)
May 13, 2025
* [
[ Agents ]
](https://www.llamaindex.ai/blog?tag=Agents)
# Improved Long &amp; Short-Term Memory for LlamaIndex Agents
By
![](https://cdn.sanity.io/images/7m9jw85w/production/3b849830b84583f97bcc1d8f4bf9db0c7bfaf6a4-416x416.jpg?w=80)
Tuana Çelik
![](https://cdn.sanity.io/images/7m9jw85w/production/add5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png?w=734)
When to Use Memory
1
* [When to Use Memory](#when-to-use-memory)
* [The Basic Usage of Memory](#the-basic-usage-of-memory)
* [Long-Term Memory Blocks](#long-term-memory-blocks)
* [Static Information as long-term memory](#static-information-as-long-term-memory)
* [Facts as long-term memory](#facts-as-long-term-memory)
* [Vector search as long-term memory](#vector-search-as-long-term-memory)
* [Customizing Memory](#customizing-memory)
* [Future Improvements](#future-improvements)
Content
* [When to Use Memory](#when-to-use-memory)
* [The Basic Usage of Memory](#the-basic-usage-of-memory)
* [Long-Term Memory Blocks](#long-term-memory-blocks)
* [Static Information as long-term memory](#static-information-as-long-term-memory)
* [Facts as long-term memory](#facts-as-long-term-memory)
* [Vector search as long-term memory](#vector-search-as-long-term-memory)
* [Customizing Memory](#customizing-memory)
* [Future Improvements](#future-improvements)
Follow us on
1
*co-authored with Logan Markewich*
For any agentic application that needs to retain basic information about past conversations, about its users, their past interactions and so on, memory is unsurprisingly a core component. So, as part of our latest round of improvements to LlamaIndex, we’ve introduced a[new and improved`Memory`component](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/#memory).
In this article, we will walk through some of the core features of the new LlamaIndex memory component, and how you can start using it in your own agentic applications. First, we’ll look into the most basic implementation of memory, where we simply store chat message history. Then, we’ll also look into more advanced implementations with our latest additions of long-term memory blocks.
## When to Use Memory
Not all AI applications need a memory implementation. A lot of agentic applications we see today don’t necessarily rely on chat history, or persistent information about their users. For example, our very own[LlamaExtract](https://www.llamaindex.ai/llamaextract)is an agentic document extraction application, that uses LLMs to extract structured information from complex files. It doesn’t rely on retaining any sort of memory while doing so.
But, if the essence of an application relies on persistent information that you’d rather not repeat (someone’s name and age for example), and/or any form of conversation flow (so virtually any application with a chat interface), memory quickly becomes quite a crucial component.
Think of this simple (and annoying) chat flow where no form of memory has been implemented:
![](https://cdn.sanity.io/images/7m9jw85w/production/a5ede0e6c636ad4ff324c7e143f622b97f17fd53-904x852.png)
Notice how by the time I get to my second question, the agent is not able to provide me with any answers. The simple reason is that it has no context as to what ‘there’ is in the sentence ‘Can you tell me what the weather is like there?’.
This can be solved with a super simple step where we store chat message history, and send that to the LLM as context. That’s exactly how the most basic form of the LlamaIndex Memory starts:
## The Basic Usage of Memory
In the most simple form, the LlamaIndex`Memory`will store any number of chat messages that fit into a given token limit, into a SQL database, which defaults to an in-memory SQLite database. In short, this memory will be able to store chat messages, up to a limit. Once that limit is reached (depending on how the memory is configured), the oldest messages in chat history will either get discarded, or flushed to long-term memory.
So, in its most basic form, the LlamaIndex Memory component allows you to build agentic workflows that are able to retain past conversations. You can store chat history into this memory either by initializing an agent with memory:
python
```
`fromllama\_index.core.agent.workflowimportFunctionAgentfromllama\_index.core.memoryimportMemorymemory=Memory.from\_defaults(session\_id="my\_session",token\_limit=40000)agent=FunctionAgent(llm=llm,tools=tools)response=awaitagent.run("&#x3C;&#x3C;question that invokes tool\>",memory=memory)`
```
Or, by using`memory.put\_messages()`to add specific chat messages into memory:
python
```
`memory=Memory.from\_defaults(session\_id="my\_session",token\_limit=40000)memory.put\_messages([ChatMessage(role="user",content="Hello, world!"),ChatMessage(role="assistant",content="Hello, world to you too!"),])chat\_history=memory.get()`
```
## Long-Term Memory Blocks
For some use-cases, the simple chat history implementation above might be enough. But for others, it makes sense to implement something a bit more advanced. We introduce 3 new “memory blocks” that act as long-term memory:
* Static memory block: which keeps track of static, non-changing information
* Fact extraction memory block: that populates a list of facts extracted from the conversation
* Vector memory block: which allows us to make use of classic embedding generation vector search as a way to both store chat history and fetch relevant parts of old conversations.
A memory component can be initialized with any number of these long-term memory blocks. Then, every time we reach the token limit for short-term memory, it will write the relevant information to these long-term memory blocks. To add long-term memory blocks we initialize memory with`memory\_blocks`:
python
```
`blocks=[&#x3C;listof long-term memory blocks\>]memory=Memory.from\_defaults(session\_id="my\_session",memory\_blocks=blocks,insert\_method="system")`
```
Let’s see in more detail how we can initialize each block.
### Static Information as long-term memory
Imagine information that might be relevant to your agentic application that is less likely to change over time. For example, your name, where you live, where you work etc. The`StaticMemoryBlock`allows you to provide this information as added context for your agent:
python
```
`fromllama\_index.core.memoryimportStaticMemoryBlockstatic\_info\_block=StaticMemoryBlock(name="core\_info",static\_content="My name is Tuana, and I live in Amsterdam. I work at LlamaIndex.",priority=0)`
```
You may think that the information here are facts, so are more appropriate for the fact extraction block. The main difference is that the fact extraction block is in itself an LLM powered extraction system, that is designed to extract facts as the conversation is happening.
### Facts as long-term memory
My personal favorite. The`FactExtractionMemoryBlock`is a unique long-term memory block that is initialized with a default prompt (which you can override), that instructs an LLM to extract a list of facts from ongoing conversations. To initialize this block:
python
```
`fromllama\_index.core.memoryimportFactExtractionMemoryBlockfromllama\_index.llms.openaiimportOpenAIllm=OpenAI(model="gpt-4o-mini")facts\_block=FactExtractionMemoryBlock(name="extracted\_info",llm=llm,max\_facts=50,priority=1)`
```
Then, for example, after a long conversation with an agent where I provide some information about myself, we may get the following:
python
```
`memory.memory\_blocks[1].facts# ['User is 29 years old.', 'User has a sister.']`
```
### Vector search as long-term memory
Finally, we also have the`VectorMemoryBlock`which has to be initialized with[a vector store](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/)like Weaviate, Qdrant or similar. The idea here is simple: once we reach the token limit for short-term memory, the memory component will write all the chat messages that have occurred to the vector store we initialized this block with. Once we do that, any ongoing conversations, where appropriate, can fetch relevant conversations from history and use that as context before replying to the user.
python
```
`fromllama\_index.core.memoryimportVectorMemoryBlockvector\_block=VectorMemoryBlock(name="vector\_memory",# required: pass in a vector store like qdrant, chroma, weaviate, milvus, etc.vector\_store=vector\_store,priority=2,embed\_model=embed\_model,similarity\_top\_k=2,)`
```
## Customizing Memory
On top of all of these, we are also making it possible for you to introduce your own memory blocks by extending the`BaseMemoryBlock`class. For example, below we define a memory block that counts the mention of a given name in the conversation:
python
```
`fromtypingimportOptional,List,Anyfromllama\_index.core.llmsimportChatMessagefromllama\_index.core.memory.memoryimportBaseMemoryBlockclassMentionCounter(BaseMemoryBlock[str]):"""A memory block that counts the number of times a user mentions a specific name."""mention\_name:str="Logan"mention\_count:int=0asyncdef\_aget(self,messages:Optional[List[ChatMessage]]=None,\*\*block\_kwargs:Any)-\>str:returnf"Logan was mentioned{self.mention\_count}times."asyncdef\_aput(self,messages:List[ChatMessage])-\>None:formessageinmessages:ifself.mention\_nameinmessage.content:self.mention\_count+=1asyncdefatruncate(self,content:str,tokens\_to\_truncate:int)-\>Optional[str]:return""`
```
## Future Improvements
We would love for you to try out these latest improvements to the memory component for agents. If you do, please let us know how you got on in our community Discord server.
But, that being said, we see a few improvements coming for this component. For example, the current implementation only allows you to make use of memory as part of the backend of an agent. We’d love to provide the option of using memory as one of the suite of tools for a tool calling agent.
Additionally, we currently only support SQL databases ([including remote databases which you can configure](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/#remote-memory)). But, we would also like to add support for NoSQL databases like MongoDB, Redis and so on.
Finally, we can imagine plenty of scenarios where the`FactExtractionMemoryBlock`might benefit from structured outputs. This would allow us to initialize the block with a predefined set of fields (facts) that the agent will fill in along the way.
To get started and learn more about how to build agents with both short-term and long-term memory:
* [Visit the Memory documentation](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory)
* [See the API docs](https://docs.llamaindex.ai/en/latest/api_reference/memory/memory/)
Related articles
## Keep Reading
* [Adding Document Understanding to Claude Code](https://www.llamaindex.ai/blog/adding-document-understanding-to-claude-code)
[![](https://cdn.sanity.io/images/7m9jw85w/production/df3f30cf36f725628aa18fed1fb44caba18f30fb-3956x1528.png)](https://www.llamaindex.ai/blog/adding-document-understanding-to-claude-code)
* [Context Engineering - What it is, and techniques to consider](https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider)
[![](https://cdn.sanity.io/images/7m9jw85w/production/93824ab037787b5d496d7380cfdf0da8ee6f9f31-734x379.png)](https://www.llamaindex.ai/blog/context-engineering-what-it-is-and-techniques-to-consider)
* [Create a Meeting Notetaker Agent for Notion with LlamaIndex and Zoom RTMS](https://www.llamaindex.ai/blog/create-a-meeting-notetaker-agent-for-notion-with-llamaindex-and-zoom-rtms)
[![](https://cdn.sanity.io/images/7m9jw85w/production/95666c02e777c9eeaf9298fd6048544b264a4c41-1468x758.png)](https://www.llamaindex.ai/blog/create-a-meeting-notetaker-agent-for-notion-with-llamaindex-and-zoom-rtms)
## Start building your first document agent today
PortableText [components.type] is missing &quot;undefined&quot;
* [Sign up for free](https://www.llamaindex.ai/signup)
* [Book a demo](https://www.llamaindex.ai/contact)
