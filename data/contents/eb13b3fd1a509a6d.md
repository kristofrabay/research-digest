# Chain-of-thought prompting elicits reasoning in large language models

**URL:** https://dl.acm.org/doi/10.5555/3600270.3602070
**Published:** 2022-11-28T00:00:00.000Z

---

## Summary

The webpage is an abstract and citation information for a research article titled **"Chain-of-thought prompting elicits reasoning in large language models"**.

The abstract states that generating a **chain of thought** (a series of intermediate reasoning steps) significantly improves the ability of large language models (LLMs) to perform complex reasoning. This is achieved through **chain-of-thought prompting**, where a few chain-of-thought demonstrations are provided as examples in the prompt. Experiments showed this method improves performance on arithmetic, commonsense, and symbolic reasoning tasks, with striking empirical gains, such as achieving state-of-the-art accuracy on the GSM8K benchmark for math word problems using a PaLM 540B model with just eight chain-of-thought exemplars.

The user query covers several topics related to LLM reasoning and planning: *reasoning & planning, Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, factuality*.

**Summary relevant to the query:**

The page directly addresses **Reasoning LLMs** and **chain-of-thought** prompting, showing that providing intermediate reasoning steps significantly improves complex reasoning abilities in LLMs across arithmetic, commonsense, and symbolic tasks.

**Topics not explicitly covered in the abstract:** inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.

---

## Full Content

Chain-of-thought prompting elicits reasoning in large language models | Proceedings of the 36th International Conference on Neural Information Processing Systems[skip to main content](#skip-to-main-content)
## Preview: ACM Digital Library Open Access
×Welcome to the preview of the new ACM Digital Library.
ACM will formally transition to[Open Access](https://dl.acm.org/action/clickThrough?id=108183&amp;url=/openaccess&amp;loc=/doi/10.5555/3600270.3602070&amp;pubId=60652939&amp;placeholderId=101025&amp;productId=108179)on January 1, 2026.
As part of this change, the Digital Library is now available in two editions:**Basic**and**Premium**. Some advanced features are available only in the Premium Edition.[Click here to learn more](https://dl.acm.org/action/clickThrough?id=108183&amp;url=/premium&amp;loc=/doi/10.5555/3600270.3602070&amp;pubId=60652939&amp;placeholderId=101025&amp;productId=108179).
**You are currently in the Basic Edition. Features requiring a subscriptionappear in grey.**
[Sign In](https://dl.acm.org/action/clickThrough?id=108183&amp;url=/action/showLogin?redirectUri=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.5555%2F3600270.3602070&amp;loc=/doi/10.5555/3600270.3602070&amp;pubId=60652939&amp;placeholderId=101025&amp;productId=108179)[Upgrade](https://dl.acm.org/action/clickThrough?id=108183&amp;url=/about/upgrade&amp;loc=/doi/10.5555/3600270.3602070&amp;pubId=60652939&amp;placeholderId=101025&amp;productId=108179)
[![](https://dl.acm.org/pb-assets/images/basic-edition-1765756198880.png)](https://dl.acm.org/premium)
[](#global-menu)
Search ACM Digital Library
SearchSearch
[Advanced Search](https://dl.acm.org/search/advanced)
10.5555/3600270.3602070guideproceedingsArticle/Chapter ViewBasic AbstractPublication PagesnipsConference Proceedingsconference-collections
[nips](#)
**## Export Citations
Select Citation formatBibTeXEndNoteACM Ref**
* Please download or close your previous search result export first before starting a new bulk export.
Preview is not available.
By clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.
* ```
```
* [Download citation**](javascript:void(0))
* [Copy citation**](javascript:void(0))
Several features on this page require Premium Access.
[Learn more](https://dl.acm.org/about/upgrade)[Sign in](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.5555/3600270.3602070)
**
You are using the Basic Edition. Features requiring a subscriptionappear in grey.
[Sign in](https://dl.acm.org/action/clickThrough?id=108178&amp;url=/action/showLogin?redirectUri=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.5555%2F3600270.3602070&amp;loc=/doi/10.5555/3600270.3602070&amp;pubId=60652939&amp;placeholderId=101024&amp;productId=108179)to your subscription or[learn more](https://dl.acm.org/action/clickThrough?id=108178&amp;url=/premium&amp;loc=/doi/10.5555/3600270.3602070&amp;pubId=60652939&amp;placeholderId=101024&amp;productId=108179)
[Upgrade](https://dl.acm.org/action/clickThrough?id=108178&amp;url=/about/upgrade&amp;loc=/doi/10.5555/3600270.3602070&amp;pubId=60652939&amp;placeholderId=101024&amp;productId=108179)
research-article
Share on
* **
* **
* **
* **
* **
# Chain-of-thought prompting elicits reasoning in large language models
AUTHORs:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)JasonWei](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)XuezhiWang](#artseq-00002),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)DaleSchuurmans](#artseq-00003),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)MaartenBosma](#artseq-00004),+ 5,[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)BrianIchter](#artseq-00005),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)FeiXia](#artseq-00006),+ 3,[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)Ed H.Chi](#artseq-00007),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)Quoc V.Le](#artseq-00008),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)DennyZhou](#artseq-00009)(Less)[Authors Info &amp; Claims](#get-premium-access)
[NIPS'22: Proceedings of the 36th International Conference on Neural Information Processing Systems](https://dl.acm.org/doi/proceedings/10.5555/3600270)
Article No.: 1800, Pages24824-24837
Published:28 November 2022[Publication History](#get-premium-access)
**...citation**...Downloads
[**](#get-premium-access)
**## New Citation Alert added!
This alert has been successfully added and will be sent to:
You will be notified whenever a record that you have chosen has been cited.
To manage your alert preferences, click on the button below.
[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)
**## New Citation Alert!
Please[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.5555/3600270.3602070)
[**](#get-premium-access)
**
**Contents
* **Information &amp; Contributors
* **Bibliometrics &amp; Citations
* **View Options
* **References
* **Figures
* **Tables
* **Media
* **Share
## Abstract
We explore how generating a*chain of thought*—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called*chain-of-thought prompting*, where a few chain of thought demonstrations are provided as exemplars in prompting.
Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.
## Supplementary Material
Additional material(3600270.3602070\_supp.pdf)
Supplemental material.
* [Download](https://dl.acm.org/doi/suppl/10.5555/3600270.3602070/suppl_file/3600270.3602070_supp.pdf)
* 448.83 KB
## References
[1]
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 2022. Do as I can, not as I say: Grounding language in robotic affordances.*arXiv preprint arXiv:2204.01691*.
[Google Scholar](https://scholar.google.com/scholar?q=Michael+Ahn,+Anthony+Brohan,+Noah+Brown,+Yevgen+Chebotar,+Omar+Cortes,+Byron+David,+Chelsea+Finn,+Keerthana+Gopalakrishnan,+Karol+Hausman,+Alex+Herzog,+et+al.+2022.+Do+as+I+can,+not+as+I+say:+Grounding+language+in+robotic+affordances.+arXiv+preprint+arXiv:2204.01691.)
[2]
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, Minneapolis, Minnesota. Association for Computational Linguistics.
[Google Scholar](https://scholar.google.com/scholar?q=Aida+Amini,+Saadia+Gabriel,+Shanchuan+Lin,+Rik+Koncel-Kedziorski,+Yejin+Choi,+and+Hannaneh+Hajishirzi.+2019.+MathQA:+Towards+interpretable+math+word+problem+solving+with+operation-based+formalisms.+In+Proceedings+of+the+2019+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics:+Human+Language+Technologies,+Volume+1+(Long+and+Short+Papers),+Minneapolis,+Minnesota.+Association+for+Computational+Linguistics.)
[3]
Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding operations and arguments with reading comprehension.*EMNLP*.
[Google Scholar](https://scholar.google.com/scholar?q=Daniel+Andor,+Luheng+He,+Kenton+Lee,+and+Emily+Pitler.+2019.+Giving+BERT+a+calculator:+Finding+operations+and+arguments+with+reading+comprehension.+EMNLP.)
[4]
Jacob Andreas, Dan Klein, and Sergey Levine. 2018. Learning with latent language.*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Jacob+Andreas,+Dan+Klein,+and+Sergey+Levine.+2018.+Learning+with+latent+language.+NAACL.)
[5]
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models.*arXiv preprint arXiv:2108.07732*.
[Google Scholar](https://scholar.google.com/scholar?q=Jacob+Austin,+Augustus+Odena,+Maxwell+Nye,+Maarten+Bosma,+Henryk+Michalewski,+David+Dohan,+Ellen+Jiang,+Carrie+Cai,+Michael+Terry,+Quoc+Le,+et+al.+2021.+Program+synthesis+with+large+language+models.+arXiv+preprint+arXiv:2108.07732.)
[6]
BIG-bench collaboration. 2021. Beyond the imitation game: Measuring and extrapolating the capabilities of language models.*In preparation*.
[Google Scholar](https://scholar.google.com/scholar?q=BIG-bench+collaboration.+2021.+Beyond+the+imitation+game:+Measuring+and+extrapolating+the+capabilities+of+language+models.+In+preparation.)
[7]
Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural language deductions.*EMNLP*.
[Google Scholar](https://scholar.google.com/scholar?q=Kaj+Bostrom,+Xinyu+Zhao,+Swarat+Chaudhuri,+and+Greg+Durrett.+2021.+Flexible+generation+of+natural+language+deductions.+EMNLP.)
[8]
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.*NeurIPS*.
[Google Scholar](https://scholar.google.com/scholar?q=Tom+Brown,+Benjamin+Mann,+Nick+Ryder,+Melanie+Subbiah,+Jared+D+Kaplan,+Prafulla+Dhariwal,+Arvind+Neelakantan,+Pranav+Shyam,+Girish+Sastry,+Amanda+Askell,+Sandhini+Agarwal,+Ariel+Herbert-Voss,+Gretchen+Krueger,+Tom+Henighan,+Rewon+Child,+Aditya+Ramesh,+Daniel+Ziegler,+Jeffrey+Wu,+Clemens+Winter,+Chris+Hesse,+Mark+Chen,+Eric+Sigler,+Mateusz+Litwin,+Scott+Gray,+Benjamin+Chess,+Jack+Clark,+Christopher+Berner,+Sam+McCandlish,+Alec+Radford,+Ilya+Sutskever,+and+Dario+Amodei.+2020.+Language+models+are+few-shot+learners.+NeurIPS.)
[9]
Jonathon Cai, Richard Shin, and Dawn Song. 2017. Making neural programming architectures generalize via recursion.*ICLR*.
[Google Scholar](https://scholar.google.com/scholar?q=Jonathon+Cai,+Richard+Shin,+and+Dawn+Song.+2017.+Making+neural+programming+architectures+generalize+via+recursion.+ICLR.)
[10]
Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations.*NeurIPS*.
[Google Scholar](https://scholar.google.com/scholar?q=Oana-Maria+Camburu,+Tim+Rocktäschel,+Thomas+Lukasiewicz,+and+Phil+Blunsom.+2018.+e-SNLI:+Natural+language+inference+with+natural+language+explanations.+NeurIPS.)
[11]
Howard Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. Can rationalization improve robustness?*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Howard+Chen,+Jacqueline+He,+Karthik+Narasimhan,+and+Danqi+Chen.+2022.+Can+rationalization+improve+robustness?+NAACL.)
[12]
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code.*arXiv preprint arXiv:2107.03374*.
[Google Scholar](https://scholar.google.com/scholar?q=Mark+Chen,+Jerry+Tworek,+Heewoo+Jun,+Qiming+Yuan,+Henrique+Ponde+de+Oliveira+Pinto,+Jared+Kaplan,+Harri+Edwards,+Yuri+Burda,+Nicholas+Joseph,+Greg+Brockman,+et+al.+2021.+Evaluating+large+language+models+trained+on+code.+arXiv+preprint+arXiv:2107.03374.)
[13]
Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2019. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension.*ICLR*.
[Google Scholar](https://scholar.google.com/scholar?q=Xinyun+Chen,+Chen+Liang,+Adams+Wei+Yu,+Denny+Zhou,+Dawn+Song,+and+Quoc+V.+Le.+2019.+Neural+symbolic+reader:+Scalable+integration+of+distributed+and+symbolic+representations+for+reading+comprehension.+ICLR.)
[14]
Ting-Rui Chiang and Yun-Nung Chen. 2019. Semantically-aligned equation generation for solving and reasoning math word problems. In*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 2656-2668, Minneapolis, Minnesota. Association for Computational Linguistics.
[Crossref](https://doi.org/10.18653/v1/N19-1272)
[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.18653/v1/N19-1272)
[15]
Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language.*IJCAI*.
[Google Scholar](https://scholar.google.com/scholar?q=Peter+Clark,+Oyvind+Tafjord,+and+Kyle+Richardson.+2020.+Transformers+as+soft+reasoners+over+language.+IJCAI.)
[16]
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems.*arXiv preprint arXiv:2110.14168*.
[Google Scholar](https://scholar.google.com/scholar?q=Karl+Cobbe,+Vineet+Kosaraju,+Mohammad+Bavarian,+Jacob+Hilton,+Reiichiro+Nakano,+Christopher+Hesse,+and+John+Schulman.+2021.+Training+verifiers+to+solve+math+word+problems.+arXiv+preprint+arXiv:2110.14168.)
[17]
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding.*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Jacob+Devlin,+Ming-Wei+Chang,+Kenton+Lee,+and+Kristina+Toutanova.+2019.+BERT:+Pre-training+of+deep+bidirectional+transformers+for+language+understanding.+NAACL.)
[18]
Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural logic machines.*ICLR*.
[Google Scholar](https://scholar.google.com/scholar?q=Honghua+Dong,+Jiayuan+Mao,+Tian+Lin,+Chong+Wang,+Lihong+Li,+and+Denny+Zhou.+2019.+Neural+logic+machines.+ICLR.)
[19]
Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Benefits of intermediate annotations in reading comprehension. ACL.
[Google Scholar](https://scholar.google.com/scholar?q=Dheeru+Dua,+Sameer+Singh,+and+Matt+Gardner.+2020.+Benefits+of+intermediate+annotations+in+reading+comprehension.+ACL.)
[20]
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies.*TACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Mor+Geva,+Daniel+Khashabi,+Elad+Segal,+Tushar+Khot,+Dan+Roth,+and+Jonathan+Berant.+2021.+Did+aristotle+use+a+laptop?+A+question+answering+benchmark+with+implicit+reasoning+strategies.+TACL.)
[21]
Yuling Gu, Bhavana Dalvi Mishra, and Peter Clark. 2022. DREAM: Uncovering mental models behind language models.*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Yuling+Gu,+Bhavana+Dalvi+Mishra,+and+Peter+Clark.+2022.+DREAM:+Uncovering+mental+models+behind+language+models.+NAACL.)
[22]
Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher Ré. 2018. Training classifiers with natural language explanations. ACL.
[Google Scholar](https://scholar.google.com/scholar?q=Braden+Hancock,+Paroma+Varma,+Stephanie+Wang,+Martin+Bringmann,+Percy+Liang,+and+Christopher+Ré.+2018.+Training+classifiers+with+natural+language+explanations.+ACL.)
[23]
Peter Hase and Mohit Bansal. 2022. When can models learn from explanations? a formal framework for understanding the roles of explanation data. ACL.
[Google Scholar](https://scholar.google.com/scholar?q=Peter+Hase+and+Mohit+Bansal.+2022.+When+can+models+learn+from+explanations?+a+formal+framework+for+understanding+the+roles+of+explanation+data.+ACL.)
[24]
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset.*arXiv preprint arXiv:2103.03874*.
[Google Scholar](https://scholar.google.com/scholar?q=Dan+Hendrycks,+Collin+Burns,+Saurav+Kadavath,+Akul+Arora,+Steven+Basart,+Eric+Tang,+Dawn+Song,+and+Jacob+Steinhardt.+2021.+Measuring+mathematical+problem+solving+with+the+math+dataset.+arXiv+preprint+arXiv:2103.03874.)
[25]
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization.*EMNLP*.
[Google Scholar](https://scholar.google.com/scholar?q=Mohammad+Javad+Hosseini,+Hannaneh+Hajishirzi,+Oren+Etzioni,+and+Nate+Kushman.+2014.+Learning+to+solve+arithmetic+word+problems+with+verb+categorization.+EMNLP.)
[26]
Zhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem solving as complex relation extraction.*arXiv preprint arXiv:2203.10316*.
[Google Scholar](https://scholar.google.com/scholar?q=Zhanming+Jie,+Jierui+Li,+and+Wei+Lu.+2022.+Learning+to+reason+deductively:+Math+word+problem+solving+as+complex+relation+extraction.+arXiv+preprint+arXiv:2203.10316.)
[27]
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models.*arXiv preprint arXiv:2001.08361*.
[Google Scholar](https://scholar.google.com/scholar?q=Jared+Kaplan,+Sam+McCandlish,+Tom+Henighan,+Tom+B+Brown,+Benjamin+Chess,+Rewon+Child,+Scott+Gray,+Alec+Radford,+Jeffrey+Wu,+and+Dario+Amodei.+2020.+Scaling+laws+for+neural+language+models.+arXiv+preprint+arXiv:2001.08361.)
[28]
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository.*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Rik+Koncel-Kedziorski,+Subhro+Roy,+Aida+Amini,+Nate+Kushman,+and+Hannaneh+Hajishirzi.+2016.+MAWPS:+A+math+word+problem+repository.+NAACL.)
[29]
Andrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. 2022. Can language models learn from explanations in context?*arXiv preprint arXiv:2204.02329*.
[Google Scholar](https://scholar.google.com/scholar?q=Andrew+K.+Lampinen,+Ishita+Dasgupta,+Stephanie+C.Y.+Chan,+Kory+Matthewson,+Michael+Henry+Tessler,+Antonia+Creswell,+James+L.+McClelland,+Jane+X.+Wang,+and+Felix+Hill.+2022.+Can+language+models+learn+from+explanations+in+context?+arXiv+preprint+arXiv:2204.02329.)
[30]
Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang, and Ee-Peng Lim. 2021. MWPToolkit: An open-source framework for deep learning-based math word problem solvers.*arXiv preprint arXiv:2109.00799*.
[Google Scholar](https://scholar.google.com/scholar?q=Yihuai+Lan,+Lei+Wang,+Qiyuan+Zhang,+Yunshi+Lan,+Bing+Tian+Dai,+Yan+Wang,+Dongxiang+Zhang,+and+Ee-Peng+Lim.+2021.+MWPToolkit:+An+open-source+framework+for+deep+learning-based+math+word+problem+solvers.+arXiv+preprint+arXiv:2109.00799.)
[31]
Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth?*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Teven+Le+Scao+and+Alexander+Rush.+2021.+How+many+data+points+is+a+prompt+worth?+NAACL.)
[32]
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning.*EMNLP*.
[Google Scholar](https://scholar.google.com/scholar?q=Brian+Lester,+Rami+Al-Rfou,+and+Noah+Constant.+2021.+The+power+of+scale+for+parameter-efficient+prompt+tuning.+EMNLP.)
[33]
Iddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles: From robust processing to precise semantics.*Proceedings of the 2nd Workshop on Text Meaning and Interpretation*.
[Crossref](https://doi.org/10.3115/1628275.1628277)
[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.3115/1628275.1628277)
[34]
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation.*ACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Xiang+Lisa+Li+and+Percy+Liang.+2021.+Prefix-tuning:+Optimizing+continuous+prompts+for+generation.+ACL.)
[35]
Zhengzhong Liang, Steven Bethard, and Mihai Surdeanu. 2021. Explainable multi-hop verbal reasoning through internal monologue.*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Zhengzhong+Liang,+Steven+Bethard,+and+Mihai+Surdeanu.+2021.+Explainable+multi-hop+verbal+reasoning+through+internal+monologue.+NAACL.)
[36]
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL.
[Google Scholar](https://scholar.google.com/scholar?q=Wang+Ling,+Dani+Yogatama,+Chris+Dyer,+and+Phil+Blunsom.+2017.+Program+induction+by+rationale+generation:+Learning+to+solve+and+explain+algebraic+word+problems.+ACL.)
[37]
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.*arXiv preprint arXiv:2107.13586*.
[Google Scholar](https://scholar.google.com/scholar?q=Pengfei+Liu,+Weizhe+Yuan,+Jinlan+Fu,+Zhengbao+Jiang,+Hiroaki+Hayashi,+and+Graham+Neubig.+2021.+Pre-train,+prompt,+and+predict:+A+systematic+survey+of+prompting+methods+in+natural+language+processing.+arXiv+preprint+arXiv:2107.13586.)
[38]
Bodhisattwa Prasad Majumder, Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley. 2021. Rationale-inspired natural language explanations with commonsense.*arXiv preprint arXiv:2106.13876*.
[Google Scholar](https://scholar.google.com/scholar?q=Bodhisattwa+Prasad+Majumder,+Oana-Maria+Camburu,+Thomas+Lukasiewicz,+and+Julian+McAuley.+2021.+Rationale-inspired+natural+language+explanations+with+commonsense.+arXiv+preprint+arXiv:2106.13876.)
[39]
Ana Marasović, Iz Beltagy, Doug Downey, and Matthew E Peters. 2022. Few-shot self-rationalization with natural language prompts.*NAACL Findings*.
[Google Scholar](https://scholar.google.com/scholar?q=Ana+Marasović,+Iz+Beltagy,+Doug+Downey,+and+Matthew+E+Peters.+2022.+Few-shot+self-rationalization+with+natural+language+prompts.+NAACL+Findings.)
[40]
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In ACL.
[Google Scholar](https://scholar.google.com/scholar?q=Joshua+Maynez,+Shashi+Narayan,+Bernd+Bohnet,+and+Ryan+McDonald.+2020.+On+faithfulness+and+factuality+in+abstractive+summarization.+In+ACL.)
[41]
Shen Yun Miao, Chao Chun Liang, and Keh Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. ACL.
[Google Scholar](https://scholar.google.com/scholar?q=Shen+Yun+Miao,+Chao+Chun+Liang,+and+Keh+Yih+Su.+2020.+A+diverse+corpus+for+evaluating+and+developing+English+math+word+problem+solvers.+ACL.)
[42]
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work?*arXiv preprint arXiv:2202.12837*.
[Google Scholar](https://scholar.google.com/scholar?q=Sewon+Min,+Xinxi+Lyu,+Ari+Holtzman,+Mikel+Artetxe,+Mike+Lewis,+Hannaneh+Hajishirzi,+and+Luke+Zettlemoyer.+2022.+Rethinking+the+role+of+demonstrations:+What+makes+in-context+learning+work?+arXiv+preprint+arXiv:2202.12837.)
[43]
Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. WT5?! Training text-to-text models to explain their predictions.*arXiv preprint arXiv:2004.14546*.
[Google Scholar](https://scholar.google.com/scholar?q=Sharan+Narang,+Colin+Raffel,+Katherine+Lee,+Adam+Roberts,+Noah+Fiedel,+and+Karishma+Malkan.+2020.+WT5?!+Training+text-to-text+models+to+explain+their+predictions.+arXiv+preprint+arXiv:2004.14546.)
[44]
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models.*arXiv preprint arXiv:2112.00114*.
[Google Scholar](https://scholar.google.com/scholar?q=Maxwell+Nye,+Anders+Johan+Andreassen,+Guy+Gur-Ari,+Henryk+Michalewski,+Jacob+Austin,+David+Bieber,+David+Dohan,+Aitor+Lewkowycz,+Maarten+Bosma,+David+Luan,+et+al.+2021.+Show+your+work:+Scratchpads+for+intermediate+computation+with+language+models.+arXiv+preprint+arXiv:2112.00114.)
[45]
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.*arXiv preprint arXiv:2203.02155*.
[Google Scholar](https://scholar.google.com/scholar?q=Long+Ouyang,+Jeff+Wu,+Xu+Jiang,+Diogo+Almeida,+Carroll+L.+Wainwright,+Pamela+Mishkin,+Chong+Zhang,+Sandhini+Agarwal,+Katarina+Slama,+Alex+Ray,+et+al.+2022.+Training+language+models+to+follow+instructions+with+human+feedback.+arXiv+preprint+arXiv:2203.02155.)
[46]
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems?*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Arkil+Patel,+Satwik+Bhattamishra,+and+Navin+Goyal.+2021.+Are+NLP+models+really+able+to+solve+simple+math+word+problems?+NAACL.)
[47]
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations.*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Matthew+E.+Peters,+Mark+Neumann,+Mohit+Iyyer,+Matt+Gardner,+Christopher+Clark,+Kenton+Lee,+and+Luke+Zettlemoyer.+2018.+Deep+contextualized+word+representations.+NAACL.)
[48]
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu Chen. 2022. Reasoning like program executors.*arXiv preprint arXiv:2201.11473*.
[Google Scholar](https://scholar.google.com/scholar?q=Xinyu+Pi,+Qian+Liu,+Bei+Chen,+Morteza+Ziyadi,+Zeqi+Lin,+Yan+Gao,+Qiang+Fu,+Jian-Guang+Lou,+and+Weizhu+Chen.+2022.+Reasoning+like+program+executors.+arXiv+preprint+arXiv:2201.11473.)
[49]
Piotr Piekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. ACL.
[Google Scholar](https://scholar.google.com/scholar?q=Piotr+Piekos,+Mateusz+Malinowski,+and+Henryk+Michalewski.+2021.+Measuring+and+improving+BERT's+mathematical+abilities+by+predicting+the+order+of+reasoning.+ACL.)
[50]
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis &amp; insights from training Gopher.*arXiv preprint arXiv:2112.11446*.
[Google Scholar](https://scholar.google.com/scholar?q=Jack+W.+Rae,+Sebastian+Borgeaud,+Trevor+Cai,+Katie+Millican,+Jordan+Hoffmann,+Francis+Song,+John+Aslanides,+Sarah+Henderson,+Roman+Ring,+Susannah+Young,+et+al.+2021.+Scaling+language+models:+Methods,+analysis+&+insights+from+training+Gopher.+arXiv+preprint+arXiv:2112.11446.)
[51]
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.*Journal of Machine Learning Research*, 21:1-67.
[Google Scholar](https://scholar.google.com/scholar?q=Colin+Raffel,+Noam+Shazeer,+Adam+Roberts,+Katherine+Lee,+Sharan+Narang,+Michael+Matena,+Yanqi+Zhou,+Wei+Li,+and+Peter+J+Liu.+2020.+Exploring+the+limits+of+transfer+learning+with+a+unified+text-to-text+transformer.+Journal+of+Machine+Learning+Research,+21:1-67.)
[52]
Dheeraj Rajagopal, Vidhisha Balachandran, Eduard H. Hovy, and Yulia Tsvetkov. 2021. SelfExplain: A self-explaining architecture for neural text classifiers.*EMNLP*.
[Google Scholar](https://scholar.google.com/scholar?q=Dheeraj+Rajagopal,+Vidhisha+Balachandran,+Eduard+H.+Hovy,+and+Yulia+Tsvetkov.+2021.+SelfExplain:+A+self-explaining+architecture+for+neural+text+classifiers.+EMNLP.)
[53]
Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! Leveraging language models for commonsense reasoning. ACL.
[Google Scholar](https://scholar.google.com/scholar?q=Nazneen+Fatema+Rajani,+Bryan+McCann,+Caiming+Xiong,+and+Richard+Socher.+2019.+Explain+yourself!+Leveraging+language+models+for+commonsense+reasoning.+ACL.)
[54]
Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading comprehension with numerical reasoning.*EMNLP*.
[Google Scholar](https://scholar.google.com/scholar?q=Qiu+Ran,+Yankai+Lin,+Peng+Li,+Jie+Zhou,+and+Zhiyuan+Liu.+2019.+NumNet:+Machine+reading+comprehension+with+numerical+reasoning.+EMNLP.)
[55]
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language generation models.*arXiv preprint arXiv:2112.12870*.
[Google Scholar](https://scholar.google.com/scholar?q=Hannah+Rashkin,+Vitaly+Nikolaev,+Matthew+Lamm,+Michael+Collins,+Dipanjan+Das,+Slav+Petrov,+Gaurav+Singh+Tomar,+Iulia+Turc,+and+David+Reitter.+2021.+Measuring+attribution+in+natural+language+generation+models.+arXiv+preprint+arXiv:2112.12870.)
[56]
Gabriel Recchia. 2021. Teaching autoregressive language models complex tasks by demonstration.*arXiv preprint arXiv:2109.02102*.
[Google Scholar](https://scholar.google.com/scholar?q=Gabriel+Recchia.+2021.+Teaching+autoregressive+language+models+complex+tasks+by+demonstration.+arXiv+preprint+arXiv:2109.02102.)
[57]
Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A recipe for arbitrary text style transfer with large language models. ACL.
[Google Scholar](https://scholar.google.com/scholar?q=Emily+Reif,+Daphne+Ippolito,+Ann+Yuan,+Andy+Coenen,+Chris+Callison-Burch,+and+Jason+Wei.+2022.+A+recipe+for+arbitrary+text+style+transfer+with+large+language+models.+ACL.)
[58]
Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm.*Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems*.
[Digital Library](https://dl.acm.org/doi/10.1145/3411763.3451760)
[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3411763.3451760)
[59]
Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems.*EMNLP*.
[Google Scholar](https://scholar.google.com/scholar?q=Subhro+Roy+and+Dan+Roth.+2015.+Solving+general+arithmetic+word+problems.+EMNLP.)
[60]
Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about Quantities in Natural Language.*TACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Subhro+Roy,+Tim+Vieira,+and+Dan+Roth.+2015.+Reasoning+about+Quantities+in+Natural+Language.+TACL.)
[61]
Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching soft rules to pre-trained language models.*EMNLP*.
[Google Scholar](https://scholar.google.com/scholar?q=Mohammed+Saeed,+Naser+Ahmadi,+Preslav+Nakov,+and+Paolo+Papotti.+2021.+RuleBERT:+Teaching+soft+rules+to+pre-trained+language+models.+EMNLP.)
[62]
Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zero-shot task generalization.*ICLR*.
[Google Scholar](https://scholar.google.com/scholar?q=Victor+Sanh,+Albert+Webson,+Colin+Raffel,+Stephen+H.+Bach,+Lintang+Sutawika,+Zaid+Alyafeai,+Antoine+Chaffin,+Arnaud+Stiegler,+Teven+Le+Scao,+Arun+Raja,+et+al.+2022.+Multitask+prompted+training+enables+zero-shot+task+generalization.+ICLR.)
[63]
Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021.
[Google Scholar](https://scholar.google.com/scholar?q=Jianhao+Shen,+Yichun+Yin,+Lin+Li,+Lifeng+Shang,+Xin+Jiang,+Ming+Zhang,+and+Qun+Liu.+2021.)
[64]
Generate &amp; rank: A multi-task framework for math word problems. In*Findings of the Association for Computational Linguistics: EMNLP 2021*.
[Google Scholar](https://scholar.google.com/scholar?q=Generate+&+rank:+A+multi-task+framework+for+math+word+problems.+In+Findings+of+the+Association+for+Computational+Linguistics:+EMNLP+2021.)
[65]
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge.*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Alon+Talmor,+Jonathan+Herzig,+Nicholas+Lourie,+and+Jonathan+Berant.+2019.+CommonsenseQA:+A+question+answering+challenge+targeting+commonsense+knowledge.+NAACL.)
[66]
Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge.*NeurIPS*.
[Google Scholar](https://scholar.google.com/scholar?q=Alon+Talmor,+Oyvind+Tafjord,+Peter+Clark,+Yoav+Goldberg,+and+Jonathan+Berant.+2020.+Leap-of-thought:+Teaching+pre-trained+models+to+systematically+reason+over+implicit+knowledge.+NeurIPS.)
[67]
Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. 2021. CommonsenseQA 2.0: Exposing the limits of ai through gamification.*NeurIPS Track on Datasets and Benchmarks*.
[Google Scholar](https://scholar.google.com/scholar?q=Alon+Talmor,+Ori+Yoran,+Ronan+Le+Bras,+Chandra+Bhagavatula,+Yoav+Goldberg,+Yejin+Choi,+and+Jonathan+Berant.+2021.+CommonsenseQA+2.0:+Exposing+the+limits+of+ai+through+gamification.+NeurIPS+Track+on+Datasets+and+Benchmarks.)
[68]
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms.*arXiv preprint arXiv:2205.05131*.
[Google Scholar](https://scholar.google.com/scholar?q=Yi+Tay,+Mostafa+Dehghani,+Vinh+Q+Tran,+Xavier+Garcia,+Dara+Bahri,+Tal+Schuster,+Huaixiu+Steven+Zheng,+Neil+Houlsby,+and+Donald+Metzler.+2022.+Unifying+language+learning+paradigms.+arXiv+preprint+arXiv:2205.05131.)
[69]
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications.*arXiv preprint arXiv:2201.08239*.
[Google Scholar](https://scholar.google.com/scholar?q=Romal+Thoppilan,+Daniel+De+Freitas,+Jamie+Hall,+Noam+Shazeer,+Apoorv+Kulshreshtha,+Heng-Tze+Cheng,+Alicia+Jin,+Taylor+Bos,+Leslie+Baker,+Yu+Du,+et+al.+2022.+LaMDA:+Language+models+for+dialog+applications.+arXiv+preprint+arXiv:2201.08239.)
[70]
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models.*arXiv preprint arXiv:2203.11171*.
[Google Scholar](https://scholar.google.com/scholar?q=Xuezhi+Wang,+Jason+Wei,+Dale+Schuurmans,+Quoc+Le,+Ed+Chi,+and+Denny+Zhou.+2022a.+Self-consistency+improves+chain+of+thought+reasoning+in+language+models.+arXiv+preprint+arXiv:2203.11171.)
[71]
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022b. Benchmarking generalization via in-context instructions on 1,600+ language tasks.*arXiv preprint arXiv:2204.07705*.
[Google Scholar](https://scholar.google.com/scholar?q=Yizhong+Wang,+Swaroop+Mishra,+Pegah+Alipoormolabashi,+Yeganeh+Kordi,+Amirreza+Mirzaei,+Anjana+Arunkumar,+Arjun+Ashok,+Arut+Selvan+Dhanasekaran,+Atharva+Naik,+David+Stap,+et+al.+2022b.+Benchmarking+generalization+via+in-context+instructions+on+1,600++language+tasks.+arXiv+preprint+arXiv:2204.07705.)
[72]
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners.*ICLR*.
[Google Scholar](https://scholar.google.com/scholar?q=Jason+Wei,+Maarten+Bosma,+Vincent+Y.+Zhao,+Kelvin+Guu,+Adams+Wei+Yu,+Brian+Lester,+Nan+Du,+Andrew+M.+Dai,+and+Quoc+V.+Le.+2022a.+Finetuned+language+models+are+zero-shot+learners.+ICLR.)
[73]
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022b. Emergent abilities of large language models.*Transactions on Machine Learning Research*.
[Google Scholar](https://scholar.google.com/scholar?q=Jason+Wei,+Yi+Tay,+Rishi+Bommasani,+Colin+Raffel,+Barret+Zoph,+Sebastian+Borgeaud,+Dani+Yogatama,+Maarten+Bosma,+Denny+Zhou,+Donald+Metzler,+et+al.+2022b.+Emergent+abilities+of+large+language+models.+Transactions+on+Machine+Learning+Research.)
[74]
Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing human-AI collaboration for generating free-text explanations.*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Sarah+Wiegreffe,+Jack+Hessel,+Swabha+Swayamdipta,+Mark+Riedl,+and+Yejin+Choi.+2022.+Reframing+human-AI+collaboration+for+generating+free-text+explanations.+NAACL.)
[75]
Sarah Wiegreffe and Ana Marasovic. 2021. Teach me to explain: A review of datasets for explainable NLP.*NeurIPS*.
[Google Scholar](https://scholar.google.com/scholar?q=Sarah+Wiegreffe+and+Ana+Marasovic.+2021.+Teach+me+to+explain:+A+review+of+datasets+for+explainable+NLP.+NeurIPS.)
[76]
Sarah Wiegreffe, Ana Marasovic, and Noah A. Smith. 2021. Measuring association between labels and free-text rationales.*EMNLP*.
[Google Scholar](https://scholar.google.com/scholar?q=Sarah+Wiegreffe,+Ana+Marasovic,+and+Noah+A.+Smith.+2021.+Measuring+association+between+labels+and+free-text+rationales.+EMNLP.)
[77]
Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie J Cai. 2022a. PromptChainer: Chaining large language model prompts through visual programming.*CHI Extended Abstracts*.
[Google Scholar](https://scholar.google.com/scholar?q=Tongshuang+Wu,+Ellen+Jiang,+Aaron+Donsbach,+Jeff+Gray,+Alejandra+Molina,+Michael+Terry,+and+Carrie+J+Cai.+2022a.+PromptChainer:+Chaining+large+language+model+prompts+through+visual+programming.+CHI+Extended+Abstracts.)
[78]
Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022b. AI chains: Transparent and controllable human-AI interaction by chaining large language model prompts.*CHI*.
[Google Scholar](https://scholar.google.com/scholar?q=Tongshuang+Wu,+Michael+Terry,+and+Carrie+Jun+Cai.+2022b.+AI+chains:+Transparent+and+controllable+human-AI+interaction+by+chaining+large+language+model+prompts.+CHI.)
[79]
Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. 2020. Neural execution engines: Learning to execute subroutines.*NeurIPS*.
[Google Scholar](https://scholar.google.com/scholar?q=Yujun+Yan,+Kevin+Swersky,+Danai+Koutra,+Parthasarathy+Ranganathan,+and+Milad+Hashemi.+2020.+Neural+execution+engines:+Learning+to+execute+subroutines.+NeurIPS.)
[80]
Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, and Xiang Ren. 2021. Refining language models with compositional explanations.*NeurIPS*.
[Google Scholar](https://scholar.google.com/scholar?q=Huihan+Yao,+Ying+Chen,+Qinyuan+Ye,+Xisen+Jin,+and+Xiang+Ren.+2021.+Refining+language+models+with+compositional+explanations.+NeurIPS.)
[81]
Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot in-context learning.*arXiv preprint arXiv:2205.03401*.
[Google Scholar](https://scholar.google.com/scholar?q=Xi+Ye+and+Greg+Durrett.+2022.+The+unreliability+of+explanations+in+few-shot+in-context+learning.+arXiv+preprint+arXiv:2205.03401.)
[82]
Yordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, and Oana-Maria Camburu. 2021. Few-shot out-of-domain transfer learning of natural language explanations.*arXiv preprint arXiv:2112.06204*.
[Google Scholar](https://scholar.google.com/scholar?q=Yordan+Yordanov,+Vid+Kocijan,+Thomas+Lukasiewicz,+and+Oana-Maria+Camburu.+2021.+Few-shot+out-of-domain+transfer+learning+of+natural+language+explanations.+arXiv+preprint+arXiv:2112.06204.)
[83]
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using "annotator rationales" to improve machine learning for text categorization.*NAACL*.
[Google Scholar](https://scholar.google.com/scholar?q=Omar+Zaidan,+Jason+Eisner,+and+Christine+Piatko.+2007.+Using+"annotator+rationales"+to+improve+machine+learning+for+text+categorization.+NAACL.)
[84]
Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute.*arXiv preprint arXiv:1410.4615*.
[Google Scholar](https://scholar.google.com/scholar?q=Wojciech+Zaremba+and+Ilya+Sutskever.+2014.+Learning+to+execute.+arXiv+preprint+arXiv:1410.4615.)
[85]
Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. STaR: Bootstrapping reasoning with reasoning.*arXiv preprint arXiv:2203.14465*.
[Google Scholar](https://scholar.google.com/scholar?q=Eric+Zelikman,+Yuhuai+Wu,+and+Noah+D.+Goodman.+2022.+STaR:+Bootstrapping+reasoning+with+reasoning.+arXiv+preprint+arXiv:2203.14465.)
[86]
Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models.*ICML*.
[Google Scholar](https://scholar.google.com/scholar?q=Tony+Z.+Zhao,+Eric+Wallace,+Shi+Feng,+Dan+Klein,+and+Sameer+Singh.+2021.+Calibrate+before+use:+Improving+few-shot+performance+of+language+models.+ICML.)
[87]
Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and Jian Tang. 2020. Towards interpretable natural language understanding with explanations as latent variables.*NeurIPS*.
[Google Scholar](https://scholar.google.com/scholar?q=Wangchunshu+Zhou,+Jinyi+Hu,+Hanlin+Zhang,+Xiaodan+Liang,+Maosong+Sun,+Chenyan+Xiong,+and+Jian+Tang.+2020.+Towards+interpretable+natural+language+understanding+with+explanations+as+latent+variables.+NeurIPS.)
## Cited By
[View all**](https://dl.acm.org/action/ajaxShowCitedBy?doi=10.5555/3600270.3602070)
* Zhang YJin ZXing YLi GLiu FZhu JDou WWei J(2025)PATCH: Empowering Large Language Model with Programmer-Intent Guidance and Collaborative-Behavior Simulation for Automatic Bug FixingACM Transactions on Software Engineering and Methodology10.1145/3718739**35**:1(1-35)Online publication date: 11-Dec-2025
[https://dl.acm.org/doi/10.1145/3718739](https://dl.acm.org/doi/10.1145/3718739)
* Li ZLiu AJia MLu YZhang TSun CZhang DLi X(2025)Gestura: A LVLM-Powered System Bridging Motion and Semantics for Real-Time Free-Form Gesture UnderstandingProceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies10.1145/3770709**9**:4(1-29)Online publication date: 2-Dec-2025
[https://dl.acm.org/doi/10.1145/3770709](https://dl.acm.org/doi/10.1145/3770709)
* Bao EPerez AParapar JCha MPark CPark NYang CBasu Roy SLi JKamps JShin KHooi BHe L(2025)ReDSM5: A Reddit Dataset for DSM-5 Depression DetectionProceedings of the 34th ACM International Conference on Information and Knowledge Management10.1145/3746252.3761610(6323-6327)Online publication date: 10-Nov-2025
[https://dl.acm.org/doi/10.1145/3746252.3761610](https://dl.acm.org/doi/10.1145/3746252.3761610)
* [Show More Cited By](javascript:void(0))
## Index Terms
1. Chain-of-thought prompting elicits reasoning in large language models
1. [Computing methodologies](https://dl.acm.org/topic/ccs2012/10010147?ContentGroupKey=10.5555/3600270&amp;expand=all)
1. [Artificial intelligence](https://dl.acm.org/topic/ccs2012/10010147.10010178?ContentGroupKey=10.5555/3600270&amp;expand=all)
1. [Knowledge representation and reasoning](https://dl.acm.org/topic/ccs2012/10010147.10010178.10010187?ContentGroupKey=10.5555/3600270&amp;expand=all)
1. [Machine learning](https://dl.acm.org/topic/ccs2012/10010147.10010257?ContentGroupKey=10.5555/3600270&amp;expand=all)
Index terms have been assigned to the content through auto-classification.
## Recommendations
* [### Tools for Thought: The History and Future of Mind-Expanding Technology
](https://dl.acm.org/doi/10.5555/518654)
[Read More](https://dl.acm.org/doi/10.5555/518654)
* [### Computers And Thought
](https://dl.acm.org/doi/10.5555/2341131)
[Read More](https://dl.acm.org/doi/10.5555/2341131)
* [### Computers and Thought
](https://dl.acm.org/doi/10.5555/601134)
[Read More](https://dl.acm.org/doi/10.5555/601134)
## Comments
[](https://dl.acm.org/doi/10.5555/3600270.3602070#disqus_thread)
#### Affiliations
![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)
JasonWei
Google Research, Brain Team
[View Profile](https://dl.acm.org/profile/99661014643)
![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)
XuezhiWang
Google Research, Brain Team
[View Profile](https://dl.acm.org/profile/99661153177)
![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)
DaleSchuurmans
Google Research, Brain Team
[View Profile](https://dl.acm.org/profile/81100182569)
![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)
MaartenBosma
Google Research, Brain Team
[View Profile](https://dl.acm.org/profile/99661130076)
![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)
BrianIchter
Google Research, Brain Team
[View Profile](https://dl.acm.org/profile/99659246572)
![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)
FeiXia
Google Research, Brain Team
[View Profile](https://dl.acm.org/profile/99661150524)
![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)
Ed H.Chi
Google Research, Brain Team
[View Profile](https://dl.acm.org/profile/99661014410)
![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)
Quoc V.Le
Google Research, Brain Team
[View Profile](https://dl.acm.org/profile/99659364049)
![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)
DennyZhou
Google Research, Brain Team
[View Profile](https://dl.acm.org/profile/81329493422)
FiguresTables
**Close figure viewer
**Back to article
Figure title goes here
****Change zoom level
[**Go to figure location within the article](#f1)
[**Download figure](#)
**Toggle share panel
Share on social media
**Toggle information panel
**
**
**
**
All figuresAll tables
**xrefBack.goTo
**xrefBack.goTo
**
* **Request permissionsExpand All
**Collapse
**Expand Table
Show all references
SHOW ALL BOOKS
[Authors Info &amp; Affiliations](#tab-contributors)
[View Table of Contents](https://dl.acm.org/doi/proceedings/10.5555/3600270)
**Your Search Results Download Request**
We are preparing your search results for download ...
We will inform you here when the file is ready.
[Download now!](#)
**Your Search Results Download Request**
Your file of search results citations is now ready.
[Download now!](#)
**Your Search Results Download Request**
Your search export query has expired. Please try again.
