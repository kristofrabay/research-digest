# Smart Chunking & Embeddings for RAG

**URL:** https://dev.to/ashokan/smart-chunking-embeddings-for-rag-4ok
**Published:** 2025-11-07T14:42:43.000Z

---

## Summary

This page provides a deep dive into **Chunking Strategies** and **Embedding Techniques** for building Retrieval-Augmented Generation (RAG) systems, using **Qdrant** as the vector database.

**Key topics covered:**

*   **Chunking Strategies:** Discusses various methods including Fixed-size + overlap, Recursive, Document-structure–aware, Sentence-window, Semantic chunking, and Hierarchical chunking, emphasizing that good chunking is crucial for RAG success.
*   **Embedding Techniques:** Reviews modern embedding models (like BGE-M3, Nomic Embed, Voyage) and discusses task-aware prompting (e.g., Nomic's prefixes) and hybrid-ready models.
*   **Vector Databases (Qdrant):** Details Qdrant's capabilities, including support for dense vectors, **sparse vectors**, and **hybrid search** (combining dense and sparse retrieval). It provides quickstarts using FastEmbed integration and manual setup.
*   **RAG Architectures/Components:** The visual overview and subsequent sections cover the flow: Chunking $\rightarrow$ Embedding $\rightarrow$ Indexing (Qdrant) $\rightarrow$ **Hybrid Retrieval** $\rightarrow$ **Reranking** (optional, e.g., with ColBERT) $\rightarrow$ LLM.
*   **RAG Alternatives/Enhancements:** Hybrid search (dense + sparse) is presented as a robust pattern, and reranking is highlighted as an impactful step after initial retrieval.

The page directly addresses **Vector databases**, **embeddings (new efficient models)**, **rerankers**, **RAG architectures**, **hybrid search**, and **chunking strategies**. It does not explicitly detail "RAG alternatives" beyond the hybrid approach, nor does it focus on new "efficient models" beyond listing current strong performers.

---

## Full Content

Smart Chunking &amp; Embeddings for RAG - DEV Community
[Skip to content](#main-content)
Navigation menu[![DEV Community](https://media2.dev.to/dynamic/image/quality=100/https://dev-to-uploads.s3.amazonaws.com/uploads/logos/resized_logo_UQww2soKuUsjaOGNB38o.png)](https://dev.to/)
Search[Powered by AlgoliaSearch](https://www.algolia.com/developers/?utm_source=devto&utm_medium=referral)
[Log in](https://dev.to/enter?signup_subforem=1)[Create account](https://dev.to/enter?signup_subforem=1&amp;state=new-user)
## DEV Community
Close
![](https://assets.dev.to/assets/heart-plus-active-9ea3b22f2bc311281db911d416166c5f430636e76b15cd5df6b3b841d830eefa.svg)Add reaction
![](https://assets.dev.to/assets/sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)Like![](https://assets.dev.to/assets/multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)Unicorn![](https://assets.dev.to/assets/exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)Exploding Head![](https://assets.dev.to/assets/raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)Raised Hands![](https://assets.dev.to/assets/fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)Fire
Jump to CommentsSaveBoost
More...
Copy linkCopy link
Copied to Clipboard
[Share to X]()[Share to LinkedIn]()[Share to Facebook](https://www.facebook.com/sharer.php?u=https://dev.to/ashokan/smart-chunking-embeddings-for-rag-4ok)[Share to Mastodon](https://toot.kytta.dev/?text=https://dev.to/ashokan/smart-chunking-embeddings-for-rag-4ok)
[Share Post via...](#)[Report Abuse](https://dev.to/report-abuse)
[![Cover image for Smart Chunking &amp; Embeddings for RAG](https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdf8e22s314zvrhdfoigz.png)](https://media2.dev.to/dynamic/image/width=1000,height=420,fit=cover,gravity=auto,format=auto/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/df8e22s314zvrhdfoigz.png)
[![Ashok Nagaraj](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F805226%2F9ae1cd5e-15d3-4782-943f-19cdcee80f37.jpg)](https://dev.to/ashokan)
[Ashok Nagaraj](https://dev.to/ashokan)
Posted onNov 7
# Smart Chunking &amp; Embeddings for RAG
[#rag](https://dev.to/t/rag)
#### [](#from-raw-docs-to-retrieval-gold-a-deep-dive-into-chunking-strategies-amp-embedding-techniques-with-qdrant)From Raw Docs to Retrieval Gold: A Deep Dive into**Chunking Strategies**&amp;**Embedding Techniques**(with**Qdrant**)
> **> TL;DR
**> : Great RAG systems don’t start at the vector DB—they start at *> chunking
*> . This post walks through when and how to chunk, how to choose and generate embeddings, and how to index/search in **> Qdrant
**> with dense, sparse, and hybrid retrieval. It includes runnable code, diagrams, and sample chunking illustrations you can paste directly into Markdown.
> ## [](#table-of-contents)Table of Contents
1. [Why chunking matters](#why-chunking-matters)
2. [Chunking strategies](#chunking-strategies)
* [Fixed-size + overlap](#1-fixed-size-window-tokens-or-characters--overlap)
* [Recursive](#2-recursive-split-paragraph--sentence--word)
* [Document-structure–aware](#3-document-structureaware-split-markdownhtmljson)
* [Sentence-window](#4-sentence-window-retrieval)
* [Semantic chunking](#5-semantic-chunking-boundary-by-meaning-not-characters)
* [Hierarchical](#6-hierarchical-chunking-multi-level-nodes)
* [Choosing sizes &amp; overlaps](#choosing-chunk-sizes--overlaps)
* [Embedding techniques](#embedding-techniques)
* [Model choices (2024–2025)](#picking-an-embedding-model-20242025-snapshot)
* [Task-aware prompts](#task-aware-prompting-for-embeddings)
* [Hybrid-ready models](#hybrid-ready-models)
* [Qdrant as the vector DB](#qdrant-as-the-vector-db)
* [Quickstart (FastEmbed)](#a-batteries-included-quickstart-fastembed-via-qdrant-client)
* [Manual control](#b-manual-control-your-own-embeddings--payload-schema)
* [Hybrid retrieval](#c-hybrid-retrieval-with-qdrant-dense--sparse)
* [Reranking](#d-reranking-optional-but-often-impactful)
* [End-to-end example](#part-iv--end-to-end-example-chunk--embed--qdrant--hybrid-query)
* [Evaluation &amp; tuning](#part-vi--evaluation--tuning)
* [Practical guidance](#part-vii--practical-guidance-battle-tested)
* [Further reading &amp; references](#further-reading--references)
## [](#why-chunking-matters)Why chunking matters
Long contexts are seductive, but LLMs still show**primacy/recency bias**and degrade when key facts live in the*middle*of long inputs ("lost in the middle"). Thoughtful chunking with overlap keeps the right facts adjacent at retrieval-time and improves end-to-end accuracy and latency.[Liu et al., 2024](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long).
Modern RAG stacks pair**well-formed chunks**with**strong embeddings**and a**vector DB**that supports**dense + sparse**retrieval and**reranking**.**Qdrant**provides production-grade vectors, filters, payloads, and hybrid retrieval in one place. See the Qdrant README and Payload docs.[[Qdrant README]](https://github.com/qdrant/qdrant/blob/master/README.md)[[Payload docs]](https://qdrant.tech/documentation/concepts/payload/).
## [](#visual-overview)Visual overview
```
`flowchart LR
A[Raw Documents] --&gt;&gt; B[Parse &amp;&amp; Clean]
B --&gt;&gt; C[Chunking Strategy\\n(Fixed / Recursive / Semantic / Hierarchical / Sentence-window)]
C --&gt;&gt; D[Embedder(s)\\nBGE-M3 / Nomic / Voyage]
D --&gt;&gt; E[Qdrant Index\\n(dense + sparse vectors,\\npayload)]
E --&gt;&gt; F[Hybrid Retrieval\\n(dense ⊕sparse)]
F --&gt;&gt; G[Reranker (optional)\\n(e.g., ColBERT)]
G --&gt;&gt; H[LLM + Prompt\\n(answers, grounded)]`
```
Enter fullscreen modeExit fullscreen mode
Qdrant supports dense vectors,**sparse vectors**, and**hybrid**workflows; reranking with late interaction (e.g., ColBERT) is a documented pattern.[[Sparse vectors in Qdrant]](https://dev.to/qdrant/sparse-vectors-in-qdrant-pure-vector-based-hybrid-search-3j64)[[Hybrid tutorial]](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/).
## [](#chunking-strategies)Chunking strategies
Below are practical chunking strategies you can mix-and-match. Each section includes**sample text**,**what the chunks look like**, and**code**where useful.
### [](#1-fixedsize-window-tokens-or-characters-overlap)1) Fixed-size window (tokens or characters) + overlap
* **When**: fast baselines, logs, transcripts.
* **Why**: predictable chunk lengths, simple to implement.
* **Risk**: can split sentences mid-thought; consider overlap to preserve context.
**Sample text**(we’ll reuse this):
```
`[Doc] The GPU kernel uses tiling to reduce global memory access. Block-level synchronization is required. See Algorithm 2 for warp-level primitives.`
```
Enter fullscreen modeExit fullscreen mode
**Fixed-size (≈ 80 chars) with 20-char overlap**
```
`Chunk 1:
"The GPU kernel uses tiling to reduce global memory access. Block-level"
Chunk 2:
"access. Block-level synchronization is required. See Algorithm 2 for"
Chunk 3:
"See Algorithm 2 for warp-level primitives."`
```
Enter fullscreen modeExit fullscreen mode
**Code (LangChain)**
```
`fromlangchain\_text\_splittersimportCharacterTextSplittersplitter=CharacterTextSplitter.from\_tiktoken\_encoder(encoding\_name="cl100k\_base",chunk\_size=80,chunk\_overlap=20)chunks=splitter.split\_text("""The GPU kernel uses tiling to reduce global memory access. Block-level synchronization is required. See Algorithm 2 for warp-level primitives.""")fori,cinenumerate(chunks,1):print(f"Chunk{i}:\\n{c}\\n")`
```
Enter fullscreen modeExit fullscreen mode
LangChain’s token/character splitters are standard, and**RecursiveCharacterTextSplitter**is the recommended default for general text.[[LangChain splitters]](https://docs.langchain.com/oss/python/integrations/splitters/index).
### [](#2-recursive-split-paragraph-→-sentence-→-word)2)**Recursive**split (paragraph →sentence →word)
* **When**: prose, docs, Markdown, HTML.
* **Why**: preserves natural boundaries first; falls back only when needed.
* **How**: tries`["\\n\\n", "\\n", " ", ""]`in order to keep larger units intact.[[Recursive splitter]](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter)
**Code (Recursive + Markdown-aware)**
```
`fromlangchain\_text\_splittersimportRecursiveCharacterTextSplittersplitter=RecursiveCharacterTextSplitter(chunk\_size=120,chunk\_overlap=30,is\_separator\_regex=False)chunks=splitter.split\_text(long\_markdown\_string)`
```
Enter fullscreen modeExit fullscreen mode
### [](#3-documentstructureaware-split-markdownhtmljson)3)**Document-structure–aware**split (Markdown/HTML/JSON)
* **When**: knowledge bases, docs, webpages, specs.
* **Why**: avoid chopping headings, lists, code blocks; align chunk meaning to structure.
LangChain provides Markdown/HTML splitters;**LlamaIndex**offers file-based node parsers (e.g.,**MarkdownNodeParser**,**HTMLNodeParser**).[[LangChain splitters]](https://docs.langchain.com/oss/python/integrations/splitters/index)[[LlamaIndex node parsers]](https://developers.llamaindex.ai/python/framework/module_guides/loading/node_parsers/modules/)
**Code (LlamaIndex HTML)**
```
`fromllama\_index.core.node\_parserimportHTMLNodeParserparser=HTMLNodeParser(tags=["h1","h2","p","li","code"])nodes=parser.get\_nodes\_from\_documents(html\_docs)`
```
Enter fullscreen modeExit fullscreen mode
### [](#4-sentencewindow-retrieval)4)**Sentence-window**retrieval
* **When**: you want precise grounding while preserving local context.
* **Why**: index at**sentence**granularity, but expand context during retrieval by adding a window of neighboring sentences.
**Code (LlamaIndex SentenceWindow)**
```
`fromllama\_index.core.node\_parserimportSentenceWindowNodeParserparser=SentenceWindowNodeParser(window\_size=2)# ±2 sentences of contextnodes=parser.get\_nodes\_from\_documents(documents)`
```
Enter fullscreen modeExit fullscreen mode
LlamaIndex provides**SentenceWindowNodeParser**specifically for this pattern.[[LlamaIndex node parsers]](https://developers.llamaindex.ai/python/framework/module_guides/loading/node_parsers/modules/)
### [](#5-semantic-chunking-boundary-by-meaning-not-characters)5)**Semantic chunking**(boundary by*meaning*, not characters)
* **When**: long-form text with shifting topics (papers, handbooks).
* **Why**: create chunk boundaries where semantic similarity between consecutive sentences*drops*.
A practical recipe: embed each sentence, compute cosine distance, start a new chunk when distance exceeds a threshold (e.g., 95th percentile).**LlamaIndex**provides a pack implementing this (“semantic chunking” popularized by Greg Kamradt).[[LlamaIndex semantic chunking pack]](https://pypi.org/project/llama-index-packs-node-parser-semantic-chunking/)
**Illustration (semantic boundaries)**
```
`S1: Intro to tiling ─┐S2: Memory coalescing ─┤(high similarity →same chunk)
S3: Warp shuffles ─┘S4: Runtime flags ←[semantic drop: new topic →new chunk]
S5: Env setup`
```
Enter fullscreen modeExit fullscreen mode
### [](#6-hierarchical-chunking-multilevel-nodes)6)**Hierarchical chunking**(multi-level nodes)
* **When**: large manuals/books; need both overview and details.
* **Why**: index multiple granularities (e.g., 2k/512/128 tokens) and let the retriever blend them.
**Code (LlamaIndex Hierarchical)**
```
`fromllama\_index.core.node\_parser.relational.hierarchicalimportHierarchicalNodeParserparser=HierarchicalNodeParser.from\_defaults(chunk\_sizes=[2048,512,128],# parent →child →grandchildchunk\_overlap=40)nodes=parser.get\_nodes\_from\_documents(documents)`
```
Enter fullscreen modeExit fullscreen mode
Hierarchical node parsers create a*flat list*of nodes but preserve parent/child relationships—ideal for multi-granularity retrieval.[[LlamaIndex hierarchical]](https://developers.llamaindex.ai/python/framework-api-reference/node_parsers/hierarchical/)
### [](#choosing-chunk-sizes-amp-overlaps)Choosing chunk sizes &amp; overlaps
* Start with**512–1,024 tokens**and**10–20% overlap**for prose; increase overlap for dense technical content and code.
* Keep chunks**semantically coherent**(recursive/semantic methods) to mitigate “lost in the middle.”[[Recursive splitter]](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter)[[Lost in the Middle]](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long)
## [](#embedding-techniques)Embedding techniques
### [](#picking-an-embedding-model-20242025-snapshot)Picking an embedding model (2024–2025 snapshot)
* **BGE-M3**(open, multilingual, can output dense + sparse + multi-vector; long-text up to \~8k tokens). Strong hybrid story.[[HF card]](https://huggingface.co/BAAI/bge-m3)[[arXiv]](https://arxiv.org/abs/2402.03216)
* **Nomic Embed (v1.5 / v2 MoE)**(open, long-context, Matryoshka—truncate dims without retraining; task-prefix prompts).[[HF v1.5]](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)[[Tech report]](https://arxiv.org/abs/2402.01613)[[V2 MoE overview]](https://simonwillison.net/2025/Feb/12/nomic-embed-text-v2/)
* **Voyage-3/3.5(-lite)**(hosted API, strong multilingual retrieval; domain variants for code/finance/law).[[Voyage docs]](https://docs.voyageai.com/docs/embeddings)
* Benchmark*by task*, not just overall averages; see**MTEB**(retrieval/STSb are most relevant for RAG).[[MTEB leaderboard]](https://huggingface.co/spaces/mteb/leaderboard)> **> Tip
**> : Retrieval usually uses **> cosine
**> on **> L2-normalized
**> vectors (most libraries handle this). Validate that your client and DB use the same similarity metric (e.g., **> COSINE
**> in Qdrant). [> [Qdrant client]
](https://python-client.qdrant.tech/)
> ### [](#taskaware-prompting-for-embeddings)Task-aware prompting for embeddings
Some models require**instruction prefixes**to ensure query/document embeddings live in compatible subspaces. For**Nomic**:
```
`fromsentence\_transformersimportSentenceTransformermodel=SentenceTransformer("nomic-ai/nomic-embed-text-v1.5",trust\_remote\_code=True)doc\_emb=model.encode(["search\_document: GPU tiling improves mem access"])qry\_emb=model.encode(["search\_query: What improves memory access on GPUs?"])`
```
Enter fullscreen modeExit fullscreen mode
These**search\_document / search\_query**prefixes are part of the model spec.[[HF v1.5]](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)
### [](#hybridready-models)Hybrid-ready models
**BGE-M3**can produce both**dense**and**sparse**signals—useful if you plan to feed**Qdrant’s hybrid**flow (dense + sparse).[[BGE-M3 docs]](https://bge-model.com/bge/bge_m3.html)
## [](#qdrant-as-the-vector-db)Qdrant as the Vector DB
### [](#why-qdrant)Why Qdrant?
* **Vectors + payloads**(schemaless JSON metadata + filters).[[Payload docs]](https://qdrant.tech/documentation/concepts/payload/)
* **Sparse vectors**and**hybrid search**patterns (dense ⊕sparse; reranking with ColBERT).[[Sparse vectors]](https://dev.to/qdrant/sparse-vectors-in-qdrant-pure-vector-based-hybrid-search-3j64)[[Hybrid tutorial]](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/)
* **Client ergonomics**: Python client supports**local`:memory:`**mode,**FastEmbed integration**(`client.add`/`client.query`), async, and Cloud.[[Qdrant Quickstart]](https://python-client.qdrant.tech/quickstart)[[PyPI]](https://pypi.org/project/qdrant-client/)
There’s also active work discussing hybrid algorithms (e.g.,**BM42**) and how IDF/statistics interplay with sparsity.[[BM42 news]](https://blocksandfiles.com/2024/07/02/qdrant-launches-combined-vector-and-keyword-search-for-rag-and-ai-apps/)[[BM25/IDF discussion]](https://github.com/qdrant/qdrant/issues/6690)
### [](#a-batteriesincluded-quickstart-fastembed-via-qdrant-client)A. “Batteries-included” quickstart (FastEmbed via Qdrant client)
**Install &amp; run Qdrant**
```
`docker run-p6333:6333 qdrant/qdrant:latest`
```
Enter fullscreen modeExit fullscreen mode
**Python client with auto-embedding and simplified APIs**
```
`# pip install "qdrant-client[fastembed]"fromqdrant\_clientimportQdrantClientclient=QdrantClient(":memory:")# or url="http://localhost:6333"docs=["Qdrant has LangChain integrations","Qdrant also has LlamaIndex integrations"]# Simple add →auto-embeds via FastEmbedids=client.add(collection\_name="demo\_collection",documents=docs)# Query by text directly (embeds the query under the hood)result=client.query(collection\_name="demo\_collection",query\_text="Which vector DB works with LangChain?",limit=2)print(result)`
```
Enter fullscreen modeExit fullscreen mode
This “**add / query**” path is documented in the official**Qdrant Python Client Quickstart**.[[Quickstart]](https://python-client.qdrant.tech/quickstart)
### [](#b-manual-control-your-own-embeddings-payload-schema)B. Manual control (your own embeddings + payload schema)
**Create a collection with COSINE distance**
```
`fromqdrant\_clientimportQdrantClientfromqdrant\_client.modelsimportDistance,VectorParams,PointStructclient=QdrantClient(host="localhost",port=6333)ifnotclient.collection\_exists("ml\_notes"):client.create\_collection(collection\_name="ml\_notes",vectors\_config=VectorParams(size=1024,distance=Distance.COSINE),# set to your model dim)`
```
Enter fullscreen modeExit fullscreen mode
**Upsert points with payloads**
```
`# assume `embeddings` is a list of 1024-d vectors
# and `texts` is the corresponding list of stringspoints=[PointStruct(id=i,vector=embeddings[i],payload={"doc\_id":"kernel\_guide","section":i,"text":texts[i],"tags":["gpu","tiling"]})foriinrange(len(embeddings))]client.upsert(collection\_name="ml\_notes",points=points)`
```
Enter fullscreen modeExit fullscreen mode
Qdrant’s Python client covers collection creation, upserts, searches, and filtering; payloads are**schemaless JSON**, filterable by field.[[Client docs]](https://python-client.qdrant.tech/)[[Payload]](https://qdrant.tech/documentation/concepts/payload/)
**Search (vector) with metadata filter**
```
`fromqdrant\_client.modelsimportFilter,FieldCondition,MatchValuehits=client.search(collection\_name="ml\_notes",query\_vector=query\_vec,query\_filter=Filter(must=[FieldCondition(key="tags",match=MatchValue(value="gpu"))]),limit=5)`
```
Enter fullscreen modeExit fullscreen mode
### [](#c-hybrid-retrieval-with-qdrant-dense-sparse)C. Hybrid retrieval with**Qdrant**(dense + sparse)
**Concept**: Store*both*dense vectors and sparse vectors per point; retrieve with a hybrid pipeline (then optionally rerank). Qdrant documents sparse vectors and shows**reranking**patterns;**LlamaIndex**exposes a simple`enable\_hybrid=True`switch powered by fastembed (e.g.,**Qdrant/bm25**or SPLADE).[[Sparse vectors]](https://dev.to/qdrant/sparse-vectors-in-qdrant-pure-vector-based-hybrid-search-3j64)[[Hybrid tutorial]](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/)[[LlamaIndex Qdrant hybrid]](https://developers.llamaindex.ai/python/examples/vector_stores/qdrant_hybrid/)
**Code (LlamaIndex + Qdrant hybrid)**
```
`# pip install -U llama-index llama-index-vector-stores-qdrant fastembed qdrant-clientfromllama\_index.coreimportSimpleDirectoryReader,VectorStoreIndex,StorageContext,Settingsfromllama\_index.vector\_stores.qdrantimportQdrantVectorStorefromqdrant\_clientimportQdrantClient,AsyncQdrantClientdocs=SimpleDirectoryReader("./data").load\_data()client=QdrantClient(host="localhost",port=6333)aclient=AsyncQdrantClient(host="localhost",port=6333)vector\_store=QdrantVectorStore("gpu\_notes",client=client,aclient=aclient,enable\_hybrid=True,# &lt;-- dense + sparsefastembed\_sparse\_model="Qdrant/bm25",# or "prithvida/Splade\_PP\_en\_v1"batch\_size=64,)storage\_context=StorageContext.from\_defaults(vector\_store=vector\_store)Settings.chunk\_size=512index=VectorStoreIndex.from\_documents(docs,storage\_context=storage\_context)# Query hybrid: specify final k and dense/sparse fused candidates under the hoodretriever=index.as\_retriever(similarity\_top\_k=10)nodes=retriever.retrieve("warp-level primitives vs block-level sync")forninnodes:print(n.metadata.get("source"),n.score)`
```
Enter fullscreen modeExit fullscreen mode
This setup and parameters are demonstrated in**LlamaIndex’s Qdrant Hybrid**example.[[LlamaIndex hybrid]](https://developers.llamaindex.ai/python/examples/vector_stores/qdrant_hybrid/)
> **> Note
**> : Qdrant docs also describe sparse vectors’ JSON shape and their role in hybrid pipelines; pairing dense semantics with sparse exact-term matching—then **> reranking
**> —is a robust recipe. [> [Qdrant course excerpt]
](https://qdrant.tech/course/essentials/day-1/embedding-models/)> [> [Hybrid tutorial]
](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/)
> ### [](#d-reranking-optional-but-often-impactful)D. Reranking (optional but often impactful)
After retrieving top-N (e.g., N=50) via hybrid, rerank with**ColBERT**or a cross-encoder for final top-k. Qdrant’s advanced tutorial covers hybrid + reranking architecture and code paths.[[Hybrid + Reranking]](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/)
## [](#part-iv-endtoend-example-chunk-→-embed-→-qdrant-→-hybrid-query)Part IV —End-to-end example:**Chunk →Embed →Qdrant →Hybrid Query**
### [](#1-chunk-recursive-overlap)1) Chunk (Recursive) + overlap
```
`fromlangchain\_text\_splittersimportRecursiveCharacterTextSplittertext="""The GPU kernel uses tiling to reduce global memory access.
Block-level synchronization is required. See Algorithm 2 for warp-level primitives."""splitter=RecursiveCharacterTextSplitter(chunk\_size=120,chunk\_overlap=30)chunks=splitter.split\_text(text)`
```
Enter fullscreen modeExit fullscreen mode
### [](#2-embed-choose-one-model)2) Embed (choose one model)
**Option A: BGE-M3 (dense)**
```
`fromsentence\_transformersimportSentenceTransformerbge=SentenceTransformer("BAAI/bge-m3")vecs=bge.encode(chunks,normalize\_embeddings=True)`
```
Enter fullscreen modeExit fullscreen mode
**Option B: Nomic Embed v1.5 (task prefixes + Matryoshka)**
```
`fromsentence\_transformersimportSentenceTransformernomic=SentenceTransformer("nomic-ai/nomic-embed-text-v1.5",trust\_remote\_code=True)vecs=nomic.encode([f"search\_document:{c}"forcinchunks])`
```
Enter fullscreen modeExit fullscreen mode
### [](#3-index-in-qdrant-manual)3) Index in Qdrant (manual)
```
`fromqdrant\_clientimportQdrantClientfromqdrant\_client.modelsimportVectorParams,Distance,PointStructclient=QdrantClient(url="http://localhost:6333")ifnotclient.collection\_exists("chunks\_demo"):client.create\_collection(collection\_name="chunks\_demo",vectors\_config=VectorParams(size=len(vecs[0]),distance=Distance.COSINE),)points=[PointStruct(id=i,vector=vecs[i],payload={"text":chunks[i],"order":i})foriinrange(len(chunks))]client.upsert(collection\_name="chunks\_demo",points=points)`
```
Enter fullscreen modeExit fullscreen mode
### [](#4-query-dense)4) Query (dense)
```
`qry=nomic.encode(["search\_query: How is global memory access reduced?"])[0]hits=client.search(collection\_name="chunks\_demo",query\_vector=qry,limit=3)forhinhits:print(h.payload["text"],"→ score:",h.score)`
```
Enter fullscreen modeExit fullscreen mode
### [](#5-hybrid-dense-⊕-sparse-via-llamaindex-wrapper)5) Hybrid (dense ⊕sparse) via LlamaIndex wrapper
For production hybrid, prefer the**Qdrant + LlamaIndex**path shown earlier—enables SPLADE/BM25 sparse vectors automatically and combines them with dense vectors before (optional) reranking.[[LlamaIndex Qdrant hybrid]](https://developers.llamaindex.ai/python/examples/vector_stores/qdrant_hybrid/)
## [](#part-v-evaluation-amp-tuning)Part V —Evaluation &amp; tuning
* **A/B test**chunk sizes/overlaps using retrieval metrics (**Recall@k**,**MRR**) and downstream QA accuracy.
* Reference datasets from**BEIR/MTEB**for repeatable measurement; focus on*retrieval*and*STS*categories to reflect RAG performance.[[MTEB]](https://huggingface.co/spaces/mteb/leaderboard)
* Watch for**middle-of-context**degradation; shorter, semantically-tight chunks often help.[[Lost in the Middle]](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long)
## [](#part-vii-practical-guidance-battletested)Part VII —Practical guidance (battle-tested)
1. **Start simple**: Recursive splitter, 512–1,024 tokens, 10–20% overlap; adjust per domain.[[Recursive splitter]](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter)
2. **Use hybrid retrieval**for noisy queries/long-tail terms (dense semantics + sparse keywords). Qdrant + LlamaIndex makes this 1-line (`enable\_hybrid=True`).[[LlamaIndex hybrid]](https://developers.llamaindex.ai/python/examples/vector_stores/qdrant_hybrid/)
3. **Normalize embeddings**and match distance functions (e.g., COSINE end-to-end).[[Qdrant client]](https://python-client.qdrant.tech/)
4. **Task-aware prompting**(e.g., Nomic’s prefixes) to avoid query–doc space drift.[[Nomic v1.5]](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)
5. **Payloads matter**: store source, span offsets, titles, section IDs for robust filtering &amp; citations. Qdrant payloads are schemaless and filterable.[[Payload docs]](https://qdrant.tech/documentation/concepts/payload/)
6. **Rerank**top candidates when quality trumps latency (ColBERT/cross-encoder).[[Hybrid + Reranking]](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/)
7. **Monitor updates**to sparse/hybrid algorithms (e.g., BM42) as the ecosystem evolves.[[BM42 news]](https://blocksandfiles.com/2024/07/02/qdrant-launches-combined-vector-and-keyword-search-for-rag-and-ai-apps/)
## [](#further-reading-amp-references)Further reading &amp; references
* **Qdrant Python Client**(Quickstart; FastEmbed`add/query`):[https://python-client.qdrant.tech/quickstart](https://python-client.qdrant.tech/quickstart)
* **Qdrant payloads**:[https://qdrant.tech/documentation/concepts/payload/](https://qdrant.tech/documentation/concepts/payload/)
* **Sparse vectors / hybrid with reranking**:
* Qdrant Hybrid Tutorial:[https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/)
* Sparse vectors intro (Qdrant):[https://dev.to/qdrant/sparse-vectors-in-qdrant-pure-vector-based-hybrid-search-3j64](https://dev.to/qdrant/sparse-vectors-in-qdrant-pure-vector-based-hybrid-search-3j64)
* **LlamaIndex**:
* Qdrant Hybrid:[https://developers.llamaindex.ai/python/examples/vector\_stores/qdrant\_hybrid/](https://developers.llamaindex.ai/python/examples/vector_stores/qdrant_hybrid/)
* Node Parsers (HTML/Sentence/Hierarchical):[https://developers.llamaindex.ai/python/framework/module\_guides/loading/node\_parsers/modules/](https://developers.llamaindex.ai/python/framework/module_guides/loading/node_parsers/modules/)
* Hierarchical API:[https://developers.llamaindex.ai/python/framework-api-reference/node\_parsers/hierarchical/](https://developers.llamaindex.ai/python/framework-api-reference/node_parsers/hierarchical/)
* **LangChain splitters**(recursive, token/character, Markdown/HTML):
* Overview:[https://docs.langchain.com/oss/python/integrations/splitters/index](https://docs.langchain.com/oss/python/integrations/splitters/index)
* Recursive:[https://docs.langchain.com/oss/python/integrations/splitters/recursive\_text\_splitter](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter)
* **Lost in the Middle**(motivation for chunk sizes/overlap):[](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long)[https://direct.mit.edu/tacl/article/doi/10.1162/tacl\_a\_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long)
* **Embedding models**:
* **BGE-M3**:[https://huggingface.co/BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3); paper[https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216); docs[https://bge-model.com/bge/bge\_m3.html](https://bge-model.com/bge/bge_m3.html)
* **Nomic Embed**:[https://huggingface.co/nomic-ai/nomic-embed-text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5); report[https://arxiv.org/abs/2402.01613](https://arxiv.org/abs/2402.01613); v2 MoE overview[https://simonwillison.net/2025/Feb/12/nomic-embed-text-v2/](https://simonwillison.net/2025/Feb/12/nomic-embed-text-v2/)
* **Voyage embeddings**:[https://docs.voyageai.com/docs/embeddings](https://docs.voyageai.com/docs/embeddings); SDK[https://github.com/voyage-ai/voyageai-python](https://github.com/voyage-ai/voyageai-python)
* **MTEB leaderboard**:[](https://huggingface.co/spaces/mteb/leaderboard)[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
* **BM42 news**:[](https://blocksandfiles.com/2024/07/02/qdrant-launches-combined-vector-and-keyword-search-for-rag-and-ai-apps/)[https://blocksandfiles.com/2024/07/02/qdrant-launches-combined-vector-and-keyword-search-for-rag-and-ai-apps/](https://blocksandfiles.com/2024/07/02/qdrant-launches-combined-vector-and-keyword-search-for-rag-and-ai-apps/)
## Top comments(0)
Subscribe
![pic](https://media2.dev.to/dynamic/image/width=256,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)
PersonalTrusted User
[Create template](https://dev.to/settings/response-templates)
Templates let you quickly answer FAQs or store snippets for re-use.
SubmitPreview[Dismiss](https://dev.to/404.html)
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's[permalink](#).
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or[reporting abuse](https://dev.to/report-abuse)
[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F805226%2F9ae1cd5e-15d3-4782-943f-19cdcee80f37.jpg)Ashok Nagaraj](https://dev.to/ashokan)
Follow
Software developer at Cisco working on Kubernetes and related cloud technologies
* Location
Bangalore
* Education
Engineering
* Work
Cisco
* Joined
Jan 28, 2022
### More from[Ashok Nagaraj](https://dev.to/ashokan)
[From PDFs to Markdown
#rag#parser#etl
](https://dev.to/ashokan/from-pdfs-to-markdown-evaluating-document-parsers-for-air-gapped-rag-systems-58eh)
![DEV Community](https://media2.dev.to/dynamic/image/width=190,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)
We&#39;re a place where coders share, stay up-to-date and grow their careers.
[Log in](https://dev.to/enter?signup_subforem=1)[Create account](https://dev.to/enter?signup_subforem=1&amp;state=new-user)
![](https://assets.dev.to/assets/sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)![](https://assets.dev.to/assets/multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)![](https://assets.dev.to/assets/exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)![](https://assets.dev.to/assets/raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)![](https://assets.dev.to/assets/fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)
