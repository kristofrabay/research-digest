# Reasoning with Language Model is Planning with World Model - ACL Anthology

**URL:** https://aclanthology.org/2023.emnlp-main.507/
**Published:** 2025-12-10T00:00:00.000Z

---

## Summary

The webpage describes a paper titled "Reasoning with Language Model is Planning with World Model," which introduces a framework called **Reasoning via Planning (RAP)**.

This framework addresses the limitations of Large Language Models (LLMs) in complex reasoning tasks (like plan generation, math, and logic) that stem from their lack of an internal "world model" for predicting future states and simulating outcomes.

RAP overcomes this by repurposing the LLM to act as **both a world model and a reasoning agent**. It incorporates a planning algorithm based on **Monte Carlo Tree Search (MCTS)** to strategically explore the reasoning space, balancing exploration and exploitation to find high-reward reasoning paths.

The paper notes that LLMs show remarkable reasoning capabilities with **Chain-of-Thought (CoT)** prompting, but RAP is shown to be superior to CoT and other baselines in challenging reasoning problems.

---

## Full Content

Reasoning with Language Model is Planning with World Model - ACL Anthology
## [Reasoning with Language Model is Planning with World Model](https://aclanthology.org/2023.emnlp-main.507.pdf)
[Shibo Hao](https://aclanthology.org/people/shibo-hao/),[Yi Gu](https://aclanthology.org/people/yi-gu/),[Haodi Ma](https://aclanthology.org/people/haodi-ma/),[Joshua Hong](https://aclanthology.org/people/joshua-hong/),[Zhen Wang](https://aclanthology.org/people/zhen-wang/),[Daisy Wang](https://aclanthology.org/people/daisy-wang/),[Zhiting Hu](https://aclanthology.org/people/zhiting-hu/)
##### Correct Metadata for
&#215;
**Important**: The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See[our corrections guidelines](https://aclanthology.org/info/corrections/)if you need to change the PDF.
TitleAdjust the title. Retain tags such as &lt;&lt;fixed-case\>.
AuthorsAdjust author names and order to match the PDF.
Add Author
AbstractCorrect abstract if needed. Retain XML formatting tags such as &lt;&lt;tex-math\>.
Verification against PDFEnsure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult[the PDF](#).)
[![]()](#)
Authors concatenated from the text boxes above:
ALL author names match the snapshot above—including middle initials, hyphens, and accents.
Submit
##### Abstract
Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs’ absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monte Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, properly balancing exploration v.s. exploitation to achieve a high-reward reasoning path efficiently. We apply RAP to a variety of challenging reasoning problems, such as plan generation, math reasoning, and logical inference. Empirical results demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency, e.g., RAP on LLaMA-33B surpasses CoT on GPT-4 with 33% relative improvement in plan generation.
Anthology ID:2023.emnlp-main.507Volume:[Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing](https://aclanthology.org/volumes/2023.emnlp-main/)Month:DecemberYear:2023Address:SingaporeEditors:[Houda Bouamor](https://aclanthology.org/people/houda-bouamor/),[Juan Pino](https://aclanthology.org/people/juan-pino/),[Kalika Bali](https://aclanthology.org/people/kalika-bali/)Venue:[EMNLP](https://aclanthology.org/venues/emnlp/)SIG:Publisher:Association for Computational LinguisticsNote:Pages:8154–8173Language:URL:[https://aclanthology.org/2023.emnlp-main.507/](https://aclanthology.org/2023.emnlp-main.507/)DOI:[10.18653/v1/2023.emnlp-main.507](https://doi.org/10.18653/v1/2023.emnlp-main.507)Bibkey:**hao-etal-2023-reasoningCite (ACL):Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023.[Reasoning with Language Model is Planning with World Model](https://aclanthology.org/2023.emnlp-main.507/). In*Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 8154–8173, Singapore. Association for Computational Linguistics.**Cite (Informal):[Reasoning with Language Model is Planning with World Model](https://aclanthology.org/2023.emnlp-main.507/)(Hao et al., EMNLP 2023)**Copy Citation:**BibTeX**Markdown**MODS XML**EndnoteMore options…PDF:[https://aclanthology.org/2023.emnlp-main.507.pdf](https://aclanthology.org/2023.emnlp-main.507.pdf)Video:[**https://aclanthology.org/2023.emnlp-main.507.mp4](https://aclanthology.org/2023.emnlp-main.507.mp4)
[**PDF](https://aclanthology.org/2023.emnlp-main.507.pdf)[**Cite](#)[**Search](https://www.semanticscholar.org/search?q=Reasoning+with+Language+Model+is+Planning+with+World+Model)[**Video](https://aclanthology.org/2023.emnlp-main.507.mp4)[**Fix data](#)
##### Export citation
&#215;
* [BibTeX](#citeBibtex)
* [MODS XML](#citeMods)
* [Endnote](#citeEndnote)
* [Preformatted](#citeMarkdown)
```
@inproceedings{hao-etal-2023-reasoning,
title = &#34;&#34;Reasoning with Language Model is Planning with World Model&#34;&#34;,
author = &#34;&#34;Hao, Shibo and
Gu, Yi and
Ma, Haodi and
Hong, Joshua and
Wang, Zhen and
Wang, Daisy and
Hu, Zhiting&#34;&#34;,
editor = &#34;&#34;Bouamor, Houda and
Pino, Juan and
Bali, Kalika&#34;&#34;,
booktitle = &#34;&#34;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing&#34;&#34;,
month = dec,
year = &#34;&#34;2023&#34;&#34;,
address = &#34;&#34;Singapore&#34;&#34;,
publisher = &#34;&#34;Association for Computational Linguistics&#34;&#34;,
url = &#34;&#34;https://aclanthology.org/2023.emnlp-main.507/&#34;&#34;,
doi = &#34;&#34;10.18653/v1/2023.emnlp-main.507&#34;&#34;,
pages = &#34;&#34;8154--8173&#34;&#34;,
abstract = &#34;&#34;Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs&#39;&#39; absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monte Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, properly balancing exploration v.s. exploitation to achieve a high-reward reasoning path efficiently. We apply RAP to a variety of challenging reasoning problems, such as plan generation, math reasoning, and logical inference. Empirical results demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency, e.g., RAP on LLaMA-33B surpasses CoT on GPT-4 with 33{\\%} relative improvement in plan generation.&#34;&#34;
}
```
**Download as File**Copy to Clipboard
```
&lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt;
&lt;modsCollection xmlns=&#34;http://www.loc.gov/mods/v3&#34;&gt;
&lt;mods ID=&#34;hao-etal-2023-reasoning&#34;&gt;
&lt;titleInfo&gt;
&lt;title&gt;Reasoning with Language Model is Planning with World Model&lt;/title&gt;
&lt;/titleInfo&gt;
&lt;name type=&#34;personal&#34;&gt;
&lt;namePart type=&#34;given&#34;&gt;Shibo&lt;/namePart&gt;
&lt;namePart type=&#34;family&#34;&gt;Hao&lt;/namePart&gt;
&lt;role&gt;
&lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
&lt;/role&gt;
&lt;/name&gt;
&lt;name type=&#34;personal&#34;&gt;
&lt;namePart type=&#34;given&#34;&gt;Yi&lt;/namePart&gt;
&lt;namePart type=&#34;family&#34;&gt;Gu&lt;/namePart&gt;
&lt;role&gt;
&lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
&lt;/role&gt;
&lt;/name&gt;
&lt;name type=&#34;personal&#34;&gt;
&lt;namePart type=&#34;given&#34;&gt;Haodi&lt;/namePart&gt;
&lt;namePart type=&#34;family&#34;&gt;Ma&lt;/namePart&gt;
&lt;role&gt;
&lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
&lt;/role&gt;
&lt;/name&gt;
&lt;name type=&#34;personal&#34;&gt;
&lt;namePart type=&#34;given&#34;&gt;Joshua&lt;/namePart&gt;
&lt;namePart type=&#34;family&#34;&gt;Hong&lt;/namePart&gt;
&lt;role&gt;
&lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
&lt;/role&gt;
&lt;/name&gt;
&lt;name type=&#34;personal&#34;&gt;
&lt;namePart type=&#34;given&#34;&gt;Zhen&lt;/namePart&gt;
&lt;namePart type=&#34;family&#34;&gt;Wang&lt;/namePart&gt;
&lt;role&gt;
&lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
&lt;/role&gt;
&lt;/name&gt;
&lt;name type=&#34;personal&#34;&gt;
&lt;namePart type=&#34;given&#34;&gt;Daisy&lt;/namePart&gt;
&lt;namePart type=&#34;family&#34;&gt;Wang&lt;/namePart&gt;
&lt;role&gt;
&lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
&lt;/role&gt;
&lt;/name&gt;
&lt;name type=&#34;personal&#34;&gt;
&lt;namePart type=&#34;given&#34;&gt;Zhiting&lt;/namePart&gt;
&lt;namePart type=&#34;family&#34;&gt;Hu&lt;/namePart&gt;
&lt;role&gt;
&lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;author&lt;/roleTerm&gt;
&lt;/role&gt;
&lt;/name&gt;
&lt;originInfo&gt;
&lt;dateIssued&gt;2023-12&lt;/dateIssued&gt;
&lt;/originInfo&gt;
&lt;typeOfResource&gt;text&lt;/typeOfResource&gt;
&lt;relatedItem type=&#34;host&#34;&gt;
&lt;titleInfo&gt;
&lt;title&gt;Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing&lt;/title&gt;
&lt;/titleInfo&gt;
&lt;name type=&#34;personal&#34;&gt;
&lt;namePart type=&#34;given&#34;&gt;Houda&lt;/namePart&gt;
&lt;namePart type=&#34;family&#34;&gt;Bouamor&lt;/namePart&gt;
&lt;role&gt;
&lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt;
&lt;/role&gt;
&lt;/name&gt;
&lt;name type=&#34;personal&#34;&gt;
&lt;namePart type=&#34;given&#34;&gt;Juan&lt;/namePart&gt;
&lt;namePart type=&#34;family&#34;&gt;Pino&lt;/namePart&gt;
&lt;role&gt;
&lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt;
&lt;/role&gt;
&lt;/name&gt;
&lt;name type=&#34;personal&#34;&gt;
&lt;namePart type=&#34;given&#34;&gt;Kalika&lt;/namePart&gt;
&lt;namePart type=&#34;family&#34;&gt;Bali&lt;/namePart&gt;
&lt;role&gt;
&lt;roleTerm authority=&#34;marcrelator&#34; type=&#34;text&#34;&gt;editor&lt;/roleTerm&gt;
&lt;/role&gt;
&lt;/name&gt;
&lt;originInfo&gt;
&lt;publisher&gt;Association for Computational Linguistics&lt;/publisher&gt;
&lt;place&gt;
&lt;placeTerm type=&#34;text&#34;&gt;Singapore&lt;/placeTerm&gt;
&lt;/place&gt;
&lt;/originInfo&gt;
&lt;genre authority=&#34;marcgt&#34;&gt;conference publication&lt;/genre&gt;
&lt;/relatedItem&gt;
&lt;abstract&gt;Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs’ absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monte Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, properly balancing exploration v.s. exploitation to achieve a high-reward reasoning path efficiently. We apply RAP to a variety of challenging reasoning problems, such as plan generation, math reasoning, and logical inference. Empirical results demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency, e.g., RAP on LLaMA-33B surpasses CoT on GPT-4 with 33% relative improvement in plan generation.&lt;/abstract&gt;
&lt;identifier type=&#34;citekey&#34;&gt;hao-etal-2023-reasoning&lt;/identifier&gt;
&lt;identifier type=&#34;doi&#34;&gt;10.18653/v1/2023.emnlp-main.507&lt;/identifier&gt;
&lt;location&gt;
&lt;url&gt;https://aclanthology.org/2023.emnlp-main.507/&lt;/url&gt;
&lt;/location&gt;
&lt;part&gt;
&lt;date&gt;2023-12&lt;/date&gt;
&lt;extent unit=&#34;page&#34;&gt;
&lt;start&gt;8154&lt;/start&gt;
&lt;end&gt;8173&lt;/end&gt;
&lt;/extent&gt;
&lt;/part&gt;
&lt;/mods&gt;
&lt;/modsCollection&gt;
```
**Download as File**Copy to Clipboard
```
%0 Conference Proceedings
%T Reasoning with Language Model is Planning with World Model
%A Hao, Shibo
%A Gu, Yi
%A Ma, Haodi
%A Hong, Joshua
%A Wang, Zhen
%A Wang, Daisy
%A Hu, Zhiting
%Y Bouamor, Houda
%Y Pino, Juan
%Y Bali, Kalika
%S Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
%D 2023
%8 December
%I Association for Computational Linguistics
%C Singapore
%F hao-etal-2023-reasoning
%X Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs’ absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monte Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, properly balancing exploration v.s. exploitation to achieve a high-reward reasoning path efficiently. We apply RAP to a variety of challenging reasoning problems, such as plan generation, math reasoning, and logical inference. Empirical results demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency, e.g., RAP on LLaMA-33B surpasses CoT on GPT-4 with 33% relative improvement in plan generation.
%R 10.18653/v1/2023.emnlp-main.507
%U https://aclanthology.org/2023.emnlp-main.507/
%U https://doi.org/10.18653/v1/2023.emnlp-main.507
%P 8154-8173
```
**Download as File**Copy to Clipboard
##### Markdown (Informal)
[Reasoning with Language Model is Planning with World Model](https://aclanthology.org/2023.emnlp-main.507/) (Hao et al., EMNLP 2023)
* [Reasoning with Language Model is Planning with World Model](https://aclanthology.org/2023.emnlp-main.507/)(Hao et al., EMNLP 2023)##### ACL
* Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023.[Reasoning with Language Model is Planning with World Model](https://aclanthology.org/2023.emnlp-main.507/). In*Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 8154–8173, Singapore. Association for Computational Linguistics.
**Copy Markdown to Clipboard**Copy ACL to Clipboard
