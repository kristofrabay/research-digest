# A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning

**URL:** https://browse.arxiv.org/html/2401.02384v1
**Published:** 2024-01-04T00:00:00.000Z

---

## Summary

The webpage describes **ChartAssistant**, a novel vision-language model specifically designed for **universal chart comprehension and reasoning**.

Key aspects relevant to your query include:

*   **Vision-Language Models (VLM) for Charts:** ChartAssistant is a VLM built on two variants: ChartAst-D (based on Donut) and ChartAst-S (based on SPHINX, leveraging LLMs).
*   **Training Methodology:** It uses a two-stage training process on a comprehensive chart-specific benchmark called **ChartSFT**:
    1.  **Pre-training on Chart-to-Table Translation:** This step aligns the chart visual information with its structured text representation (like a table). This directly relates to **document understanding** and **PDF parsing/chart extraction** aspects of your query, as charts are a form of structured document.
    2.  **Multitask Instruction Tuning:** Fine-tuning on various chart-related tasks (QA, summarization, etc.) without requiring task-specific fine-tuning for downstream performance.
*   **Performance:** ChartAssistant reportedly outperforms previous state-of-the-art methods like UniChart and even advanced general-purpose models like **GPT-4V(ision)** on real-world chart data.
*   **Chart Extraction/Understanding:** The model excels at tasks like **chart-to-table translation** (extracting structured data from charts), **chart numerical QA** (requiring mathematical reasoning), and **chart summarization**.

While the page focuses specifically on charts rather than general multimodal RAG or report generation from arbitrary documents, it details advanced techniques for **vision-language models** applied to **structured document understanding (charts)**, including **extraction** (chart-to-table) and **reasoning**, which are core components of your query.

---

## Full Content

HTML conversions [sometimes display errors](https://info.dev.arxiv.org/about/accessibility_html_error_messages.html) due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.

- failed: xtab
- failed: epic

Authors: achieve the best HTML results from your LaTeX submissions by following these [best practices](https://info.arxiv.org/help/submit_latex_best_practices.html).

License: arXiv.org perpetual non-exclusive license

arXiv:2401.02384v1 \[cs.CV\] 04 Jan 2024

# ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning

Fanqing Meng1,212{}^{1,2}start\_FLOATSUPERSCRIPT 1 , 2 end\_FLOATSUPERSCRIPT, Wenqi Shao1‚Å£‚Ä†1‚Ä†{}^{1\\dagger}start\_FLOATSUPERSCRIPT 1 ‚Ä† end\_FLOATSUPERSCRIPT, Quanfeng Lu1,414{}^{1,4}start\_FLOATSUPERSCRIPT 1 , 4 end\_FLOATSUPERSCRIPT, Peng Gao11{}^{1}start\_FLOATSUPERSCRIPT 1 end\_FLOATSUPERSCRIPT

Kaipeng Zhang11{}^{1}start\_FLOATSUPERSCRIPT 1 end\_FLOATSUPERSCRIPT, Yu Qiao11{}^{1}start\_FLOATSUPERSCRIPT 1 end\_FLOATSUPERSCRIPT, Ping Luo3,1‚Å£‚Ä†31‚Ä†{}^{3,1\\dagger}start\_FLOATSUPERSCRIPT 3 , 1 ‚Ä† end\_FLOATSUPERSCRIPT

11{}^{1}start\_FLOATSUPERSCRIPT 1 end\_FLOATSUPERSCRIPTOpenGVLab, Shanghai AI Laboratory ‚ÄÉ22{}^{2}start\_FLOATSUPERSCRIPT 2 end\_FLOATSUPERSCRIPTShanghai Jiao Tong University

33{}^{3}start\_FLOATSUPERSCRIPT 3 end\_FLOATSUPERSCRIPTThe University of Hong Kong ‚ÄÉ44{}^{4}start\_FLOATSUPERSCRIPT 4 end\_FLOATSUPERSCRIPTNanjing University

###### Abstract

Charts play a vital role in data visualization, understanding data patterns, and informed decision-making. However, their unique combination of graphical elements (e.g., bars, lines) and textual components (e.g., labels, legends) poses challenges for general-purpose multimodal models. While vision-language models trained on chart data excel in comprehension, they struggle with generalization and require task-specific fine-tuning. To address these challenges, we propose ChartAssistant, a chart-based vision-language model for universal chart comprehension and reasoning. ChartAssistant leverages ChartSFT, a comprehensive dataset covering diverse chart-related tasks with basic and specialized chart types. It undergoes a two-stage training process, starting with pre-training on chart-to-table parsing to align chart and text, followed by multitask instruction-following fine-tuning. This approach enables ChartAssistant to achieve competitive performance across various chart tasks without task-specific fine-tuning. Experimental results demonstrate significant performance gains over the state-of-the-art UniChart method, outperforming OpenAI‚Äôs GPT-4V(ision) on real-world chart data. The code and data are available at [https://github.com/OpenGVLab/ChartAst](hhttps://github.com/OpenGVLab/ChartAst).

‚Ä†‚Ä†footnotetext: ‚Ä†‚Ä†\\dagger‚Ä† Corresponding Authors: shaowenqi@pjlab.org.cn; pluo@cs.hku.edu

This work was done when Fanqing Meng and Quanfeng Lu were interning at Shanghai AI Laboratory.

## 1 Introduction

![Refer to caption](https://browse.arxiv.org/html/2401.02384v1/x1.png)Figure 1: A comparison between previous chart-based models and our proposed ChartAssistant. ChartAssistant first aligns the chart and the text by pre-training on the chart-to-table translation task. After performing multitask instruction tuning, it can solve various downstream tasks without task-specific fine-tuning procedures.

People around the world generate a multitude of charts on a daily basis, including data visualizations for business reports, market analysis, scientific experiments, and data-driven presentations \[ [11](https://browse.arxiv.org/html/2401.02384v1/#bib.bib11), [9](https://browse.arxiv.org/html/2401.02384v1/#bib.bib9), [10](https://browse.arxiv.org/html/2401.02384v1/#bib.bib10)\].
Charts are an effective tool for understanding data patterns, such as the distributional properties depicted in histograms and growth trends illustrated in line graphs.
Developing chart learning methods enables the design of machine analysts with enhanced capabilities to solve various chart-related downstream tasks such as chart question answering (QA) \[ [29](https://browse.arxiv.org/html/2401.02384v1/#bib.bib29), [14](https://browse.arxiv.org/html/2401.02384v1/#bib.bib14), [31](https://browse.arxiv.org/html/2401.02384v1/#bib.bib31)\], chart summarization \[ [12](https://browse.arxiv.org/html/2401.02384v1/#bib.bib12), [36](https://browse.arxiv.org/html/2401.02384v1/#bib.bib36)\].

However, chart comprehension is challenging due to the intricate visual marks ( _e.g._ lines, bars and symbols), implicit numerical information, and complex spatial relationships between elements ( _e.g._ axes and labels). Interpreting charts requires specialized knowledge, spatial reasoning, and numerical understanding. The advanced general-purpose multimodal models \[ [47](https://browse.arxiv.org/html/2401.02384v1/#bib.bib47), [21](https://browse.arxiv.org/html/2401.02384v1/#bib.bib21), [46](https://browse.arxiv.org/html/2401.02384v1/#bib.bib46)\] such as GPT-4V(ision) \[ [3](https://browse.arxiv.org/html/2401.02384v1/#bib.bib3)\], trained on natural images, struggle with chart-related tasks due to the specific complexities and relationships unique to charts. Although recent multimodal literate models \[ [28](https://browse.arxiv.org/html/2401.02384v1/#bib.bib28), [19](https://browse.arxiv.org/html/2401.02384v1/#bib.bib19)\] have achieved impressive results in processing various document-level tasks, they still face difficulties in accurately answering chart-related questions.

In pursuit of universal chart reasoning and comprehension, prior works propose pre-training vision-language models on chart-related tasks as shown in Fig. [1](https://browse.arxiv.org/html/2401.02384v1/#S1.F1)(a). For example,
both Matcha \[ [25](https://browse.arxiv.org/html/2401.02384v1/#bib.bib25)\] and UniChart \[ [30](https://browse.arxiv.org/html/2401.02384v1/#bib.bib30)\] undergo multitask instructional tuning and task-specific fine-tuning, exhibiting good performance on several downstream tasks.
However, these models still have severe downsides.
Firstly, prior models fail to explicitly align the chart and the associated structured text-form table, which is essential to interpret the relationships between elements in the chart. Secondly, the existing training data \[ [31](https://browse.arxiv.org/html/2401.02384v1/#bib.bib31), [29](https://browse.arxiv.org/html/2401.02384v1/#bib.bib29)\] is deficient in image-text annotations aimed at improving the model‚Äôs comprehension of visual elements and mathematical reasoning, as well as annotated data from the specialized chart types such as box-plots. Due to the above factors, existing chart-based models have poor generalization and require task-specific fine-tuning to achieve promising results on various downstream tasks as illustrated in Fig. [1](https://browse.arxiv.org/html/2401.02384v1/#S1.F1)(a).

To address these challenges, we propose ChartAssistant, a new vision-language model for universal chart comprehension and reasoning. We implement ChartAssistant with two variants, _i.e._ ChartAst-D and ChartAst-S, ChartAst-D is built upon Donut \[ [16](https://browse.arxiv.org/html/2401.02384v1/#bib.bib16)\], a powerful vision-language model for visual document understanding. It employs Swin Transformer \[ [27](https://browse.arxiv.org/html/2401.02384v1/#bib.bib27)\] as the visual encoder to process high-resolution charts and Bart \[ [20](https://browse.arxiv.org/html/2401.02384v1/#bib.bib20)\] decoder to produce the expected output conditioned on the encoded image and text. While ChartAst-S is built upon SPHINX \[ [23](https://browse.arxiv.org/html/2401.02384v1/#bib.bib23)\], a large vision-language model for universal multimodal comprehension. SPHINX employs multiple visual encoders for mixed encoding and leverages the powerful language capabilities of LLM \[ [47](https://browse.arxiv.org/html/2401.02384v1/#bib.bib47)\]. Additionally, it incorporates specialized processing techniques for high-resolution images, resulting in enhanced chart comprehension abilities. Compared to ChartAst-D, ChartAst-S is a more advanced version, offering increased robustness and usability. It excels in its ability to handle a wide range of tasks and exhibits strong performance in various scenarios.
To improve generalization, ChartAssistant is trained on a large-scale chart-specific benchmark dubbed ChartSFT. The training process involves a two-stage pre-training pipeline which employs chart-to-table pre-training to align the chart and its structured text and then perform joint tuning on multiple chart-related tasks as shown in Fig. [1](https://browse.arxiv.org/html/2401.02384v1/#S1.F1)(b).

Specifically, we first construct ChartSFT by collecting instruction-following data from various chart-related tasks. To address the limitations of existing chart-based benchmarks \[ [31](https://browse.arxiv.org/html/2401.02384v1/#bib.bib31), [29](https://browse.arxiv.org/html/2401.02384v1/#bib.bib29), [14](https://browse.arxiv.org/html/2401.02384v1/#bib.bib14)\], we introduce several modifications to improve the quality of data annotation: 1) instruction-following data involving various topics for chart-to-table translation is added, which we find helps align the chart and the associated structured text; 2) the chain-of-thought annotations for chart numerical QA task are generated to improve mathematical reasoning abilities \[ [41](https://browse.arxiv.org/html/2401.02384v1/#bib.bib41)\]; 3) the task of chart referring question answering is created to enhance the understanding of visual elements and their relationships \[ [4](https://browse.arxiv.org/html/2401.02384v1/#bib.bib4), [45](https://browse.arxiv.org/html/2401.02384v1/#bib.bib45)\]; 4) chart with specialized types such as radar and box plot are included to improve the generalization. Overall, ChartSFT encompasses a larger corpus of instruction-following data, incorporates a wider range of chart-related tasks and types, and features more comprehensive data annotations compared to previous benchmarks \[ [29](https://browse.arxiv.org/html/2401.02384v1/#bib.bib29), [31](https://browse.arxiv.org/html/2401.02384v1/#bib.bib31), [14](https://browse.arxiv.org/html/2401.02384v1/#bib.bib14)\].

Before conducting multitask instruction tuning, as done in existing research \[ [30](https://browse.arxiv.org/html/2401.02384v1/#bib.bib30), [25](https://browse.arxiv.org/html/2401.02384v1/#bib.bib25)\], we begin by pre-training ChartAssistant on the chart-to-table translation task as shown in Fig. [1](https://browse.arxiv.org/html/2401.02384v1/#S1.F1)(b). This task involves parsing a chart and generating a Markdown table. It shares similarities with dense captioning for natural images, allowing the model to interpret the elements and relationships within the chart. Similar to the role of image captioning in training multimodal models \[ [26](https://browse.arxiv.org/html/2401.02384v1/#bib.bib26), [37](https://browse.arxiv.org/html/2401.02384v1/#bib.bib37), [43](https://browse.arxiv.org/html/2401.02384v1/#bib.bib43)\], chart-to-table translation facilitates alignment between the chart and its structured text.
Following pre-training, we proceed with multitask instruction tuning using ChartSFT. This two-stage training approach enables ChartAssistant to achieve strong performance across a range of chart-related tasks without requiring task-specific fine-tuning.

The contributions of this paper can be summarized as follows.
1) We present ChartAssistant, a vision-language model for chart comprehension and reasoning. ChartAssistant is versatile enough to solve various chart-related tasks across a wide range of chart types.
2) We build a chart-specific visual instruction-following benchmark dubbed ChartSFT. ChartSFT surpasses existing chart-based benchmarks with its larger instruction-following data corpus, a broader range of tasks and chart types, and more comprehensive data annotations. 3) Extensive experimental results on various downstream tasks demonstrate that ChartAssistant surpasses the previous SoTA method UniChart \[ [30](https://browse.arxiv.org/html/2401.02384v1/#bib.bib30)\] by 1.0%, 33.8%, 210.0%, 479.0%, 10.1% performance gain on chart-to-table translation, chart open-ended QA, numerical QA, referring QA and summarization, respectively. Notably, ChartAssistant even outperforms advanced multimodal models such as OpenAI‚Äôs GPT-4V(ision) \[ [3](https://browse.arxiv.org/html/2401.02384v1/#bib.bib3)\] and Google‚Äôs Bard \[ [6](https://browse.arxiv.org/html/2401.02384v1/#bib.bib6)\] on chart data in real scenarios.

![Refer to caption](https://browse.arxiv.org/html/2401.02384v1/x2.png)Figure 2: ChartAssistant is pre-trained on vast and various chart-related tasks, and can adeptly perform a range of chart comprehension and reasoning tasks including chart-to-table translation, numerical QA, referring QA, open-ended QA and chart summarization.

## 2 Related Work

### 2.1 Multimodal Foundation Model

Multimodal foundation models \[ [21](https://browse.arxiv.org/html/2401.02384v1/#bib.bib21), [50](https://browse.arxiv.org/html/2401.02384v1/#bib.bib50)\] mainly focus on natural images, which have shown remarkable progress, advancing in areas like image captioning \[ [40](https://browse.arxiv.org/html/2401.02384v1/#bib.bib40)\] and visual question answering \[ [40](https://browse.arxiv.org/html/2401.02384v1/#bib.bib40), [13](https://browse.arxiv.org/html/2401.02384v1/#bib.bib13)\]. SPHINX \[ [23](https://browse.arxiv.org/html/2401.02384v1/#bib.bib23)\] leverages LLM and multiple visual encoders to achieve advanced performance on multiple multi-modal tasks. Among these, visual document understanding is a topic of both industrial importance and research challenge. Donut \[ [16](https://browse.arxiv.org/html/2401.02384v1/#bib.bib16)\] proposed an OCR-free Transformer trained in end-to-end manner,which is a powerful document understanding model. Nougat \[ [2](https://browse.arxiv.org/html/2401.02384v1/#bib.bib2)\] is fine-tuned on Donut and useful for academic documents understanding. However, extracting information from real-world images like charts and plots presents unique challenges as compared to natural images or documents. Furthermore, the complexity of queries increases, often involving sophisticated mathematical calculations. As a result, contemporary document models and multimodal foundation models often fall short when tasked with handling chart-related tasks, demonstrating a significant decline in performance \[ [25](https://browse.arxiv.org/html/2401.02384v1/#bib.bib25)\].

### 2.2 Chart-specific Vision-Language Model

Some methods modify vision-language models for chart-related tasks. Matcha \[ [25](https://browse.arxiv.org/html/2401.02384v1/#bib.bib25)\] extends Pix2Struct \[ [19](https://browse.arxiv.org/html/2401.02384v1/#bib.bib19)\] by integrating mathematical reasoning and chart data extraction tasks, excelling at chart question answering and chart summarization. Unichart \[ [30](https://browse.arxiv.org/html/2401.02384v1/#bib.bib30)\] undergoes multitask instruction tuning for more chart-related tasks, establishing itself as the most versatile and effective chart vision-language model currently available. However, these methods have limitations. They require task-specific fine-tuning for promising results, risking catastrophic forgetting \[ [44](https://browse.arxiv.org/html/2401.02384v1/#bib.bib44), [5](https://browse.arxiv.org/html/2401.02384v1/#bib.bib5)\] and sacrificed task generalization. Furthermore, these models struggle with mathematical computations, limiting their effectiveness and range of applicable chart types.

Contrastingly, we propose ChartSFT, the most extensive dataset to date, supporting a wide variety of chart tasks and types. We develop ChartAssistant using ChartSFT with a two-stage training strategy, capable of handling diverse chart-related tasks without requiring task-specific fine-tuning.

## 3 ChartSFT

Table 1: Summary of utilized datasets and data volumes for each task.

| | | | | | | | | | | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ChartQA\[ [29](https://browse.arxiv.org/html/2401.02384v1/#bib.bib29)\] | PlotQA\[ [31](https://browse.arxiv.org/html/2401.02384v1/#bib.bib31)\] | OpenCQA\[ [14](https://browse.arxiv.org/html/2401.02384v1/#bib.bib14)\] | ScigraphQA\[ [22](https://browse.arxiv.org/html/2401.02384v1/#bib.bib22)\] | Vistext\[ [38](https://browse.arxiv.org/html/2401.02384v1/#bib.bib38)\] | Chart-to-text\[ [15](https://browse.arxiv.org/html/2401.02384v1/#bib.bib15)\] | ChartSumm\[ [36](https://browse.arxiv.org/html/2401.02384v1/#bib.bib36)\] | arXiv | Data Aug. | SpecializedTypes | Total |
| Chart-to-Table Translation |
| 17141 | 224386 | 0 | 0 | 0 | 0 | 0 | 132719 | 220050 | 317662 | 911958 |
| Numerical Question Answering |
| 0 | 3997388 | 0 | 0 | 0 | 0 | 0 | 0 | 5318500 | 15178693 | 24494581 |
| Referring Question Answering |
| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2139567 | 3760275 | 5899842 |
| Open-ended Question Answering |
| 30219 | 4362236 | 7724 | 659309 | 0 | 0 | 0 | 408658 | 128105 | 1478952 | 7075203 |
| Chart Summarization |
| 0 | 157070 | 7724 | 0 | 12441 | 44096 | 84363 | 0 | 356248 | 419895 | 1006738 |

We construct a large-scale chart-specific benchmark called ChartSFT by collecting instruction-following data from various tasks. The composition of ChartSFT is shown in Table [1](https://browse.arxiv.org/html/2401.02384v1/#S3.T1), as extensively described below. Our ChartSFT consists of 39388322393883223938832239388322 pieces of chart-text annotated data, 4.754.754.754.75 and 5.625.625.625.62 times larger than MatCha \[ [25](https://browse.arxiv.org/html/2401.02384v1/#bib.bib25)\] and UniChart \[ [30](https://browse.arxiv.org/html/2401.02384v1/#bib.bib30)\], respectively, as illustrated in Fig. [3](https://browse.arxiv.org/html/2401.02384v1/#S3.F3). ChartSFT contains chart types including both base and specialized types. Specifically, our ChartSFT encompasses nine types of charts by collecting data from various sources as shown in table [8](https://browse.arxiv.org/html/2401.02384v1/#S6.T8). First, most charts with base types including bar, line, dot-line, and pie are collected from several existing datasets \[ [29](https://browse.arxiv.org/html/2401.02384v1/#bib.bib29), [31](https://browse.arxiv.org/html/2401.02384v1/#bib.bib31), [14](https://browse.arxiv.org/html/2401.02384v1/#bib.bib14), [36](https://browse.arxiv.org/html/2401.02384v1/#bib.bib36), [22](https://browse.arxiv.org/html/2401.02384v1/#bib.bib22), [38](https://browse.arxiv.org/html/2401.02384v1/#bib.bib38), [15](https://browse.arxiv.org/html/2401.02384v1/#bib.bib15)\]. Second, we also generate some charts with base types from arxiv tables \[ [1](https://browse.arxiv.org/html/2401.02384v1/#bib.bib1)\] and data augmentation techniques ( _e.g._ various APIs and figure parameters). In particular, we use ChatGPT to suggest the proper chart type given each table data from arxiv. Third, we synthesize table data which is appropriate for depicting charts with specialized types.

Table 2: Chart type distribution of the multitask instruction tuning, we are not including SciGraphQA \[ [22](https://browse.arxiv.org/html/2401.02384v1/#bib.bib22)\] and ChartSumm \[ [36](https://browse.arxiv.org/html/2401.02384v1/#bib.bib36)\] because these datasets do not contain information about chart types.

| Datasets | Bar | Line | Dot-line | Pie | Area | Hist | Radar | Bubble | Box |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ChartQA \[ [29](https://browse.arxiv.org/html/2401.02384v1/#bib.bib29)\] | 84.8% | 12.2% | 0.0% | 3.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |
| PlotQA \[ [31](https://browse.arxiv.org/html/2401.02384v1/#bib.bib31)\] | 67.0% | 16.5% | 16.5% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |
| OpenCQA \[ [14](https://browse.arxiv.org/html/2401.02384v1/#bib.bib14)\] | 71.7% | 24.6% | 0.6% | 3.0% | 0.1% | 0.0% | 0.0% | 0.0% | 0.0% |
| Vistext \[ [38](https://browse.arxiv.org/html/2401.02384v1/#bib.bib38)\] | 50.1% | 24.2% | 0.0% | 0.0% | 25.6% | 0.0% | 0.0% | 0.0% | 0.0% |
| Chart-to-text \[ [15](https://browse.arxiv.org/html/2401.02384v1/#bib.bib15)\] | 82.8% | 13.6% | 1.6% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |
| arXiv | 71.6% | 17.1% | 11.3% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |
| Data Aug. | 56.5% | 17.0% | 11.5% | 15.0% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |
| Synthetic | 0.0% | 0.0% | 0.0% | 0.0% | 23.1% | 25.8% | 20.9% | 19.1% | 11.1% |
| Total | 44.3% | 11.3% | 8.0% | 3.6% | 7.8% | 8.4% | 6.8% | 6.2% | 3.6% |

### 3.1 Chart with Base Types

We collect instruction-following data with base chart types ( _i.e._ bars, lines, dot-lines, and pies) from 5555 chart-rated tasks, including chart-to-table translation, chart numerical QA, chart referring QA, chart open-ended QA, and chart summarization as shown in Fig. [2](https://browse.arxiv.org/html/2401.02384v1/#S1.F2). Instead of directly utilizing existing chart-based benchmarks, we introduce several modifications to improve the data annotation quality.
For each task, we present the details of data collection as follows.

#### 3.1.1 Chart-to-Table Translation

The task of chart-to-table translation aims at parsing a chart into its underlying data table in text form. Pre-training with chart-to-table translation enables our ChartAssistant to comprehend the chart‚Äôs elements and their relationships, facilitating alignment of the chart and its underlying structured text.

Data Collection. We collect 17141171411714117141 and 224386224386224386224386 pieces of chart-text data from ChartQA and PlotQA for chart-to-table translation. However, these benchmarks vary little in chart styles and involve limited topics. We propose two strategies to address the issue.

- ‚Ä¢


More Chart Styles. We re-plot the chart with diverse visualization tools for tables in ChartQA and PlotQA. Specifically, we utilize 5555 APIs in Python, including ggplot, plotly, matplotlib, seaborn, and pyecharts, along with over 20202020 variations in parameters color, size, font type, background, and more. After style augmentation, 220050220050220050220050 pieces of chart-text data are created for chart-to-table translation from PlotQA, respectively.

- ‚Ä¢


Table from arXiv Papers. We collect more real table data to increase the topic diversity. To this end, we crawl 1301932130193213019321301932 papers involving various topics such as computer science, biology, finance, and more from arXiv platform \[ [1](https://browse.arxiv.org/html/2401.02384v1/#bib.bib1)\]. For each paper, we extract the table from the source LaTeX code where table data can be localized in the table environment. We employ ChatGPT \[ [33](https://browse.arxiv.org/html/2401.02384v1/#bib.bib33)\] to transform the latex table into the markdown table. We also make the chart in a specific base type ( _e.g._ pies) by following ChatGPT‚Äôs suggestion. We find that ChatGPT works well to generate text in the target format and give appropriate advice for chart types. There are 132719132719132719132719 pieces of chart-text data obtained from the arXiv.


#### 3.1.2 Chart Numerical Question Answering

Chart numerical QA targets at responding to the request about mathematical reasoning given a chart. It requires an accurate understanding of the chart, as well as reasoning and math calculation abilities.

Data Collection. The data for numerical QA mainly comes from the PlotQA benchmark. However, PlotQA generates numerical QA data from 40404040 templates with limited types of questions and direct final answers, resulting in poor generalization and math reasoning. with our proposed two strategies to improve the data quality below, more than 24M QA pairs are collected.

Table 3: Comparison of templates for numerical QA between PlotQA and our ChartSFT. ‚ÄòNum.‚Äô denotes the number of templates. We use 4444 statistics to measure the complexity of templates, including ‚ÄòLen.‚Äô, ‚ÄòCOT Steps‚Äô and ‚ÄòFun.‚Äô. They denote the average token length, the number of steps in COT annotation, and how many kinds of functions are needed to obtain the final answer, respectively. Besides templates in PlotQA, ChartSFT newly created 61616161 templates for numerical QA with higher complexity.

| | Num. | Len. | COT Steps | Func. |
| --- | --- | --- | --- | --- |
| PlotQA | 40 | 32.83 | 3.48 | 2.95 |
| ChartSFT | 61 (101) | 39.54 | 5.02 | 3.90 |

- ‚Ä¢


More Templates. We create 101101101101 templates to generate numerical QA questions automatically involving various types of questions with complex calculations. Here is one template for analyzing the correlation between two items: ‚ÄòAcross all << >>, are the << >> values of << >> and << >> negatively correlated?‚Äô The comparison between templates in our ChartAssistant and PlotQA is provided in Table [3](https://browse.arxiv.org/html/2401.02384v1/#S3.T3) where we can see that our improved templates encompass larger token lengths and more complex calculations. We present all templates in the Appendix Sec.A.

- ‚Ä¢


Chain-of-Though (COT) Annotations. Instead of utilizing the final answer as the response annotation, we generate COT annotation for the final answer, which has been proven to improve the model‚Äôs mathematical reasoning ability \[ [41](https://browse.arxiv.org/html/2401.02384v1/#bib.bib41)\]. We first define a set of available functions to segment the problem‚Äôs solution into smaller steps, each encompassing function calls and parameters. These steps are then organized into a JSON-formatted text. As shown in Fig. [2](https://browse.arxiv.org/html/2401.02384v1/#S1.F2), the maximum extraction problem is decomposed into a step of data retrieval and a step of maximum calculation. When computing the answers, the backend executes the calculations by following the ordered function calls within the text. This approach not only enhances reasoning ability but also mitigates calculation errors.


#### 3.1.3 Chart Referring Question Answering

We create a new task for chart named referring question answering, considering that users may utilize a set of marks to denote some pieces to their interest in the chart as shown in Fig. [2](https://browse.arxiv.org/html/2401.02384v1/#S1.F2). Note that referring question answering with a bounding box has been explored in general-purpose multimodal models such as GPT4ROI \[ [48](https://browse.arxiv.org/html/2401.02384v1/#bib.bib48)\] and Shikra \[ [4](https://browse.arxiv.org/html/2401.02384v1/#bib.bib4)\] where the referential QA has been shown to benefit comprehending spatial relationships. The task of referring QA is expected to enhance the understanding of visual elements and their relationship in the chart.

Data Collection. We extend a part of COT annotations for numerical QA in Sec. [3.1.2](https://browse.arxiv.org/html/2401.02384v1/#S3.SS1.SSS2) to the task of Referring QA. Three steps are conducted to produce referring QA pairs with diverse patterns. i) The color, size, and width are randomly selected to make the mark. ii) We use several marks such as an arrow and a bounding box to refer to an item in the chart. iii) Multiple marks can be depicted in the same chart to describe the relationships between elements. Overall, we collect 5899842589984258998425899842 pieces of data for the chart referring QA.

#### 3.1.4 Chart Open-ended QA

Chart open-ended QA (OpenQA) deals with open-ended questions regarding charts as illustrated in Fig. [2](https://browse.arxiv.org/html/2401.02384v1/#S1.F2). It requires both low-level Chart comprehension and high-level reasoning abilities.

Data Collection. We collect data from existing benchmarks, such as plotQA \[ [31](https://browse.arxiv.org/html/2401.02384v1/#bib.bib31)\], ChartQA \[ [29](https://browse.arxiv.org/html/2401.02384v1/#bib.bib29)\], OpenCQA \[ [14](https://browse.arxiv.org/html/2401.02384v1/#bib.bib14)\] and ScigraphQA \[ [22](https://browse.arxiv.org/html/2401.02384v1/#bib.bib22)\]. We further introduce our collected table data from arXiv in Sec. [3.1.1](https://browse.arxiv.org/html/2401.02384v1/#S3.SS1.SSS1) for this task.

Open-ended QA data by ChatGPT. Other than tabular data crawled in Sec. [3.1.1](https://browse.arxiv.org/html/2401.02384v1/#S3.SS1.SSS1), we extract corresponding captions, and the first paragraph describing the table from the source code of the paper. By utilizing ChatGPT, we generate 3333 open-ended QA pairs for each table by feeding the table and the descriptive information.

By putting the above benchmarks together, our ChartSFT covers diverse topics for Open-ended QA. In total, there are 7075243707524370752437075243 pieces of data for this task.

![Refer to caption](https://browse.arxiv.org/html/2401.02384v1/x3.png)Figure 3: Comparison between ChartSFT and datasets from previous methods. Our dataset surpasses the best previous dataset in UniChart \[ [30](https://browse.arxiv.org/html/2401.02384v1/#bib.bib30)\] by 4.64.64.64.6 times in total and supports a greater variety of chart tasks and types.![Refer to caption](https://browse.arxiv.org/html/2401.02384v1/x4.png)Figure 4: ChartAst-D and ChartAst-S network architecture.

#### 3.1.5 Chart Summarization

Chart Summarization is a vital task aimed at generating concise and informative summaries for various types of charts, which has been studied extensively \[ [8](https://browse.arxiv.org/html/2401.02384v1/#bib.bib8), [38](https://browse.arxiv.org/html/2401.02384v1/#bib.bib38), [15](https://browse.arxiv.org/html/2401.02384v1/#bib.bib15)\].

Data Collection. We collected a substantial amount of existing open-source datasets \[ [38](https://browse.arxiv.org/html/2401.02384v1/#bib.bib38), [15](https://browse.arxiv.org/html/2401.02384v1/#bib.bib15), [36](https://browse.arxiv.org/html/2401.02384v1/#bib.bib36), [14](https://browse.arxiv.org/html/2401.02384v1/#bib.bib14)\], but the scale is still not sufficient. Therefore, we further incorporate a large-scale chart summarization dataset generated through Knowledge Distillation by Unichart \[ [30](https://browse.arxiv.org/html/2401.02384v1/#bib.bib30)\] into our training process. There are 1006738100673810067381006738 pieces of data for the chart summarization task.

### 3.2 Charts with Specialized Types

Previous Chart-based models perform poorly on charts with specialized types including radar, area, histogram, bubble, and box-plot. To improve generalization, we also train our ChartAssistant on charts with these specialized types.
We collect chart data on specialized types through the following pipeline: i) pairs of chart and table generation, and ii) multitask dataset generation.

Chart-to-Table data generation. Considering the significant challenge of collecting real-world data on a large scale, we generate data synthetically. The (chart, table) pairs are generated in the following steps:

i) Table generation: We randomly select one distribution from over twenty predefined probability density distributions, and then generate tabular data by utilizing this distribution for randomization. For radar, bubble and area charts, We directly utilize randomly generated numbers as the table data. For histogram and box plot, we generate an array of extensive numbers using this distribution and calculate the statistical metrics of this array to serve as the table data (e.g., frequencies corresponding to histograms, upper whiskers corresponding to box plots). And then we use the generated data to prompt ChatGPT for creating titles, legends, and labels that align with the numerical characteristics.

ii) Chart generation: We utilize multiple Python plotting libraries to generate a variety of charts, including styles from matplotlib, plotly, pyecharts, ggplot, seaborn, altair, and more. To ensure diversity in the charts, we randomly select the following parameters: line style, font style, colors, markers, the position of legends and so on. Besides our own synthetic data, we also use the table from PlotQA \[ [31](https://browse.arxiv.org/html/2401.02384v1/#bib.bib31)\], ChartQA \[ [29](https://browse.arxiv.org/html/2401.02384v1/#bib.bib29)\], ChartSumm \[ [36](https://browse.arxiv.org/html/2401.02384v1/#bib.bib36)\] and Chart-To-Text \[ [15](https://browse.arxiv.org/html/2401.02384v1/#bib.bib15)\] to plot the charts for area and radar charts.

Multitask dataset generation. For the chart summarization and open-ended QA tasks, we instruct ChatGPT to build datasets by supplying both the table and the corresponding types of charts. For numerical QA and referring QA tasks, we adhere to the approach of the chart with base types by crafting a series of mathematical question templates tailored to the distinct characteristics of various chart types. Subsequently, we manually generate answers with COT annotations.

## 4 Our ChartAssistant

### 4.1 Architecture

The key to completing the tasks related to charts lies in accurately understanding the content of the charts. As shown in Fig. [4](https://browse.arxiv.org/html/2401.02384v1/#S3.F4), we implement ChartAssistant with two variants, _i.e._ ChartAst-D and ChartAst-S, which have 260M and 13B parameters in total, respectively. Both ChartAst-D and ChartAst-S perform well in many chart-related tasks but ChartAst-D has a smaller size and ChartAst-S enjoys better generalization.

ChartAst-D is a vision-language model for chart understanding built upon Donut \[ [17](https://browse.arxiv.org/html/2401.02384v1/#bib.bib17)\]. It consists of a visual encoder Swin-Base \[ [27](https://browse.arxiv.org/html/2401.02384v1/#bib.bib27)\] and a textual BART decoder \[ [20](https://browse.arxiv.org/html/2401.02384v1/#bib.bib20)\]. For an input image XVsubscriptùëãùëâX\_{V}italic\_X start\_POSTSUBSCRIPT italic\_V end\_POSTSUBSCRIPT, the visual encoder employs fixed-sized non-overlapping windows to divide the image and performs self-attention layers to consolidate information across these windows, which transforms the image into a set of tokens ZV={ùê≥i‚à£ùê≥i‚àà‚Ñùd,1‚â§i‚â§n}subscriptùëçùëâconditional-setsubscriptùê≥ùëñformulae-sequencesubscriptùê≥ùëñsuperscript‚Ñùùëë1ùëñùëõZ\_{V}=\\left\\{\\mathbf{z}\_{i}\\mid\\mathbf{z}\_{i}\\in\\mathbb{R}^{d},1\\leq i\\leq n\\right\\}italic\_Z start\_POSTSUBSCRIPT italic\_V end\_POSTSUBSCRIPT = { bold\_z start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ‚à£ bold\_z start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ‚àà blackboard\_R start\_POSTSUPERSCRIPT italic\_d end\_POSTSUPERSCRIPT , 1 ‚â§ italic\_i ‚â§ italic\_n }, where nùëõnitalic\_n is encoded token length and dùëëditalic\_d is the token size.
By taking ZVsubscriptùëçùëâZ\_{V}italic\_Z start\_POSTSUBSCRIPT italic\_V end\_POSTSUBSCRIPT as key and value and tokens of text instruction XqsubscriptùëãùëûX\_{q}italic\_X start\_POSTSUBSCRIPT italic\_q end\_POSTSUBSCRIPT as the query, the BART decoder generates the corresponding response Yq=(ùê≤i)i=1msubscriptùëåùëûsuperscriptsubscriptsubscriptùê≤ùëñùëñ1ùëöY\_{q}=\\left(\\mathbf{y}\_{i}\\right)\_{i=1}^{m}italic\_Y start\_POSTSUBSCRIPT italic\_q end\_POSTSUBSCRIPT = ( bold\_y start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ) start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_m end\_POSTSUPERSCRIPT, and mùëömitalic\_m is the length of responses.

ChartAst-S is a large vision-language model for chart understanding built upon Sphinx \[ [23](https://browse.arxiv.org/html/2401.02384v1/#bib.bib23)\]. For high-resolution images, it preserves the original information through sampling and partitioning methods, ensuring greater fidelity to the image content. Moreover, Sphinx leverages the abundant prior knowledge of LLM \[ [39](https://browse.arxiv.org/html/2401.02384v1/#bib.bib39)\] to handle various tasks such as visual question answering and image summarization. Specifically, for an input image XVsubscriptùëãùëâX\_{V}italic\_X start\_POSTSUBSCRIPT italic\_V end\_POSTSUBSCRIPT. ChartAst-S incorporates multiple visual encoders to extract more informative visual features ZVsubscriptùëçùëâZ\_{V}italic\_Z start\_POSTSUBSCRIPT italic\_V end\_POSTSUBSCRIPT, such as DINOv2 \[ [32](https://browse.arxiv.org/html/2401.02384v1/#bib.bib32)\], CLIP \[ [34](https://browse.arxiv.org/html/2401.02384v1/#bib.bib34)\], and ConvNeXt \[ [42](https://browse.arxiv.org/html/2401.02384v1/#bib.bib42)\]. Unlike ChartAst-D where visual tokens are involved in a language decoder with a cross-attention module, ChartAst-S directly appends visual tokens to the text tokens XqsubscriptùëãùëûX\_{q}italic\_X start\_POSTSUBSCRIPT italic\_q end\_POSTSUBSCRIPT. The merged tokens are then fed into the LLM to generate the response. Thanks to the intricate design of the visual encoder and the powerful reasoning ability of LLM, ChartAst-D performs well in various real-world chart-related applications.

### 4.2 Training

In our ChartSFT, we have a corresponding instruction XqsubscriptùëãùëûX\_{q}italic\_X start\_POSTSUBSCRIPT italic\_q end\_POSTSUBSCRIPT and response YqsubscriptùëåùëûY\_{q}italic\_Y start\_POSTSUBSCRIPT italic\_q end\_POSTSUBSCRIPT for each image XVsubscriptùëãùëâX\_{V}italic\_X start\_POSTSUBSCRIPT italic\_V end\_POSTSUBSCRIPT. We input these image-text pairs into the model. The objective is to minimize the cross-entropy loss of predicting the next token.
To improve the generalization in various downstream tasks, we adopt a two-stage training pipeline to train our ChartAst-D and ChartAst-S below.

Stage I: Pretraining on Chart-to-table Translation. Given a chart XVc‚Å¢2‚Å¢tsuperscriptsubscriptùëãùëâùëê2ùë°X\_{V}^{c2t}italic\_X start\_POSTSUBSCRIPT italic\_V end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_c 2 italic\_t end\_POSTSUPERSCRIPT, our goal is to convert the chart into a text-form table Yqc‚Å¢2‚Å¢tsuperscriptsubscriptùëåùëûùëê2ùë°Y\_{q}^{c2t}italic\_Y start\_POSTSUBSCRIPT italic\_q end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_c 2 italic\_t end\_POSTSUPERSCRIPT under the instruction Xqc‚Å¢2‚Å¢tsuperscriptsubscriptùëãùëûùëê2ùë°X\_{q}^{c2t}italic\_X start\_POSTSUBSCRIPT italic\_q end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_c 2 italic\_t end\_POSTSUPERSCRIPT. Here the superscript c‚Å¢2‚Å¢tùëê2ùë°c2titalic\_c 2 italic\_t indicates the instruction-following data comes from the task of chart-to-table translation. Our training loss function for Stage I is given by

| | | | |
| --- | --- | --- | --- |
| | ‚ÑíStage1=‚àí‚àëi=1mlog‚Å°PŒ∏‚Å¢(Yq,ic‚Å¢2‚Å¢t\|XVc‚Å¢2‚Å¢t,Xqc‚Å¢2‚Å¢t,Yq, V}^{c2t},X\_{q}^{c2t},Y\_{q, Y\_{q,i}^{k}\|X\_{V}^{k},X\_{q}^{k},Y\_{q,
