# MASTER: A Multi-Agent System with LLM Specialized MCTS

**URL:** https://huggingface.co/papers/2501.14304
**Published:** 2025-02-20T00:00:00.000Z

---

## Summary

The MASTER framework is a novel approach that improves problem-solving in Large Language Models (LLMs) by integrating a **Multi-Agent System** with **Tactical Execution** and a specialized **MCTS (Monte Carlo Tree Search)** algorithm.

The paper addresses limitations of using standard MCTS with LLMs, specifically:
1.  The difficulty in obtaining **objective rewards** for tasks like question answering, unlike games like Go.
2.  The **excessive token usage and time consumption** required for statistically significant reward estimations (typically needing over 30 simulations).

MASTER uses LLM-specialized MCTS to coordinate agent recruitment and communication, autonomously adjusting the number of agents based on task complexity and ensuring focused communication. Experiments show it achieves state-of-the-art performance on HotpotQA (76% accuracy) and WebShop (80% accuracy).

While the query covers broad topics like **reasoning and planning, chain-of-thought, self-reflection, grounding, and hallucination reduction**, the paper specifically focuses on enhancing **planning capability** using a specialized **MCTS** within a **Multi-Agent System** framework.

---

## Full Content

Paper page - MASTER: A Multi-Agent System with LLM Specialized MCTS
[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)
[Papers](https://huggingface.co/papers)
arxiv:2501.14304
# MASTER: A Multi-Agent System with LLM Specialized MCTS
Published on Jan 24
[
Upvote
-
](https://huggingface.co/login?next=/papers/2501.14304)
Authors:
Bingzheng Gan,
Yufan Zhao,
Tianyi Zhang,
Jing Huang,
Yusu Li,
Shu Xian Teo,
![](https://cdn-avatars.huggingface.co/v1/production/uploads/67ea7f597382053ae1ff676f/zy6b17SFRBfxZvTnq5m3Z.jpeg)[Changwang Zhang](https://huggingface.co/mLeoKing),
Wei Shi
## Abstract
A novel MASTER framework improves problem-solving in Large Language Models by integrating Multi-Agent System and Tactical Execution with a specialized MCTS algorithm, achieving high accuracy across different tasks.
AI-generated summary
[Large Language Models (LLM)](https://huggingface.co/papers?q=Large%20Language%20Models%20(LLM))are increasingly being explored for
problem-solving tasks. However, their[strategic planning](https://huggingface.co/papers?q=strategic%20planning)capability is often
viewed with skepticism. Recent studies have incorporated the Monte Carlo Tree
Search (MCTS) algorithm to augment the planning capacity of LLM. Despite its
potential, MCTS relies on extensive sampling simulations to approximate the
true[reward distribution](https://huggingface.co/papers?q=reward%20distribution), which leads to two primary issues. Firstly, MCTS is
effective for tasks like the Game of Go, where[simulation results](https://huggingface.co/papers?q=simulation%20results)can yield
objective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such
as question answering, the result of a simulation is the answer to the
question, which cannot yield an objective reward without the[ground truth](https://huggingface.co/papers?q=ground%20truth).
Secondly, obtaining statistically significant reward estimations typically
requires a sample size exceeding 30 simulations, resulting in excessive token
usage and time consumption. To address these challenges, we present the[Multi-Agent System](https://huggingface.co/papers?q=Multi-Agent%20System)with[Tactical Execution](https://huggingface.co/papers?q=Tactical%20Execution)and Reasoning using LLM Specialized
MCTS (MASTER), a novel framework that coordinates agent recruitment and
communication through LLM specialized MCTS. This system autonomously adjusts
the number of agents based on[task complexity](https://huggingface.co/papers?q=task%20complexity)and ensures[focused communication](https://huggingface.co/papers?q=focused%20communication)among them. Comprehensive experiments across various tasks demonstrate the
effectiveness of our proposed framework. It achieves 76% accuracy on[HotpotQA](https://huggingface.co/papers?q=HotpotQA)and 80% on[WebShop](https://huggingface.co/papers?q=WebShop), setting new state-of-the-art performance on these datasets.
[View arXiv page](https://arxiv.org/abs/2501.14304)[View PDF](https://arxiv.org/pdf/2501.14304)[Add to collection](https://huggingface.co/login?next=/papers/2501.14304)
### Community
![](https://huggingface.co/avatars/3240502f9d3806b6c5760a54cd01cb6d.svg)[countkc](https://huggingface.co/countkc)
[Feb 20](#67b77b575b714bb42e649986)
[@librarian-bot](https://huggingface.co/librarian-bot)recommend
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg)](https://huggingface.co/librarian-bot)
* 1 reply·
![](https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg)[librarian-bot](https://huggingface.co/librarian-bot)
[Feb 20](#67b77b5d17e6386726598774)
This is an automated message from the[Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper.
The following papers were recommended by the Semantic Scholar API
* [Policy Guided Tree Search for Enhanced LLM Reasoning](https://huggingface.co/papers/2502.06813)(2025)
* [Scaling Autonomous Agents via Automatic Reward Modeling And Planning](https://huggingface.co/papers/2502.12130)(2025)
* [QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search](https://huggingface.co/papers/2502.02584)(2025)
* [PoAct: Policy and Action Dual-Control Agent for Generalized Applications](https://huggingface.co/papers/2501.07054)(2025)
* [Multi-agent Architecture Search via Agentic Supernet](https://huggingface.co/papers/2502.04180)(2025)
* [rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking](https://huggingface.co/papers/2501.04519)(2025)
* [Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search](https://huggingface.co/papers/2502.02508)(2025)
Please give a thumbs up to this comment if you found it helpful!
If you want recommendations for any Paper on Hugging Face checkout[this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers)Space
You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:`[@librarian-bot](https://huggingface.co/librarian-bot)recommend`
EditPreview
Upload images, audio, and videos by dragging in the text input, pasting, orclicking here.
Tap or paste here to upload images
Comment
·[Sign up](https://huggingface.co/join?next=/papers/2501.14304)or[log in](https://huggingface.co/login?next=/papers/2501.14304)to comment
[
Upvote
-
](https://huggingface.co/login?next=/papers/2501.14304)
## Models citing this paper0
No model linking this paper
Cite arxiv.org/abs/2501.14304 in a model README.md to link it from this page.
## Datasets citing this paper0
No dataset linking this paper
Cite arxiv.org/abs/2501.14304 in a dataset README.md to link it from this page.
### Spaces citing this paper0
No Space linking this paper
Cite arxiv.org/abs/2501.14304 in a Space README.md to link it from this page.
## Collections including this paper0
No Collection including this paper
Add this paper to a[collection](https://huggingface.co/new-collection)to link it from this page.
