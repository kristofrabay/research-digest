# (PDF) Chain of Thought Prompting Elicits Reasoning in Large Language Models

**URL:** https://www.researchgate.net/publication/358232899_Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models
**Published:** 2022-01-27T00:00:00.000Z

---

## Summary

The webpage describes a paper titled **"Chain of Thought Prompting Elicits Reasoning in Large Language Models."**

The core idea is that while scaling up language models improves performance on many tasks, they still struggle with complex **reasoning tasks** (like math word problems, symbolic manipulation, and commonsense reasoning).

The paper introduces **Chain of Thought (CoT) prompting** as a method to enable these large models to perform better on these reasoning tasks. CoT involves prompting the model to generate a series of short sentences that mimic a human's step-by-step reasoning process before giving the final answer.

Key findings related to your query:

*   **Reasoning LLMs & Chain-of-Thought:** CoT prompting allows sufficiently large language models (around 100B parameters or more) to better perform reasoning tasks where standard prompting results in flat scaling curves.
*   **Inference-time Compute:** The paper suggests CoT allows models to allocate more computation (intermediate tokens) to problems requiring more reasoning steps.
*   **Hallucination Reduction/Factuality (Indirectly):** By examining incorrect answers, the study found that many errors in CoT were minor mistakes or logical errors, suggesting the generated chain provides a window into the reasoning path, which can be debugged (though full characterization is an open question). Using an external calculator to verify equations within the chain further improved performance.
*   **Planning with LLMs (Related Concept):** While the paper focuses on eliciting reasoning steps rather than explicit planning algorithms like MCTS, the CoT approach decomposes a multi-step problem into intermediate steps, which is foundational to planning.

The paper demonstrates the effectiveness of CoT across **arithmetic reasoning, symbolic reasoning, and commonsense reasoning** tasks, showing significant performance improvements over standard prompting, especially for challenging problems where model scale alone was insufficient.

The provided webpage text is primarily a list of references and appendices from a research paper titled "(PDF) Chain of Thought Prompting Elicits Reasoning in Large Language Models."

While the title and the content of the appendices discuss **Chain of Thought Prompting** and its application to **reasoning** (especially in math word problems), the text does not directly contain detailed information or definitions for all the specific concepts listed in your query, such as:

*   inference-time compute
*   self-reflection
*   planning with LLMs
*   MCTS (Monte Carlo Tree Search) for language models
*   test-time scaling
*   hallucination reduction and detection
*   grounding
*   factuality

The text focuses heavily on demonstrating how Chain of Thought (CoT) prompting elicits reasoning, analyzing errors in CoT, and comparing CoT to finetuning, particularly on arithmetic reasoning tasks (like GSM8K).

**Summary based on the query and the text:**

The document is about **Chain of Thought Prompting** and how it **elicits reasoning** in Large Language Models, particularly for solving math word problems. It analyzes the correctness and types of errors in the reasoning chains generated by the model. While it strongly relates to the "Reasoning LLMs" and "chain-of-thought" aspects of your query, it does not cover the other advanced topics like MCTS, self-reflection, or hallucination reduction mentioned.

**Conclusion:**

No answer found for the full scope of your query, as the text is focused specifically on Chain of Thought prompting and reasoning analysis, not the broader set of advanced techniques you listed.

---

## Full Content

PreprintPDF Available

# Chain of Thought Prompting Elicits Reasoning in Large Language Models

- January 2022

DOI: [10.48550/arXiv.2201.11903](http://dx.doi.org/10.48550/arXiv.2201.11903)

- License
- [CC BY 4.0](https://www.researchgate.net/deref/https%3A%2F%2Fcreativecommons.org%2Flicenses%2Fby%2F4.0%2F)

Authors:

[Jason Wei](https://www.researchgate.net/scientific-contributions/Jason-Wei-2149924474)

[Jason Wei](https://www.researchgate.net/scientific-contributions/Jason-Wei-2149924474)

- This person is not on ResearchGate, or hasn't claimed this research yet.


[Xuezhi Wang](https://www.researchgate.net/scientific-contributions/Xuezhi-Wang-2141575721)

[Xuezhi Wang](https://www.researchgate.net/scientific-contributions/Xuezhi-Wang-2141575721)

- This person is not on ResearchGate, or hasn't claimed this research yet.


[Dale Schuurmans](https://www.researchgate.net/scientific-contributions/Dale-Schuurmans-33801279)

[Dale Schuurmans](https://www.researchgate.net/scientific-contributions/Dale-Schuurmans-33801279)

- This person is not on ResearchGate, or hasn't claimed this research yet.


[Maarten Bosma](https://www.researchgate.net/scientific-contributions/Maarten-Bosma-2199768876)

[Maarten Bosma](https://www.researchgate.net/scientific-contributions/Maarten-Bosma-2199768876)

- This person is not on ResearchGate, or hasn't claimed this research yet.


Show all 7 authorsHide

Preprints and early-stage research may not have been peer reviewed yet.

[Download file PDF](https://www.researchgate.net/publication/358232899_Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models/fulltext/61f7535a007fb50447277bad/Chain-of-Thought-Prompting-Elicits-Reasoning-in-Large-Language-Models.pdf)

[Read file](https://www.researchgate.net/publication/358232899_Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models#read)

[Download citation](https://www.researchgate.net/publication/358232899_Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models/citation/download)

Copy link Link copied

[Read file](https://www.researchgate.net/publication/358232899_Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models#read) [Download citation](https://www.researchgate.net/publication/358232899_Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models/citation/download)
Copy link Link copied

## Abstract and Figures

Although scaling up language model size has reliably improved performance on a range of NLP tasks, even the largest models currently struggle with certain reasoning tasks such as math word problems, symbolic manipulation, and commonsense reasoning. This paper explores the ability of language models to generate a coherent chain of thought -- a series of short sentences that mimic the reasoning process a person might have when responding to a question. Experiments show that inducing a chain of thought via prompting can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves.

[Chain of thought (highlighted) facilitates multistep reasoning in large language models. The output here is from a 137B parameter language model.\
\
…](https://www.researchgate.net/figure/Chain-of-thought-highlighted-facilitates-multistep-reasoning-in-large-language-models_fig1_358232899)

[Employing chain of thought enables language models to solve problems for which standard prompting has a mostly flat scaling curve.\
\
…](https://www.researchgate.net/figure/Employing-chain-of-thought-enables-language-models-to-solve-problems-for-which-standard_fig2_358232899)

[Of 50 random exam- ples where the model returned the correct final answer, all\
\
…](https://www.researchgate.net/figure/Of-50-random-exam-ples-where-the-model-returned-the-correct-final-answer-all_tbl1_358232899)

[Ablation study for different variants of chain of thought. These results are for the 137B parameter model. Standard deviation is shown over 5 random seeds which shuffled the order of the few- shot exemplars.\
\
…](https://www.researchgate.net/figure/Ablation-study-for-different-variants-of-chain-of-thought-These-results-are-for-the-137B_tbl2_358232899)

[+7](https://www.researchgate.net/figure/Example-of-correct-chains-of-thought-produced-by-the-model-for-the-GSM8K-dataset_tbl3_358232899)

[Example of correct chains of thought produced by the model for the GSM8K dataset.\
\
…](https://www.researchgate.net/figure/Example-of-correct-chains-of-thought-produced-by-the-model-for-the-GSM8K-dataset_tbl3_358232899)

Figures - available via license: [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/)

Content may be subject to copyright.

**Discover the world's research**

- 25+ million members
- 160+ million publication pages
- 2.3+ billion citations

[Join for free](https://www.researchgate.net/signup.SignUp.html)

Available via license: [CC BY 4.0](https://www.researchgate.net/deref/https%3A%2F%2Fcreativecommons.org%2Flicenses%2Fby%2F4.0%2F)

Content may be subject to copyright.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Jason Wei 1XuezhiWang 1Dale Schuurmans 1MaartenBosma 1EdChi 1QuocLe 1DennyZhou 1

Abstract

Although scaling up language model size has re-

liably improved performance on a range of NLP

tasks, even the largest models currently struggle

with certain reasoning tasks such as math word

problems, symbolic manipulation, and common-

sense reasoning.This paper explores the ability

of language models to generate a coherent chain

of thought—a series of short sentences that mimic

the reasoning process a person might have when

responding to a question. Experiments show that

inducing a chain of thought via prompting can en-

able sufﬁciently large language models to better

perform reasoning tasks that otherwise have ﬂat

scaling curves.

1\. Introduction

The recent NLP landscape has been revolutionized by lan-

guage models (Devlin et al.,2019;Brown et al.,2020,inter

alia).Scaling up the size of language models has led to

improvements in performance and sample efﬁciency, with

trends spanning many orders of magnitude (Kaplan et al.,

2020).Language models at the scale of 100B or more pa-

rameters have achieved strong performance on NLP tasks

such as sentiment analysis and topic classiﬁcation, even in

few-shot and zero-shot settings (Brown et al.,2020;Wei

et al.,2021,inter alia). One interpretation of this ﬁnding is

that large language models successfully perform system-1

tasks (Stanovich & West,2000), which are done quickly

and intuitively by humans.

The other class of tasks—system-2 tasks—requires slow and

deliberate thinking (often with multiple steps), and includes

logical and mathematical reasoning tasks, among others.

Language models often struggle on tasks of this kind, even

when scaled to hundreds of billions of parameters (Rae et al.,

2021). In other words, many tasks in this category have ﬂat

scaling curves—simply increasing model scale does not

lead to substantive performance gains.

1

Google Brain.Correspondence to:Jason Wei, Denny Zhou.

Q: Shawn has five toys. For Christmas, he got two

toys each from his mom and dad. How many toys

does he have now?

A: Shawn started with 5 toys. If he got 2 toys each

from his mom and dad, then that is 4 more toys. 5

\+ 4 = 9. The answer is 9.

John takes care of 10 dogs. Each dog takes .5

hours a day to walk and take care of their

business. So that is 10 x .5 = 5 hours a day. 5

hours a day x 7 days a week = 35 hours a week.

The answer is 35 hours a week.

Chain of thought prompting

Q: Shawn has five toys. For Christmas, he got two

toys each from his mom and dad. How many toys

does he have now?

A: The answer is 9.

The answer is 50.

Input:

Model !

output:

Standard prompting

Input:

Model !

output:

Q: John takes care of 10 dogs. Each dog takes .5

hours a day to walk and take care of their

business. How many hours a week does he

spend taking care of dogs?

A:

…

Q: John takes care of 10 dogs. Each dog takes

.5 hours a day to walk and take care of their

business. How many hours a week does he

spend taking care of dogs?

A:

…

Figure 1.

Chain of thought (highlighted) facilitates multistep rea-

soning in large language models. The output here is from a 137B

parameter language model.

In this paper, we explore chain of thought prompting as

a method for improving the ability of language models to

perform reasoning tasks. Similar to how humans naturally

deliberate when presented with a multi-step reasoning prob-

lem, it might be beneﬁcial if language models could analo-

gously generate a coherent chain of thought before arriving

at the answer. The intuition is that a chain of thought allows

language models to decompose a multi-step problem into

intermediate steps that are solved individually, instead of

arXiv:2201.11903v1 \[cs.CL\] 28 Jan 2022

Chain of Thought Prompting Elicits Reasoning in Large Language Models

solving an entire multi-hop problem in a single forward

pass.Chain of thought prompting is simple and improves

performance across various reasoning tasks where standard

few-shot prompting is insufﬁcient.Crucially, our experi-

ments suggest that successful chain of thought prompting is

an emergent property of model scale—that is, the beneﬁts

of chain of thought prompting only materialize at sufﬁcient

model scale (around 100B parameters).

2\. Chain of Thought

Consider one’s own thought process when solving a type-2

task such as a multi-step math word problem, where it is

typical to decompose the problem into intermediate steps

and solve each before giving the ﬁnal answer (e.g., “After

Jane gives 2 ﬂowers to her mom she has 10

. . .

then after

she gives 3 to her dad she will have 7

. . .

so the answer is

7.”).The goal of this paper is to endow language models

with the ability to generate a similar chain of thought—a

coherent series of short sentences that lead to the answer

for a reasoning problem.We will show that sufﬁciently

large language models can generate chains of thought if

demonstrations of chain of thought reasoning are provided

in the exemplars for few-shot prompting.

Figure 1shows an example of a model producing a chain

of thought to solve a math word problem that it would have

otherwise gotten incorrect. The chain of thought in this case

resembles a solution and can interpreted as one, but we still

opt to call it a chain of thought to better capture the idea that

it mimics the step-by-step thought process for arriving at

the answer (and also, solutions/explanations typically come

after the ﬁnal answer; e.g., Narang et al. (2020)).

Chain of thought prompting has several attractive properties

as an approach for facilitating reasoning in language models.

(1)

First, producing a chain of thought, in principle, allows

models to decompose multi-step problems into intermediate

steps, which means that additional computation can be allo-

cated to problems that require more reasoning steps.

(2)

Sec-

ond, a generated chain of thought provides an interpretable

window into the behavior of the model, suggesting how it

might have arrived at a particular answer and providing op-

portunities to debug where the reasoning path went wrong

(although fully characterizing a model’s computations that

support an answer remains an open question).

(3)

Third,

chain of thought reasoning can be used for a range of tasks

including math word problems, symbolic manipulation, and

commonsense reasoning, and is applicable (at least in prin-

ciple) to any task that humans can solve via language.

(4)

Finally, chain of thought reasoning can be readily elicited in

sufﬁciently large off-the-shelf language models simply by

including examples of chain of thought sequences into the

exemplars of few-shot prompting. In empirical experiments

on arithmetic reasoning (Section 3), symbolic reasoning

(Section 4), and commonsense reasoning (Section 5), we

show that chain of thought prompting dramatically improves

performance on several datasets where standard few-shot

prompting is insufﬁcient.

3\. Arithmetic Reasoning

We begin the investigation by considering math word prob-

lems of the form depicted in Figure 1, which probe the

ability of language models perform arithmetic reasoning,

focusing on problems solvable by elementary school chil-

dren (ages 6-10). Though simple, arithmetic reasoning is a

task where language models can exhibit a ﬂat scaling curve.

Strikingly, we ﬁnd that providing a prompt comprising a

few chain of thought reasoning patterns can dramatically

improve performance on difﬁcult math word problems for

language models of sufﬁcient scale.

3.1. Experimental Setup

Datasets.

Following both the Math Word Problem Repos-

itory (Koncel-Kedziorski et al.,2016), and more-recent

benchmarks (Cobbe et al.,2021), we consider the following

six datasets: (1) SingleOp (Roy et al.,2015), (2) SingleEq

(Koncel-Kedziorski et al.,2015), (3) AddSub (Hosseini

et al.,2014), (4) ASDiv (Miao et al.,2020), (5) MultiArith

(Roy & Roth,2015), and (6) GSM8K (Cobbe et al.,2021).

Example problems are shown in Table 10 in the Appendix.

Standard prompting.

As a baseline approach, we consider

standard few-shot prompting, popularized by Brown et al.

(2020), in which a language model is given in-context exem-

plars of input–output pairs before outputting a prediction for

a test-time example. Exemplars are formatted as questions

and answers before being fed into the model, and the model

gives the answer directly, as shown in Figure 1(top).

Chain of thought prompting.

The proposed approach is to

augment each exemplar in few-shot prompting with a chain

of thought for an associated answer, as illustrated in Figure 1

(bottom). As most of the datasets only have an evaluation

split, we manually composed a single set of eight generic

few-shot exemplars with chains of thought for prompting—

Figure 1(bottom) shows one chain of thought exemplar, and

the full set of exemplars is given in Table 19 in the Appendix.

(These exemplars did not undergo prompt engineering, and

we assess robustness to alternative exemplars and chains of

thought in Section 3.4.)

To investigate whether chain of thought prompting in this

form can successfully elicit successful reasoning across a

wide range of math word problems, we used this single

set of eight chain of thought exemplars for all six datasets.

For every experiment, we report averaged results over ﬁve

random seeds, where each seed had a different randomly

shufﬂed order of exemplars.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

110 100

0

50

100

75

25

SingleOp SingleEqAddSub ASDiv

Model scale (# parameters, in billions)

Solve rate (%)

110 100110 100110 100

Standard prompting

Chain of thought prompting

Figure 2.

When scaling up the model alone already allows models

to solve math word problems, chain of thought prompting does as

well or better.

Language model.

We use a collection of dense left-to-

right, decoder-only transformer language models.These

models have sizes of 422M, 2B, 8B, 68B, and 137B param-

eters respectively.Models were pretrained on a mixture of

web documents, dialog data, and Wikipedia, tokenized into

2.49T BPE tokens with a 32k vocabulary using the Senten-

cePiece library (Kudo & Richardson,2018). Full training

details are given in (Thoppilan et al.,2022, LaMDA-PT)

We sample from the model using greedy decoding.

3.2. Results

We ﬁrst compare the performance of chain of thought

prompting to standard prompting.The results can be divided

by dataset into two groups. First, as shown in Figure 2, we

see that standard prompting already leads to upward scal-

ing curves in the four datasets that contain easier problems

(SingleOp, SingleEq, AddSub, and ASDiv), achieving rea-

sonable problem solving ability at the model scale of 100B

parameters. In these cases, adding chain of thought prompt-

ing improves accuracy only slightly for the larger models.

For the two more challenging datasets, on the other hand,

the effect of chain of thought prompting becomes salient. As

seen in Figure 3, increasing model scale for standard prompt-

ing does not improve performance on these datasets—the

scaling curve is mostly ﬂat. When adding chain of thought

prompting, however, the model is now able to achieve per-

formance that increases with model scale. Notably, chain of

thought prompting does better than standard prompting only

at the scale of

∼

100B parameters; models of smaller scale

produced ﬂuent but illogical chains of thought, leading to

lower performance than standard prompting.

To better understand why chain of thought prompting works,

we manually examine model-generated chains of thought for

both correct and incorrect answers in the GSM8K dataset.

Some examples are shown in Table 1. Of 50 random exam-

ples where the model returned the correct ﬁnal answer, all

0

20

40

30

10

MultiArith GSM8K

Model scale (# parameters, in billions)

Solve rate (%)

110 100

Standard prompting

Chain of thought prompting

110 100

50

0

5

10

15

Figure 3.

Employing chain of thought enables language models

to solve problems for which standard prompting has a mostly ﬂat

scaling curve.

of the generated chains of thought were also logically and

mathematically correct except for one, which coincidentally

arrived at the correct answer through incorrect reasoning.

1

We also randomly examine 50 random samples for which

the model gave the wrong answer.The summary of this

analysis is that 46% of the chains of thought were almost

correct, barring minor mistakes (calculator error, symbol

mapping error, or one reasoning step missing), and that

the other 54% of the chains of thought had major errors in

semantic understanding or coherence.2

Notably, using an external calculator is a complementary

method for improving chain of thought prompting. We use

a Python program as a calculator to evaluate all equations

in generated chains of thought; when there are multiple

equations in a chain of thought, we propagate the external

calculator results from one equation to the following equa-

tions via string matching.

3

We ﬁnd that the performance

of chain of thought prompting on GSM8K improves from

14.8% to 19.5%.Despite the fact that chain of thought

prompting result only 8 few-shot exemplars, this compares

favorably with GPT-3 175B with a calculator ﬁnetuned on

500 examples (Cobbe et al.,2021), which gets 18%.

3.3. Ablation Study

The observed beneﬁts of using chain of thought prompting

raises the natural question of why it helps, and whether the

same performance improvements can be conferred via other,

similar types of prompting.So we perform an ablation study

with three variations of chain of thought. These variations

are described below, with the results shown in Table 2(we

focus on the two datasets with biggest performance improve-

ment, MultiArith and GSM8K).

1For the full analysis, see Appendix A.1.

2For the full analysis, see Appendix A.2.

3

E.g., “20+10=25, 25+5=30” will be updated to “20+10=30,

30+5=35” after adding the calculator.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Table 1.

Examples of correct and incorrect chains of thought pro-

duced by the 137B model on the GSM8K dataset.Even more

examples are given in Table 5–Table 8in the Appendix.

QUESTION:

Ducks need to eat 3.5 pounds of insects each

week to survive.If there is a ﬂock of ten ducks, how many

pounds of insects do they need per day?

MODEL ANSWER (CORRECT):

Ducks need 3.5 pounds

of insects each week. If there is a ﬂock of 10 ducks, then they

need 3.5 x 10 = 35 pounds of insects each week. If they need

35 pounds of insects each week, then they need 35 / 7 = 5

pounds of insects each day. The answer is 5. X

QUESTION:

It takes Matthew 3 minutes to dig a small hole

for shrubs and 10 minutes to dig a large hole for trees.How

many hours will it take him to dig 30 small holes and 15 large

holes?

MODEL ANSWER (CORRECT):

It takes Matthew 3 min-

utes to dig a small hole and 10 minutes to dig a large hole. So

it takes Matthew 3 x 30 = 90 minutes to dig 30 small holes. It

takes Matthew 10 x 15 = 150 minutes to dig 15 large holes. So

it takes Matthew 90 + 150 = 240 minutes to dig 30 small holes

and 15 large holes.240 minutes is 4 hours.The answer is 4

hours. X

QUESTION:

I have 10 liters of orange drink that are two-thirds

water and I wish to add it to 15 liters of pineapple drink that is

three-ﬁfths water. But as I pour it, I spill one liter of the orange

drink. How much water is in the remaining 24 liters?

MODEL ANSWER (INCORRECT):

I have 10 liters of or-

ange drink that are two-thirds water. I have 15 liters of pineap-

ple drink that is three-ﬁfths water. So I have 10 + 15 = 25 liters

of drink. I spill one liter of orange drink.So I have 24 liters of

drink. 25 - 24 is 1.The answer is 1. 7

Equation only.

One hypothesis for why chain of thought

prompting might help is that it produces the mathematical

equation to be solved, and so we test a variation where the

model is prompted to only output a mathematical equation

before giving the answer.Although equation only prompting

helps on the four easy datasets where scaling helps (result

shown in Table 11 in the Appendix), for the two challenging

datasets with ﬂat scaling curves, equation only prompting

does not help, as shown in Table 2.This result implies

that the semantics of the questions in these datasets are too

challenging to directly translate into an equation without the

intermediate reasoning steps provided by a chain of thought.

Variable compute only.

Another compelling intuition be-

hind chain of thought reasoning is that it allows the model to

spend more computation (i.e., intermediate tokens) solving

harder problems. To isolate the effect of variable computa-

tion from chain of thought reasoning, we test a conﬁguration

where the model is prompted to only output a sequence of

dots (

. . .

) equal to the number of characters in the equa-

tion needed to solve the problem.This variant performs

about the same as the baseline, which suggests that variable

computation by itself is not the reason for the success of

Table 2.

Ablation study for different variants of chain of thought.

These results are for the 137B parameter model.Standard deviation

is shown over 5 random seeds which shufﬂed the order of the few-

shot exemplars.

MultiArith GSM8k

Standard prompting7.6 ±0.4 6.3 ±0.4

Chain of thought prompting45.0 ±1.6 14.8 ±0.7

Ablations:

Equation only5.7 ±0.6 5.7 ±0.3

Variable compute only7.7 ±0.4 6.0 ±0.3

Thought after answer7.6 ±0.6 5.9 ±0.5

chain of thought prompting, and that there appears to be

meaningful utility from expressing intermediate steps via

natural language, at least in this particular setup.

Chain of thought after answer.

Another potential reason

for the beneﬁt of chain of thought prompting could simply

be that such prompts allow the model to better access rele-

vant knowledge acquired during pretraining. Therefore, we

test an alternative conﬁguration where the chain of thought

prompt is only given after the answer, isolating whether the

model actually depends on the produced chain of thought

to give the ﬁnal answer.This variant performs about the

same as the baseline, which suggests that the sequential

reasoning embodied in the chain of thought is useful for

reasons beyond just activating knowledge.

3.4. Robustness of Chain of Thought

As sensitivity to prompt engineering and particular few-

shot exemplars is a key weakness of prompting approaches

(Reynolds & McDonell,2021;Zhao et al.,2021), in this

last subsection we evaluate robustness to chains of thought

written by different annotators.In addition to the results

above, which used chains of thought written by an Annotator

A, two other co-authors of the paper (Annotators B and C)

independently wrote chains of thought for the same few-shot

exemplars.

4

We run the same evaluations on MultiArith

and GSM8K using these chains of thought, and show the

results in Figure 4.In addition, Annotator A also wrote a

separate chain of thought that was more concise than the

original, following the style of solutions given in Cobbe

et al. (2021).

5

We ﬁnd that although there is some variance

between different chain of thought annotations, as might be

expected when using exemplar-based prompting (Le Scao &

Rush,2021;Reynolds & McDonell,2021), all sets of chain

4

These additional chains of thought are shown in Appendix D.

5

For instance, whereas original chain of thought uses several

short sentences (“’There were originally 9 computers.For each of

4 days, 5 more computers were added. So 5 \* 4 = 20 computers

were added. 9 + 20 is 29.”), the concise chain of thought would

read “5 \* 4 = 20 new computers were added. So there are 9 + 20

= 29 new computers in the server room now”.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

0 15105 20

GSM8K:

Chain of thought prompting:

Standard prompting

Annotators A, B, and C

Concise style

Diﬀerent set of exemplars

A

400 2060

MultiArith: AB C

B C

Figure 4.

Chain of thought prompting signiﬁcantly outperforms

standard prompting for chain of thought prompts written by various

annotators, as well as for another set of few-shot exemplars.

of thought prompts outperform the standard baseline by a

signiﬁcant amount. We interpret this result as implying that

successful use of chain of thought does not depend on any

particular linguistic style.

To conﬁrm that successful chain of thought prompting does

not depend on the particular set of eight exemplars that we

composed for this task, we also run experiments with three

sets of eight exemplars randomly sampled from the GSM8K

training set, an independent source (examples in this dataset

already included reasoning steps like a chain of thought).

6

As Figure 4shows, this set of prompts performed compara-

bly with our manually written exemplars, also substantially

outperforming standard prompting.

As further robustness tests, we run two other experiments.

First, since we used eight few-shot exemplars (which ﬁlled

the input length), we conﬁrm that the performance gains of

chain of thought prompting remain robust for varying num-

bers of few-shot exemplars (Appendix A.3 and Figure 7).

Second, we show that chain of thought prompting general-

izes to other language models by running experiments using

GPT-3 (Brown et al.,2020).We ﬁnd that the results for

GPT-3

davinci

are comparable with those for our 137B

model (for full analysis, see Appendix A.3 and Figure 8).

4\. Symbolic Reasoning

We next investigate the ability of language models to per-

form symbolic reasoning tasks.Although the symbolic

reasoning tasks we consider are simple for humans, lan-

guage models typically exhibit a ﬂat scaling curve on them.

We show that solving intermediate steps of a symbolic rea-

soning task via chain of thought allows models to perform

tasks that are not solvable with standard prompting alone.

6

We sample examples

≤60

tokens to ﬁt into our input context

window, and also limit the examples to

≤2

steps to solve for a fair

comparison with the eight exemplars that we composed.

4.1. Symbolic Reasoning Tasks

We use the following three toy tasks that test a diverse set

of symbolic manipulation functions.

•Last letter concatenation.

This ﬁrst task asks the model

to concatenate the last letters of words in a name (e.g.,

“Amy Brown”

→

“yn”). It is a more challenging version

of ﬁrst letter concatenation, which language models can

already perform with good accuracy without chain of

thought.

7

We generate full names by randomly concate-

nating names from the top one-thousand ﬁrst and last

names from name census data.8

•Reverse list.

This task asks the model to reverse the order

of a list of objects (e.g., “sunglasses, monkey, phone, key”

→

“key, phone, monkey, sunglasses”). We generate these

lists of words by randomly sampling everyday objects.9

•Coin ﬂip.

This ﬁnal task asks the model to answer

whether a coin is still heads up after people either ﬂip

or do not ﬂip the coin (e.g., “A coin is heads up. Phoebe

ﬂips the coin. Osvaldo does not ﬂip the coin.Is the coin

still heads up?” →“no”).

As the construction of these symbolic reasoning tasks is

well-deﬁned, for each task we split our test set into an in-

domain test set for which examples had the same number

of steps as the training/few-shot exemplars, as well as two

out-of-domain (OOD) test sets, for which evaluation exam-

ples had more steps than those in training.For last letter

concatenation, the model only sees exemplars of names with

two words, and then performs last letter concatenation on

names with three and four words.

10

We do the same for

the number of items in the reverse list task (in-domain = 5,

OOD = {6, 7}) and the number of potential ﬂips in the coin

ﬂip task (in-domain = 2, OOD = {3, 4}).

4.2. Methods and Models

We use the same methods and models as in the prior sec-

tion, comparing standard prompting with chain of thought

prompting. We again manually compose chains of thought

for the few-shot exemplars for each task, which are illus-

trated in Table 3on the next page.

4.3. Results

Figure 5shows the results of these in-domain and OOD

evaluations. For in-domain evaluations, standard prompting

achieves low but non-zero accuracy, whereas employing

7

We tested 10 common names using GPT-3

davinci

and it

got all but one correct.

8Data from https://namecensus.com/.

9https://www.vocabulary.com/lists/189583

10

For names of length longer than 2 words, we concatenate

multiple ﬁrst and last names together.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Table 3.

Few-shot exemplars for chain of thought prompting for

symbolic reasoning datasets.Chains of thought are highlighted.

Exact prompts with all exemplars are shown in Table 20–Table 22

in the Appendix.

LAST LETTER CONCATENATION

Q:

Take the last letters of the words in "Elon Musk" and con-

catenate them.

A:

The last letter of “Elon” is “n”. The last letter of “Musk” is

“k”. Concatenating them is “nk”. So the answer is nk.

REVERSE LIST

Q: Reverse the sequence "glasses, pen, alarm, license".

A:

First is glasses. Second is pen. Third is alarm. Fourth is

license. Now to reverse, change the order to: Fourth is license.

Third is alarm. Second is pen. First is glasses. So the answer is

"license, alarm, pen, glasses".

COIN FLIP

Q:

A coin is heads up. Maybelle ﬂips the coin.Shalonda does

not ﬂip the coin. Is the coin still heads up?

A:

The coin was ﬂipped by Maybelle. So the coin was ﬂipped

1 time, which is an odd number. The coin started heads up, so

after an odd number of ﬂips, it will be tails up. So the answer

is no.

chain of thought prompting leads to a large majority of the

problems being solved.Note that these in-domain eval-

uations are “toy tasks” in the sense that perfect solution

structures are already provided by the chains of thought in

the few-shot exemplars; all the model has to do is repeat

the same steps with the new symbols in the test-time exam-

ple.And yet, small language models still fail—the ability

to perform abstract manipulations on unseen symbols for

these three tasks only arises at the scale of 100B model

parameters.

As for the OOD evaluations, observe that standard prompt-

ing fails almost completely. With chain of thought prompt-

ing on the other hand, language models achieve upward

scaling curves, though performance is lower than in the

in-domain setting. Interestingly, for the last letter concate-

nation and reverse list tasks, there is a larger model scale

requirement for OOD generalization—while the 68B model

had signiﬁcant boosts from chain of thought prompting for

in-domain evaluation, only the 137B model showed OOD

generalization on these two tasks.

5\. Commonsense Reasoning

In this ﬁnal experimental investigation, we consider whether

the language-based nature of chain of thought also makes

it applicable to commonsense reasoning, which involves

reasoning about physical and human interactions under the

presumption of general background knowledge. Common-

sense reasoning is key for building models that interact with

1

10

100

2 flips

(in domain)

Accuracy (%)

OOD: 3 flips

OOD: 4 flips

1

10

100

1

10

100

50

60

70

80

100

90

Model scale (# parameters in billions)

Coin Flip

1

10

100

1

10

100

1

10

100

100

0

50

75

25

Solve rate (%)

OOD: 6 items

OOD: 7 items

5 items

(in domain)

Reverse List

1

10

100

1

10

100

1

10

100

Solve rate (%)

OOD: 3 words

OOD: 4 words

2 words

(in domain)

Last Letter Concatenation

100

0

50

75

25

Standard prompting

Chain of thought prompting

Figure 5.

For three symbolic reasoning tasks, employing chain

of thought facilitates good performance when standard few-shot

prompting is insufﬁcient. Examples of model-produced chains of

thought are shown in Table 12–Table 14 in the Appendix.

the world; although this type of reasoning seems trivial

for humans, it is still beyond the reach of current natural

language understanding systems (Talmor et al.,2021).

5.1. Datasets and Prompts

For evaluation, we choose four datasets to cover a diverse

range of commonsense reasoning types. The popular Com-

monsenseQA (Talmor et al.,2019) asks commonsense rea-

soning questions about the world involving complex se-

mantics that often require prior knowledge.StrategyQA

(Geva et al.,2021) requires models to infer a multi-hop

strategy to answer questions. We also choose two special-

ized evaluation sets from the BIG-bench effort (BIG-bench

collaboration,2021): Date Understanding, which involves

inferring a date from a given context, and Sports Under-

standing, which involves determining whether a sentence

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Table 4.

Few-shot exemplars for chain of thought prompting for

commonsense reasoning datasets.Chains of thought are high-

lighted. Exact prompts with all exemplars are shown in Table 23–

Table 26 in the Appendix.

COMMONSENSEQA

Q:

Sammy wanted to go to where the people were.Where

might he go?

Options:

(a) race track(b) populated areas(c) desert

(d) apartment(e) roadblock

A:

The answer must be a place with a lot of people. Race tracks,

desert, apartments, and roadblocks don’t have a lot of people,

but populated areas do. So the answer is (b).

STRATEGYQA

Q:

Yes or no:Could a llama birth twice during War in Vietnam

(1945-46)?

A:

The War in Vietnam was 6 months. The gestation period

for a llama is 11 months. So a llama could not give birth twice

during the War in Vietnam. So the answer is no.

DATE UNDERSTANDING

Q:

The concert was scheduled to be on 06/01/1943, but was

delayed by one day to today. What is the date 10 days ago in

MM/DD/YYYY?

A:

One day after 06/01/1943 is 06/02/1943, so today is

06/02/1943. 10 days before today is 05/23/1943. So the an-

swer is 05/23/1943.

SPORTS UNDERSTANDING

Q:

Is the following sentence plausible? "Joao Moutinho caught

the screen pass in the NFC championship."

A:

Joao Moutinho is a soccer player. The NFC championship

is part of American football, not soccer. So the answer is no.

relating to sports is plausible or implausible. Table 4shows

examples of exemplars for each of the four datasets.

We again follow the same experimental setup as the prior

two sections.For CommonsenseQA and StrategyQA, we

randomly selected eight examples from the training set and

manually composed chains of thought for them to use as

few-shot exemplars. The two BIG-bench tasks do not have

training sets, so we selected the ﬁrst ten examples as exem-

plars in the evaluation set as few-shot exemplars and report

numbers on the rest of the evaluation set.

5.2. Results

The results of these experiments are shown in Figure 6.

Using standard prompting, CommonsenseQA, StrategyQA,

and Date Understanding all showed considerable improve-

ments as model size increased. For these tasks, employing

chain of thought prompting led to small but noteworthy im-

provements in performance, allowing the few-shot model

to surpass ﬁnetuned BERT-large for CommonsenseQA and

ﬁnetuned RoBERTa for StrategyQA. Sports understanding

1

10

100

1

10

100

1

10

100

1

10

100

20

30

40

50

60

45

50

55

60

65

0

10

20

30

50

60

70

80

90

Sports

Understanding

Date

Understanding

StrategyQA

CommonsenseQA

Model size (# parameters in billions)

Accuracy (%)

Accuracy (%)

BERT-large

RoBERTa

Sports enthusiast

Human with limited

interest in sports

Standard prompting

Chain of thought prompting

Figure 6.

Compared with standard prompting, chain of thought

prompting also improves performance on various types of com-

monsense reasoning tasks. Examples of model-produced chains of

thought are shown in Table 15–Table 18 in the Appendix.

had the biggest improvement from chain of thought prompt-

ing, with the 68B model’s chain of thought performance

surpassing that of a human with limited interest in sports,

and the 137B model’s chain of thought performance surpass-

ing that of an unaided sports enthusiast (see task description

in BIG-bench collaboration,2021).

6\. Discussion

We have explored chain of thought prompting as a simple

mechanism for eliciting multi-step reasoning behavior in

large language models.We observe that chainof thought

prompting substantially improves performance on challeng-

ing math word problems, yielding improvements that are

much stronger than ablations and robust to different an-

notators and models (Section 3).We then extended the

evaluation to symbolic reasoning, showing that chain of

thought prompting facilitates OOD generalization, and that

sufﬁcient model scale is critical even when exact structural

chains of thought are present in exemplars (Section 4). Fi-

nally, experiments on several commonsense reasoning tasks

demonstrate how the linguistic nature of chain of thought

reasoning makes it generally applicable (Section 5). We em-

phasize that in all experiments, chain of thought reasoning

is elicited simply by prompting an off-the-shelf language

model (i.e., no language models were ﬁnetuned).

Chain of Thought Prompting Elicits Reasoning in Large Language Models

The emergence of chain of thought reasoning as a conse-

quence of model scale has been a prevailing theme of these

experiments.For six reasoning tasks where standard prompt-

ing has a ﬂat scaling curve, chain of thought prompting leads

to dramatically increasing scaling curves for sufﬁciently

large language models. Chain of thought prompting appears

to expand the set of tasks that large language models can per-

form successfully—in other words, our work underscores

that standard prompting only provides a lower bound on the

capabilities of large language models in principle. This ob-

servation likely raises more questions than it answers—for

instance, how much more can we expect reasoning ability

to improve with a further increase in model scale?What

other prompting methods might expand the range of tasks

that language models can solve?

Ironically, the dependence on chain of thought prompting

and sufﬁciently large models are both key components and

major limitations of our proposed approach.Although

the cost of manually augmenting exemplars with chains

of thought is minimal in the few-shot setting, the annota-

tion costs could be prohibitive for ﬁnetuning (though this

could potentially be surmounted with synthetic data genera-

tion, or zero-shot generalization). The emergence of chain

of thought reasoning only at large model scale makes it

costly to serve in real-world applications; future work could

explore how to induce reasoning at smaller model scales.

Moreover, chain of thought prompting improving the scal-

ing curve is not equivalent to solving the tasks; compared

with human accuracy, there is still notable headroom on all

tasks we explore. Finally, we qualify that although chain of

thought emulates the thought processes of human reasoners,

this does not answer whether the neural network is actually

“reasoning”, which we leave as an open question.

7\. Related Work

The recent success of large-scale language models has led

to growing interest in improving their capability to perform

tasks via prompting (Brown et al. (2020), and see Liu et al.

(2021) for a survey), as well specialized methods to enhance

their reasoning abilities (Andor et al.,2019;Ran et al.,2019;

Geva et al.,2020;Pi˛ekos et al.,2021;Lourie et al.,2021).

This paper falls in the category of general prompting ap-

proaches; but whereas most prompting techniques focus on

how to optimize the inputs/prompts for a given task (Li &

Liang,2021;Lester et al.,2021;Reif et al.,2021;Wei et al.,

2021,inter alia), we leverage prompting in an orthogonal

way by guiding the model to produce self-assisting outputs.

Another closely related direction uses natural language ex-

planations (NLEs) to improve model interpretability (Han-

cock et al.,2018;Wiegreffe & Marasovi´

c,2021,inter alia).

That line of work typically focuses on natural language infer-

ence (Camburu et al.,2018;Yordanov et al.,2021;Bostrom

et al.,2021), and produces explanations either simultane-

ously to or after the ﬁnal prediction (Narang et al.,2020;

Majumder et al.,2021;Wiegreffe et al.,2021).By contrast,

the chain of thought processing considered in this paper

occurs before the ﬁnal answer.And while NLE aims mostly

to improve neural network interpretability (Rajagopal et al.,

2021), the goal of chain of thought prompting is to allow

models to decompose multi-hop reasoning tasks into multi-

ple steps—interpretability is just a side effect.Marasovi´

c

et al. (2021) show that prompt-based ﬁnetuning with NLE

improves NLI and classiﬁcation performance; in compar-

ison, our work focuses on a range of tasks that require

multi-hop reasoning.

Other works have explored ideas similar to chain of thought,

including expressing formal rules using language (Clark

et al.,2020;Saeed et al.,2021), ﬁnetuning language models

on manually annotated or synthetic datasets (Rajani et al.,

2019;Talmor et al.,2020;Liang et al.,2021;Gu et al.,

2021), and inducing programs (Ling et al.,2017;Chen et al.,

2021).Compared with these methods that use ﬁnetuning

and are often task-speciﬁc, our work is the ﬁrst to use a

prompting-based approach to show that reasoning can be

elicited in off-the-shelf language models of sufﬁcient scale.

Two recent works perhaps relate most closely to the method

in this paper. Cobbe et al. (2021) ﬁnetune language models

to solve math word problems by generating intermediate

reasoning steps, showing that performance can be further

improved by using a separate veriﬁer to check the reasoning

steps.The approach in our paper is far simpler—instead

of ﬁnetuning, we use prompting, which does not modify

the model and thus maintains the ability of a single large

language model to perform many tasks.Nye et al. (2021)

show that language models can evaluate mathematical ex-

pressions and Python programs by performing intermediate

computations on a “scratchpad”.Whereas that work focuses

on using language models to execute programs, chain of

thought prompting generalizes such domain-speciﬁc primi-

tives to natural language, making it open-domain and rele-

vant to all of NLP in principle (one could even view chains

of thought as “natural language programs”).

8\. Conclusions

We have explored chain of thought prompting as a sim-

ple and broadly applicable method for enhancing reason-

ing in language models.Through experiments on arith-

metic, symbolic, and commonsense reasoning, we ﬁnd that

chain of thought processing is an emergent property of

model scale that allows sufﬁciently large language models

to perform reasoning tasks that otherwise have ﬂat scaling

curves.Broadening the range of reasoning tasks that lan-

guage models can perform will hopefully inspire further

work on language-based approaches to reasoning.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Reproducibility Statement

As our results make use of a set of large language models

that is not publicly available, we take the following actions

to facilitate reproducibility.First, we provide the exact input

prompts for all tasks in Table 19–Table 26 in Appendix C

(and emphasize that we do not perform any ﬁnetuning and

only apply prompting to off-the-shelf language models).

Second, we include results using the publicly available GPT-

3

davinci

as an additional language model (see Figure 8).

Finally, we make exact inputs, targets, and predictions for

each task available as a zip ﬁle at

https://github.com/

jasonwei20/chain-of-thought-prompting.

Acknowledgements

We thank Claire Cui,Andrew Dai, and Ellie Pavlick for

providing feedback on the paper. We also thank Yuhuai Wu

for helpful discussions.

References

Andor, D., He, L., Lee, K., and Pitler, E.Giving BERT

a calculator:Finding operations and arguments with

reading comprehension.In Proceedings of the 2019

Conference on Empirical Methods in Natural Language

Processing and the 9th International Joint Conference

on Natural Language Processing (EMNLP-IJCNLP),

2019.doi: 10.18653/v1/D19-1609.URL

https:

//aclanthology.org/D19-1609.

BIG-bench collaboration.Beyond the imitation game: Mea-

suring and extrapolating the capabilities of language mod-

els. In preparation, 2021.URL

https://github.

com/google/BIG-bench/.

Bostrom, K., Zhao, X., Chaudhuri, S., and Durrett, G.Flexi-

ble generation of natural language deductions. In Proceed-

ings of the 2021 Conference on Empirical Methods in Nat-

ural Language Processing, 2021.doi:10.18653/v1/2021.

emnlp-main.506. URL

https://aclanthology.

org/2021.emnlp-main.506.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,

J.D., Dhariwal, P., Neelakantan, A., Shyam, P.,

Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,

Krueger, G., Henighan, T., Child, R., Ramesh, A.,

Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M.,

Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,

Berner, C., McCandlish, S., Radford, A., Sutskever,

I., and Amodei,D.Language models are few-shot

learners.In Advances in Neural Information Processing

Systems, 2020.URL

https://proceedings.

neurips.cc/paper/2020/file/

1457c0d6bfcb4967418bfb8ac142f64a-Paper.

pdf.

Camburu, O.-M., Rocktäschel, T., Lukasiewicz, T., and

Blunsom, P.e-SNLI: Natural language inference with

natural language explanations.In Advances in Neural

Information Processing Systems, 2018.URL

https:

//arxiv.org/pdf/1812.01193.pdf.

Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,

Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,

G., et al.Evaluating large language models trained on

code. arXiv preprint arXiv:2107.03374, 2021.URL

https://arxiv.org/abs/2107.03374.

Clark, P., Tafjord, O., and Richardson, K.Transformers

as soft reasoners over language.Proceedings of the In-

ternational Joint Conference on Artiﬁcial Intelligence

(IJCAI), 2020.URL

https://www.ijcai.org/

proceedings/2020/0537.pdf.

Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano,

R., Hesse, C., andSchulman, J.Trainingveri-

ﬁers to solve math word problems.arXiv preprint

arXiv:2110.14168, 2021.URL

https://arxiv.

org/abs/2110.14168.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.BERT:

Pre-training of deep bidirectional transformers for lan-

guage understanding.In Proceedings of the 2019 Confer-

ence of the North American Chapter of the Association

for Computational Linguistics: Human Language Tech-

nologies, Volume 1 (Long and Short Papers), 2019.URL

https://aclanthology.org/N19-1423.

Geva, M., Gupta, A., and Berant, J.Injecting numerical

reasoning skills into language models.In Proceedings

of the 58th Annual Meeting of the Association for Com-

putational Linguistics, 2020.doi:10.18653/v1/2020.

acl-main.89. URL

https://aclanthology.org/

2020.acl-main.89.

Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and

Berant, J.Did aristotle use a laptop?A question an-

swering benchmark with implicit reasoning strategies.

Transactions of the Association for Computational Lin-

guistics, 2021\. doi:10.1162/tacl\_a\_00370. URL

https:

//aclanthology.org/2021.tacl-1.21.

Gu, Y., Mishra, B. D., and Clark, P.DREAM: Uncov-

ering mental models behind language models.arXiv

preprint arXiv:2112.08656, 2021.URL

https://

arxiv.org/pdf/2112.08656.pdf.

Hancock, B., Varma, P., Wang, S., Bringmann, M., Liang,

P., and Ré, C.Training classiﬁers with natural language

explanations.In Proceedings of the 56th Annual Meeting

of the Association for Computational Linguistics (Volume

1:Long Papers), 2018\. doi:10.18653/v1/P18-1175. URL

https://aclanthology.org/P18-1175.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Hosseini, M. J., Hajishirzi, H., Etzioni, O., and Kushman,

N.Learning to solve arithmetic word problems with

verb categorization.In Proceedings of the 2014 Confer-

ence on Empirical Methods in Natural Language Process-

ing (EMNLP),2014\. doi: 10.3115/v1/D14-1058. URL

https://aclanthology.org/D14-1058.

Hu, M., Peng, Y., Huang, Z., and Li, D.A multi-type

multi-span network for reading comprehension that re-

quires discrete reasoning.In Proceedings of the 2019

Conference on Empirical Methods in Natural Language

Processing and the 9th International Joint Conference

on Natural Language Processing (EMNLP-IJCNLP),

2019.doi: 10.18653/v1/D19-1170.URL

https:

//aclanthology.org/D19-1170.

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,

Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and

Amodei, D.Scaling laws for neural language models.

arXiv preprint arXiv:2001.08361, 2020.URL

https:

//arxiv.org/abs/2001.08361.

Koncel-Kedziorski, R., Hajishirzi, H., Sabharwal, A., Et-

zioni, O., and Ang, S. D.Parsing Algebraic Word Prob-

lems into Equations.Transactions of the Association for

Computational Linguistics, 2015.doi:10.1162/tacl\_a\_

00160\. URL

https://doi.org/10.1162/tacl\_

a\_00160.

Koncel-Kedziorski, R., Roy, S., Amini, A., Kushman,

N., and Hajishirzi, H.MAWPS: A math word prob-

lem repository.In Proceedings of the 2016 Confer-

ence of the North American Chapter of the Associa-

tion for Computational Linguistics:Human Language

Technologies, 2016.doi: 10.18653/v1/N16-1136.URL

https://aclanthology.org/N16-1136.

Kudo, T. and Richardson, J.SentencePiece:A sim-

ple and language independent subword tokenizer and

detokenizer for neural text processing.In Proceed-

ings of the 2018 Conference on Empirical Methods in

Natural Language Processing:System Demonstrations,

2018.doi: 10.18653/v1/D18-2012.URL

https:

//aclanthology.org/D18-2012.

Lan, Y., Wang, L., Zhang, Q., Lan, Y., Dai, B. T., Wang, Y.,

Zhang, D., and Lim, E.-P.MWPToolkit:An open-source

framework for deep learning-based math word problem

solvers. arXiv preprint arXiv:2109.00799, 2021.URL

https://arxiv.org/abs/2109.00799.

Le Scao, T. and Rush, A.How many data points is a

prompt worth?In Proceedings of the 2021 Confer-

ence of the North American Chapter of the Associa-

tion for Computational Linguistics:Human Language

Technologies, 2021.doi: 10.18653/v1/2021.naacl-main.

208\. URL

https://aclanthology.org/2021.

naacl-main.208.

Lester, B., Al-Rfou, R., and Constant, N. The power of scale

for parameter-efﬁcient prompt tuning.In Proceedings

of the 2021 Conference on Empirical Methods in Natu-

ral Language Processing, 2021.doi:10.18653/v1/2021.

emnlp-main.243. URL

https://aclanthology.

org/2021.emnlp-main.243.

Li, X. L. and Liang, P.Preﬁx-tuning:Optimizing con-

tinuous prompts for generation.In Proceedings of the

59th Annual Meeting of the Association for Computa-

tional Linguistics and the 11th International Joint Con-

ference on Natural Language Processing (Volume 1:

Long Papers), 2021.doi:10.18653/v1/2021.acl-long.

353\. URL

https://aclanthology.org/2021.

acl-long.353.

Liang, Z., Bethard, S., andSurdeanu, M.Explain-

able multi-hop verbal reasoning through internal mono-

logue.In Proceedings of the 2021 Conference of the

North American Chapter of the Association for Computa-

tional Linguistics: Human Language Technologies, 2021.

doi: 10.18653/v1/2021.naacl-main.97.URL

https:

//aclanthology.org/2021.naacl-main.97.

Ling, W., Yogatama, D., Dyer, C., and Blunsom, P.Pro-

gram induction by rationale generation:Learning to

solve and explain algebraic word problems.In Pro-

ceedings of the 55th Annual Meeting of the Associa-

tion for Computational Linguistics (Volume 1:Long

Papers), 2017.doi:10.18653/v1/P17-1015.URL

https://aclanthology.org/P17-1015.

Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig,

G.Pre-train, prompt,and predict: A systematic survey

of prompting methods in natural language processing.

arXiv preprint arXiv:2107.13586, 2021.URL

https:

//arxiv.org/abs/2107.13586.

Lourie, N., Le Bras, R., Bhagavatula, C., and Choi, Y. UNI-

CORN on RAINBOW: A universal commonsense reason-

ing model on a new multitask benchmark.Proceedings

of the AAAI Conference on Artiﬁcial Intelligence, 2021.

URL https://arxiv.org/abs/2103.13009.

Majumder, B.P., Camburu, O.-M., Lukasiewicz, T.,

and McAuley,J.Rationale-inspired natural lan-

guage explanations with commonsense.arXiv preprint

arXiv:2106.13876, 2021.URL

https://arxiv.

org/abs/2106.13876.

Marasovi´

c, A., Beltagy, I., Downey, D., andPeters,

M. E.Few-shot self-rationalization with natural language

prompts. arXiv preprint arXiv:2111.08284, 2021\. URL

http://arxiv.org/abs/2111.08284.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Miao, S. Y., Liang,C. C.,and Su,K. Y.A diverse cor-

pus for evaluating and developing English math word

problem solvers.In Proceedings of the 58th Annual

Meeting of the Association for Computational Linguistics,

2020\. doi: 10.18653/v1/2020.acl-main.92. URL

https:

//aclanthology.org/2020.acl-main.92.

Narang, S., Raffel, C., Lee, K., Roberts, A., Fiedel,

N., and Malkan,K.WT5?!Training text-to-text

models to explain their predictions.arXiv preprint

arXiv:2004.14546, 2020.URL

https://arxiv.

org/abs/2004.14546.

Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski,

H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A.,

Bosma, M., Luan, D., et al.Show your work:Scratch-

pads for intermediate computation with language mod-

els. arXiv preprint arXiv:2112.00114, 2021.URL

http://arxiv.org/abs/2112.00114.

Pi˛ekos, P., Malinowski, M., and Michalewski, H. Measuring

and improving BERT’s mathematical abilities by predict-

ing the order of reasoning.In Proceedings of the 59th

Annual Meeting of the Association for Computational Lin-

guistics and the 11th International Joint Conference on

Natural Language Processing (Volume 2: Short Papers),

2021\. doi: 10.18653/v1/2021.acl-short.49. URL

https:

//aclanthology.org/2021.acl-short.49.

Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoff-

mann, J., Song, F., Aslanides, J., Henderson, S., Ring,

R., Young, S., et al.Scaling language models: Meth-

ods, analysis & insights from training Gopher.arXiv

preprint arXiv:2112.11446, 2021.URL

https://

arxiv.org/abs/2112.11446.

Rajagopal, D., Balachandran, V., Hovy, E. H., and Tsvetkov,

Y.SelfExplain:A self-explaining architecture for neu-

ral text classiﬁers.In Proceedings of the 2021 Con-

ference on Empirical Methods in Natural Language

Processing, 2021.doi:10.18653/v1/2021.emnlp-main.

64\. URL

https://aclanthology.org/2021.

emnlp-main.64.

Rajani, N. F., McCann, B., Xiong, C., and Socher, R.Ex-

plain yourself!leveraging language models for com-

monsense reasoning.In Proceedings of the 57th An-

nual Meeting of the Association for Computational Lin-

guistics,2019\. doi:10.18653/v1/P19-1487. URL

https://aclanthology.org/P19-1487.

Ran, Q., Lin, Y., Li, P., Zhou, J., and Liu, Z.NumNet:

Machine reading comprehension with numerical reason-

ing.In Proceedings of the 2019 Conference on Empir-

ical Methods in Natural Language Processing and the

9th International Joint Conference on Natural Language

Processing (EMNLP-IJCNLP), 2019\. doi:10.18653/v1/

D19-1251. URL

https://aclanthology.org/

D19-1251.

Reif, E., Ippolito, D., Yuan, A., Coenen, A., Callison-

Burch, C., and Wei, J.A recipe for arbitrary text style

transfer with large language models.arXiv preprint

arXiv:2109.03910, 2021.URL

https://arxiv.

org/abs/2109.03910.

Reynolds, L. and McDonell, K.Prompt programming for

large language models: Beyond the few-shot paradigm.

In Extended Abstracts of the 2021 CHI Conference on

Human Factors in Computing Systems,pp. 1–7, 2021.

URL https://arxiv.org/abs/2102.07350.

Roy, S. and Roth, D.Solving general arithmetic word

problems.In Proceedings of the 2015 Conference on

Empirical Methods in Natural Language Processing,

2015.doi: 10.18653/v1/D15-1202.URL

https:

//aclanthology.org/D15-1202.

Roy, S., Vieira, T., and Roth, D.Reasoning about Quantities

in Natural Language.Transactions of the Association for

Computational Linguistics, 2015.doi:10.1162/tacl\_a\_

00118\. URL

https://doi.org/10.1162/tacl\_

a\_00118.

Saeed, M., Ahmadi, N., Nakov, P., andPapotti, P.Rule-

BERT: Teaching soft rules to pre-trained language mod-

els.In Proceedings of the 2021 Conference on Empirical

Methods in Natural Language Processing, 2021.doi:

10.18653/v1/2021.emnlp-main.110. URL

https://

aclanthology.org/2021.emnlp-main.110.

Stanovich, K. E. and West, R. F.Individual differ-

ences in reasoning:Implications for the rationality de-

bate? Behavioral and brain sciences, 23(5):645–665,

2000\. URL

https://pubmed.ncbi.nlm.nih.

gov/11301544/.

Talmor, A., Herzig, J., Lourie, N., and Berant, J.Com-

monsenseQA: A question answering challenge target-

ing commonsense knowledge.In Proceedings of the

2019 Conference of the North American Chapter of

the Association for Computational Linguistics: Human

Language Technologies, Volume 1 (Long and Short Pa-

pers),2019\. doi: 10.18653/v1/N19-1421. URL

https:

//aclanthology.org/N19-1421.

Talmor, A., Tafjord, O., Clark, P., Goldberg, Y., and Berant,

J.Leap-of-thought: Teaching pre-trained models to sys-

tematically reason over implicit knowledge.Advances

in Neural Information Processing Systems, 2020.URL

https://arxiv.org/abs/2006.06609.

Talmor, A., Yoran, O., Bras, R. L., Bhagavatula, C., Gold-

berg, Y., Choi, Y., and Berant, J.Commonsenseqa 2.0:

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Exposing the limits of ai through gamiﬁcation.Ad-

vances on Neural Information Processing Systems Track

on Datasets and Benchmarks, 2021.URL

https:

//arxiv.org/abs/2201.05320.

Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul-

shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,

Du, Y., et al.Lamda: Language models for dialog appli-

cations. arXiv preprint arXiv:2201.08239, 2022.

Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,

B., Du, N., Dai, A. M., and Le, Q. V.Finetuned lan-

guage models are zero-shot learners.Proceedings of the

International Conference on Learning Representations,

2021\. URL

https://openreview.net/forum?

id=gEZrGCozdqR.

Wiegreffe, S. and Marasovi´

c, A.Teach me to explain: A

review of datasets for explainable nlp. Advances in Neural

Information Processing Systems, 2021.URL

https:

//arxiv.org/abs/2102.12060.

Wiegreffe, S., Hessel, J., Swayamdipta, S., Riedl, M.,

and Choi,Y.Reframing human-AI collaboration

for generating free-text explanations.arXiv preprint

arXiv:2112.08674, 2021.URL

https://arxiv.

org/abs/2112.08674.

Yordanov, Y., Kocijan, V., Lukasiewicz, T., andCam-

buru, O.-M.Few-shot out-of-domain transfer learn-

ing of natural language explanations.arXiv preprint

arXiv:2112.06204, 2021.URL

http://arxiv.org/

abs/2112.06204.

Zhao, T. Z., Wallace, E., Feng, S., Klein, D., and Singh, S.

Calibrate before use:Improving few-shot performance

of language models.Proceedings of the International

Conference on Machine Learning, 2021\. URL

https:

//arxiv.org/abs/2102.09690.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

A. Appendix: Additional Analysis

A.1. Correct Chain of Thought Analysis

As mentioned in the main text, we analyze 50 random exam-

ples of chains of thought that led to correct answers in the

GSM8K dataset.Of these 50, only one arrived at the cor-

rect answer through incorrect reasoning (shown in Table 6:

“correct by chance”).The other 49 had correct logic and

math, with examples shown in Table 5. Four had minor im-

perfections while maintaining coherent and understandable

logic:

•

One had underspeciﬁed statements (shown in Table 6:

“correct but underspeciﬁed statements”).

•

One made an unrelated but correct statement (shown in

Table 6:“correct but unrelated statement”).

•

Two had the correct logic but omitted speciﬁc steps in

the equation, though it was still understandable (shown in

Table 6:“correct but imperfect equation”).

Table 5.

Example of correct chains of thought produced by the

model for the GSM8K dataset.

QUESTION:

James decides to run 3 sprints 3 times a week.

He runs 60 meters each sprint. How many total meters does he

run a week?

MODEL ANSWER (CORRECT):

James decides to run 3

sprints 3 times a week. He runs 60 meters each sprint.So he

runs 60 meters x 3 sprints x 3 times a week. That is 60 meters

x 9. The answer is 540.X

QUESTION:

Brandon’s iPhone is four times as old as Ben’s

iPhone. Ben’s iPhone is two times older than Suzy’s iPhone. If

Suzy’s iPhone is 1 year old, how old is Brandon’s iPhone?

MODEL ANSWER (CORRECT):

Brandon’s iPhone is 4

times as old as Ben’s iPhone.Ben’s iPhone is 2 times older

than Suzy’s iPhone. So Brandon’s iPhone is 4 x 2 = 8 times

older than Suzy’s iPhone.Suzy’s iPhone is 1 year old.So

Brandon’s iPhone is 8 x 1 = 8 years old. The answer is 8. X

QUESTION:

Grandma Jones baked 5 apple pies for the ﬁre-

man’s luncheon. She cut each pie into 8 pieces and set the ﬁve

pies out on the buffet table for the guests to serve themselves.

At the end of the evening, after the guests had taken and eaten

their pieces of pie, there were 14 pieces of pie remaining. How

many pieces were taken by the guests?

MODEL ANSWER (CORRECT):

5 pies were baked and cut

into 8 pieces each. The 5 pies were then served to the guests.8

x 5 = 40 pieces of pie. The guests ate 14 pieces of pie.40 - 14

= 26 pieces of pie were left. The answer is 26.X

QUESTION:

Jean has 30 lollipops. Jean eats 2 of the lollipops.

With the remaining lollipops, Jean wants to package 2 lollipops

in one bag. How many bags can Jean ﬁll?

MODEL ANSWER (CORRECT):

Jean started with 30 lol-

lipops.She ate 2 of them.So she has 28 lollipops left.She

wants to package 2 lollipops in one bag. So she can package

28 / 2 = 14 bags. The answer is 14.X

Table 6.

Of 50 examples that the model got correct, there were 5

salient cases where the chain of thought the model generated was

imperfect. We found the other 45 to be equivalent to what a human

would potentially generate as a ground truth.

QUESTION:

There are 36 penguins sunbathing in the snow.

One-third of them jump in and swim in the ocean.Another

one-third go inside the cave to eat their dinner.How many

penguins are still left sunbathing?

MODEL ANSWER (CORRECT BY CHANCE):

There are 36

penguins.One-third of them jump in and swim in the ocean.

So that is 12 penguins. Another one-third go inside the cave to

eat their dinner. So that is 12 penguins. The answer is 12.

QUESTION:

John has 2 houses with 3 bedrooms each. Each

bedroom has 2 windows each. There are an additional 4 win-

dows in each house not connected to bedrooms.How many

total windows are there between the houses?

MODEL ANSWER (CORRECT BUT UNDERSPECIFIED

STATEMENTS):

There are 2 houses with 3 bedrooms each.

Each bedroom has 2 windows each.So there are 2 x 3 = 6

windows in each house. There are an additional 4 windows in

each house not connected to bedrooms.So there are 4 + 4 =

8 windows in each house. So there are 6 x 2 = 12 windows in

each house. So there are 12 + 8 = 20 windows in both houses.

The answer is 20.

QUESTION:

If Ann is 9 years old and her brother is twice her

age, how old will her brother be in 3 years?

MODEL ANSWER (CORRECT BUT UNRELATED STATE-

MENT):

Ann is 9 years old.Her brother is twice her age.

So her brother is 18 years old.In 3 years, she will be 12.So

her brother will be 18 + 3 = 21 years old. The answer is 21.

QUESTION:

A local gas station is selling gas for $3.00 a

gallon. An app company is offering $.20 cashback per gallon

if you ﬁll up at this station. If someone buys 10 gallons of gas,

how much with their gas be, after the cashback rewards?

MODEL ANSWER (CORRECT BUT IMPERFECT EQUA-

TION):

The gas is originally 3 dollars per gallon.If you

buy 10 gallons, that is 3 x 10 = 30 dollars. If you get 20 cents

off per gallon, that is 20 x 10 = 2 dollars. So the total is 30 - 2

= 28 dollars. The answer is 28 dollars.

QUESTION:

Nissa hires 60 seasonal workers to play elves in

her department store’s Santa village. A third of the elves quit

after children vomit on them, then 10 of the remaining elves

quit after kids kick their shins. How many elves are left?

MODEL ANSWER (CORRECT BUT IMPERFECT EQUA-

TION):

Nissa hires 60 seasonal workers.A third of them

quit. So 60 - 1/3 = 40 elves are left. Then 10 elves quit.So 40 -

10 = 30 elves are left. The answer is 30 elves.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

A.2. Incorrect Chain of Thought Analysis

We also manually analyze 50 randomly sampled outputs of

the model that were incorrect.There are many ways that

a chain of thought can be incorrect, making the design of

error categorization non-trivial. We decided to categorize

errors into what changes are needed to make the chain of

thought correct, with the goal of elucidating how the model

can be improved in the future.

We found that many chains of thought can be made correct

with one of the following three classes of modiﬁcation.

•Calculator error only.

We found that 8% of the chains

of thought were completely correct except for a calculator

error—in other words, applying an external calculator to

equations, as done in Cobbe et al. (2021), would make

the chain of thought correct. An example of this type of

error is shown in Table 7: “calculator erroronly”. Indeed,

the solve rate of chain of thought prompting on GSM8K

went up from 14.8% to 19.5% when we added a Python

program as an external calculator, as shown in Table 11.

Also, 34% of the examples contained calculator errors in

addition to other types of errors.However, we perform the

rest of the error categorization independently of calculator

errors.

•Symbol mapping error.

We next found that 16% percent

of the chains of thought were correct except for what we

call symbol mapping errors. We deﬁne a symbol mapping

error as when the chain of thought is correct except for

the number symbols, and it could be made totally correct

by modifying only the equations and not the words.As

one might argue that they could simply place the correct

ﬁnal equation in any chain of thought, we constrain this

category to chains of thought where the chain of thought

can be modiﬁed to be a completely correct reasoning

process (not just ﬁnal answer). An example of this error

category is shown in Table 7:“symbol mapping error”.

•One step missing error.

Our next category of error is

chains of thought which were correct except that they

were missing a single step.In other words, these chains of

thoughts could be rewritten to be correct by adding in an

additional reasoning step that was missed by the model.

An example of this error category is shown in Table 7:

“one step missing error”. We found that 22% percent of

the errors fell into this category.

We found that the remaining chains of thought (27 of 50;

54%) would require substantial edits to make into a correct

chain of thought.Almost all cases here involved some

error in semantic understanding (see Table 8: “semantic

understanding error”), and 8 of the 27 also had incoherent

chain of thoughts, meaning that some statements in the

generated chain of thought did not follow from prior ones or

violated basic world knowledge (see Table 8: “incoherent

chain of thought error”).

Table 7.

Example of incorrect chains of thought, categorized as

described in Appendix A.2.

QUESTION:

Dan plants 3 rose bushes.Each rose bush has

25 roses. Each rose has 8 thorns.How many thorns are there

total?

MODEL ANSWER (INCORRECT;CALCULATOR ERROR

ONLY):

Dan plants 3 rose bushes.Each rose bush has 25

roses. Each rose has 8 thorns.So 3 x 25 x 8 = 300. The answer

is 300. 7

EXPLANATION FOR ERROR CATEGORY:

The produced

chain of thought could be made correct just by running the

equation through an external calculator, instead of asking the

model to do the computation.

QUESTION:

Jill gets paid $20 per hour to teach and $30 to be

a cheerleading coach. If she works 50 weeks a year, 35 hours a

week as a teacher and 15 hours a week as a coach, what’s her

annual salary?

MODEL ANSWER (INCORRECT;SYMBOL MAPPING ER-

ROR):

Jill gets paid 20 dollars per hour to teach and 30 dollars

per hour to be a cheerleading coach. If she works 50 weeks a

year, 35 hours a week as a teacher and 15 hours a week as a

coach, then she works 50 x 35 = 1750 hours as a teacher and

15 x 30 = 450 hours as a coach.So she works 1750 + 450 =

2200 hours. She gets paid 20 dollars per hour for 1750 hours

and 30 dollars per hour for 450 hours. So her annual salary is

20 x 1750 + 30 x 450 = $36,500. The answer is $36,500.7

EXPLANATION FOR ERROR CATEGORY:

The model incor-

rectly mapped “15 x 30” instead of “15 x 50”, and the chain of

thought would be made correct by ﬁxing just that one change

(with an external calculator).

QUESTION:

Kelian has two recipes for preparing dishes, one

having 20 instructions and the second one having twice as

many instructions as the ﬁrst one. How many instructions does

Kelian have to read to prepare the two dishes?

MODEL ANSWER (INCORRECT;ONE STEP MISSING ER-

ROR):

Kelian has two recipes. One has 20 instructions.The

other has twice as many instructions as the ﬁrst one.So the

second one has 40 instructions.So Kelian has to read 40 in-

structions to prepare the two dishes. The answer is 40.7

EXPLANATION FOR ERROR CATEGORY:

The model an-

swer could be made correct just adding one additional step (in

this case, adding 20 and 40).

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Table 8.

Example of incorrect chains of thought, categorized as

described in Appendix A.2. These chains of thought had multiple

mistakes that could not be ﬁxed with minor edits.

QUESTION:

Gretchen has 110 coins. There are 30 more gold

coins than silver coins. How many gold coins does Gretchen

have?

MODEL ANSWER (INCORRECT;SEMANTIC UNDER-

STANDING ERROR):

Gretchen has 110 coins.There are 30

more gold coins than silver coins. So there are 110 - 30 = 80

silver coins. So there are 80 silver coins and 110 - 80 = 30 gold

coins. The answer is 30.7

EXPLANATION FOR ERROR CATEGORY:

The model does

not understand that 110 coins are in total and that it must solve

an equation to get the number of gold and silver coins.

QUESTION:

In a dance class of 20 students, 20% enrolled in

contemporary dance, 25% of the remaining enrolled in jazz

dance, and the rest enrolled in hip-hop dance. What percentage

of the entire students enrolled in hip-hop dance?

MODEL ANSWER (INCORRECT;INCOHERENT CHAIN OF

THOUGHT ERROR):

20% of the students enrolled in contem-

porary dance. 25% of the remaining students enrolled in jazz

dance. Therest enrolled in hip-hop dance.So the percentage of

the entire students enrolled in hip-hop dance is the percentage

of the students enrolled in hip-hop dance, minus the percent-

age of the students enrolled in contemporary dance, minus

the percentage of the students enrolled in jazz dance.So the

percentage of the entire students enrolled in hip-hop dance is

(25 + 20) - (25 + 20) = 100%. The answer is 100%.7

EXPLANATION FOR ERROR CATEGORY:

This chain of

thought is incoherent in that the percent of entire students

enrolled in hip-hope dance cannot be the percent of student

enrolled in hip-hop dance minus another term.

A.3. Additional Robustness Analysis

As the experiments in the main paper use a ﬁxed number of

few-shot exemplars (8; as constrained by the input length

of 1024 tokens), we verify that the chain of thought prompt-

ing is robust to various numbers of few-shot exemplars.

We run experiments for the 137B model, comparing chain

of thought prompting with standard prompting for the six

datasets where standard prompting had a mostly ﬂat scaling

curve. As shown in Figure 7, the improvement of chain of

thought prompting over standard prompting remains robust

to varying the number of few-shot exemplars in the prompt.

As another robustness test, we check that chain of thought

prompting works not just for the set of language models

used in the main results, but also for GPT-3 language mod-

els (Brown et al.,2020).We evaluate the MultiArith and

GSM8K results using the OpenAI GPT-3 API, comparing

standard prompting and chain of thought prompting.As

shown in Figure 8, the overall ﬁnding remains unchanged

for GPT-3

davinci

versus the 137B language model we

use. Though the 137B-parameter model and GPT-3 did not

Reverse list

2

4

6

8

Coin ﬂip

2

4

6

8

Sports understanding

2

4

6

8

50

60

70

80

90

100

50

60

70

80

90

100

0

25

50

75

100

Last letter concat.

1

2

3

4

0

25

50

75

100

MultiArith

2

4

6

8

0

10

20

30

40

50

GSM8K

2

4

6

8

0

5

10

15

Number of exemplars in few-shot prompting

Solve rate (%)

Standard prompting

Chain of thought prompting

Figure 7.

The improvement of chain of thought prompting over

standard prompting is robust to varying the number of few-shot

exemplars in the prompt. The results shown here are for the 137B

parameter model.

Standard

Chain of

thought

25

50

75

100

0

Sports Understanding

Standard

Chain of

thought

Standard

Chain of

thought

0

10

20

30

40

50

0

5

10

15

MultiArith

GSM8K

Standard

Chain of

thought

0

20

40

60

80

Last letter concatenation

Standard

Chain of

thought

0

20

40

60

80

Reverse list (5 objects)

Standard

Chain of

thought

25

50

75

100

0

Coinﬂip (3 ﬂips)

Standard

Chain of

thought

25

50

75

100

0

Coinﬂip (2 ﬂips)

GPT-3: davinci

137B LM

Solve rate (%)

Solve rate (%)

Solve rate (%)

Solve rate (%)

Figure 8.

Chain of thought prompting outperforms standard

prompting for both the 137B LM we use in the main paper, as well

as for GPT-3 Da Vinci.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

perform exactly the same

11

for every single task, chain of

thought prompting still outperforms standard prompting by

a large margin on all tasks.

A.4. External Calculator

As discussed in Appendix A.2, some examples can be made

correct by ﬁxing the calculation errors; in other words, some

chains of thought (along with the equations produced) were

correct, except the model performed a calculation incor-

rectly.A similar observation was made in Cobbe et al.

(2021). Hence, we further add a Python program as an ex-

ternal calculator (using the Python

eval

function) to all the

equations in the generated chain of thought. When there are

multiple equations in a chain of thought, we propagate the

external calculator results from one equation to the follow-

ing equations via string matching.As shown in Table 11, we

see that adding a calculator signiﬁcantly boosts performance

of chain of thought prompting on all tasks.

In analyzing why the language model made certain calcula-

tor errors, we decomposed the error rate by number of hops

(a hop is deﬁned as one math operation in “

+, -, \*, /

”,

e.g.,

2+3

is 1-hop,

2+3-1

is 2-hop). We found that the lan-

guage model performs signiﬁcantly worse when the number

of hops increases in the equations—the 137B model has an

83% accuracy for 1-hop equations, but drops to 36% for

2-hop equations and 9.5% for 3-hop equations, respectively.

This leads us to wonder whether chain of thought helps only

because it decomposes multi-hop calculations into single-

hop calculations, thus improving the calculation accuracy

over each hop.Hence, we further employ an external cal-

culator to both chain of thought prompting as well as the

equation-only variant, as shown in Table 11.

Although using a calculator closes the gap between equation

only and chain of thought prompting on the four easy rea-

soning tasks (SingleOp, SingleEq, AddSub, ASDiv), chain

of thought prompting still helps signiﬁcantly on the com-

plex reasoning tasks (MultiArith and GSM8k). This result

implies that chain of thought also reduces problem complex-

ity by decomposing a complex problem into easier-to-solve

sub-problems, thus increasing the accuracy of semantic un-

derstanding in each sub-problem.

A.5. Comparison with Finetuned Models

We further compare chain of thought prompting to directly

ﬁnetuning the same 137B model on a training dataset. We

11

Compared with our 137B model, chain of thought prompting

using GPT-3 was worse on last letter concatenation (28.4% vs

77.7%). However, this number goes up to 55.8% when using GPT-

3

text-davinci

instead of

davinci

.Compared with our

137B model, GPT-3 does much better on Coinﬂip with 2 potential

ﬂips with standard prompting. However, GPT-3’s coinﬂip ability

with standard prompting went away with 3 potential ﬂips.

Table 9.

Comparison with ﬁnetuning the 137B model on GSM8K.

We ﬁnetune on both the full GSM8k train set as wellsubsets from

splitting the training set by the number of math operations (#hops).

Prompting surpasses the ﬁnetuning result with

≤2

hops and is

comparable to ﬁnetuning with ≤3hops.

Method GSM8K

solve rate

Finetuned 137B model

≤2hops (N=2,261) 14.0%

≤3hops (N=4,080) 20.5%

Full training set (N=7,473) 31.0%

Chain of thought prompting (≤2hops) 19.5%

focus on GSM8K (Cobbe et al.,2021), which has a training

set annotated with intermediate steps to arrive at the ﬁnal

answer (similar to a chain of thought, except slightly more

terse).We use an external calculator on generated chains

of thought for both prompting and ﬁnetuning. We compare

with models ﬁnetuned on both the full GSM8k train set, as

well as subsets limited by the number of hops in the train

examples, which allows us to test the model’s generalization

ability on problems that require more hops.The results

are shown in Table 9.All eight exemplars we composed

for chain of thought prompting have at most 2 hops but

achieved a solve rate of 19.5%, which is much higher than

the ﬁnetuned results with

≤2

hops from the GSM8k train

set (2,261 examples; solve rate = 14.0%). Chain of thought

prompting is also comparable with ﬁnetuned results for

≤3

hops (4,080 examples; solve rate = 20.5%).This compar-

ison suggests that, compared with ﬁnetuning, prompting

can exhibit better generalizability to harder problems while

being far more sample-efﬁcient.

A small note is that compared to ﬁnetuning, prompting is

less likely to force the language model to always output a

ﬁnal answer in a ﬁxed format. For example, on GSM8K, the

ﬁnetuned 137B model has a 98% probability of producing

a format of “{thoughts}.#### {ﬁnal answer}”, consis-

tent with the format in GSM8k training, while with our

prompting the 137B model only has a

80%

probability of

producing a format of “{thoughts}.The answer is {ﬁnal

answer}”, which was used in all the exemplars. To ensure a

fair comparison, we err on the conservative side for prompt-

ing results, and consider all predictions that do not have

“The answer is” as wrong predictions.12

12

This could lead to a slight underestimate of our model’s true

performance since the model could still predict the answer cor-

rectly even without the correct format, e.g., “...hence the total

number is 50.” instead of “The answer is 50.”.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Table 10.

Summary of math word problem dataset we use in this paper with examples.

N

: number of evaluation examples. avg # words:

average number of words in the question.

Dataset N

avg #

wordsExample problem

SingleOp

(single operation arithmetic)562 21.0 If there are 7 bottle caps in a box and Linda puts 7 more bottle caps inside,

how many bottle caps are in the box?

SingleEq

(single equation problems)508 27.4 Benny bought a soft drink for 2 dollars and 5 candy bars. He spent a total of

27 dollars. How much did each candy bar cost?

AddSub

(addition and subtraction)395 31.5 There were 6 roses in the vase. Mary cut some roses from her ﬂower garden.

There are now 16 roses in the vase. How many roses did she cut?

ASDiv

(diverse math word problems)2,096 30.9 Ellen has six more balls than Marin. Marin has nine balls.How many balls

does Ellen have?

MultiArith

(multi-step arithmetic)600 31.8

The school cafeteria ordered 42 red apples and 7 green apples for students

lunches. But, if only 9 students wanted fruit, how many extra did the cafeteria

end up with?

GSM8K

(grade-school math)1,319 46.9

Josh decides to try ﬂipping a house. He buys a house for $80,000 and then

puts in $50,000 in repairs. This increased the value of the house by 150%.

How much proﬁt did he make?

Table 11.

Full table showing ablation studies, effect of using an external calculator, and comparison with baselines.Using an external

calculator signiﬁcantly improves performance of both chain of thought as well as its equation only variant (model only outputs the

equation before giving the ﬁnal answer). Chain of thought prompting outperforms the variable compute only and thought after answer

variants (see Section 3.3 for descriptions), whereas equation only actually performs better on the easy datasets where standard prompting

already achieves good performance. Chain of thought prompting far outperforms unsupervised baselines and compares well with GPT-3

175B ﬁnetuned on 500/1,000 examples on GSM8K.

a

: GenBERT (Geva et al.,2020),

b

: MTMSN (Hu et al.,2019),

c

: relevance classiﬁer

(Roy & Roth,2015),

d

: GPT-2 ﬁnetuned with

k

-fold CV (Lan et al.,2021),

e

: LCA operation classiﬁer (Roy & Roth,2015),

f

: GPT-3

175B with calculator ﬁnetuned on 500 examples (Cobbe et al.,2021),

g

: GPT-3 175B with calculator ﬁnetuned on 1,000 examples (Cobbe

et al.,2021).

SingleOp SingleEqAddSubASDiv MultiArithGSM8K

Unsupervised model28.3a32.5b32.2b\-\- \-

Supervised model--94.9c61.4d60.5e18f/ 21g

Standard prompting72.8 ±1.2 47.4 ±0.8 42.1 ±1.1 40.7 ±0.7 7.6 ±0.4 6.3 ±0.4

Chain of thought prompting76.0 ±1.2 56.6 ±0.5 50.8 ±2.1 46.9 ±0.7 45.0 ±1.6 14.8 ±0.7

\+ calculator82.0 ±0.9 76.3 ±0.3 69.1 ±2.1 54.9 ±0.7 55.4 ±1.5 19.5 ±0.6

ABLATIONS:

Equation only78.4 ±1.1 58.1 ±1.0 54.6 ±0.8 46.9 ±0.5 5.7 ±0.6 5.7 ±0.3

\+ calculator85.2 ±0.8 79.6 ±0.6 73.9 ±0.3 57.6 ±0.2 29.9 ±0.6 15.2 ±0.4

Variable compute only70.9 ±1.1 45.4 ±1.0 39.2 ±1.8 39.7 ±0.4 7.7 ±0.4 6.0 ±0.3

Thought after answer73.5 ±0.8 47.9 ±1.1 42.6 ±1.9 40.3 ±0.4 7.6 ±0.6 5.9 ±0.5

Chain of Thought Prompting Elicits Reasoning in Large Language Models

B. Appendix: Input/Output Examples

Table 12.

Examples of correct and incorrect chains of thought

produced by the model on the letter concatenation task.

QUESTION:

Take the last letters of the words in “Waldo

Schmidt” and concatenate them.

MODEL ANSWER (CORRECT):

The last letter of “Waldo”

is “o”. The last letter of “Schmidt” is “t”.Concatenating them

is “ot”. So the answer is ot.X

QUESTION:

Take the last letters of the words in “Daniel Fried-

man” and concatenate them.

MODEL ANSWER (INCORRECT):

The last letter of

“Daniel” is “l”.The last letter of “Friedman” is “m”.Con-

catenating them is “lm”. So the answer is lm.7

Table 13.

Examples of correct and incorrect chains of thought

produced by the model on the reverse list task.

QUESTION:

Reverse the sequence "umbrella, head, camera,

battery, scissors".

MODEL ANSWER (CORRECT):

First is umbrella. Second

is head.Third is camera.Fourth is battery.Fifth is scissors.

Now to reverse, change the order to: Fifth is scissors. Fourth is

battery. Third is camera. Second is head.First is umbrella. So

the answer is "scissors, battery, camera, head, umbrella". X

QUESTION:

Reverse the sequence "button, stamp, light, tissue,

sweet".

MODEL ANSWER (INCORRECT):

First is button. Second

is stamp. Third is light.Fourth is tissue. Fifth is sweet.Now

to reverse, change the order to: Fifth is sweet. Fourth is tissue.

Third is light. Second is stamp.First is button. So the answer

is “sweet, tissue, button, stamp, light”.7

Table 14.

Examples of correct and incorrect chains of thought

produced by the model on the coinﬂip task.

QUESTION:

A coin is heads up. Andree ﬂips the coin.Audrie

does not ﬂip the coin. Is the coin still heads up?

MODEL ANSWER (CORRECT):

The coin was ﬂipped by

Andree.So the coin was ﬂipped 1 time, which is an odd number.

The coin started heads up, so after an odd number of ﬂips, it

will be tails up. So the answer is no.X

QUESTION:

A coin is heads up.Kristian does not ﬂip the

coin. Dallas does not ﬂip the coin.Is the coin still heads up?

MODEL ANSWER (INCORRECT):

The coin was ﬂipped

by Kristian.So the coin was ﬂipped 1 time, which is an odd

number. The coin started heads up, so after an odd number of

ﬂips, it will be tails up. So the answer is no.7

Table 15.

Examples of correct and incorrect chains of thought

produced by the model on CommonsenseQA.

QUESTION:

When a person is beginning work, what are they

building?

Answer Choices: (a) time (b) accomplishing (c) working (d)

momentum (e) tiredness

MODEL ANSWER (CORRECT):

The answer must be some-

thing that is built.Of the above choices, only momentum is

built. So the answer is (d).X

QUESTION:Where are you likely to ﬁnd a hamburger?

Answer Choices: (a) fast food restaurant (b) pizza (c) ground

up dead cows (d) mouth (e) cow carcus

MODEL ANSWER (CORRECT):

The answer must be a

place where hamburgers are found.Of the above choices,

only fast food restaurants serve hamburgers. So the answer is

(a). X

QUESTION:

Aside from water and nourishment what does

your dog need?

Answer Choices:(a) bone (b) charm (c) petted (d) lots of

attention (e) walked

MODEL ANSWER (INCORRECT):

The answer must be

something that a dog needs. Of the above choices, only bone

is something that a dog needs. So the answer is (a).

7

(Ground

truth is (d).)

QUESTION:

What are you waiting alongside with when you’re

in a reception area?

Answer Choices: (a) motel (b) chair (c) hospital (d) people (e)

hotels

MODEL ANSWER (INCORRECT):

The answer must be

something that is waiting with you in a reception area. Of the

above choices, only people are waiting with you in a reception

area. So the answer is (e).7(Ground truth is (d).)

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Table 16.

Examples of correct and incorrect chains of thought

produced by the model on StrategyQA.

QUESTION:Will The Exorcist stimulate limbic system?

MODEL ANSWER (CORRECT):

The Exorcist is a horror

movie. Horror movies are scary. The limbic system is involved

in fear. Thus, The Exorcist will stimulate the limbic system.

So the answer is yes. X

QUESTION:

Was Jackson Pollock trained by Leonardo da

Vinci?

MODEL ANSWER (CORRECT):

Leonardo da Vinci lived

in the 15th century. Jackson Pollock lived in the 20th century.

Thus, Jackson Pollock could not have been trained by Leonardo

da Vinci. So the answer is no. X

QUESTION:

Can Harry Potter book a ﬂight on Asiana Air-

lines?

MODEL ANSWER (INCORRECT):

Harry Potter is a ﬁc-

tional character. Thus, Harry Potter can do anything.So the

answer is yes. 7(Ground truth is “no”.)

QUESTION:Would a sophist use an épée?

MODEL ANSWER (INCORRECT):

A sophist is a person

who is skilled in the art of persuasion.An épée is a type of

sword. Thus, a sophist could use an épée. So the answer is yes.

7(Ground truth is “no”.)

Table 17.

Examples of correct and incorrect chains of thought

produced by the model on Date Understanding.

QUESTION:

May 6, 1992 is like yesterday to Jane, but that

is actually ten years ago.What is the date a month ago in

MM/DD/YYYY?

MODEL ANSWER (CORRECT):

May 6, 1992 is ten years

ago, so today is May 6, 2002. So a month ago will be April 6,

2002\. So the answer is 04/06/2002.X

QUESTION:

This is the last day of 1899. What is the date 24

hours later in MM/DD/YYYY?

MODEL ANSWER (CORRECT):

Today is 12/31/1899. 24

hours later will be 01/01/1900.So the answer is 01/01/1900.

X

QUESTION:

Jane scheduled 3 apointments with 5 poeple for

tomorrow (Tue, 7/9/1972). What is the date one week ago from

today in MM/DD/YYYY?

MODEL ANSWER (INCORRECT):

Tomorrow is 7/9/1972.

One week ago from today is 7/2/1972.So the answer is

7/2/1972. 7(Ground truth is 07/01/1972.)

QUESTION:

Today is the palindrome day of 2020, because

the MMDDYYYY format of the date is the same backwards as

forwards. What is the date tomorrow in MM/DD/YYYY?

MODEL ANSWER (INCORRECT):

Today is 02/29/2020.

Tomorrow will be 03/01/2020.So the answer is 03/01/2020.

7

(Ground truth is “02/03/2020”.)

Table 18.

Examples of correct and incorrect chains of thought

produced by the model on Sports Understanding.

QUESTION:

Is the following sentence plausible?“Joao

Moutinho was out at third.”

MODEL ANSWER (CORRECT):

Joao Moutinho is a soccer

player. Being out at third is part of baseball, not soccer. So the

answer is no. X

QUESTION:

Is the following sentence plausible? “Malcolm

Brogdon eurostepped to the basket in the NBA Championship.”

MODEL ANSWER (CORRECT):

Malcolm Brogdon is a

basketball player. Eurostepping to the basket is part of basket-

ball. So the answer is yes.X

QUESTION:

Is the following sentence plausible?“Derrick

White backhanded a shot.”

MODEL ANSWER (INCORRECT):

Derrick White is a bas-

ketball player. Backhanding a shot is part of basketball. So the

answer is yes.

7

(Ground truth is “no”, backhanding a shot is

part of hockey.)

QUESTION:

Is the following sentence plausible?“Nazem

Kadri was out at home.”

MODEL ANSWER (INCORRECT):

Nazem Kadri is a

hockey player.Being out at home is part of hockey.So the

answer is yes.

7

(Ground truth is “no”, being out at home is

part of baseball.)

Chain of Thought Prompting Elicits Reasoning in Large Language Models

C. Appendix: Full Prompts

Table 19.

Few-shot exemplars for full chain of thought prompt for

math word problems. This set of exemplars was used for all math

word problem datasets.

PROMPT FOR MATH WORD PROBLEMS

Q:

There are 15 trees in the grove. Grove workers will plant

trees in the grove today.After they are done, there will be 21

trees. How many trees did the grove workers plant today?

A:

There are 15 trees originally. Then there were 21 trees after

some more were planted. So there must have been 21 - 15 = 6.

The answer is 6.

Q:

If there are 3 cars in the parking lot and 2 more cars arrive,

how many cars are in the parking lot?

A:

There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.

The answer is 5.

Q:

Leah had 32 chocolates and her sister had 42.If they ate

35, how many pieces do they have left in total?

A:

Originally, Leah had 32 chocolates. Her sister had 42. So in

total they had 32 + 42 = 74. After eating 35, they had 74 - 35 =

39\. The answer is 39.

Q:

Jason had 20 lollipops.He gave Denny some lollipops.

Now Jason has 12 lollipops.How many lollipops did Jason

give to Denny?

A:

Jason started with 20 lollipops. Then he had 12 after giving

some to Denny. So he gave Denny 20 - 12 = 8. The answer is

8.

Q:

Shawn has ﬁve toys. For Christmas, he got two toys each

from his mom and dad. How many toys does he have now?

A:

Shawn started with 5 toys. If he got 2 toys each from his

mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer

is 9.

Q:

There were nine computers in the server room. Five more

computers were installed each day, from monday to thursday.

How many computers are now in the server room?

A:

There were originally 9 computers. For each of 4 days, 5

more computers were added. So 5 \* 4 = 20 computers were

added. 9 + 20 is 29. The answer is 29.

Q:

Michael had 58 golf balls. On tuesday, he lost 23 golf balls.

On wednesday, he lost 2 more.How many golf balls did he

have at the end of wednesday?

A:

Michael started with 58 golf balls. After losing 23 on tues-

day, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 =

33 golf balls. The answer is 33.

Q:

Olivia has $23. She bought ﬁve bagels for $3 each.How

much money does she have left?

A:

Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x

3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The

answer is 8.

Table 20.

Few-shot exemplars for full chain of thought prompt for

the last letter concatenation task.

PROMPT FOR LAST LETTER CONCATENATION

Q:

Take the last letters of the words in "Elon Musk" and con-

catenate them.

A: The last letter of "Elon" is "n". The last letter of "Musk" is

"k". Concatenating them is "nk". The answer is nk.

Q:

Take the last letters of the words in "Larry Page" and con-

catenate them.

A: The last letter of "Larry" is "y". The last letter of "Page" is

"e". Concatenating them is "ye". The answer is ye.

Q:

Take the last letters of the words in "Sergey Brin" and

concatenate them.

A:

The last letter of "Sergey" is "y". The last letter of "Brin" is

"n". Concatenating them is "yn". The answer is yn.

Q:

Take the last letters of the words in "Bill Gates" and con-

catenate them.

A:

The last letter of "Bill" is "l". The last letter of "Gates" is

"s". Concatenating them is "ls". The answer is ls.

Chain of Thought Prompting Elicits Reasoning in Large Language Models

Table 21.

Few-shot exemplars for full chain of thought prompt for

the reverse list of objects task. We found that prompt engineering

improved performance on this task speciﬁcally (we did not do

prompt engineering on any other tasks).

PROMPT FOR REVERSE LIST

Q: Reverse the sequence "cigar, umbrella, key, gum, alarm".

A:

First is cigar. Second is umbrella. Third is key. Fourth is

gum. Fifth is alarm. Now to reverse, change the order to: Fifth

is alarm. Fourth is gum. Third is key. Second is umbrella. First

is cigar. So the answer is "alarm, gum, key, umbrella, cigar".

Q:

Reverse the sequence "player, passport, umbrella, bottle,

watch".

A:

First is player. Second is passport. Third is umbrella. Fourth

is bottle. Fifth is watch. Now to reverse, change the order

to: Fifth is watch. Fourth is bottle. Third is umbrella. Second

is passport. First is player. So the answer is "watch, bottle,

umbrella, passport, player".

Q: Reverse the sequence "coin, postcard, case, pen, wallet".

A:

First is coin. Second is postcard. Third is case. Fourth is

pen. Fifth is wallet. Now to reverse, change the order to: Fifth

is wallet. Fourth is pen. Third is case. Second is postcard. First

is coin. So the answer is "wallet, pen, case, postcard, coin".

Q: Reverse the sequence "laptop, lipstick, pen, bin, clock".

A:

First is laptop. Second is lipstick. Third is pen. Fourth is

bin. Fifth is clock. Now to reverse, change the order to: Fifth

is clock. Fourth is bin. Third is pen. Second is lipstick. First is

laptop. So the answer is "clock, bin, pen, lipstick, laptop".

Q: Reverse the sequence "key, pen, screen, ﬁle, cigar".

A:

First is key. Second is pen. Third is screen. Fourth is ﬁle.

Fifth is cigar. Now to reverse, change the order to: Fifth is cigar.

Fourth is ﬁle. Third is screen. Second is pen. First is key. So

the answer is "cigar, ﬁle, screen, pen, key".

Q: Reverse the sequence "card, stamp, book, water, glasses".

A:

First is card. Second is stamp. Third is book. Fourth is water.

Fifth is glasses. Now to reverse, change the order to: Fifth is

glasses. Fourth is water. Third is book. Second is stamp. First

is card. The answer is 29.

Q:

Reverse the sequence "clock, coin, bottle, head, postcard".

A:

First is clock. Second is coin. Third is bottle. Fourth is head.

Fifth is postcard. Now to reverse, change the order to: Fifth is

postcard. Fourth is head. Third is bottle. Second is coin. First

is clock. So the answer is "postcard, head, bottle, coin, clock".

Q:

Reverse the sequence "battery, glasses, lighter, water, scis-

sors".

A:

First is battery. Second is glasses. Third is lighter. Fourth

is water. Fifth is scissors. Now to reverse, change the order

to: Fifth is scissors. Fourth is water. Third is lighter. Second

is glasses. First is battery. So the answer is "scissors, water,

lighter, glasses, battery".

Table 22.

Few-shot exemplars for full chain of thought prompt for

the coinﬂip task.

PROMPT FOR COIN FLIP

Q:

Q: A coin is heads up. Ka ﬂips the coin.Sherrie ﬂips the

coin. Is the coin still heads up?

A:

The coin was ﬂipped by Ka and Sherrie. So the coin was

ﬂipped 2 times, which is an even number. The coin started

heads up, so after an even number of ﬂips, it will still be heads

up. So the answer is yes.

Q:

A coin is heads up. Jamey ﬂips the coin.Teressa ﬂips the

coin. Is the coin still heads up?

A:

The coin was ﬂipped by Jamey and Teressa. So the coin

was ﬂipped 2 times, which is an even number. The coin started

heads up, so after an even number of ﬂips, it will still be heads

up. So the answer is yes.

Q:

A coin is heads up. Maybelle ﬂips the coin.Shalonda does

not ﬂip the coin. Is the coin still heads up?

A:

The coin was ﬂipped by Maybelle. So the coin was ﬂipped

1 time, which is an odd number. The coin started heads up, so

after an odd number of ﬂips, it will be tails up. So the answer

is no.

Q:

A coin is heads up.Millicent does not ﬂip the coin.Con-

ception ﬂips the coin. Is the coin still heads up?

A:

The coin was ﬂipped by Conception. So the coin was ﬂipped

1 time, which is an odd number. The coin started heads up, so

after an odd number of ﬂips, it will be tails up. So the answer

is no.

Q:

A coin is heads up.Sal ﬂips the coin.Raymond does not

ﬂip the coin. Is the coin still heads up?

A:

The coin was ﬂipped by Sal. So the coin was ﬂipped 1 time,

which is an odd number. The coin started heads up, so after an

odd number of ﬂips, it will be tails up. So the answer is no.

Q:

A coin is heads up. Conception ﬂips the coin.Kristian does

not ﬂip the coin. Is the coin still heads up?

A:

The coin was ﬂipped by Conception. So the coin was ﬂipped

1 time, which is an odd number. The coin started heads up, so

after an odd number of ﬂips, it will be tails up. So the answer

is no.

Q:

A coin is heads up. Inga does not ﬂip the coin.Elanor does

not ﬂip the coin. Is the coin still heads up?

A:

The coin was ﬂipped by no one. So the coin was ﬂipped 0

times. The coin started head
