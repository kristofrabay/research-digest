# SOCIAL MEDIA TITLE TAG

**URL:** https://llm-mcts.github.io/
**Published:** 2023-01-01T00:00:00.000Z

---

## Summary

The webpage describes a paper titled "Large Language Models as Commonsense Knowledge for Large-Scale Task Planning." It focuses on using Large Language Models (LLMs) within the **Monte Carlo Tree Search (MCTS)** framework for large-scale task planning.

Specifically, the paper proposes an **LLM-MCTS algorithm** where:
1.  The LLM acts as a **commonsense world model**, providing a prior belief for MCTS to enable effective **reasoning**.
2.  The LLM also acts as a **heuristic policy** to guide the search, improving efficiency.

The results show that this LLM-MCTS approach outperforms MCTS alone and policies induced solely by LLMs for complex tasks. The authors suggest that using the LLM as a world model for model-based planning is likely better than using it only as a policy when the description length of the world model is substantially smaller than that of the policy (related to the **minimum description length (MDL)** principle).

While the user query covers a broad range of topics related to **reasoning and planning with LLMs** (including chain-of-thought, self-reflection, inference-time compute, hallucination reduction, grounding, etc.), this specific page directly addresses **planning with LLMs** using **MCTS** and leveraging the LLM as a **world model** for **reasoning**.

---

## Full Content

# Large Language Models as Commonsense Knowledge for Large-Scale Task Planning

NeurIPS 2023

[Zirui Zhao](https://1989ryan.github.io/),[Wee Sun Lee](https://comp.nus.edu.sg/~leews/),[David Hsu](https://comp.nus.edu.sg/~dyhsu/)

National University of Singapore

[arXiv](https://arxiv.org/abs/2305.14078)[Paper](https://llm-mcts.github.io/static/pdfs/paper.pdf)[Code](https://github.com/llm-mcts/llm-mcts)

## We use Large Language Models as both the commonsense world model and the heuristic policy within the Monte Carlo Tree Search framework, enabling better-reasoned decision-making for daily tasks.

## Abstract

Large-scale task planning is a major challenge. Recent work exploits large
language models (LLMs) directly as a _policy_ and shows surprisingly
interesting results. This paper shows that LLMs provide a
_commonsense model_ of the world in addition to a policy that acts on
it. The world model and the policy can be combined in a search
algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task
planning. In our new LLM-MCTS algorithm, the LLM-induced world model
provides a commonsense prior belief for MCTS to achieve effective reasoning;
the LLM-induced policy acts as a heuristic to guide the search, vastly
improving search efficiency. Experiments show that LLM-MCTS outperforms
both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide
margin, for complex, novel tasks.
Further experiments and analyses on multiple tasks---multiplication, multi-hop travel planning, object rearrangement---suggest _minimum description length_ (MDL)
as a general guiding principle: if the
description length of the world model is substantially smaller than that of the
policy, using LLM as a world model for model-based planning is likely better
than using LLM solely as a policy.

## BibTeX

```
@inproceedings{
 zhao2023large,
 title={Large Language Models as Commonsense Knowledge for Large-Scale Task Planning},
 author={Zirui Zhao and Wee Sun Lee and David Hsu},
 booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
 year={2023},
 url={https://openreview.net/forum?id=Wjp1AYB8lH}
}
```
