# Building a Multimodal RAG That Responds with Text, Images, and Tables from Sources

**URL:** https://towardsdatascience.com/building-a-multimodal-rag-with-text-images-tables-from-sources-in-response/
**Published:** 2025-11-03T00:00:00.000Z

---

## Summary

The webpage describes an **improved Multimodal Retrieval-Augmented Generation (RAG) pipeline** designed to reliably return text, images, and tables from complex source documents.

Key aspects covered include:

*   **Problem:** Standard multimodal RAG often fails because image captions/summaries lack the necessary context to distinguish between similar elements (like tables for "producers" vs. "processors") in complex documents.
*   **Improved Pipeline Changes:**
    1.  **Context-Aware Image Summaries:** Instead of just summarizing the image, the system extracts up to 200 characters of text immediately *before and after* the figure/table to create a contextually rich caption.
    2.  **Text Response Guided Image Selection:** Image retrieval is performed *after* the textual response is generated, matching the best images to the generated text rather than directly matching the user query to image embeddings.
*   **Implementation Details:** The process uses the **Adobe PDF Extract API** for reliable parsing of PDFs (including figures and tables), GPT-4o for quality checking images and generating summaries, and FAISS for indexing embeddings.
*   **Results:** The tests show that this enhanced approach successfully retrieves highly relevant figures and tables for specific queries related to financial reports, research papers (like VectorPainter and CLIP distillation), demonstrating improved accuracy in multimodal retrieval.

In summary, the page details a method to build a more robust multimodal RAG system by focusing on **contextualizing visual elements** during ingestion and **decoupling image selection from the initial user query** during retrieval.

---

## Full Content

Building a Multimodal RAG That Responds with Text, Images, and Tables from Sources | Towards Data Science
[![Towards Data Science](https://towardsdatascience.com/wp-content/uploads/2025/02/TDS-Vector-Logo.svg)](https://towardsdatascience.com/)
Publish AI, ML &amp; data-science insights to a global community of data professionals.
Sign in
[Submit an Article](https://contributor.insightmediagroup.io/)
* [LinkedIn](https://www.linkedin.com/company/towards-data-science/?originalSubdomain=ca)
* [X](https://x.com/TDataScience)
Toggle Search
Search
[Large Language Models](https://towardsdatascience.com/category/artificial-intelligence/large-language-models/)
# Building a Multimodal RAG That Responds with Text, Images, and Tables from Sources
Why do few chatbots return figures from source documents in their responses?
[Partha Sarkar](https://towardsdatascience.com/author/partha_sarkar/)
Nov 3, 2025
11 min read
Share
![](https://towardsdatascience.com/wp-content/uploads/2025/11/Unsplash.jpg)[Photo](https://unsplash.com/photos/a-computer-circuit-board-with-a-brain-on-it-_0iV9LmPDn0)by Steve Johnson on Unsplash
**Retrieval-AugmentedGeneration (RAG)**has been one of the earliest and most successful applications of Generative AI. Yet, few chatbots return*images, tables, and figures*from source documents alongside textual answers.
In this post, I explore why it’s difficult to build a reliable, truly multimodal RAG system, especially for*complex documents*such as research papers and corporate reports — which often include dense text, formulae, tables, and graphs.
Also, here I present an approach for an**improved multimodal RAG pipeline**that delivers consistent, high-quality multimodal results across these document types.
## Dataset and Setup
To illustrate, I built a small multimodal knowledge base using the following documents:
1. [*Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners*](https://arxiv.org/pdf/2407.04003)
2. [*VectorPainter: Advanced Stylized Vector Graphics Synthesis Using Stroke-Style Priors*](https://arxiv.org/pdf/2405.02962)
3. [*Marketing Strategy for Financial Services: Financing Farming &amp; Processing the Cassava, Maize and Plantain Value Chains in Côte d’Ivoire*](https://www.ifc.org/content/dam/ifc/doc/2023/marketing-strategy-for-financial-services-en.pdf)
The language model used is**GPT-4o**, and for embeddings I used**text-embedding-3-small**.
## The Standard Multimodal RAG Architecture
In theory, a multimodal RAG bot should:
* Accept**text and image**queries.
* Return**text and image**responses.
* Retrieve**context**from both text and image sources.
A typical pipeline looks like this:
1. **Ingestion**
* **Parsing &amp; chunking:**Split documents into text segments and extract images.
* **Image summarization:**Use an LLM to generate captions or summaries for each image.
* **Multi-vector embeddings:**Create embeddings for text chunks, image summaries, and optionally for the raw image features (e.g., using CLIP).
**2. Indexing**
* Store embeddings and metadata in a vector database.
**3. Retrieval**
* For a user query, perform similarity search on:
* Text embeddings (for textual matches)
* Image summary embeddings (for image relevance)
**4. Generation**
* Use a multimodal LLM to synthesize the final response using both retrieved text and images.## The Inherent Assumption
This approach assumes that**the caption or summary of an image generated from its content, always contains enough context**about the text or themes that appear in the document, for which this image would be an appropriate response.
In real-world documents, this often isn’t true.
**Example: Context Loss in Corporate Reports**
Take the “Marketing Strategy for Financial Services (#3 in dataset)” report in the dataset. In its Executive Summary, there are two similar-looking tables showing*Working Capital*requirements — one for**primary producers (farmers)**and one for**processors**. They are the following:
![](https://contributor.insightmediagroup.io/wp-content/uploads/2025/11/Fig-1.png)Working Capital Table for Primary Producers![](https://contributor.insightmediagroup.io/wp-content/uploads/2025/11/Fig-2.png)Working Capital Table for Processors
GPT-4o generates the following for the first table:
*“The table outlines various types of working capital financing options for agricultural businesses, including their purposes and availability across different situations”*
And the following for the second table:
*“The table provides an overview of working capital financing options, detailing their purposes and potential applicability in different scenarios for businesses, particularly exporters and stock purchasers”*
Both seem fine individually — but neither captures the**context**that distinguishes*producers*from*processors*.
This means they will be**retrieved incorrectly**for queries specifically asking about producers or processors only. There are other tables such as CAPEX, Funding opportunities where the same issue can be seen.
For the VectorPainter paper, where Fig 3 in the paper shows the VectorPainter pipeline, GPT-4o generates the caption as*“Overview of the proposed framework for stroke-based style extraction and stylized SVG synthesis with stroke-level constraints,”*missing the fact that it represents the core theme of the paper, named “VectorPainter” by the authors.
And for the Vision Language similarity distillation loss formula defined in Sec 3.3 of the CLIP finetuning paper, the caption generated is*“Equation representing the Variational Logit Distribution (VLD) loss, defined as the sum of Kullback–Leibler (KL) divergences between predicted and target logit distributions over a batch of inputs.”,*where the context of vision and language correlation is absent.
It is also to be noted that in the research papers, the figures and tables have a author provided caption, however, during the extraction process, this is extracted not as part of the image, but as part of the text. And also the positioning of the caption is sometimes above and at other times below the figure. As for the Marketing Strategy reports, the embedded tables and other images do not even have an attached caption describing the figure.
What the above has illustrated is that the real-world documents do not follow any standard format of text, images, tables and captions, thereby making the process of associating context to the figures difficult.
## The New and Improved Multimodal RAGpipeline
To solve this, I made two key changes.
**1. Context-Aware Image Summaries**
Instead of asking the LLM to summarize the image, I extract the**text immediately before and after the figure**— up to 200 characters in each direction.
This way, the image caption includes:
* The*author-provided*caption (if any)
* The*surrounding narrative*that gives it meaning
Even if the document lacks a formal caption, this provides a contextually accurate summary.
**2. Text Response Guided Image Selection at Generation Time**
During retrieval, I**don’t**match the user query directly with image captions. This is because the user query often is too short to provide adequate context for image retrieval (eg; What is…?)
Instead:
* First, generate the**textual response**using the top text chunks retrieved for context.
* Then,**select the best two images**for the text response matched to the image captions
This ensures the final images are chosen**in relation to the actual response**, not the query alone.
Here is a diagram for the**Extraction to Embedding**pipeline:
![](https://contributor.insightmediagroup.io/wp-content/uploads/2025/11/Fig-3.png)Extraction to Embedding Pipeline
And the pipeline for**Retrieval and Response Generation**is as follows:
![](https://contributor.insightmediagroup.io/wp-content/uploads/2025/11/Fig-4-1.png)Retrieval and Response Generation## Implementation Details
**Step 1: Extract Text and Images**
Use[Adobe PDF Extract API](https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/quickstarts/python/)to parse PDFs into:
* figures/ and tables/ folders with.png files
* A structuredData.json file containing positions, text, and file paths
I found this API to be far more reliable than libraries like PyMuPDF, especially for extracting formulas and diagrams.
**Step 2: Create a Text File**
Concatenate all textual elements from the JSON to create the raw text corpus:
```
`# Extract text, sorted by Page and vertical order (Bounds[1])
elements = data.get(&quot;&quot;elements&quot;&quot;, [])
# Concatenate text
all\_text = []
for el in elements:
if &quot;&quot;Text&quot;&quot; in el:
all\_text.append(el[&quot;&quot;Text&quot;&quot;].strip())
final\_text = &quot;&quot;\\n&quot;&quot;.join(all\_text)`
```
**Step 3: Build Image Captions**: Walk through each element of `structuredData.json`, check if the element filepath ends in `.png`. Load the file from figures and tables folder of the document, then use the LLM to perform a quality check on the image. This is needed as the extraction process will find some illegible, small images, header and footer, company logos etc which need to be excluded from any user responses.
Note that we are not asking the LLM to interpret the images; just comment if it is clear and relevant enough to be included in the database. The prompt for the LLM would be like:
```
`Analyse the given image for quality, clarity, size etc. Is it a good quality image that can be used for further processing ? The images that we consider good quality are tables of facts and figures, scientific images, formulae, everyday objects and scenes etc. Images of poor quality would be any company logo or any image that is illegible, small, faint and in general would not look good in a response to a user query.
Answer with a simple Good or Poor. Do not be verbose`
```
Next we create the image summary. For this, in the `structuredData.json`, we look at the elements behind and ahead of the `.png` element, and collect up to 200 characters in each direction for a total of 400 characters. This forms the image caption or summary. The code snippet is as follows:
```
`# Collect before
j = i - 1
while j &gt;&gt;= 0 and len(text\_before) &lt;&lt; 200:
if &quot;&quot;Text&quot;&quot; in elements[j] and not (&quot;&quot;Table&quot;&quot; in elements[j][&quot;&quot;Path&quot;&quot;] or &quot;&quot;Figure&quot;&quot; in elements[j][&quot;&quot;Path&quot;&quot;]):
text\_before = elements[j][&quot;&quot;Text&quot;&quot;].strip() + &quot;&quot; &quot;&quot; + text\_before
j -= 1
text\_before = text\_before[-200:]
# Collect after
k = i + 1
while k &lt;&lt; len(elements) and len(text\_after) &lt;&lt; 200:
if &quot;&quot;Text&quot;&quot; in elements[k]:
text\_after += &quot;&quot; &quot;&quot; + elements[k][&quot;&quot;Text&quot;&quot;].strip()
k += 1
text\_after = text\_after[:200]`
```
We perform this for each figure and table for every document in our database, and store the image captions as metadata. In my case, I store as a `image\_captions.json` file.
This simple change makes a**huge difference**— the resulting captions include meaningful context. For instance, the captions I get for the two Working Capital tables from the Marketing Strategy report are as follows. Note how the contexts are now**clearly differentiated**and include farmers and processors.
```
`&quot;&quot;caption&quot;&quot;: &quot;&quot;o farmers for their capital expenditure needs as well as for their working capital needs. The table below shows the different products that would be relevant for the small, medium, and large farmers. Working Capital Input Financing For purchase of farm inputs and labour Yes Yes Yes Contracted Crop Loan\* For purchase of inputs for farmers contracted by reputable buyers Yes Yes Yes Structured Loan&quot;&quot;`
```
```
`&quot;&quot;caption&quot;&quot;: &quot;&quot;producers and their buyers b)\\t Potential Loan products at the processing level At the processing level, the products that would be relevant to the small scale and the medium\_large processors include Working Capital Invoice discounting\_ Factoring Financing working capital requirements by use of accounts receivable as collateral for a loan Maybe Yes Warehouse receipt-financing Financing working ca&quot;&quot;`
```
**Step 4: Chunk Text and Generate Embeddings**
The text file of the document is split into chunks of 1000 characters, using*` RecursiveCharacterTextSplitter`*from `*langchain*` and stored. Embeddings created for the text chunks and image captions, normalized and stored as `*faiss*` indexes
**Step 5: Context Retrieval and Response Generation**
The user query is matched and the top 5 text chunks are retrieved as context. Then we use these retrieved chunks and user query to get the text response using the LLM.
In the next step, we take the generated text response and find the top 2 closest image matches (based on caption embeddings) to the response. This is different from the traditional way of matching the user query to the image embeddings and provides much better results.
There is one final step. Our image captions were based on 400 characters around the image in the document, and may not form a logical and concise caption for display. Therefore, for the final selected 2 images, we ask the LLM to take the image captions along with the images and create a brief caption ready for display in the final response.
Here is the code for the above logic:
```
`# Retrieve context
result = retrieve\_context\_with\_images\_from\_chunks(
user\_input,
content\_chunks\_json\_path,
faiss\_index\_path,
top\_k=5,
text\_only\_flag= True
)
text\_results = result.get(&quot;&quot;top\_chunks&quot;&quot;, [])
# Construct prompts
payload\_1 = construct\_prompt\_text\_only (user\_input, text\_results)
# Collect responses (synchronously for tool)
assistant\_text, caption\_text = &quot;&quot;&quot;&quot;, &quot;&quot;&quot;&quot;
for chunk in call\_gpt\_stream(payload\_1):
assistant\_text += chunk
lst\_final\_images = retrieve\_top\_images (assistant\_text, caption\_faiss\_index\_path, captions\_json\_path, top\_n=2)
if len(lst\_final\_images) &gt;&gt; 0:
payload = construct\_img\_caption (lst\_final\_images)
for chunk in call\_gpt\_stream(payload):
caption\_text += chunk
response = {
&quot;&quot;answer&quot;&quot;: assistant\_text + (&quot;&quot;\\n\\n&quot;&quot; + caption\_text if caption\_text else &quot;&quot;&quot;&quot;),
&quot;&quot;images&quot;&quot;: [x[&#039;&#039;image\_name&#039;&#039;] for x in lst\_final\_images],
}
return response`
```
## Test Results
Let’s run the queries mentioned at the beginning of this blog to see if the images retrieved are relevant to the user query. For simplicity, I am printing only the images and their captions displayed and not the text response.
**Query 1:***What are the loan and working capital requirement of the primary producer?*
Figure 1: Overview of working capital financing options for small, medium, and large farmers.
Figure 2: Capital expenditure financing options for medium and large farmers.
![](https://contributor.insightmediagroup.io/wp-content/uploads/2025/11/Fig-5.png)Image Result for Query 1
**Query 2:***What are the loan and working capital requirement of the processors?*
Figure 1: Overview of working capital loan products for small-scale and medium-large processors.
Figure 2: CAPEX loan products for machinery purchase and business expansion at the processing level.
![](https://contributor.insightmediagroup.io/wp-content/uploads/2025/11/Fig-6.png)Image Result for Query 2
**Query 3**:*What is vision language distillation?*
Figure 1: Vision-language similarity distillation loss formula for transferring modal consistency from pre-trained CLIP to fine-tuned models.
Figure 2: Final objective function combining distillation loss, supervised contrastive loss, and vision-language similarity distillation loss with balancing hyperparameters.
![](https://contributor.insightmediagroup.io/wp-content/uploads/2025/11/Fig-7.png)Formula Retrieval for Query 3
**Query 4**:*What is VectorPainter pipeline?*
Figure 1: Overview of the stroke style extraction and SVG synthesis process, highlighting stroke vectorization, style-preserving loss, and text-prompt-based generation.
Figure 2: Comparison of various methods for style transfer across raster and vector formats, showcasing the effectiveness of the proposed approach in maintaining stylistic consistency.
![](https://contributor.insightmediagroup.io/wp-content/uploads/2025/11/Fig-8.png)Image Retrieval for Query 4## Conclusion
This enhanced pipeline demonstrates how**context-aware image summarization**and**text response based image selection**can dramatically improve multimodal retrieval accuracy.
The approach produces**rich, multimodal answers**that combine text and visuals in a coherent way — essential for research assistants, document intelligence systems, and AI-powered knowledge bots.
Try it out… leave your comments and connect with me at[www.linkedin.com/in/partha-sarkar-lets-talk-AI](http://www.linkedin.com/in/partha-sarkar-lets-talk-AI)
## Resources
1.[*Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners*](https://arxiv.org/pdf/2407.04003): Mushui Liu, Bozheng Li, Yunlong Yu Zhejiang University
2.[*VectorPainter: Advanced Stylized Vector Graphics Synthesis Using Stroke-Style Priors*](https://arxiv.org/pdf/2405.02962): Juncheng Hu, Ximing Xing, Jing Zhang, Qian Yu† Beihang University
3.[*Marketing Strategy for Financial Services: Financing Farming &amp; Processing the Cassava, Maize and Plantain Value Chains in Côte d’Ivoire*](https://www.ifc.org/content/dam/ifc/doc/2023/marketing-strategy-for-financial-services-en.pdf)from[https://www.ifc.org](https://www.ifc.org)
Written By
Partha Sarkar
[See all from Partha Sarkar](https://towardsdatascience.com/author/partha_sarkar/)
[Llm](https://towardsdatascience.com/tag/llm/),[Multimodal](https://towardsdatascience.com/tag/multimodal/),[Multimodal Learning](https://towardsdatascience.com/tag/multimodal-learning/),[Rag](https://towardsdatascience.com/tag/rag/)
Share This Article
* [Share on Facebook]()
* [Share on LinkedIn]()
* [Share on X]()
Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.
[Write for TDS](https://towardsdatascience.com/questions-96667b06af5/)
## Related Articles
* ![](https://towardsdatascience.com/wp-content/uploads/2023/11/17MonaLlO4zTNeFQT0ATKbw.png)
## [Using LLMs to evaluate LLMs](https://towardsdatascience.com/using-llms-to-evaluate-llms-ce390ae575c6/)
[Artificial Intelligence](https://towardsdatascience.com/category/artificial-intelligence/)
You can ask ChatGPT to act in a million different ways: as your nutritionist, language&hellip;
[Maksym Petyak](https://towardsdatascience.com/author/petyak-mi/)
November 10, 2023
8 min read
* ![](https://towardsdatascience.com/wp-content/uploads/2023/11/1R-fWNo-SVYYmm_tepIoVuw.jpeg)
## [A beginner&#8217;s guide to building a Retrieval Augmented Generation (RAG) application from scratch](https://towardsdatascience.com/a-beginners-guide-to-building-a-retrieval-augmented-generation-rag-application-from-scratch-e52921953a5d/)
[Large Language Models](https://towardsdatascience.com/category/artificial-intelligence/large-language-models/)
# Retrieval Augmented Generation, or RAG, is all the rage these days because it introduces&hellip;
[Bill Chambers](https://towardsdatascience.com/author/wachambers/)
November 2, 2023
12 min read
* ## [Take a Look Under the hood](https://towardsdatascience.com/take-a-look-under-the-hood-24e40281c900/)
[Large Language Models](https://towardsdatascience.com/category/artificial-intelligence/large-language-models/)
Using Monosemanticity to understand the concepts a Large Language Model learned
[Dorian Drost](https://towardsdatascience.com/author/doriandrost/)
June 13, 2024
13 min read
* ## [Interacting with large language models](https://towardsdatascience.com/interacting-with-large-language-models-76c11cfd6290/)
[Large Language Models](https://towardsdatascience.com/category/artificial-intelligence/large-language-models/)
Enriching prompts to steer the model &#8211; explained for non-experts
[Dorian Drost](https://towardsdatascience.com/author/doriandrost/)
May 24, 2023
12 min read
* ## [5 Ways Generative AI Changes How Companies Approach Data (And How It Doesn&#8217;t)](https://towardsdatascience.com/5-ways-generative-ai-changes-how-companies-approach-data-and-how-it-doesnt-6e87c1f3c41/)
[Data Engineering](https://towardsdatascience.com/category/data-science/data-engineering/)
Experts from venture capital, Snowflake, and more discuss how generative AI will benefit data teams&hellip;
[Michael Segner](https://towardsdatascience.com/author/michaelrsegner/)
August 10, 2023
13 min read
* ## [Different ways of training LLMs](https://towardsdatascience.com/different-ways-of-training-llms-c57885f388ed/)
[Large Language Models](https://towardsdatascience.com/category/artificial-intelligence/large-language-models/)
And why prompting is none of them
[Dorian Drost](https://towardsdatascience.com/author/doriandrost/)
July 21, 2023
13 min read
* ![](https://towardsdatascience.com/wp-content/uploads/2023/09/1J16KiNF4M_XxPkALZFUJHw-scaled.jpeg)
## [10 Ways to Improve the Performance of Retrieval Augmented Generation Systems](https://towardsdatascience.com/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c/)
[Large Language Models](https://towardsdatascience.com/category/artificial-intelligence/large-language-models/)
Tools to go from prototype to production
[Matt Ambrogi](https://towardsdatascience.com/author/mattambrogi/)
September 18, 2023
11 min read
