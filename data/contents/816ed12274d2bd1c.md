# Unified MCP Client Library

**URL:** https://mcpservers.org/servers/mcp-use/mcp-use
**Published:** 2024-06-20T00:00:00.000Z

---

## Summary

The webpage describes the **Unified MCP Client Library (mcp-use)**, an open-source Python library designed to connect **any LLM to any MCP server**, enabling the creation of custom agents with tool access.

Here is a summary of how the page relates to your query terms:

*   **agent\_infrastructure: MCP servers, tool use, agent frameworks:** The core purpose of the library is to facilitate agent infrastructure by connecting LLMs to **MCP servers** to enable **tool use**. It integrates with **agent frameworks** like **LangChain** (via providers).
*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** The library explicitly supports integration with **LangChain** for connecting to various LLMs (like **OpenAI** and **Anthropic**). It does not explicitly mention LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK, though it supports LLMs from those ecosystems via LangChain.
*   **function calling, structured outputs:** The documentation notes that only models with **tool calling capabilities** (which implies function calling) can be used with `mcp_use`.
*   **agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context, supervised fine-tuning, reinforcement learning:** None of these advanced agent development, training, or evaluation concepts are directly discussed in the context of the `mcp-use` library features.
*   **agent memory, agentic memory, context compression:** These memory and context management techniques are **not mentioned** in the provided text.

**In summary:** The page details a library for building tool-using agents by connecting LLMs (via LangChain) to MCP servers, supporting features like streaming, multi-server orchestration, and sandboxed execution. It **does not cover** agent memory, context compression, or advanced training/evaluation methods listed in your query.

---

## Full Content

Unified MCP Client Library | Awesome MCP Servers
# Unified MCP Client Library
An open-source library to connect any LLM to any MCP server, enabling the creation of custom agents with tool access.
[GitHub](https://github.com/mcp-use/mcp-use)
üåêMCP-Use is the open source way to connect**any LLM to any MCP server**and build custom MCP agents that have tool access, without using closed source or application clients.
üí°Let developers easily connect any LLM to tools like web browsing, file operations, and more.
* If you want to get started quickly check out[mcp-use.com website](https://mcp-use.com/)to build and deploy agents with your favorite MCP servers.
* Visit the[mcp-use docs](https://docs.mcp-use.com/)to get started with mcp-use library
* For the TypeScript version, visit[mcp-use-ts](https://github.com/mcp-use/mcp-use-ts)|Supports||
**Primitives**|[![Tools](https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-tools&amp;label=Tools&amp;style=flat)](https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml)[![Resources](https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-resources&amp;label=Resources&amp;style=flat)](https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml)[![Prompts](https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-prompts&amp;label=Prompts&amp;style=flat)](https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml)[![Sampling](https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-sampling&amp;label=Sampling&amp;style=flat)](https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml)[![Elicitation](https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-elicitation&amp;label=Elicitation&amp;style=flat)](https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml)[![Authentication](https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=primitive-authentication&amp;label=Authentication&amp;style=flat)](https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml)|
**Transports**|[![Stdio](https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=transport-stdio&amp;label=Stdio&amp;style=flat)](https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml)[![SSE](https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=transport-sse&amp;label=SSE&amp;style=flat)](https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml)[![Streamable HTTP](https://img.shields.io/github/actions/workflow/status/pietrozullo/mcp-use/tests.yml?job=transport-streamableHttp&amp;label=Streamable%20HTTP&amp;style=flat)](https://github.com/pietrozullo/mcp-use/actions/workflows/tests.yml)|
## Features
# Quick start
With pip:
```
`pip install mcp-use`
```
Or install from source:
```
`git clone https://github.com/mcp-use/mcp-use.git
cd mcp-use
pip install -e .`
```
### Installing LangChain Providers
mcp\_use works with various LLM providers through LangChain. You&#x27;&#x27;ll need to install the appropriate LangChain provider package for your chosen LLM. For example:
```
`# For OpenAI
pip install langchain-openai
# For Anthropic
pip install langchain-anthropic`
```
For other providers, check the[LangChain chat models documentation](https://python.langchain.com/docs/integrations/chat/)and add your API keys for the provider you want to use to your`.env`file.
```
`OPENAI\_API\_KEY=
ANTHROPIC\_API\_KEY=`
```
> **> Important
**> : Only models with tool calling capabilities can be used with mcp_use. Make sure your chosen model supports function calling or tool use.
> ### Spin up your agent:
```
`import asyncio
import os
from dotenv import load\_dotenv
from langchain\_openai import ChatOpenAI
from mcp\_use import MCPAgent, MCPClient
async def main():
# Load environment variables
load\_dotenv()
# Create configuration dictionary
config = {
&quot;&quot;mcpServers&quot;&quot;: {
&quot;&quot;playwright&quot;&quot;: {
&quot;&quot;command&quot;&quot;: &quot;&quot;npx&quot;&quot;,
&quot;&quot;args&quot;&quot;: [&quot;&quot;@playwright/mcp@latest&quot;&quot;],
&quot;&quot;env&quot;&quot;: {
&quot;&quot;DISPLAY&quot;&quot;: &quot;&quot;:1&quot;&quot;
}
}
}
}
# Create MCPClient from configuration dictionary
client = MCPClient.from\_dict(config)
# Create LLM
llm = ChatOpenAI(model=&quot;&quot;gpt-4o&quot;&quot;)
# Create agent with the client
agent = MCPAgent(llm=llm, client=client, max\_steps=30)
# Run the query
result = await agent.run(
&quot;&quot;Find the best restaurant in San Francisco&quot;&quot;,
)
print(f&quot;&quot;\\nResult: {result}&quot;&quot;)
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
asyncio.run(main())`
```
You can also add the servers configuration from a config file like this:
```
`client = MCPClient.from\_config\_file(
os.path.join(&quot;&quot;browser\_mcp.json&quot;&quot;)
)`
```
Example configuration file (`browser\_mcp.json`):
```
`{
&quot;mcpServers&quot;: {
&quot;playwright&quot;: {
&quot;command&quot;: &quot;npx&quot;,
&quot;args&quot;: [&quot;@playwright/mcp@latest&quot;],
&quot;env&quot;: {
&quot;DISPLAY&quot;: &quot;:1&quot;
}
}
}
}`
```
For other settings, models, and more, check out the documentation.
## Streaming Agent Output
MCP-Use supports asynchronous streaming of agent output using the`stream`method on`MCPAgent`. This allows you to receive incremental results, tool actions, and intermediate steps as they are generated by the agent, enabling real-time feedback and progress reporting.
### How to use
Call`agent.stream(query)`and iterate over the results asynchronously:
```
`async for chunk in agent.stream(&quot;Find the best restaurant in San Francisco&quot;):
print(chunk[&quot;messages&quot;], end=&quot;&quot;, flush=True)`
```
Each chunk is a dictionary containing keys such as`actions`,`steps`,`messages`, and (on the last chunk)`output`. This enables you to build responsive UIs or log agent progress in real time.
#### Example: Streaming in Practice
```
`import asyncio
import os
from dotenv import load\_dotenv
from langchain\_openai import ChatOpenAI
from mcp\_use import MCPAgent, MCPClient
async def main():
load\_dotenv()
client = MCPClient.from\_config\_file(&quot;&quot;browser\_mcp.json&quot;&quot;)
llm = ChatOpenAI(model=&quot;&quot;gpt-4o&quot;&quot;)
agent = MCPAgent(llm=llm, client=client, max\_steps=30)
async for chunk in agent.stream(&quot;&quot;Look for job at nvidia for machine learning engineer.&quot;&quot;):
print(chunk[&quot;&quot;messages&quot;&quot;], end=&quot;&quot;&quot;&quot;, flush=True)
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
asyncio.run(main())`
```
This streaming interface is ideal for applications that require real-time updates, such as chatbots, dashboards, or interactive notebooks.
# Example Use Cases
## Web Browsing with Playwright
```
`import asyncio
import os
from dotenv import load\_dotenv
from langchain\_openai import ChatOpenAI
from mcp\_use import MCPAgent, MCPClient
async def main():
# Load environment variables
load\_dotenv()
# Create MCPClient from config file
client = MCPClient.from\_config\_file(
os.path.join(os.path.dirname(\_\_file\_\_), &quot;&quot;browser\_mcp.json&quot;&quot;)
)
# Create LLM
llm = ChatOpenAI(model=&quot;&quot;gpt-4o&quot;&quot;)
# Alternative models:
# llm = ChatAnthropic(model=&quot;&quot;claude-3-5-sonnet-20240620&quot;&quot;)
# llm = ChatGroq(model=&quot;&quot;llama3-8b-8192&quot;&quot;)
# Create agent with the client
agent = MCPAgent(llm=llm, client=client, max\_steps=30)
# Run the query
result = await agent.run(
&quot;&quot;Find the best restaurant in San Francisco USING GOOGLE SEARCH&quot;&quot;,
max\_steps=30,
)
print(f&quot;&quot;\\nResult: {result}&quot;&quot;)
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
asyncio.run(main())`
```
## Airbnb Search
```
`import asyncio
import os
from dotenv import load\_dotenv
from langchain\_anthropic import ChatAnthropic
from mcp\_use import MCPAgent, MCPClient
async def run\_airbnb\_example():
# Load environment variables
load\_dotenv()
# Create MCPClient with Airbnb configuration
client = MCPClient.from\_config\_file(
os.path.join(os.path.dirname(\_\_file\_\_), &quot;&quot;airbnb\_mcp.json&quot;&quot;)
)
# Create LLM - you can choose between different models
llm = ChatAnthropic(model=&quot;&quot;claude-3-5-sonnet-20240620&quot;&quot;)
# Create agent with the client
agent = MCPAgent(llm=llm, client=client, max\_steps=30)
try:
# Run a query to search for accommodations
result = await agent.run(
&quot;&quot;Find me a nice place to stay in Barcelona for 2 adults &quot;&quot;
&quot;&quot;for a week in August. I prefer places with a pool and &quot;&quot;
&quot;&quot;good reviews. Show me the top 3 options.&quot;&quot;,
max\_steps=30,
)
print(f&quot;&quot;\\nResult: {result}&quot;&quot;)
finally:
# Ensure we clean up resources properly
if client.sessions:
await client.close\_all\_sessions()
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
asyncio.run(run\_airbnb\_example())`
```
Example configuration file (`airbnb\_mcp.json`):
```
`{
&quot;mcpServers&quot;: {
&quot;airbnb&quot;: {
&quot;command&quot;: &quot;npx&quot;,
&quot;args&quot;: [&quot;-y&quot;, &quot;@openbnb/mcp-server-airbnb&quot;]
}
}
}`
```
## Blender 3D Creation
```
`import asyncio
from dotenv import load\_dotenv
from langchain\_anthropic import ChatAnthropic
from mcp\_use import MCPAgent, MCPClient
async def run\_blender\_example():
# Load environment variables
load\_dotenv()
# Create MCPClient with Blender MCP configuration
config = {&quot;&quot;mcpServers&quot;&quot;: {&quot;&quot;blender&quot;&quot;: {&quot;&quot;command&quot;&quot;: &quot;&quot;uvx&quot;&quot;, &quot;&quot;args&quot;&quot;: [&quot;&quot;blender-mcp&quot;&quot;]}}}
client = MCPClient.from\_dict(config)
# Create LLM
llm = ChatAnthropic(model=&quot;&quot;claude-3-5-sonnet-20240620&quot;&quot;)
# Create agent with the client
agent = MCPAgent(llm=llm, client=client, max\_steps=30)
try:
# Run the query
result = await agent.run(
&quot;&quot;Create an inflatable cube with soft material and a plane as ground.&quot;&quot;,
max\_steps=30,
)
print(f&quot;&quot;\\nResult: {result}&quot;&quot;)
finally:
# Ensure we clean up resources properly
if client.sessions:
await client.close\_all\_sessions()
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
asyncio.run(run\_blender\_example())`
```
# Configuration Support
## HTTP Connection Example
MCP-Use supports HTTP connections, allowing you to connect to MCP servers running on specific HTTP ports. This feature is particularly useful for integrating with web-based MCP servers.
Here&#x27;s an example of how to use the HTTP connection feature:
```
`import asyncio
import os
from dotenv import load\_dotenv
from langchain\_openai import ChatOpenAI
from mcp\_use import MCPAgent, MCPClient
async def main():
&quot;&quot;&quot;&quot;&quot;&quot;Run the example using a configuration file.&quot;&quot;&quot;&quot;&quot;&quot;
# Load environment variables
load\_dotenv()
config = {
&quot;&quot;mcpServers&quot;&quot;: {
&quot;&quot;http&quot;&quot;: {
&quot;&quot;url&quot;&quot;: &quot;&quot;http://localhost:8931/sse&quot;&quot;
}
}
}
# Create MCPClient from config file
client = MCPClient.from\_dict(config)
# Create LLM
llm = ChatOpenAI(model=&quot;&quot;gpt-4o&quot;&quot;)
# Create agent with the client
agent = MCPAgent(llm=llm, client=client, max\_steps=30)
# Run the query
result = await agent.run(
&quot;&quot;Find the best restaurant in San Francisco USING GOOGLE SEARCH&quot;&quot;,
max\_steps=30,
)
print(f&quot;&quot;\\nResult: {result}&quot;&quot;)
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
# Run the appropriate example
asyncio.run(main())`
```
This example demonstrates how to connect to an MCP server running on a specific HTTP port. Make sure to start your MCP server before running this example.
# Multi-Server Support
MCP-Use allows configuring and connecting to multiple MCP servers simultaneously using the`MCPClient`. This enables complex workflows that require tools from different servers, such as web browsing combined with file operations or 3D modeling.
## Configuration
You can configure multiple servers in your configuration file:
```
`{
&quot;mcpServers&quot;: {
&quot;airbnb&quot;: {
&quot;command&quot;: &quot;npx&quot;,
&quot;args&quot;: [&quot;-y&quot;, &quot;@openbnb/mcp-server-airbnb&quot;, &quot;--ignore-robots-txt&quot;]
},
&quot;playwright&quot;: {
&quot;command&quot;: &quot;npx&quot;,
&quot;args&quot;: [&quot;@playwright/mcp@latest&quot;],
&quot;env&quot;: {
&quot;DISPLAY&quot;: &quot;:1&quot;
}
}
}
}`
```
## Usage
The`MCPClient`class provides methods for managing connections to multiple servers. When creating an`MCPAgent`, you can provide an`MCPClient`configured with multiple servers.
By default, the agent will have access to tools from all configured servers. If you need to target a specific server for a particular task, you can specify the`server\_name`when calling the`agent.run()`method.
```
`# Example: Manually selecting a server for a specific task
result = await agent.run(
&quot;&quot;Search for Airbnb listings in Barcelona&quot;&quot;,
server\_name=&quot;&quot;airbnb&quot;&quot; # Explicitly use the airbnb server
)
result\_google = await agent.run(
&quot;&quot;Find restaurants near the first result using Google Search&quot;&quot;,
server\_name=&quot;&quot;playwright&quot;&quot; # Explicitly use the playwright server
)`
```
## Dynamic Server Selection (Server Manager)
For enhanced efficiency and to reduce potential agent confusion when dealing with many tools from different servers, you can enable the Server Manager by setting`use\_server\_manager=True`during`MCPAgent`initialization.
When enabled, the agent intelligently selects the correct MCP server based on the tool chosen by the LLM for a specific step. This minimizes unnecessary connections and ensures the agent uses the appropriate tools for the task.
```
`import asyncio
from mcp\_use import MCPClient, MCPAgent
from langchain\_anthropic import ChatAnthropic
async def main():
# Create client with multiple servers
client = MCPClient.from\_config\_file(&quot;&quot;multi\_server\_config.json&quot;&quot;)
# Create agent with the client
agent = MCPAgent(
llm=ChatAnthropic(model=&quot;&quot;claude-3-5-sonnet-20240620&quot;&quot;),
client=client,
use\_server\_manager=True # Enable the Server Manager
)
try:
# Run a query that uses tools from multiple servers
result = await agent.run(
&quot;&quot;Search for a nice place to stay in Barcelona on Airbnb, &quot;&quot;
&quot;&quot;then use Google to find nearby restaurants and attractions.&quot;&quot;
)
print(result)
finally:
# Clean up all sessions
await client.close\_all\_sessions()
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
asyncio.run(main())`
```
# Tool Access Control
MCP-Use allows you to restrict which tools are available to the agent, providing better security and control over agent capabilities:
```
`import asyncio
from mcp\_use import MCPAgent, MCPClient
from langchain\_openai import ChatOpenAI
async def main():
# Create client
client = MCPClient.from\_config\_file(&quot;&quot;config.json&quot;&quot;)
# Create agent with restricted tools
agent = MCPAgent(
llm=ChatOpenAI(model=&quot;&quot;gpt-4&quot;&quot;),
client=client,
disallowed\_tools=[&quot;&quot;file\_system&quot;&quot;, &quot;&quot;network&quot;&quot;] # Restrict potentially dangerous tools
)
# Run a query with restricted tool access
result = await agent.run(
&quot;&quot;Find the best restaurant in San Francisco&quot;&quot;
)
print(result)
# Clean up
await client.close\_all\_sessions()
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
asyncio.run(main())`
```
# Sandboxed Execution
MCP-Use supports running MCP servers in a sandboxed environment using E2B&#x27;s cloud infrastructure. This allows you to run MCP servers without having to install dependencies locally, making it easier to use tools that might have complex setups or system requirements.
## Installation
To use sandboxed execution, you need to install the E2B dependency:
```
`# Install mcp-use with E2B support
pip install &quot;mcp-use[e2b]&quot;
# Or install the dependency directly
pip install e2b-code-interpreter`
```
You&#x27;ll also need an E2B API key. You can sign up at[e2b.dev](https://e2b.dev)to get your API key.
## Configuration
To enable sandboxed execution, use the sandbox parameter when creating your`MCPClient`:
```
`import asyncio
import os
from dotenv import load\_dotenv
from langchain\_openai import ChatOpenAI
from mcp\_use import MCPAgent, MCPClient
from mcp\_use.types.sandbox import SandboxOptions
async def main():
# Load environment variables (needs E2B\_API\_KEY)
load\_dotenv()
# Define MCP server configuration
server\_config = {
&quot;&quot;mcpServers&quot;&quot;: {
&quot;&quot;everything&quot;&quot;: {
&quot;&quot;command&quot;&quot;: &quot;&quot;npx&quot;&quot;,
&quot;&quot;args&quot;&quot;: [&quot;&quot;-y&quot;&quot;, &quot;&quot;@modelcontextprotocol/server-everything&quot;&quot;],
}
}
}
# Define sandbox options
sandbox\_options: SandboxOptions = {
&quot;&quot;api\_key&quot;&quot;: os.getenv(&quot;&quot;E2B\_API\_KEY&quot;&quot;), # API key can also be provided directly
&quot;&quot;sandbox\_template\_id&quot;&quot;: &quot;&quot;base&quot;&quot;, # Use base template
}
# Create client with sandboxed mode enabled
client = MCPClient(
config=server\_config,
sandbox=True,
sandbox\_options=sandbox\_options,
)
# Create agent with the sandboxed client
llm = ChatOpenAI(model=&quot;&quot;gpt-4o&quot;&quot;)
agent = MCPAgent(llm=llm, client=client)
# Run your agent
result = await agent.run(&quot;&quot;Use the command line tools to help me add 1+1&quot;&quot;)
print(result)
# Clean up
await client.close\_all\_sessions()
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
asyncio.run(main())`
```
## Sandbox Options
The`SandboxOptions`type provides configuration for the sandbox environment:
|Option|Description|Default|
`api\_key`|E2B API key. Required - can be provided directly or via E2B\_API\_KEY environment variable|None|
`sandbox\_template\_id`|Template ID for the sandbox environment|&quot;base&quot;|
`supergateway\_command`|Command to run supergateway|&quot;npx -y supergateway&quot;|
## Benefits of Sandboxed Execution
* **No local dependencies**: Run MCP servers without installing dependencies locally
* **Isolation**: Execute code in a secure, isolated environment
* **Consistent environment**: Ensure consistent behavior across different systems
* **Resource efficiency**: Offload resource-intensive tasks to cloud infrastructure# Direct Tool Calls (Without LLM)
You can call MCP server tools directly without an LLM when you need programmatic control:
```
`import asyncio
from mcp\_use import MCPClient
async def call\_tool\_example():
config = {
&quot;&quot;mcpServers&quot;&quot;: {
&quot;&quot;everything&quot;&quot;: {
&quot;&quot;command&quot;&quot;: &quot;&quot;npx&quot;&quot;,
&quot;&quot;args&quot;&quot;: [&quot;&quot;-y&quot;&quot;, &quot;&quot;@modelcontextprotocol/server-everything&quot;&quot;],
}
}
}
client = MCPClient.from\_dict(config)
try:
await client.create\_all\_sessions()
session = client.get\_session(&quot;&quot;everything&quot;&quot;)
# Call tool directly
result = await session.call\_tool(
name=&quot;&quot;add&quot;&quot;,
arguments={&quot;&quot;a&quot;&quot;: 1, &quot;&quot;b&quot;&quot;: 2}
)
print(f&quot;&quot;Result: {result.content[0].text}&quot;&quot;) # Output: 3
finally:
await client.close\_all\_sessions()
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
asyncio.run(call\_tool\_example())`
```
See the complete example:[examples/direct\_tool\_call.py](examples/direct_tool_call.py)
# Build a Custom Agent:
You can also build your own custom agent using the LangChain adapter:
```
`import asyncio
from langchain\_openai import ChatOpenAI
from mcp\_use.client import MCPClient
from mcp\_use.adapters.langchain\_adapter import LangChainAdapter
from dotenv import load\_dotenv
load\_dotenv()
async def main():
# Initialize MCP client
client = MCPClient.from\_config\_file(&quot;&quot;examples/browser\_mcp.json&quot;&quot;)
llm = ChatOpenAI(model=&quot;&quot;gpt-4o&quot;&quot;)
# Create adapter instance
adapter = LangChainAdapter()
# Get LangChain tools with a single line
tools = await adapter.create\_tools(client)
# Create a custom LangChain agent
llm\_with\_tools = llm.bind\_tools(tools)
result = await llm\_with\_tools.ainvoke(&quot;&quot;What tools do you have available ? &quot;&quot;)
print(result)
if \_\_name\_\_ == &quot;&quot;\_\_main\_\_&quot;&quot;:
asyncio.run(main())`
```
# Debugging
MCP-Use provides a built-in debug mode that increases log verbosity and helps diagnose issues in your agent implementation.
## Enabling Debug Mode
There are two primary ways to enable debug mode:
### 1. Environment Variable (Recommended for One-off Runs)
Run your script with the`DEBUG`environment variable set to the desired level:
```
`# Level 1: Show INFO level messages
DEBUG=1 python3.11 examples/browser\_use.py
# Level 2: Show DEBUG level messages (full verbose output)
DEBUG=2 python3.11 examples/browser\_use.py`
```
This sets the debug level only for the duration of that specific Python process.
Alternatively you can set the following environment variable to the desired logging level:
```
`export MCP\_USE\_DEBUG=1 # or 2`
```
### 2. Setting the Debug Flag Programmatically
You can set the global debug flag directly in your code:
```
`import mcp\_use
mcp\_use.set\_debug(1) # INFO level
# or
mcp\_use.set\_debug(2) # DEBUG level (full verbose output)`
```
### 3. Agent-Specific Verbosity
If you only want to see debug information from the agent without enabling full debug logging, you can set the`verbose`parameter when creating an MCPAgent:
```
`# Create agent with increased verbosity
agent = MCPAgent(
llm=your\_llm,
client=your\_client,
verbose=True # Only shows debug messages from the agent
)`
```
This is useful when you only need to see the agent&#x27;s steps and decision-making process without all the low-level debug information from other components.
## Star History
[![Star History Chart](https://api.star-history.com/svg?repos=pietrozullo/mcp-use&amp;type=Date)](https://www.star-history.com/#pietrozullo/mcp-use&amp;Date)
# Contributing
We love contributions! Feel free to open issues for bugs or feature requests. Look at[CONTRIBUTING.md](CONTRIBUTING.md)for guidelines.
## Contributors
Thanks to all our amazing contributors!
## Top Starred Dependents
# Requirements
* Python 3.11+
* MCP implementation (like Playwright MCP)
* LangChain and appropriate model libraries (OpenAI, Anthropic, etc.)# License
MIT
# Citation
If you use MCP-Use in your research or project, please cite:
```
`@software{mcp\_use2025,
author = {Zullo, Pietro},
title = {MCP-Use: MCP Library for Python},
year = {2025},
publisher = {GitHub},
url = {https://github.com/pietrozullo/mcp-use}
}`
```
## Related Servers
[
* ### Scout Monitoring MCP
sponsor
Put performance and error data directly in the hands of your AI assistant.
](https://mcpservers.org/servers/scoutamp)[
* ### Alpha Vantage MCP Server
sponsor
Access financial market data: realtime &amp; historical stock, ETF, options, forex, crypto, commodities, fundamentals, technical indicators, &amp; more
](https://mcpservers.org/servers/alphavantage/alpha_vantage_mcp)[
* ### Trustwise
Advanced evaluation tools for AI safety, alignment, and performance using the Trustwise API.
](https://mcpservers.org/servers/trustwiseai/trustwise-mcp-server)[
* ### Remote MCP Server (Authless)
An example of a remote MCP server deployable on Cloudflare Workers without authentication.
](https://mcpservers.org/servers/rsweetland/remote-mcp-server-authless)[
* ### DocuMind MCP Server
An MCP server for analyzing documentation quality using advanced neural processing.
](https://mcpservers.org/servers/Sunwood-ai-labs/documind-mcp-server)[
* ### Awesome LLMs Txt
Access documentation from the Awesome-llms-txt repository directly in your conversations.
](https://mcpservers.org/servers/SecretiveShell/MCP-llms-txt)[
* ### xctools
üçéMCP server for Xcode&#x27;s xctrace, xcrun, xcodebuild.
](https://mcpservers.org/servers/nzrsky/xctools-mcp-server)[
* ### Volatility3 MCP Server
Perform advanced memory forensics analysis using Volatility3 via a conversational interface. Requires user-specified memory dump files.
](https://mcpservers.org/servers/Kirandawadi/volatility3-mcp)[
* ### MCP Sourcify Server
Verify and retrieve smart contract source code using the Sourcify API.
](https://mcpservers.org/servers/soyrubio/mcp-sourcify)[
* ### MCP Pyrefly
A server for real-time Python code validation using Pyrefly, designed to prevent common coding errors from LLMs.
](https://mcpservers.org/servers/kimasplund/mcp-pyrefly)[
* ### Azure DevOps
Interact with Azure DevOps Work Items using the Azure DevOps REST API.
](https://mcpservers.org/servers/wuhuangjia/azure-devops-mcp-server)[
* ### Authless Remote MCP Server
An example of a remote MCP server without authentication, deployable on Cloudflare Workers or runnable locally via npm.
](https://mcpservers.org/servers/tempi-tech/remote-mcp-server-authless)
