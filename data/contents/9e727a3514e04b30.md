# Build an AI-powered multimodal RAG system with Docling and Granite

**URL:** https://www.ibm.com/think/tutorials/build-multimodal-rag-langchain-with-docling-granite
**Published:** None

---

## Summary

This page describes how to build an AI-powered **multimodal Retrieval-Augmented Generation (RAG)** system using **IBM Docling** and **IBM Granite** models.

The tutorial covers:
*   **Multimodal RAG:** Explaining that RAG can be extended beyond text to use multimodal LLMs (MLLMs) to process data like images, in addition to text. It mentions popular MLLMs like Gemini, Llama 3.2, GPT-4, and GPT-4o, and notes that the tutorial uses an IBM Granite model.
*   **Document Preprocessing:** Using **Docling** to parse documents (like PDFs), extract text and images, chunk the text, and use a Granite MLLM to generate descriptions for the images. Tables are converted to markdown.
*   **Vector Database Population:** Storing the text chunks, table markdown, and image descriptions (vectorized using a Granite Embeddings model) into a Milvus vector database.
*   **RAG Pipeline:** Setting up a RAG chain using **LangChain** to retrieve relevant document chunks (including image descriptions) based on a user query and use a Granite language model to generate an answer grounded in that context.

The system specifically focuses on answering real-time user queries from unstructured data in a PDF by combining vision and text understanding.

---

## Full Content

Build an AI-powered multimodal RAG system with Docling and Granite | IBM
# Build an AI-powered multimodal RAG system with Docling and Granite
## Authors
[BJ Hargrave](https://www.ibm.com/think/author/bj-hargrave)
Open Source Developer, STSM
[Erika Russi](https://www.ibm.com/think/author/erika-russi)
Data Scientist
IBM
In this tutorial, you will use IBM’s[Docling](https://github.com/DS4SD/docling)and open source[IBM Granite vision](https://huggingface.co/ibm-granite/granite-vision-3.2-2b), text-based[embeddings](https://huggingface.co/collections/ibm-granite/granite-embedding-models-6750b30c802c1926a35550bb)and[generative AI](https://huggingface.co/collections/ibm-granite/granite-32-language-models-67b3bc8c13508f6d064cff9a)models to create a RAG system. These models are available via various open source frameworks. In this tutorial, we will use[Replicate](https://replicate.com/)to connect to the IBM Granite vision and generative AI models and[HuggingFace](https://huggingface.co/models)to connect to the embeddings model.
## Multimodal retrieval-augmented generation
[Retrieval-augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation)is a technique used with large language models (LLMs) to connect the model with a knowledge base of information outside the data the LLM has been trained on without having to perform[fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning). Traditional RAG is limited to text-based use cases such as text summarization and chatbots.
Multimodal RAG can use[multimodal](https://www.ibm.com/think/topics/multimodal-ai)LLMs (MLLM) to process information from multiple types of data to be included as part of the external knowledge base used in RAG. Multimodal data can include text, images, audio, video or other forms. Popular multimodal LLMs include Google’s Gemini, Meta’s Llama 3.2 and OpenAI’s GPT-4 and GPT-4o.
For this recipe, you will use an IBM Granite model capable of processing different modalities. You will create an AI system to answer real-time user queries from unstructured data in a PDF.
## Tutorial overview
Welcome to this Granite tutorial. In this tutorial, you’ll learn how to harness the power of advanced tools to build an AI-powered multimodal RAG pipeline. This tutorial will guide you through the following processes:
* **Document preprocessing:**Learn how to handle documents from various sources, parse and transform them into usable formats and store them in vector databases by using Docling. You will use a Granite MLLM to generate image descriptions of images in the documents.
* **RAG:**Understand how to connect LLMs such as Granite with external knowledge bases to enhance query responses and generate valuable insights.
* **LangChain for workflow integration:**Discover how to use LangChain to streamline and orchestrate document processing and retrieval workflows, enabling seamless interaction between different components of the system.
This tutorial uses three cutting-edge technologies:
1. **[Docling](https://github.com/DS4SD/docling):**An open-source toolkit used to parse and convert documents.
2. **[Granite](https://www.ibm.com/granite/docs/models/granite/):**A state-of-the-art LLM that provides robust natural language capabilities and a vision language model that provides image to text generation.
3. **[LangChain](https://github.com/langchain-ai/langchain):**A powerful framework used to build applications powered by language models, designed to simplify complex workflows and integrate external tools seamlessly.
By the end of this tutorial, you will accomplish the following:
* Gain proficiency in document preprocessing, chunking and image understanding.
* Integrate vector databases to enhance retrieval capabilities.
* Use RAG to perform efficient and accurate data retrieval for real-world applications.
This tutorial is designed for AI developers, researchers and enthusiasts looking to enhance their knowledge of document management and advanced natural language processing (NLP) techniques. The tutorial can also be found in the IBM Granite Community’s[Granite Snack Cookbook GitHub](https://github.com/ibm-granite-community/granite-snack-cookbook/blob/main/recipes/RAG/Granite_Multimodal_RAG.ipynb)in the form of a Jupyter Notebook.
### Prerequisites
* Familiarity with Python programming.
* Basic understanding of LLMs, NLP concepts and computer vision.
## Steps
### Step 1: Install dependencies
```
`! echo &#34;&#34;::group::Install Dependencies&#34;&#34;
%pip install uv
! uv pip install git&#43;&#43;https://github.com/ibm-granite-community/utils.git \\
transformers \\
pillow \\
langchain\_classic \\
langchain\_core \\
langchain\_huggingface sentence\_transformers \\
langchain\_milvus &#39;&#39;pymilvus[milvus\_lite]&#39;&#39; \\
docling \\
&#39;&#39;langchain\_replicate &#64;&#64; git&#43;&#43;https://github.com/ibm-granite-community/langchain-replicate.git&#39;&#39;
! echo &#34;&#34;::endgroup::&#34;&#34;`
```
### Step 2: Selecting the AI models
#### Logging
To see some logging information, we can configure INFO log level.
NOTE: It is okay to skip running this cell.
import logging
logging.basicConfig(level&#61;logging.INFO)
### Load the Granite models
Specify the embeddings model to use for generating text embedding vectors. Here we will use one of the[Granite Embeddings models](https://huggingface.co/collections/ibm-granite/granite-embedding-models-6750b30c802c1926a35550bb)
To use a different embeddings model, replace this code cell with one from[this Embeddings Model recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Embeddings_Models.ipynb).
from langchain\_huggingface import HuggingFaceEmbeddings
from transformers import AutoTokenizer
embeddings\_model\_path &#61;&#61; “ibm-granite/granite-embedding-30m-english”
embeddings\_model &#61;&#61; HuggingFaceEmbeddings(
model\_name&#61;&#61;embeddings\_model\_path,
)
embeddings\_tokenizer &#61;&#61; AutoTokenizer.from\_pretrained(embeddings\_model\_path)
Specify the MLLM to use for image understanding. We will use the Granite vision model.
from ibm\_granite\_community.notebook\_utils import get\_env\_var
from langchain\_community.llms import Replicate
from transformers import AutoProcessor
vision\_model\_path &#61;&#61; “ibm-granite/granite-vision-3.2-2b”
vision\_model &#61;&#61; Replicate(
model&#61;&#61;vision\_model\_path,
replicate\_api\_token&#61;&#61;get\_env\_var(“REPLICATE\_API\_TOKEN”),
model\_kwargs&#61;&#61;{
“max\_tokens”: embeddings\_tokenizer.max\_len\_single\_sentence, # Set the maximum number of tokens to generate as output.
“min\_tokens”: 100, # Set the minimum number of tokens to generate as output.
},
)
vision\_processor &#61;&#61; AutoProcessor.from\_pretrained(vision\_model\_path)
Specify the language model to use for the RAG generation operation.  Here we use the Replicate LangChain client to connect to a Granite model from the[ibm-granite](https://replicate.com/ibm-granite)org on Replicate.
To get set up with Replicate, see[Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb). To connect to a model on a provider other than Replicate, substitute this code cell with one from the[LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb).
To connect to a model on a provider other than Replicate, substitute this code cell with one from the[LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb).
```
`from langchain\_replicate import ChatReplicate
model\_path &#61;&#61; &#34;&#34;ibm-granite/granite-4.0-h-small&#34;&#34;
model &#61;&#61; ChatReplicate(
model&#61;&#61;model\_path,
replicate\_api\_token&#61;&#61;get\_env\_var(&#34;&#34;REPLICATE\_API\_TOKEN&#34;&#34;),
model\_kwargs&#61;&#61;{
&#34;&#34;max\_tokens&#34;&#34;: 1000, # Set the maximum number of tokens to generate as output.
&#34;&#34;min\_tokens&#34;&#34;: 100, # Set the minimum number of tokens to generate as output.
},
)`
```
### Step 3: Preparing the documents for the vector database
In this example, from a set of source documents, we use[Docling](https://github.com/DS4SD/docling)to convert the documents into text and images. The text is then split into chunks. The images are processed by the MLLM to generate image summaries.
#### Use Docling to download the documents and convert to text and images
Docling will download the PDF documents and process them so we can obtain the text and images the documents contain. In the PDF, there are various data types, including text, tables, graphs and images.
from docling.document\_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base\_models import InputFormat
from docling.datamodel.pipeline\_options import PdfPipelineOptions
pdf\_pipeline\_options &#61;&#61; PdfPipelineOptions(
do\_ocr&#61;&#61;False,
generate\_picture\_images&#61;&#61;True,
)
format\_options &#61;&#61; {
InputFormat.PDF: PdfFormatOption(pipeline\_options&#61;&#61;pdf\_pipeline\_options),
}
converter &#61;&#61; DocumentConverter(format\_options&#61;&#61;format\_options)
sources &#61; [
“https://midwestfoodbank.org/images/AR\_2020\_WEB2.pdf”,
]
conversions &#61; { source: converter.convert(source&#61;source).document for source in sources }
With the documents processed, we then further process the text elements in the documents. We chunk them into appropriate sizes for the embeddings model we are using. A list of LangChain documents are created from the text chunks.
from docling\_core.transforms.chunker.hybrid\_chunker import HybridChunker
from docling\_core.types.doc.document import TableItem
from langchain\_core.documents import Document
doc\_id &#61;&#61; 0
texts: list[Document] &#61; []
for source, docling\_document in conversions.items():
for chunk in HybridChunker(tokenizer&#61;&#61;embeddings\_tokenizer).chunk(docling\_document):
items &#61;&#61; chunk.meta.doc\_items
if len(items) &#61;&#61; 1 and isinstance(items[0], TableItem):
continue # we will process tables later
refs &#61;&#61; ““.join(map(lambda item: item.get\_ref().cref, items))
print(refs)
text &#61; chunk.text
document &#61; Document(
page\_content&#61;&#61;text,
metadata&#61;{
“doc\_id”: (doc\_id:&#61;&#61;doc\_id&#43;&#43;1),
“source”: source,
“ref”: refs,
},
)
texts.append(document)
print(f”{len(texts)} text document chunks created”)
Next we process any tables in the documents. We convert the table data to markdown format for passing into the language model. A list of LangChain documents are created from the table’s markdown renderings.
from docling\_core.types.doc.labels import DocItemLabel
doc\_id &#61;&#61; len(texts)
tables: list[Document] &#61; []
for source, docling\_document in conversions.items():
for table in docling\_document.tables:
if table.label in [DocItemLabel.TABLE]:
ref &#61;&#61; table.get\_ref().cref
print(ref)
text &#61;&#61; table.export\_to\_markdown()
document &#61; Document(
page\_content&#61;&#61;text,
metadata&#61;{
“doc\_id”: (doc\_id:&#61;&#61;doc\_id&#43;&#43;1),
“source”: source,
“ref”: ref
},
)
tables.append(document)
print(f”{len(tables)} table documents created”)
Finally we process any images in the documents. Here we use the vision language model to understand the content of an image. In this example, we are interested in any textual information in the image. You might want to experiment with different prompt text to see how it might improve the results.
NOTE: Processing the images can take a very long time depending upon the number of images and the service running the vision language model.
import base64
import io
import PIL.Image
import PIL.ImageOps
from IPython.display import display
def encode\_image(image: PIL.Image.Image, format: str &#61;&#61; “png”) -&gt;&gt; str:
image &#61;&#61; PIL.ImageOps.exif\_transpose(image) or image
image &#61; image.convert(“RGB”)
buffer &#61; io.BytesIO()
image.save(buffer, format)
encoding &#61; base64.b64encode(buffer.getvalue()).decode(“utf-8”)
uri &#61; f”data:image/{format};base64,{encoding}”
return uri
# Feel free to experiment with this prompt
image\_prompt &#61;&#61; “If the image contains text, explain the text in the image.”
conversation &#61; [
{
“role”: “user”,
“content”: [
{“type”: “image”},
{“type”: “text”, “text”: image\_prompt},
],
},
]
vision\_prompt &#61;&#61; vision\_processor.apply\_chat\_template(
conversation&#61;conversation,
add\_generation\_prompt&#61;&#61;True,
)
pictures: list[Document] &#61; []
doc\_id &#61;&#61; len(texts) &#43;&#43; len(tables)
for source, docling\_document in conversions.items():
for picture in docling\_document.pictures:
ref &#61;&#61; picture.get\_ref().cref
print(ref)
image &#61;&#61; picture.get\_image(docling\_document)
if image:
text &#61;&#61; vision\_model.invoke(vision\_prompt, image&#61;&#61;encode\_image(image))
document &#61; Document(
page\_content&#61;&#61;text,
metadata&#61;{
“doc\_id”: (doc\_id:&#61;&#61;doc\_id&#43;&#43;1),
“source”: source,
“ref”: ref,
},
)
pictures.append(document)
print(f”{len(pictures)} image descriptions created”)
We can then display the LangChain documents created from the input documents.
import itertools
from docling\_core.types.doc.document import RefItem
# Print all created documents
for document in itertools.chain(texts, tables):
print(f”Document ID: {document.metadata[‘doc\_id’]}”)
print(f”Source: {document.metadata[‘source’]}”)
print(f”Content:\\n{document.page\_content}”)
print(“&#61;&#61;” \* 80) # Separator for clarity
for document in pictures:
print(f”Document ID: {document.metadata[‘doc\_id’]}”)
source &#61; document.metadata[‘source’]
print(f”Source: {source}”)
print(f”Content:\\n{document.page\_content}”)
docling\_document &#61;&#61; conversions[source]
ref &#61; document.metadata[‘ref’]
picture &#61;&#61; RefItem(cref&#61;&#61;ref).resolve(docling\_document)
image &#61;&#61; picture.get\_image(docling\_document)
print(“Image:”)
display(image)
print(“&#61;&#61;” \* 80) # Separator for clarity
### Populate the vector database
Using the embedding model, we load the documents from the text chunks and generated image captioning into a vector database. Creating this vector database allows us to easily conduct a semantic similarity search across our documents.
NOTE: Population of the vector database can take some time depending on your embedding model and service.
#### Choose your vector database
Specify the database to use for storing and retrieving embedding vectors.
To connect to a vector database other than Milvus, replace this code cell with one from[this Vector Store recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_Vector_Stores.ipynb).
import tempfile
from langchain\_core.vectorstores import VectorStore
from langchain\_milvus import Milvus
db\_file &#61;&#61; tempfile.NamedTemporaryFile(prefix&#61;&#61;”vectorstore\_”, suffix&#61;&#61;”.db”, delete&#61;&#61;False).name
print(f”The vector database will be saved to {db\_file}”)
vector\_db: VectorStore &#61;&#61; Milvus(
embedding\_function&#61;&#61;embeddings\_model,
connection\_args&#61;&#61;{“uri”: db\_file},
auto\_id&#61;&#61;True,
enable\_dynamic\_field&#61;&#61;True,
index\_params&#61;&#61;{“index\_type”: “AUTOINDEX”},
)
We now add all the LangChain documents for the text, tables and image descriptions to the vector database.
import itertools
documents &#61; list(itertools.chain(texts, tables, pictures))
ids &#61;&#61; vector\_db.add\_documents(documents)
print(f”{len(ids)} documents added to the vector database”)
### Step 4: RAG with Granite
Now that we have successfully converted our documents and vectorized them, we can set up out RAG pipeline.
#### Retrieve relevant chunks
Here we test the vector database by searching for chunks with relevant information to our query in the vector space. We display the documents associated with the retrieved image description.
Feel free to try different queries.
query &#61; &#34;How much was spent on food distribution relative to the amount of food distributed?&#34;
for doc in vector\_db.as\_retriever().invoke(query):
print(doc)
print(&#34;&#34;&#61;&#61;&#34;&#34; \* 80) # Separator for clarity
The returned document should be responsive to the query. Let&#39;s go ahead and construct our RAG pipeline.
#### Create the RAG pipeline for Granite
First we create the prompts for Granite to perform the RAG query. We use the Granite chat template and supply the placeholder values that the LangChain RAG pipeline will replace.
Next, we construct the RAG pipeline by using the Granite prompt templates previously created.
```
`from ibm\_granite\_community.langchain.chains.combine\_documents import create\_stuff\_documents\_chain
from langchain\_classic.chains.retrieval import create\_retrieval\_chain
from langchain\_core.prompts import ChatPromptTemplate
# Create a Granite prompt for question-answering with the retrieved context
prompt\_template &#61;&#61; ChatPromptTemplate.from\_template(&#34;&#34;{input}&#34;&#34;)
# Assemble the retrieval-augmented generation chain
combine\_docs\_chain &#61;&#61; create\_stuff\_documents\_chain(
llm&#61;&#61;model,
prompt&#61;&#61;prompt\_template,
)
rag\_chain &#61;&#61; create\_retrieval\_chain(
retriever&#61;&#61;vector\_db.as\_retriever(),
combine\_docs\_chain&#61;&#61;combine\_docs\_chain,
)`
```
#### Generate a retrieval-augmented response to a question
The pipeline uses the query to locate documents from the vector database and use them as context for the query.
```
`from ibm\_granite\_community.notebook\_utils import wrap\_text
output &#61;&#61; rag\_chain.invoke({&#34;&#34;input&#34;&#34;: query})
print(wrap\_text(output[&#39;&#39;answer&#39;&#39;]))`
```
Awesome! We have created an AI application that can successfully leverage knowledge from the source documents&#39; text and images.
## Next Steps
* Explore advanced RAG workflows for other industries.
* Experiment with other document types and larger datasets.
* Optimize prompt engineering for better Granite responses.
Link copied
[]()[](https://www.facebook.com/share.php?u=https://www.ibm.com/think/tutorials/build-multimodal-rag-langchain-with-docling-granite)[]()
[EbookUnlock the power of generative AI and ML
Learn how to confidently incorporate generative AI and machine learning into your business.
Read the ebook](https://www.ibm.com/account/reg/signup?formid=urx-52356)
## Resources
[TutorialIBM Developer: RAG tutorials
Explore all IBM Developer retrieval augmented generation (RAG) tutorials.
Start learning](https://developer.ibm.com/technologies/rag/tutorials/)
[ReportIBM is named a Leader in Data Science &amp; Machine Learning
Learn why IBM has been recognized as a Leader in the 2025 Gartner® Magic Quadrant™ for Data Science and Machine Learning Platforms.
Read the report](https://www.ibm.com/account/reg/signup?formid=urx-53728)
[BlogIBM RAG Cookbook
Explore a comprehensive collection of best practices, considerations, and tips for building RAG solutions tailored to business applications.
Read the blog](https://developer.ibm.com/blogs/awb-introducing-ibm-rag-cookbook/)
[ArticleIBM Developer: RAG articles
Explore all IBM Developer retrieval augmented generation (RAG) articles.
Get started](https://developer.ibm.com/technologies/rag/articles/)
[ArchitectureRetrieval augmented generation (RAG) architecture
Discover proven architecture patterns that accelerate the creation of technology solutions to meet your business challenges.
Explore architecture](https://www.ibm.com/architectures/hybrid/genai-rag)
[TutorialBuild a RAG agent to answer complex questions
Use Python, LangGraph, watsonx.ai, Elasticsearch, and Tavily to build a customized, modular agentic AI system.
Start learning](https://developer.ibm.com/tutorials/awb-build-rag-llm-agents/)
[TutorialQuick start: Prompt a foundation model with the retrieval-augmented generation pattern
Learn how to use foundation models in IBM watsonx.ai to generate factually accurate output grounded in information in a knowledge base by applying the retrieval augmented generation pattern.
Start learning](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/get-started-fm-notebook.html?context=wx)
[TutorialRetrieval augmented generation (RAG) pattern
Build a retrieval-augmented generation (RAG) pattern to generate factual output that is grounded in information from a knowledge base.
Start learning](https://www.ibm.com/account/reg/signup?formid=urx-52350)
Related solutions
IBM® watsonx Orchestrate™
Easily design scalable AI assistants and agents, automate repetitive tasks and simplify complex processes with IBM® watsonx Orchestrate™.
[Explore watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)
AI for developers
Move your applications from prototype to production with the help of our AI development solutions.
[Explore AI development tools](https://www.ibm.com/solutions/ai-for-developers)
AI consulting and services
Reinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.
[Explore AI services](https://www.ibm.com/consulting/artificial-intelligence)
Take the next step
Whether you choose to customize pre-built apps and skills or build and deploy custom agentic services using an AI studio, the IBM watsonx platform has you covered.
[Explore watsonx Orchestrate](https://www.ibm.com/products/watsonx-orchestrate)[Explore AI development tools](https://www.ibm.com/solutions/ai-for-developers)
