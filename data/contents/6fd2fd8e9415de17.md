# What are Embeddings and Vector Databases?

**URL:** https://huggingface.co/blog/qdrddr/what-are-embeddings-and-vector-databases
**Published:** 2024-08-20T00:00:00.000Z

---

## Summary

The webpage explains **Embeddings** as numerical representations of information that allow computers to determine similarity for tasks like search and classification. These embeddings act like "digital fingerprints" (vectors) for data, enabling fast, semantic search by finding data points whose vectors are mathematically closest to the query's vector.

The text also covers:
*   **Vector Databases (Vector DB):** Where the encoded numerical representations (vectors) of a dataset are stored, allowing for quick retrieval of relevant information chunks, often as the first phase in a **RAG (Retrieval-Augmented Generation)** application.
*   **How Embeddings Models Work:** They are trained on large datasets to find correlations (e.g., recognizing that "Pride and Prejudice" is semantically related to "First Impressions").
*   **Why Use Embeddings:** It is faster and easier for computers to process and understand relationships numerically than by searching raw text directly.
*   **Advantages & Disadvantages:** A key advantage is simplifying initial data retrieval without needing a schema. A major disadvantage is the **lack of transitivity** (if A is similar to B, and B is similar to C, A is not necessarily similar to C) and difficulty in synthesizing summarized concepts over large datasets, which can lead to less than 100% accurate RAG results.
*   **Retrieval:** Vector database retrieval is presented as one method, and the text notes that vector search can be just the first step before using more complex techniques.

The query asks about **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies**.

The page directly discusses **Vector databases**, **embeddings**, **RAG architectures** (by describing the two phases of RAG), and **chunking strategies** (mentioning that information is split into overlapping chunks before being stored).

The page **does not** explicitly mention:
*   New efficient embedding models (it only mentions that models are typically flavors of BERT).
*   Rerankers.
*   RAG alternatives.
*   Hybrid search.

Since the page does not cover all the requested topics, the summary focuses on what is present:

**Summary:**
The page defines **Embeddings** as numerical vectors representing data for fast, semantic similarity search, and **Vector Databases** as the storage system for these vectors, often used in the first phase

---

## Full Content

What are Embeddings and Vector Databases?
[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)
[Back to Articles](https://huggingface.co/blog)
# [](#what-are-embeddings-and-vector-databases)What are Embeddings and Vector Databases?
[Community Article](https://huggingface.co/blog/community)Published
August 20, 2024
[
Upvote
9
](https://huggingface.co/login?next=/blog/qdrddr/what-are-embeddings-and-vector-databases)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/6340651b388c3fa40f9a5bc0/av1C4_S7bHGxAzOu8lOmG.jpeg)](https://huggingface.co/lunarflu)
* [![](https://huggingface.co/avatars/4fbd7fd70abb3499593315bf22d3e868.svg)](https://huggingface.co/qdrddr)
* [![](https://huggingface.co/avatars/db1d6235022fb51544bd4af78c7705d9.svg)](https://huggingface.co/gunav)
* [![](https://huggingface.co/avatars/ef63c93c6e5b8f420cf70ec2a563528c.svg)](https://huggingface.co/ashutoshsoni1997)
* [![](https://huggingface.co/avatars/4ac2c5adb6948a4fe2dde4ef23c62e8f.svg)](https://huggingface.co/Upmanis)
* [![](https://huggingface.co/avatars/6c4c09997447b7c1193e49d939d1d26d.svg)](https://huggingface.co/randybuw)
* +3
[![Damien B's avatar](https://huggingface.co/avatars/4fbd7fd70abb3499593315bf22d3e868.svg)](https://huggingface.co/qdrddr)
[Damien B
qdrddr
Follow
](https://huggingface.co/qdrddr)
Embeddings are numerical representations of any information. They allow us to determine similarity, to empower quick search, classification, and recommendations. Imagine a digital library with a vast collection (our dataset). Each book is represented by coordinates ‚Äîa numerical vector that captures the essence of the book‚Äôs content, genre, style, and other features with a unique ‚Äòdigital fingerprint‚Äô for every book. When a user is searching for a book, they provide a search prompt. The library‚Äôs search system converts this prompt into vector coordinates using the**same embeddings method**it used for all the books to search through the library‚Äôs database. The system looks for the book vectors that are most similar to the prompt vectors. The books with the closest matching coordinates are then**recommended**to the user in the search results based on the initial request.
Another simplest use-case example would be if you are looking for a synonymous of a word. Embeddings can help you to find similar or ‚Äúclose‚Äù words, but it can do more than that. Semantic search is a very effective way to find related information to your prompt fast and that‚Äôs how Google Search Engine works.
The classic novel ‚ÄúPride and Prejudice‚Äù by Jane Austen is known by a different name in some countries ‚Äîit‚Äôs called ‚ÄúFirst Impressions‚Äù in some translations and adaptations. Despite the different names and languages, embedding these in a vector database would reveal their close semantic relationship, placing them near each other in the vector space. How do embeddings models work? Embeddings models specifically trained on large datasets to reveal these correlations, including "Pride and Prejudice" = "First Impressions", so if a model is not trained on that particular pair, it will not be as accurate in finding correlation.
Let me give you another example. This is better understood in comparison to how humans look at data vs. computers: Imagine you are looking for nearby cities to Chicago, IL on a map. If the computer knows the coordinates are {**41¬∞**88‚Äô18"N,**-87¬∞**62‚Äô31"W}, to find a city close to Chicago, it doesn‚Äôt need a map, just the list of coordinates of all other cities! Among the cities this spot {**41¬∞**84‚Äô56"N,**-87¬∞**75‚Äô39"W}, is the closest - Cicero, IL. For computers now this task is a mathematical problem to solve. Notice how latitude and longitude coordinate numbers are close. Now we can add additional ‚Äúdimension‚Äù with the size of the city by population, and if the user requests to find the closest city to Chicago with a similar size, the answer could be different for the given prompt. We can add more dimensions. Computers can find similarities in TV comedies, clothes, or many other types of information using this algorithm. In scientific language, it would formulate as ‚ÄúPlacing semantically similar inputs close together in the embedding space‚Äù. And FYI these coordinates are also referred to as latent space.
[![image/png](https://cdn-uploads.huggingface.co/production/uploads/643383a95277e3b24ef53185/Cd8sXoZUG3n2r2uAO6r88.png)](https://cdn-uploads.huggingface.co/production/uploads/643383a95277e3b24ef53185/Cd8sXoZUG3n2r2uAO6r88.png)
Embeddings is a very powerful tool to modify user prompt by enriching them with relevant information by placing the user search prompt into categories it belongs to and finding similar information via common categories from other sources. A good example would be daily news that our model is not aware of yet. Instead of baking this new information into the model daily, we simply retrieve the news from other sources and provide the closest and relevant information as additional context with the original user prompt to the model.
*Why do we need to encode and represent our dataset in a converted state as embeddings and convert user prompt into embeddings and then search vectors instead of just searching in the original dataset the text of the prompt directly? Because it's fast to process and easy for computers to understand the relationships between information this way. In other words, numerically similar embeddings of a text are also semantically similar.*
In preparing the first phase for our RAG application, the information in our entire dataset is split into overlapping chunks and stored in a database (called Vector DB) with encoded numerical representations so that later in the second phase, you can quickly retrieve a smaller portion of relevant information as an additional context for the user prompt. Embeddings encode text from our dataset into an index of vectors at the first phase and store them both in the vector database. Then on the second phase of application runtime, the user prompt is also encoded with the same Embeddings model, and the index with generated vectors for the user prompt is used to search and retrieve from the Vector DB chunks of text similarly to search engines work. That‚Äôs why they called Bi-encoder models. To encode text with numerical vector representations embeddings model is used which is typically much smaller, than LLMs. The beauty of searching Embeddings similarities stored in Vector DB is no need to know your data nor any schema to make this work. Today, virtually all embeddings are some flavors of the BERT model.
## [](#advantages--disadvantages-of-embeddings)Advantages &amp; Disadvantages of Embeddings:
Embeddings, despite their popularity, have a notable limitation: they lack transitivity and summarized concepts over large data. This has implications for interpreting and responding to queries in RAG systems. In vector space, when traversing disparate chunks of information through their shared attributes to provide new synthesized insights if vector A is similar to vector B, and vector B is similar to vector C, it does not necessarily mean that vector A is similar to vector C. When a user‚Äôs query, represented as vector A, gets B but seeks information that aligns with vector C, the direct similarity might not be immediately apparent via vector B. Also, disadvantages of embeddings are evident when trying to provide synthesized insights or holistically understand summarized semantic concepts over large data.
These limitations can lead to suboptimal situations where RAG systems, return only 60%, 70%, or 90% correct answers, rather than consistently achieving 100% accuracy.
While embeddings may not always be correct, they always return something, making them reliable in that regard. You might start thinking about what use of such relatability is if no quality is guaranteed though its simplicity often is a prerequisite to work with more complex data such as Semantic Layer, making Vector search just a first step to retrieve your data, more about it in my next posts. One of the key advantages is that you do not need to understand your data or have a schema to retrieve information, simplifying the initial stages of working with complex data. When implemented correctly and combined with other techniques, embeddings can have a positive compounding effect, which explains their widespread use despite their inherent limitations.
Retrieving from a Vector database is not the only way, you can retrieve data in many ways, from a Relational Database from tables or via APIs such as Google Maps or Yelp. You may want to use Vector database if you don‚Äôt have any other more convenient ways of storing and retrieving your data.
[https://huggingface.co/blog/getting-started-with-embeddings](https://huggingface.co/blog/getting-started-with-embeddings)[https://quamernasim.medium.com/mastering-rag-choosing-the-right-vector-embedding-model-for-your-rag-application-bbb57517890e](https://quamernasim.medium.com/mastering-rag-choosing-the-right-vector-embedding-model-for-your-rag-application-bbb57517890e)[https://github.com/alfredodeza/learn-retrieval-augmented-generation/tree/main](https://github.com/alfredodeza/learn-retrieval-augmented-generation/tree/main)
# [](#enjoyed-this-story)Enjoyed This Story?
If you like this topic and you want to support me:
1. Upvote ‚¨ÜÔ∏èmy article; that will help me out
2. [Follow me on Hugging Face](https://huggingface.co/qdrddr)Blog to get my latest articles and Join[AI Sky Discord Server](https://targeterpro.com/discord-sky-ai)ü´∂
3. Share this article on[social media](https://www.linkedin.com/feed/?shareActive=true&amp;shareUrl=https://huggingface.co/blog/qdrddr/what-are-embeddings-and-vector-databases)‚û°Ô∏èüåê
4. Give me feedback in the comments üí¨on LinkedIn. It‚Äôll help me better understand that this work was useful, even a simple ‚Äúthanks‚Äù will do. Give me good, or bad, whatever you think as long as you tell me the place to improve and how.
5. Connect with me or follow me on[LinkedIn](https://www.linkedin.com/in/damien-e)or[Discord](https://bit.ly/discord-qdrddr).
Disclaimer: This blog is not affiliated with, endorsed by, or sponsored in any way by any companies or any of their subsidiaries. Any references to products, services, logos, or trademarks are used solely to provide information and commentary and belong to respective owners. The views and opinions expressed in this blog post are the author‚Äôs own and do not necessarily reflect the views or opinions of corresponding companies.
### Community
EditPreview
Upload images, audio, and videos by dragging in the text input, pasting, orclicking here.
Tap or paste here to upload images
Comment
¬∑[Sign up](https://huggingface.co/join?next=/blog/qdrddr/what-are-embeddings-and-vector-databases)or[log in](https://huggingface.co/login?next=/blog/qdrddr/what-are-embeddings-and-vector-databases)to comment
[
Upvote
9
](https://huggingface.co/login?next=/blog/qdrddr/what-are-embeddings-and-vector-databases)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/6340651b388c3fa40f9a5bc0/av1C4_S7bHGxAzOu8lOmG.jpeg)](https://huggingface.co/lunarflu)
* [![](https://huggingface.co/avatars/4fbd7fd70abb3499593315bf22d3e868.svg)](https://huggingface.co/qdrddr)
* [![](https://huggingface.co/avatars/db1d6235022fb51544bd4af78c7705d9.svg)](https://huggingface.co/gunav)
* [![](https://huggingface.co/avatars/ef63c93c6e5b8f420cf70ec2a563528c.svg)](https://huggingface.co/ashutoshsoni1997)
* [![](https://huggingface.co/avatars/4ac2c5adb6948a4fe2dde4ef23c62e8f.svg)](https://huggingface.co/Upmanis)
* [![](https://huggingface.co/avatars/6c4c09997447b7c1193e49d939d1d26d.svg)](https://huggingface.co/randybuw)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/667a0b9fc9caef49dd02f4d2/YaiyazxX_g5sM3TkCDC5d.jpeg)](https://huggingface.co/WilRook3)
* [![](https://huggingface.co/avatars/52a8564d9ec62dcc80d5326b9c5d594e.svg)](https://huggingface.co/huggerfacet)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qT9-mGx1vCTsXbgRL4CUR.png)](https://huggingface.co/nkayy)
*
