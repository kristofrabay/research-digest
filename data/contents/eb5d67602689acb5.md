# Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights

**URL:** https://www.researchgate.net/publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights
**Published:** 2025-02-18T00:00:00.000Z

---

## Summary

The webpage summarizes a study titled "Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights," which examines the reasoning and planning capabilities of Large Language Models (LLMs) using inference-time techniques.

The study focuses on:
*   **Reasoning and Planning in LLMs:** Analyzing their ability to solve complex tasks across five categories: arithmetic, logical, common sense, algorithmic reasoning, and planning.
*   **Inference-Time Techniques:** Evaluating methods like **Chain-of-Thought (CoT)**, **Self-Consistency (SC)**, **Tree-of-Thought (ToT)**, and **Reasoning as Planning with World Models (RAP)** (which uses **MCTS**).
*   **Benchmark Creation:** Introducing **Sys2Bench**, a comprehensive benchmark with eleven diverse tasks to evaluate these techniques.
*   **Key Findings:** The study concludes that **simply scaling inference-time computation has limitations**, as no single technique consistently performs well across all reasoning and planning tasks. Tree search methods (ToT, RAP) often struggle with increasing complexity or tasks requiring self-verification, while CoT and SC perform well on arithmetic tasks. Large Reasoning Models (LRMs) like OpenAI's o1 show strong performance, especially in arithmetic and planning, but still struggle with tasks requiring advanced spatial reasoning (like Rubik's Cube). The research suggests a need for more diverse approaches beyond just scaling computation.

The user query asks about several specific concepts: **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, factuality.**

The page directly addresses:
*   **Reasoning LLMs:** The entire paper is about their reasoning and planning capabilities.
*   **Chain-of-Thought (CoT):** Explicitly mentioned and evaluated as a baseline inference-time technique.
*   **Inference-Time Compute/Scaling:** The core focus is on examining and finding limitations in scaling inference-time techniques.
*   **Planning with LLMs:** A major category of tasks evaluated in Sys2Bench.
*   **MCTS for Language Models:** Mentioned as part of the RAP technique, which uses Monte Carlo Tree Search.
*   **Factuality/Hallucination:** The discussion section notes that LLMs

---

## Full Content

(PDF) Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights
* [Home](directory/publications)
* [Philosophy Of Science](topic/Philosophy-Of-Science/publications)
* [Philosophy](topic/Philosophy/publications)
* [Reasoning](topic/Reasoning/publications)
PreprintPDF Available
# Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights
* February 2025
DOI:[10.48550/arXiv.2502.12521](https://doi.org/10.48550/arXiv.2502.12521)
* License
* [CC BY 4.0](https://www.researchgate.net/deref/https://creativecommons.org/licenses/by/4.0/)
Authors:
[](scientific-contributions/Shubham-Parashar-2268377793)
[Shubham Parashar](scientific-contributions/Shubham-Parashar-2268377793)
[Shubham Parashar](scientific-contributions/Shubham-Parashar-2268377793)
* This person is not on ResearchGate, or hasn't claimed this research yet.
[](scientific-contributions/Blake-Olson-2290097605)
[Blake Olson](scientific-contributions/Blake-Olson-2290097605)
[Blake Olson](scientific-contributions/Blake-Olson-2290097605)
* This person is not on ResearchGate, or hasn't claimed this research yet.
[](scientific-contributions/Sambhav-Khurana-2296306295)
[Sambhav Khurana](scientific-contributions/Sambhav-Khurana-2296306295)
[Sambhav Khurana](scientific-contributions/Sambhav-Khurana-2296306295)
* This person is not on ResearchGate, or hasn't claimed this research yet.
[](scientific-contributions/Eric-Li-2305925966)
[Eric Li](scientific-contributions/Eric-Li-2305925966)
[Eric Li](scientific-contributions/Eric-Li-2305925966)
* This person is not on ResearchGate, or hasn't claimed this research yet.
Show all 7 authorsHide
[Download file PDF](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights/fulltext/67b5f5fc4c479b26c9e6537a/Inference-Time-Computations-for-LLM-Reasoning-and-Planning-A-Benchmark-and-Insights.pdf)[Read file](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#read)
Preprints and early-stage research may not have been peer reviewed yet.
[Download file PDF](https://www.researchgate.net/publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights/fulltext/67b5f5fc4c479b26c9e6537a/Inference-Time-Computations-for-LLM-Reasoning-and-Planning-A-Benchmark-and-Insights.pdf)
[Read file](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#read)
[Download citation](https://www.researchgate.net/publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights/citation/download)
Copy linkLink copied
[
Read file
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#read)[
Download citation
](https://www.researchgate.net/publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights/citation/download)
Copy linkLink copied
## Abstract
We examine the reasoning and planning capabilities of large language models (LLMs) in solving complex tasks. Recent advances in inference-time techniques demonstrate the potential to enhance LLM reasoning without additional training by exploring intermediate steps during inference. Notably, OpenAI's o1 model shows promising performance through its novel use of multi-step reasoning and verification. Here, we explore how scaling inference-time techniques can improve reasoning and planning, focusing on understanding the tradeoff between computational cost and performance. To this end, we construct a comprehensive benchmark, known as Sys2Bench, and perform extensive experiments evaluating existing inference-time techniques on eleven diverse tasks across five categories, including arithmetic reasoning, logical reasoning, common sense reasoning, algorithmic reasoning, and planning. Our findings indicate that simply scaling inference-time computation has limitations, as no single inference-time technique consistently performs well across all reasoning and planning tasks.
**Discover the world's research**
* 25+ million members
* 160+ million publication pages
* 2.3+ billion citations[Join for free](signup.SignUp.html)
[](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#read-preview)
Available via license:[CC BY 4.0](deref/https://creativecommons.org/licenses/by/4.0/)
Content may be subject to copyright.
Inference-TimeComputationsforLLMReasoningand
Planning:ABenchmarkandInsights
ShubhamParashar∗∗BlakeOlson∗SambhavKhurana∗EricLi∗HongyiLing
JamesCaverleeShuiwangJi
DepartmentofComputerScience&amp;Engineering
TexasA&amp;MUniversity
Abstract
Weexaminethereasoningandplanningcapabilitiesoflargelanguagemodels
(LLMs)insolvingcomplextasks.Recentadvancesininference-timetechniques
demonstratethepotentialtoenhanceLLMreasoningwithoutadditionaltraining
byexploringintermediatestepsduringinference.Notably,OpenAI’so1model
showspromisingperformancethroughitsnoveluseofmulti-stepreasoningand
veriﬁcation.Here,weexplorehowscalinginference-timetechniquescanim-
provereasoningandplanning,focusingonunderstandingthetradeoffbetween
computationalcostandperformance.Tothisend,weconstructacomprehensive
benchmark,knownasSys2Bench,andperformextensiveexperimentsevaluating
existinginference-timetechniquesonelevendiversetasksacrossﬁvecategories,
includingarithmeticreasoning,logicalreasoning,commonsensereasoning,al-
gorithmicreasoning,andplanning.Ourﬁndingsindicatethatsimplyscaling
inference-timecomputationhaslimitations,asnosingleinference-timetechnique
consistentlyperformswellacrossallreasoningandplanningtasks.
1Introduction
Largelanguagemodels(LLMs)[Brownetal.,2020]havedemonstratedexceptionalperformance
acrossarangeofnaturallanguageprocessing(NLP)tasks,includingquestionanswering,machine
translation,sentimentanalysis,andtextsummarization[Devlinetal.,2019,Vaswanietal.,2017].
BeyondNLP,LLMshavealsobeenadaptedformultimodaltasksinvolvingvision[Parasharetal.,
2024,Linetal.,2025]andaudio[Wuetal.,2024].Buildingontheirsuccessinthesediverse
domains,researchersareincreasinglyusingLLMsasAIagents[Dengetal.,2024,Wangetal.,2024]
forcomplextasks,suchasrobotics[Liuetal.,2023]andscientiﬁcdiscovery[Wangetal.,2024].
ThesetasksrequirethereasoningandplanningcapabilitiesofLLMs,extendingbeyondsimplertext
comprehension.
ReasoningandplanninginLLMsrefertotheirabilitytosolvecomplexproblemsbyunderstanding,
processing,andgeneratingsolutionsacrossvariousdomains[Haoetal.,2024].Thesecapabilities
canbeanalyzedfrommultipleperspectives;weproposeaclassiﬁcationthatorganizesreasoning
andplanningtasksintoﬁvecategories,namelyarithmetic,logical,commonsense,algorithmic,and
plangenerationchallenges.Recentadvancesininference-timetechniquesdemonstratethepotential
toenhanceLLMreasoningandplanningwithoutadditionaltraining.Thesetechniquesfocuson
decomposingcomplexproblemsintosimplerintermediatestepsduringinference.Forinstance,
Chain-of-Thought[Weietal.,2022]encouragesstep-by-stepreasoning,whileTree-of-Thought[Yao
etal.,2024]choosesoptimalreasoningpathsusingtreesearch.Notably,OpenAI’sO1model,a
largereasoningmodel(LRM)[Valmeekametal.,2024],achievesstate-of-the-artperformanceon
∗Co-ﬁrstauthors;Projectpageathttps://github.com/divelab/sys2bench.
Preprint.Underreview.
arXiv:2502.12521v1 [cs.AI] 18 Feb 2025
[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](deref/https://github.com/divelab/sys2bench)
Table1:Summaryofthe11datasetsincludedinSys2Bench.
AlgorithmicReasoningPlanning
Gameof24BinpackingBlocksworldTripPlanCalendarPlanRubik’sCube
TaskProposeanarithmetic
expressiontoreach24.
Packitemsintothe
fewestbins.
Planactionstotransform
blocksfrominitialtogoalstate.
Planatripacrosscities
forasetnumberofdays.
Scheduleameetingconsidering
timeconstraintsofpeople.
Unscrambleascrambled
2×2Rubik’sCube.
InputAlistof4numbers.Listofitemweights
andbincapacity.
Initialstateofblocks
andgoalstate.
Cities,dayspercity,total
days,andpossibleﬂights.
Calendarswithmeetingsand
timeconstraints.
Ascrambled2×2Rubik’s
Cube.
OutputAnarithmetic
expression.
Finallistwithitems
arrangedinbins.
Asequenceofactions
astheplan.Atripitinerary.Ameetingtimeﬁttingall
schedules.
Asequenceofrotations
thatunscramblethecube.
ArithmeticReasoningLogicalReasoningCommonSenseReasoning
GSM8KAQuAProntoQAStrategyQAHotPotQA
TaskSolvehighschool
arithmeticproblems.
Solvealgebraic
problems.
Drawalogicalconclusion
fromasetofpredicates.
Answergeneralknowledge
questions.
Answergeneralknowledge
questionsusingprovidedfacts.
InputArithmeticproblem
description.
Algebraicproblem
description.
Aclausetoverifyastrueor
falseusinglogicalpredicates.Ayes/noquestion.Generalknowledgequestion
withsupportingfacts.
OutputAnumericalvalue.Amultiple-choiceoption.TrueorFalse,withreasoning.YesorNo.Shortanswerof1or2words.
variousreasoningtasks,demonstratingtheeffectivenessofinference-timetechniques.Thissuccess
hasinspiredtheresearchcommunitytofocusmoreonscalinginference-timetechniquesinthehope
ofsimilarperformanceimprovements.
AlthoughinferencetimetechniqueshaveimprovedLLMreasoningandplanning,evaluationof
thesemethodshasbeenlimitedtospeciﬁctasks,models,anddatasets.Moreover,thesemethods
haveadditionalcomputationalcosts,presentingatrade-offbetweencomputationaloverheadand
performancegains.ToovercomethislimitationweintroduceSys2Bench,acomprehensivebenchmark
coveringmultipletasksandmodels.Speciﬁcally,weperformexperimentsonelevendatasetsand
sevendifferentLLMs,testingfourwidelyusedinference-timetechniques.Basedonourﬁndings,we
arguethatsimplyscalinginference-timecomputationhaslimitations.Instead,weneedtoexplore
diverseapproachestoenhancetheholisticreasoningcapabilitiesofLLMs,asnosingleinference-time
techniqueconsistentlyoutperformsothersacrossalltasks.
2RelatedWork
LLMReasoningistheabilityofLLMstologicallyprocessinformationanddrawcoherentconclu-
sions,enablingthemtosolvecomplexproblems[SaparovandHe,2023].ThesuccessofLLMsin
NaturalLanguageGeneration[Radfordetal.,2018]andNaturalLanguageUnderstanding[Vaswani
etal.,2017,Devlinetal.,2019]hassparkedinterestinexploringreasoningcapabilities.Arangeof
datasetshavebeenintroducedtoevaluatereasoning,coveringtasksinarithmetic[Lingetal.,2017,
Cobbeetal.,2021],logic[Chollet,2019,Wangetal.,2022],commonsense[Yangetal.,2018,Geva
etal.,2021],andalgorithmicreasoning[Yaoetal.,2024].Weintroducethesetasksinmoredetailin
Section3,andreportresultsacrossthesetasksinSection5.
LLMPlanninginvolvesconstructingasequenceofactionstoachievedeﬁnedgoals[Valmeekam
etal.,2023,Zhengetal.,2024].LLMshavebeenemployedasplannersorhigh-levelcontrollersfor
robotictasksLiuetal.[2023],Huangetal.[2022]andasagentsforwebnavigation[Dengetal.,
2024],scientiﬁcdiscovery[Wangetal.,2024],andautonomousvehicles[Yangetal.,2023].Despite
theirbroadadoption,studiesrevealthatLLMsoftenstruggletogeneratevalidplansforcomplex
tasks[Kambhampatietal.,2024,Xieetal.,2024].Weprovidedetailsonevaluatedplanningproblems
inSection3,withresultsandanalysesinSection5.
InferenceTimeTechniquesforLLMsaremethodsappliedduringoutputgenerationtoimprove
performance,andalignmentwithdownstreamtasks[Wellecketal.,2024].Thesetechniquesaid
reasoningandplanningbybreakingcomplextasksintosmaller,manageablestepsforsystematic
problem-solving.Forinstance,Chain-of-Thoughtprompting(CoT)[Weietal.,2022]anditsvari-
ants[Zhouetal.,2023,Kojimaetal.,2022]decomposeproblemsintosequentialsteps,while
self-consistency[Wangetal.,2023]reﬁnesCoTbyaggregatingmultipleresponsesthroughvoting.
TreeofThought[Yaoetal.,2024],GraphofThought[Bestaetal.,2024],andMonteCarloTree
Search[Haoetal.,2023,Zhouetal.,2024]enhanceproblem-solvingbysystematicallyexploring
reasoningpaths.Detailsoninference-timemethodsareinSection4,withresultsinSection5.
2
[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf3)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf5)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf3)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf5)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf4)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf5)
3Sys2BenchProblemsandDatasets
Inthissection,weintroduceSys2Bench,abenchmarkdesignedtosystematicallyevaluatethe
reasoningandplanningcapabilitiesofLargeLanguageModels(LLMs)acrossdiversetasks.The
nameSys2BenchreﬂectsitsfocusonevaluatingSystematicReasoningandPlanning,providinga
structuredframeworkforassessinginference-timetechniques.
Akeymotivationforthisbenchmarkistodemonstratethelimitationsofsimplyscalinginference-
timecomputation,showingthatitdoesnotconsistentlyleadtobetterreasoningorproblem-solving
abilities.Whileinference-timetechniqueshavegainedtractioninimprovingLLMperformance,no
singleapproachconsistentlyoutperformsothersacrossalltasks.Thus,wearguethatamoreholistic
explorationofreasoningstrategiesisessential.Sys2BenchfacilitatesthisbybenchmarkingLLMs
onelevendatasets,categorizedintoﬁveprimaryreasoningtypes:ArithmeticReasoning,Logical
Reasoning,CommonSenseReasoning,AlgorithmicReasoning,andPlanning(summarizedinTable
1).
3.1ArithmeticReasoning
TheabilityofLargeLanguageModels(LLMs)tosolvemulti-steparithmeticproblemsremainsan
activeareaofresearchSnelletal.[2024],Kumaretal.[2024b],Hendrycksetal.[2021].Additionally,
OpenAI’so1models[OpenAI,2024]havepromptedtheresearchcommunitytoexploreinference-
timetechniquestoimprovethearithmeticreasoningofLLMs[Zhaoetal.,2024a].Weevaluate
thearithmeticreasoningofLLMs,onGSM8K[Cobbeetal.,2021]andAQuA[Lingetal.,2017]
benchmark.
GSM8Kisapopulardatasetofhigh-quality,linguisticallydiverseelementaryschoolmathword
problems,designedtoevaluatemulti-steparithmeticreasoning.Theproblemstypicallyrequire2to8
stepsofarithmeticoperations,testingtheabilityofLLMstoperformlogicaldeductionandbasic
calculations.
AQuAisadatasetofaround100,000algebraicwordproblemswithmultiple-choiceanswersand
detailedrationales.Itisdesignedtoevaluatethearithmeticreasoningandproblem-solvingcapabilities
ofmodels,makingitachallengingbenchmarkforLLMs.
3.2LogicalReasoning
Logicalreasoninginvolvesderivingconclusionsbasedonastructuredsequenceofrules,orpremises.
TheevaluationoftheabilitytoreasonlogicallybyLLMhelpsassesstheirabilitytosolvestructured
andcomplexdecision-makingproblems[Chollet,2019].WeuseProntoQA[SaparovandHe,2023]
toevaluatethelogicalreasoningabilityofLLMs.
ProntoQAisadatasetdevelopedtoevaluateanLLM’sabilitytoreasonandgenerateexplicit
reasoningchainsforﬁrst-orderlogic-basedqueries[Barwise,1977].Itchallengesmodelstonot
onlyproducecorrectanswersbutalsoprovidedetailed,step-by-stepreasoningpathsthatjustifytheir
conclusions.
3.3CommonSenseReasoning
CommonSenseReasoningistheprocessofdrawingconclusionsfromimpliciteverydayknowledge.
EvaluatingthisskillensuresthatLLMsprovideaccurateandcontextuallyappropriateresponses.We
evaluatethistypeofreasoningusingtheStrategyQAGevaetal.[2021]andHotPotQAYangetal.
[2018]datasets.
StrategyQAisabenchmarkdesignedtoassessamodel’sabilitytoperformimplicitmulti-step
reasoningusinggeneralknowledgeorcommonsensefacts.Itconsistsofyes/noquestionswherethe
goalistoarriveatthecorrectanswerbygeneratingandverifyingintermediatereasoningsteps.
HotPotQAisalarge-scaledatasetdesignedtoevaluatehoweffectivelymodelscombineinformation
frommultipledocumentstoanswergeneralknowledgequestions.Itfeaturesdiversequestiontypes
andteststheuseofsentence-levelevidenceforaccurateandexplainablemulti-hopreasoning.
3
[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf2)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)
3.4AlgorithmicReasoning
WefocusonapplyingLLMstosolvecomplexNP-hardandNP-completetasks,requiringthemto
evaluateconstraintsandproposeoptimizedalgorithmsthatachievepracticalandeffectivesolutions.
SuchproblemsassesstheapplicationofLLMstocombinatorialoptimizationandresourceallocation
tasks[Liuetal.,2024,Romera-Paredesetal.,2024].WeuseGameof24[Yaoetal.,2024],anda
noveldataset,BinPacking.
Gameof24isadatasetwherethegoalistoformanarithmeticexpressionevaluatingto24using
’+’,’-’,’\*’,or’/’withalistoffournumbers.AsanNP-completeproblemwithmultiplesolutions,it
challengesanLLMtoefﬁcientlygenerateexpressionsbyfocusingonlyonoperationsthatcanleadto
thetargetvalue.
BinPackingisanewtaskintroducedbyus,inspiredbythecombinatorialoptimizationproblems
studiedbyLiuetal.[2024],Romera-Paredesetal.[2024].Inthistask,thegoalistoﬁndtheleast
numberofbinsneededtopackalistofitems.Speciﬁcally,alistof
N
itemsofweight
[W1,W2,...Wn]
isgiven,whichmustbedividedintobins
B1,B2,B3...Bm
.Thesumofweightsineachbinmustnot
exceedthebincapacity
C
,andtheobjectiveistominimizethetotalnumberofbins
m
.Formally,the
taskcanbewrittenas:
minmsubjectto
Sm
j=1Bj={1,...,n},Bj∩Bj′=∅(∀j=j′),
Pi∈BjWi≤C(∀j).
(1)
3.5Planning
Aplanningproblemisdeﬁnedby
(S0,A,G)
,where
S0
standsforaninitialstate,
A
isthesetof
actionsneededtoachievethegoal
G
.PlanningproblemsrequireLLMstodemonstratemultistep
reasoning,andsounddecisionmakingtoarriveatcorrectsolutions.Theseproblemshavebroad
applicationsinroboticsandagent-basedsystems.Ourevaluationfocusesonfourplanningprob-
lems:BlocksWorld[Valmeekametal.,2023],Rubik’sCube[Dingetal.,2024],TripPlan,and
CalendarPlan[Zhengetal.,2024].
BlocksWorldisapopulardatasettoevaluatetheplanningcapabilitiesofLLMs.Eachtaskinvolves
transitioningfromaninitialblockconﬁgurationtoatargetconﬁguration,whichrequiresLLMsto
generateasequenceofactionstoachievethegoal.
Rubik’sCuberequiresanLLMtosolveascrambled
2×2
cubebyrestoringeachfacetoauniform
color.Startingfromascrambledcube,theLLMmustgenerateavalidplanofcuberotationsto
achievethegoal.
TripPlanchallengesanLLMtoplanatravelitinerarythatsatisﬁesconstraintsoncities,dates,and
ﬂightconnectivity,ensuringthatallcitiesarevisitedasspeciﬁed.
CalendarPlanisadatasetdesignedtoscheduleameetingbyaligningtheavailabilityofagroup
ofpeople.Thegoalistoﬁndafeasibletimeslotthataccommodatesalltheconstraintsofthe
participants.
4Sys2BenchBaselineMethods
InSys2Benchweevaluatepopularinference-timetechniquescommonlyusedtoenhanceSystem2
abilitiesofLLMs.Whilethesetechniqueshavetypicallybeenappliedtospeciﬁctasks,weanalyze
theirperformancecomprehensivelyinSys2Bench.Sys2Benchallowsustouncoverpatternsand
limitationsthatmaynotbepreviouslyevident.WesummarizethesemethodsinFig.1.
ChainofThought(CoT)enablesLLMstosolvecomplexproblemsbybreakingthemintointermedi-
atereasoningsteps,improvingtheirlogicalcoherenceandaccuracyWeietal.[2022].CoTenhances
structuredproblem-solvingofLLMsbyprovidingin-contextexamplesofstep-by-stepreasoning
duringinference.
SelfConsistency(SC)extendsCoTbygeneratingmultiplereasoningpathsforaproblemand
selectingthemostconsistentanswerthroughmajorityvoting[Wangetal.,2023].
4
[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf5)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)
3,3,4
Input: 1,2,3,4
1+2
3+3
6\*4
(1+2+3)\*4
Input: 1,2,3,4
(1+2+3)\*4
1+2
1-2
3+33+33-1
6\*42\*4
Input: 1,2,3,4
6\*4
.....
1-2
1+22\*3
Other
Operations
3+3
3-31+44-1
4\*64-26/4
1+2
(1+2+3)\*4
Input: 1,2,3,4
r1
r2
r3r5
r6
r7
r0r8
(1+2+3)\*4
1+2
3+3
6\*4
3-32\*1
12-2
2-1
r4
4-2
Chain of Thought
(CoT)Self Consistency
(SC)Tree of Thoughts
(ToT)Reasoning is Planning
withWorld Model
(RAP)
6,4
24
3,3,4
3,3,4-1,3,4
6,4
24
6,42,4
24821.524
6,46,56,4
242
0,4
-1,3,43,3,43,3,4
3,6
10
1,12
1,6,4
2,12
1,2,12
3\*4
0,4
Figure1:OverviewofInference-TimeTechniquesevaluatedontheGameof24dataset.Weevaluatefour
inference-timereasoningtechniques.ChainofThought(CoT)[Weietal.,2022]solvesproblemsthrougha
linearsequenceofreasoningsteps.Self-Consistency(SC)[Wangetal.,2023]extendsCoTbyselectinganswers
throughmajorityvotingovermultiplereasoningchains.TreeofThoughts(ToT)[Yaoetal.,2024]usestree
searchtoexploreandexpandreasoningpaths.ReasoningasPlanning(RAP)Haoetal.[2023]combinesMonte
CarloTreeSearch(MCTS)withtheLLMasaworldmodeltorewardreasoningstepsandguidetreegrowth
towardtheanswer.
TreeofThoughts(ToT)usesstructuredtreesearchtoenhancereasoninginLLMsbysystematically
exploringmultiplepaths,withtheLLMevaluatingitsownintermediategenerationstodecidewhich
pathstoexpand[Yaoetal.,2024].EvaluationcanbeperformedbyratingLLMgenerationonascale
of1-10orusinglogitsforscoring.
ToThasthreesearchstrategies:depth-ﬁrstsearch(DFS),breadth-ﬁrstsearch(BFS),andbeamsearch.
Inourexperiments,weusebeamsearchbecauseitperformsthebestamongstallvariants.
ReasoningasPlanningwithWorldModels(RAP)reformulatesreasoningasaplanningproblem,
wheretheLLMactsasboththereasoningagentandtheworldmodel[Haoetal.,2023].Thereasoning
agentgeneratespotentialreasoningpaths,whiletheworldmodelsimulatesandevaluatesthesepaths.
Speciﬁcally,RAPusesMonteCarloTreeSearch(MCTS)[Coulom,2006]toexploreandreﬁne
reasoningpaths.
UnlikeToT,whichdoesexhaustivetreesearch,RAPdynamicallyprioritizeshigh-potentialpaths
usingMCTS,resultinginimprovedperformance.RAPrequireslogitsforMCTS,whichiswhyitis
exclusivelyimplementedonLLaMA.SinceRAPrequiresextensivepromptengineeringtoframeall
tasksasplanningproblems,weevaluateitonasubsetoftasks,includingGSM8K,AQuA,ProntoQA,
StrategyQA,Gameof24,Binpacking,Blocksworld,andRubik’sCube.
5Experiments
Inthissection,wepresenttheexperimentsconductedonvarioustasksintheSys2Benchbenchmark.
Webeginbyoutliningtheexperimentalsetup,detailingthemodelsandimplementationspeciﬁcsof
theinference-timemethods.Next,theresultsforthedifferentinference-timemethodsareshownin
Table2andTable3.
5.1Setup
Inthissubsection,weprovidedetailsabouttheexperimentalsetupusedtoevaluatetheperformance
ofvariousinference-timetechniquesinSys2Bench.Wedescribethemodels,theimplementation
speciﬁcsoftheinference-timemethods,andthemetricsusedforevaluation.
ModelsevaluatedinSys2Bench,consistofthreeLLaMa3.1models,twoGPT-basedmodels,and
twolargereasoningmodels(LRMs).TheLLaMa3.1variantsare8B,70B,and405B,whilethe
5
[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf8)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf8)
GPT-basedmodelsincludeGPT-4oandGPT-4o-mini.Additionally,theO1andO1-minimodelsare
testedaspartofourLRMevaluation.Bydefault,weuseatemperatureof0.8acrossallmodelsfor
generation.
ChainofThought(CoT)involvesincludingin-contextlearningexamplesintheprompt.Inour
benchmark,welimitthistoﬁveexamplesperprompt.Theseexamplesareselectedfromthein-
contextexamplesprovidedbythedatasetorthetrainingset.Ifneitherisavailable,weuseasubsetof
testexamplesandevaluatetheremainingtestinstances.
SelfConsistency(SC)followsthesamesettingsasCoT.WegenerateﬁveCoTresponsesfromthe
LLManddeterminetheﬁnaloutputthroughmajorityvoting.
TreeofThought(ToT)implementationinSys2Benchusesbeamsearch.Thebeamsizeis5for
mosttasksexceptplanningtasks,wherethenumberofpossibleactionsateachstateislarger,thus,
thebeamsizeisincreasedto10.Bydefault,beamrankingisperformedbyaskingtheLLMtorate
outputsonascaleof1to10,exceptforLLaMa3.18B,wherelogitsareusedinstead.Finally,the
searchdepthistask-dependent,rangingfrom4forGameof24to20forTripPlan.
ReasoningasPlanning(RAP)usesMonteCarloTreeSearch(MCTS)withupto10rolloutsduring
inference.Duetoitsrelianceonarewardmodelthatrequireslogits,RAPisimplementedexclusively
onLLaMa3.18B.SimilartoToT,thesearchdepthinRAPvariesdependingonthetask.
InputOutputPrompting(IO)isutilizedwithLRMs,astheygeneratetheirownreasoningstepsand
donotrequirein-contextlearningexamples.Instead,weprovidethenecessaryformatandinstruct
themodelstorespondinthesameformat.
Metricusedacrossalltasksisaccuracy.Notethatthecontextofaccuracydiffersdependingeach
task.Forarithmetic,commonsense,andalgorithmicreasoningtasks,accuracyismeasuredonthe
correctnessoftheﬁnalanswer.Logicalreasoningtasks,namely,ProntoQA,accuracymeasuresthe
abilityofanLLMtogeneratethecorrectreasoningchain.Finally,forplanningtasks,accuracy
measuresthecorrectnessoftheproposedplan.
5.2Results
Inthissubsection,wepresenttheresultsoftheSys2Benchbenchmark,organizedbythetypesof
reasoningoutlinedinSection3.Thisgroupingallowsforaclearercomparisonofperformanceacross
tasks,demonstratingthestrengthsandlimitationsofdifferentinference-timetechniques.
ArithmeticReasoningtasksinTable2havestrongresultswithCoT.Performancefurtherimproves
withSC,asitreducestheimpactofrandomnessintheCoTanswers.However,thisstrongperformance
doesnottransfertotreesearchmethods.ToTsigniﬁcantlyunderperformsonthistask,asitsapproach
ofpromptingtheLLMtoexploremultiplereasoningpathsreliesontheLLMgeneratingandselecting
correctintermediatereasoningsteps.SinceLLMsstrugglewithself-veriﬁcation[Huangetal.,2024],
itselectsincorrectintermediatearithmeticsteps,leadingtowronganswers.Incontrast,RAPshows
modestgainsontheGSM8Kdataset,beneﬁtingfromtheLLM’sroleasaworldmodeltoselect
betterarithmeticsteps.However,RAPstillunderperformsSConAQuA,indicatingthattreesearch
methodsarenotwell-suitedforarithmeticreasoningtasks.Meanwhile,LRMsdeliverexceptional
arithmeticreasoningperformance,asshowninTable3,highlightingtheirstrengthinarithmetic.
LogicalReasoningresultsinTable2showinterestingtrends.Forinstance,SCimprovesperformance
overCoTonLLaMa3.18Band70B.However,forLLaMa3.1405BandGPT-basedmodels,
SCresultsinperformancedrops,asitincreasesthelikelihoodofgeneratingmultipleincorrect
reasoningchainsintheProntoQAtask,whereevaluationfocusesontheaccuracyofthesechains.
MajorityvotingdoesnothelpwhentheLLMoutputsmultiplewrongreasoningchains.Consistent
witharithmeticreasoning,treesearchmethodssuchasToTandRAPalsounderperforminthis
task,indicatingtheirlimitationsinlogicalreasoning.Finally,asshowninTable3,LRMsdonot
consistentlyoutperformLLMsonthistask,withO1performingworsethanGPT-4oonthistask.
CommonSenseReasoningperformanceofCoTandSCimproveswithincreasingLLMsize.
However,treesearchmethodsshowuniquetrends.Speciﬁcally,bothRAPandToTgenerate
supportingfactsforeachquestion,buttheireffectivenessvariesbytask.Tobespeciﬁc,inStrategyQA,
thebinaryoutput(yesorno)enablesLLaMAmodelstoeffectivelyutilizethegeneratedfacts,leading
toimprovedperformance.Incontrast,forHotPotQA,treesearchisnoteffectiveastheLLMneeds
6
[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf3)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf8)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf8)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf8)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf8)
tooutputshortanswers.AdditionalfactsoftencauseLLMhallucinationsandincreasederrorrates.
Furthermore,comparedtoothertasks,performanceimprovementsseenwithLRMsarelimited(see
Table3).
AlgorithmicReasoningtasksincludetheGameof24andBinpackingdatasets,asdescribedin
Section3.Table2showsthatbothCoTandSCunderperformonthesetasks.Duetothecombinatorial
optimizationnatureofthesetasks,thatrequireextensivesearch,treesearchmethodsperformwellon
allmodels,exceptLLaMa3.18B.ThesmallersizeofLLaMa3.18Blimitsthemodeltoaccurately
evaluateanddeterminethenextstepstowardasolution.WhencomparingLLMstoLRMs,resultsin
Table3highlightthepotentialofO1-miniandO1insolvingNP-HardandNP-Completeproblems,
withO1slightlyunderperformingO1-minionGameof24.
PlanningtasksarethemostchallenginginSys2Bench.Generally,CoTandSCperformanceimproves
withlargermodelsizes,andSCconsistentlyoutperformingCoT.
Treesearchmethodsshowmixedresultsacrosstasksandmodels.Onsmallermodels,suchas
LLaMa3.18BandGPT-4o-mini,ToTshowsimprovementsontaskslikeBlocksworldandTripPlan.
Howeverforlargermodelsandothertasks,ToToftendecreasesperformance.Thisisbecause
planningtasksrequireLLMstogenerateactionstosolveproblems,andincorrectactionscanlead
toincorrectsolutions.AlthoughToTisintendedtohelpLLMsexploremultiplereasoningpaths,
whichinplanningmeansconsideringdifferentactions,LLMsoftenfailtogenerateaccurateactions,
ultimatelyreducingperformance.Theothertreesearchmethod,RAP,performsexceptionallywell
onBlocksworldbyleveragingtheLLMasaworldmodeltopredictfuturestatesandrewards.
ComparedtoLLMs,LRMsperformsigniﬁcantlybetteronplanningtasks,withO1achievingnear-
perfectresultsonBlocksworld.However,theRubik’sCubetaskremainschallengingforallmethods
andmodels,asitrequiresadvancedspatialreasoningandprecisepredictionoftheconsequencesof
eachaction.BothLLMsandLRMscurrentlylackthereasoningcapabilitiesneededforthistask,
makingitout-of-distribution(OOD)forcurrentlanguagemodels.
5.3Insights
Weextendourmainexperimentstoprovideadditionalinsightsanduncoverimportanttrends.As
theresearchcommunityshowsincreasinginterestininference-timetechniquesandimprovingLLM
reasoning,theseﬁndingsoffervaluablecontributionstoongoingdiscussions.
Inference-timecomputescalingislimitedbyLLMbias.ThesetechniquesaimtoimproveLLM
reasoningbyguidingthemtogenerateintermediatesteps,simplifyingcomplextasksintosmaller,
manageableparts.However,thispremiseisﬂawedasLLMsdonotexhaustivelysearchforall
reasoningpathsandremainbiasedtowardcertainones.Asinference-timecomputescales,thisbias
persists,limitingexplorationandleadingtodiminishedperformance.Astaskcomplexityincreases,
thisissuebecomesworse,exacerbatingerrorsinreasoninganddecision-making.
OurSys2Benchexperimentsshowthistrendinarithmeticandlogicalreasoning.Inthesetasks,LLMs
excelwithCoTbutstrugglewithtreesearch,failingtoexplorereasoningpathsandselectthecorrect
one.
Treesearchstruggleswithincreasingcomplexity,performingsigniﬁcantlyworsethanCoT.
AsshowninFig2,itsbeneﬁtsdiminishbeyondadepthof4fortheTripPlanningandBlocksworld
tasksonLLaMa3.1405B.Notethat,LLaMa3.1405BhasastrongCoTperformanceinchallenging
planningtasksandideallyToTshouldleadtofurtherimprovements.However,ascomplexity
grows,generatingtherightintermediatestepsbecomescrucial,leadingtoworseperformanceof
ToTcomparedtoCoT.ApotentialexplanationforthisobservationistheinherentbiasofLLMsat
eachstepofthereasoningprocess.Thesebiasesmaypropagatethroughsuccessivesteps,leadingto
cumulativeerrorsthatdegradeToTperformance.
Languagemodelsrelyonretrievalratherthantrueunderstanding.Despiteadvancementsin
reasoningabilitieswithLRMssuchasO1andO1-Mini,theystillappeartobepatternmatching
ratherthangenuinereasoning.ThisissuehasbeenobservedinpriorstudiesforLLMs[Valmeekam
etal.,2023],butwearetheﬁrsttodemonstrateitforLRMs,includingO1andO1-Mini.
7
[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf8)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf3)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf8)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf8)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf9)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)
Table2:ResultsofInferenceTimeTechniquesacrossdiversetasksshowthatasmodelsizeincreases,perfor-
manceofCoT(CoT)[Weietal.,2022]andSelfConsistency(SC)[Wangetal.,2023]improves.However,this
trenddoesn’textendtotreesearchmethodslikeTreeofThought(ToT)[Yaoetal.,2024],whereperformance
doesnotimprovewiththebiggermodels.Furthermore,acomparisonbetweenToTandReasoningasPlanning
withWorldModels(RAP)[Haoetal.,2023]showsthatRAPoutperformsToTinplanningandarithmetic
reasoningtasksbutlagsincommonsensereasoningwhileperformingequallyinalgorithmicreasoningtasks.All
methodsandLLMsfailtosolvetheRubik’sCubeplanningtask.Thisfailurecanbeattributedtothespatial
understandingcapabilitiesrequiredforthetask,whicharecurrentlyoutofdistribution(OOD)forexistingLLMs.
Methods
AlgorithmicReasoningLogicalReasoning
GSM8KAQuAProntoQA
LLaMa3.1GPTLLaMa3.1GPTLLaMa3.1GPT
8B70B405B4omini4o8B70B405B4omini4o8B70B405B4omini4o
ChainofThoughtMethods
CoT79.895.597.092.694.758.777.278.073.679.945.882.691.061.491.8
SC@586.796.597.593.394.970.985.886.279.983.954.288.489.058.091.4
TreeSearchMethods
ToT60.091.596.091.593.544.878.085.881.178.013.524.262.642.032.8
RAP87.3––––68.1––––0.0––––
CommonSenseReasoningAlgorithmicReasoning
HotPotQAStrategyQAGameof24Binpacking
LLaMa3.1GPTLLaMa3.1GPTLLaMa3.1GPTLLaMA3.1GPT
8B70B405B4omini4o8B70B405B4omini4o8B70B405B4omini4o8B70B405B4omini4o
ChainofThoughtMethods
CoT13.830.641.038.652.846.061.576.076.679.26.08.07.013.014.06.033.045.031.075.0
SC@520.636.645.640.652.653.566.078.576.079.86.08.06.015.018.06.045.064.041.086.0
TreeSearchMethods
ToT23.030.031.531.438.268.082.079.567.573.51.059.069.042.062.01.046.081.053.077.0
RAP–––––58.5––––1.0––––1.0––––
Planning
BlocksworldTripPlanCalendarPlanRubik’sCube
LLaMa3.1GPTLLaMa3.1GPTLLaMa3.1GPTLLaMa3.1GPT
8B70B405B4omini4o8B70B405B4omini4o8B70B405B4omini4o8B70B405B4omini4o
ChainofThoughtMethods
CoT3.526.148.718.437.512.329.527.05.36.310.431.244.826.047.00.60.00.00.60.0
SC@54.530.752.121.241.512.032.334.35.05.811.638.045.629.647.40.00.60.60.60.6
TreeSearchMethods
ToT13.94.619.923.112.42.032.529.57.819.516.832.040.029.041.40.60.60.60.00.6
RAP46.8––––––––––––––0.6––––
Table3:Resultsoflargereasoningmodels(LRMs).WereporttheresultsofIOpromptingonLRMs,
includingOpenAIO1-miniandO1,withoutprovidinganyin-contextlearningexamples,asrecommendedby
OpenAI[OpenAI,2024].Overall,LRMsachievestate-of-the-artperformance,withO1outperformingO1-mini
onalltasksexcepttheGameof24.SimilartoLLMs,LRMsalsostrugglewiththeRubik’sCubetask,indicating
alackofspatialunderstanding.
Arithmetic
Reasoning
Logical
Reasoning
CommonSense
Reasoning
Algorithmic
ReasoningPlanning
GSM8KAQuAProntoQAHotPotQAStrategyQAGameof24BinpackingBlocksworldTripPlanCalendarPlanRubik’sCube
O1Mini98.092.064.035.074.077.090.048.324.088.20.0
O198.091.074.059.081.073.099.099.258.390.00.6
AsshowninFig.3,wecompareGPT-4o,O1-Mini,andO1basedonhowfrequentlytheirgenerated
movesintheRubik’sCubetaskalignwithknownonlinealgorithms.OuranalysisshowsthatGPT-4o
andO1-Minirepeatthesepopularmovesinnearlyallcases,withratesof75%and90%,respectively.
WhileO1performsbetter,itstillfollowscommonmovesequencesabout20%ofthetime.
Thereisatradeoffbetweenperformanceandthecostofinference-timemethods.Table4
includesresultsacrossBlocksworld,Gameof24,andGSM8KusingLLaMa3.18B,alongwiththe
numberoftokensgeneratedpertask.ComparedtoCoTandSC,ToTandRAPhavesigniﬁcantly
8
[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf9)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf9)
(a)TripPlan
46810
Cities
0
20
40
60
80
Accuracy
CoT
ToT
(b)Blocksworld
24681012
Steps
20
40
60
Accuracy
CoT
ToT
Figure2:TreeofThought(ToT)performancedeclinesastaskcomplexityincreases.In(a)TripPlanand(b)
Blocksworld,thenumberofstepsorcitiesrepresentstherequiredtreedepthforLLMinference-timesearch.
WhileToTperformswellforsmallerdepths,performancedeterioratesasproblemcomplexitygrows,eventually
fallingbelowCoT.Notably,CoTachievesbetterperformancewithsigniﬁcantlylowercomputationalresources,
asshowninTable4.TheseresultsareontheLLaMa405Bmodel.
Top 5Other
Moves Popularity
0
20
40
60
80
100
Percentage
Model
4o
O1 mini
O1
Figure3:Movesaregroupedbasedontheirpopu-
larityinonlineRubik’sCubesolutions.LLMslike
GPT-4ofrequentlygeneratemovescommonlyfound
inonlinealgorithms.Thisissueisevenmorepro-
nouncedinO1-mini,wherenearly90%ofthemoves
arerepeated!O1exhibitsthisbehaviorlessfrequently,
butitremainsanotablepattern.
Table4:TokencountcomparisonofCoT,SC,ToT,
andRAPonLLaMA3.18BacrossBlocksworld,
Gameof24,andGSM8K.Theresultshighlightthat
scalingupinference-timetechniquesincreasescompu-
tationalcostwithoutproportionateperformancegains,
contrastingwiththetrendsobservedbySnelletal.
[2024].
BlocksworldGameof24GSM8K
Acc↑Tokens↓Acc↑Tokens↓Acc↑Tokens↓
CoT3.53.2×1046.08.0×10379.81.9×104
SC4.51.7×1056.04.0×10486.72.4×105
ToT13.91.1×1061.03.6×10760.04.4×106
RAP46.84.9×1051.01.5×10787.39.9×106
highercomputationalcosts.However,increasedtokenusagedoesnotmeanbetterperformance.
Additionally,solving100Gameof24problemswithGPT-4oandToTcostsaround$60duetohigh
tokenusage,withcostsrisingforlargermodelsandhardertasks.Thistradeoffunderscoresthat
increasinginference-timecomputationdoesnotnecessarilytranslatetoproportionalimprovementsin
performance.
6Discussions
Whileourstudyhighlightscertainlimitations,thereisgrowinginterestinscalinginference-time
computationtofurtherenhanceLLMperformance[Snelletal.,2024].Thesemethodshavedemon-
stratedstrongresultsinarithmetic[Kumaretal.,2024a]andgameplaytasks[Schultzetal.,2024].
However,theseimprovementsrelyonveriﬁersorexternalmodelstoguidereasoning,andwithout
them,performancegainsdisappear.Moreover,taskslikeCommonSenseReasoninglackveriﬁers,
higlightingthelimitationofveriﬁer-dependentinference-timescaling.
Ontheotherhand,LRMslikeO1leveragereinforcementlearning(RL)withinference-timesearchto
improvereasoning[OpenAI,2024,Zhaoetal.,2024b].Thesemodelsaretrainedtogeneratecorrect
reasoningsteps,enablingbetterperformance(seeTable2).Despitesigniﬁcantgains,ourexperiments
highlightlimitations,particularlyinplanningandcommonsensereasoningtasks.Withinference
costsreaching60×thatofstandardLLMs,itiscrucialtoassesstheirlimitations.
9
[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf9)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfb)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfc)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfd)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pf8)
Incontrasttoalternativeviews,wearguethatsimplyscalinginference-timecomputationisnotthe
solution.Instead,improvingLLMreasoningrequiresamorestrategicapproach,andcombinationof
RLwithinference-timemethodshasbeenpromising[DeepSeek-AIetal.,2025].Therecentrelease
ofDeepSeek-R1hintsatprogressinthisdirection,offeringaglimpseofwhatmorereﬁnedmodels
couldachieveinthefuture.
7Conclusion
Thispaperexaminestheimpactofscalinginference-timecomputationonimprovingthereasoning
andplanningabilitiesofLLMs.Weshowthatscalinginference-timecomputationhaslimitations.
Instead,weneedtoexplorediverseapproachestoenhancetheholisticreasoningcapabilitiesof
LLMs.WeexplorethisbyintroducingSys2Bench,anewbenchmark,andconductextensive
experimentsevaluatinginference-timetechniquesacrosselevendiversetasksspanningﬁvecategories,
namely,arithmeticreasoning,logicalreasoning,commonsensereasoning,algorithmicreasoning,and
planning.Ourﬁndingsprovideimportantinsightsintothelimitationsofinference-timetechniques.
Finally,wediscussalternativeperspectivesfromtheliterature,criticallyanalyzetheirimplications,
andoutlinepotentialdirectionsforfutureresearch.
Acknowledgments
ThisworkwassupportedinpartbyNationalInstitutesofHealthundergrantU01AG070112and
NationalScienceFoundationundergrantCNS-2328395.
References
JonBarwise.Anintroductiontoﬁrst-orderlogic.InStudiesinLogicandtheFoundationsofMathematics,
volume90,pages5–46.Elsevier,1977.
MaciejBesta,NilsBlach,AlesKubicek,RobertGerstenberger,MichalPodstawski,LukasGianinazzi,Joanna
Gajda,TomaszLehmann,HubertNiewiadomski,PiotrNyczyk,etal.Graphofthoughts:Solvingelaborate
problemswithlargelanguagemodels.InProceedingsoftheAAAIConferenceonArtiﬁcialIntelligence,
volume38,pages17682–17690,2024.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,ArvindNeelakan-
tan,PranavShyam,GirishSastry,AmandaAskell,etal.Languagemodelsarefew-shotlearners.InAdvances
inNeuralInformationProcessingSystems(NeurIPS),volume33,pages1877–1901,2020.
FrançoisChollet.Onthemeasureofintelligence.arXivpreprintarXiv:1911.01547,2019.
KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,MatthiasPlappert,
JerryTworek,JacobHilton,ReiichiroNakano,ChristopherHesse,andJohnSchulman.Trainingveriﬁersto
solvemathwordproblems.arXivpreprintarXiv:2110.14168,2021.
RémiCoulom.Efﬁcientselectivityandbackupoperatorsinmonte-carlotreesearch.InInternationalconference
oncomputersandgames,pages72–83.Springer,2006.
DeepSeek-AI,DayaGuo,DejianYang,HaoweiZhang,JunxiaoSong,RuoyuZhang,RunxinXu,QihaoZhu,
ShirongMa,PeiyiWang,XiaoBi,XiaokangZhang,XingkaiYu,YuWu,Z.F.Wu,ZhibinGou,Zhihong
Shao,ZhuoshuLi,ZiyiGao,AixinLiu,BingXue,BingxuanWang,BochaoWu,BeiFeng,ChengdaLu,
ChenggangZhao,ChengqiDeng,ChenyuZhang,ChongRuan,DamaiDai,DeliChen,DongjieJi,ErhangLi,
FangyunLin,FucongDai,FuliLuo,GuangboHao,GuantingChen,GuoweiLi,H.Zhang,HanBao,Hanwei
Xu,HaochengWang,HonghuiDing,HuajianXin,HuazuoGao,HuiQu,HuiLi,JianzhongGuo,JiashiLi,
JiaweiWang,JingchangChen,JingyangYuan,JunjieQiu,JunlongLi,J.L.Cai,JiaqiNi,JianLiang,Jin
Chen,KaiDong,KaiHu,KaigeGao,KangGuan,KexinHuang,KuaiYu,LeanWang,LecongZhang,Liang
Zhao,LitongWang,LiyueZhang,LeiXu,LeyiXia,MingchuanZhang,MinghuaZhang,MinghuiTang,
MengLi,MiaojunWang,MingmingLi,NingTian,PanpanHuang,PengZhang,QianchengWang,Qinyu
Chen,QiushiDu,RuiqiGe,RuisongZhang,RuizhePan,RunjiWang,R.J.Chen,R.L.Jin,RuyiChen,
ShanghaoLu,ShangyanZhou,ShanhuangChen,ShengfengYe,ShiyuWang,ShuipingYu,ShunfengZhou,
ShutingPan,S.S.Li,ShuangZhou,ShaoqingWu,ShengfengYe,TaoYun,TianPei,TianyuSun,T.Wang,
WangdingZeng,WanjiaZhao,WenLiu,WenfengLiang,WenjunGao,WenqinYu,WentaoZhang,W.L.
Xiao,WeiAn,XiaodongLiu,XiaohanWang,XiaokangChen,XiaotaoNie,XinCheng,XinLiu,XinXie,
XingchaoLiu,XinyuYang,XinyuanLi,XuechengSu,XuhengLin,X.Q.Li,XiangyueJin,XiaojinShen,
10
[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)[
](publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights#pfa)
XiaoshaChen,XiaowenSun,XiaoxiangWang,XinnanSong,XinyiZhou,XianzuWang,XinxiaShan,Y.K.
Li,Y.Q.Wang,Y.X.Wei,YangZhang,YanhongXu,YaoLi,YaoZhao,YaofengSun,YaohuiWang,YiYu,
YichaoZhang,YifanShi,YiliangXiong,YingHe,YishiPiao,YisongWang,YixuanTan,YiyangMa,Yiyuan
Liu,YongqiangGuo,YuanOu,YuduanWang,YueGong,YuhengZou,YujiaHe,YunfanXiong,Yuxiang
Luo,YuxiangYou,YuxuanLiu,YuyangZhou,Y.X.Zhu,YanhongXu,YanpingHuang,YaohuiLi,YiZheng,
YuchenZhu,YunxianMa,YingTang,YukunZha,YutingYan,Z.Z.Ren,ZehuiRen,ZhangliSha,ZheFu,
ZheanXu,ZhendaXie,ZhengyanZhang,ZhewenHao,ZhichengMa,ZhigangYan,ZhiyuWu,ZihuiGu,
ZijiaZhu,ZijunLiu,ZilinLi,ZiweiXie,ZiyangSong,ZizhengPan,ZhenHuang,ZhipengXu,Zhongyu
Zhang,andZhenZhang.Deepseek-r1:Incentivizingreasoningcapabilityinllmsviareinforcementlearning,
2025.URLhttps://arxiv.org/abs/2501.12948.
XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamStevens,BoshiWang,HuanSun,andYuSu.Mind2web:
Towardsageneralistagentfortheweb.AdvancesinNeuralInformationProcessingSystems,36,2024.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.BERT:Pre-trainingofdeepbidirectional
transformersforlanguageunderstanding.InProceedingsofthe2019ConferenceoftheNorthAmerican
ChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages4171–4186,
2019.URLhttps://arxiv.org/abs/1810.04805.
RuomengDing,ChaoyunZhang,LuWang,YongXu,MinghuaMa,WeiZhang,SiQin,SaravanRajmohan,
QingweiLin,andDongmeiZhang.Everythingofthoughts:Defyingthelawofpenrosetriangleforthought
generation.InLun-WeiKu,AndreMartins,andVivekSrikumar,editors,FindingsoftheAssociationfor
ComputationalLinguistics:ACL2024,pages1638–1662,Bangkok,Thailand,August2024.Associationfor
ComputationalLinguistics.doi:10.18653/v1/2024.ﬁndings-acl.95.URL
https://aclanthology.org/
2024.findings-acl.95/.
MorGeva,DanielKhashabi,EladSegal,TusharKhot,DanRoth,andJonathanBerant.Didaristotleusea
laptop?aquestionansweringbenchmarkwithimplicitreasoningstrategies.TransactionsoftheAssociation
forComputationalLinguistics,9:346–361,2021.
ShiboHao,YiGu,HaodiMa,JoshuaJiahuaHong,ZhenWang,DaisyZheWang,andZhitingHu.Reasoning
withlanguagemodelisplanningwithworldmodel.InThe2023ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,2023.URLhttps://openreview.net/forum?id=VTWWvYtF1R.
ShiboHao,YiGu,HaotianLuo,TianyangLiu,XiyanShao,XinyuanWang,ShuhuaXie,HaodiMa,Adithya
Samavedhi,QiyueGao,etal.Llmreasoners:Newevaluation,library,andanalysisofstep-by-stepreasoning
withlargelanguagemodels.arXivpreprintarXiv:2404.05221,2024.
DanHendrycks,CollinBurns,SauravKadavath,AkulArora,StevenBasart,EricTang,DawnSong,andJacob
Steinhardt.Measuringmathematicalproblemsolvingwiththemathdataset.arXivpreprintarXiv:2103.03874,
2021.
JieHuang,XinyunChen,SwaroopMishra,HuaixiuStevenZheng,AdamsWeiYu,XinyingSong,andDenny
Zhou.Largelanguagemodelscannotself-correctreasoningyet.InTheTwelfthInternationalConferenceon
LearningRepresentations,2024.URLhttps://openreview.net/forum?id=IkmD3fKBPQ.
WenlongHuang,PieterAbbeel,DeepakPathak,andIgorMordatch.Languagemodelsaszero-shotplanners:
Extractingactionableknowledgeforembodiedagents.InInternationalconferenceonmachinelearning,
pages9118–9147.PMLR,2022.
SubbaraoKambhampati,KarthikValmeekam,LinGuan,MuditVerma,KayaStechly,SiddhantBhambri,
LucasPaulSaldyt,andAnilBMurthy.Position:Llmscan’tplan,butcanhelpplanninginllm-modulo
frameworks.InForty-ﬁrstInternationalConferenceonMachineLearning,2024.
TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa.Largelanguage
modelsarezero-shotreasoners.Advancesinneuralinformationprocessingsystems,35:22199–22213,2022.
AviralKumar,VincentZhuang,RishabhAgarwal,YiSu,JohnDCo-Reyes,AviSingh,KateBaumli,Shariq
Iqbal,ColtonBishop,RebeccaRoelofs,LeiMZhang,KayMcKinney,DishaShrivastava,CosminPaduraru,
GeorgeTucker,DoinaPrecup,FeryalBehbahani,andAleksandraFaust.Traininglanguagemodelsto
self-correctviareinforcementlearning,2024a.URLhttps://arxiv.org/abs/2409.12917.
AviralKumar,VincentZhuang,RishabhAgarwal,YiSu,JohnDCo-Reyes,AviSingh,KateBaumli,Shariq
Iqbal,ColtonBishop,RebeccaRoelofs,etal.Traininglanguagemodelstoself-correctviareinforcement
learning.arXivpreprintarXiv:2409.12917,2024b.
ZhiqiuLin,DeepakPathak,BaiqiLi,JiayaoLi,XideXia,GrahamNeubig,PengchuanZhang,andDeva
Ramanan.Evaluatingtext-to-visualgenerationwithimage-to-textgeneration.InEuropeanConferenceon
ComputerVision,pages366–384.Springer,2025.
11
[
](deref/https://arxiv.org/abs/2501.12948)[
](deref/https://arxiv.org/abs/1810.04805)[
](deref/https://aclanthology.org/2024.findings-acl.95/)[
](deref/https://aclanthology.org/2024.findings-acl.95/)[
](deref/https://openreview.net/forum?id=VTWWvYtF1R)[
](deref/https://openreview.net/forum?id=IkmD3fKBPQ)[
](deref/https://arxiv.org/abs/2409.12917)
WangLing,DaniYogatama,ChrisDyer,andPhilBlunsom.Programinductionbyrationalegeneration:
Learningtosolveandexplainalgebraicwordproblems.InReginaBarzilayandMin-YenKan,editors,
Proceedingsofthe55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:Long
Papers),pages158–167,Vancouver,Canada,July2017.AssociationforComputationalLinguistics.doi:
10.18653/v1/P17-1015.URLhttps://aclanthology.org/P17-1015/.
BoLiu,YuqianJiang,XiaohanZhang,QiangLiu,ShiqiZhang,JoydeepBiswas,andPeterStone.Llm+p:
Empoweringlargelanguagemodelswithoptimalplanningproﬁciency.arXivpreprintarXiv:2304.11477,
2023.
FeiLiu,TongXialiang,MingxuanYuan,XiLin,FuLuo,ZhenkunWang,ZhichaoLu,andQingfuZhang.
Evolutionofheuristics:Towardsefﬁcientautomaticalgorithmdesignusinglargelanguagemodel.In
Forty-ﬁrstInternationalConferenceonMachineLearning,2024.
OpenAI.Openaio1systemcard,2024.URLhttps://openai.com/index/openai-o1-system-card/.
ShubhamParashar,ZhiqiuLin,TianLiu,XiangjueDong,YananLi,DevaRamanan,JamesCaverlee,and
ShuKong.Theneglectedtailsinvision-languagemodels.InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages12988–12997,2024.
AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever.Improvinglanguageunderstand-
ingbygenerativepre-training.OpenAI,2018.URL
https://cdn.openai.com/research-covers/
language-unsupervised/language\_understanding\_paper.pdf.
BernardinoRomera-Paredes,MohammadaminBarekatain,AlexanderNovikov,MatejBalog,MPawanKumar,
EmilienDupont,FranciscoJRRuiz,JordanSEllenberg,PengmingWang,OmarFawzi,etal.Mathematical
discoveriesfromprogramsearchwithlargelanguagemodels.Nature,625(7995):468–475,2024.
AbulhairSaparovandHeHe.Languagemodelsaregreedyreasoners:Asystematicformalanalysisof
chain-of-thought.InTheEleventhInternationalConferenceonLearningRepresentations,2023.URL
https://openreview.net/forum?id=qFVVBzXxR2V.
JohnSchultz,JakubAdamek,MatejJusup,MarcLanctot,MichaelKaisers,SarahPerrin,DanielHennes,Jeremy
Shar,CannadaLewis,AnianRuoss,TomZahavy,PetarVeliˇ
ckovi´
c,LaurelPrince,SatinderSingh,EricMalmi,
andNenadTomašev.Masteringboardgamesbyexternalandinternalplanningwithlanguagemodels,2024.
URLhttps://arxiv.org/abs/2412.12119.
CharlieSnell,JaehoonLee,KelvinXu,andAviralKumar.Scalingllmtest-timecomputeoptimallycanbemore
effectivethanscalingmodelparameters.arXivpreprintarXiv:2408.03314,2024.
KarthikValmeekam,MatthewMarquez,SarathSreedharan,andSubbaraoKambhampati.Ontheplanning
abilitiesoflargelanguagemodels-acriticalinvestigation.AdvancesinNeuralInformationProcessingSystems,
36:75993–76005,2023.
KarthikValmeekam,KayaStechly,andSubbaraoKambhampati.Llmsstillcan’tplan;canlrms?apreliminary
evaluationofopenai’so1onplanbench.arXivpreprintarXiv:2409.13373,2024.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,
andIlliaPolosukhin.Attentionisallyouneed.InAdvancesinNeuralInformationProcessingSystems,
volume30,2017.URLhttps://arxiv.org/abs/1706.03762.
LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,JiakaiTang,
XuChen,YankaiLin,etal.Asurveyonlargelanguagemodelbasedautonomousagents.Frontiersof
ComputerScience,18(6):186345,2024.
SiyuanWang,ZhongkunLiu,WanjunZhong,MingZhou,ZhongyuWei,ZhuminChen,andNanDuan.From
lsat:Theprogressandchallengesofcomplexreasoning.IEEE/ACMTransactionsonAudio,Speech,and
LanguageProcessing,2022.
XuezhiWang,JasonWei,DaleSchuurmans,QuocVLe,EdH.Chi,SharanNarang,AakankshaChowdhery,
andDennyZhou.Self-consistencyimproveschainofthoughtreasoninginlanguagemodels.InTheEleventh
InternationalConferenceonLearningRepresentations,2023.URL
https://openreview.net/forum?
id=1PL1NIMMrw.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,DennyZhou,etal.
Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.Advancesinneuralinformation
processingsystems,35:24824–24837,2022.
12
[
](deref/https://aclanthology.org/P17-1015/)[
](deref/https://openai.com/index/openai-o1-system-card/)[
](deref/https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)[
](deref/https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)[
](deref/https://openreview.net/forum?id=qFVVBzXxR2V)[
](deref/https://arxiv.org/abs/2412.12119)[
](deref/https://arxiv.org/abs/1706.03762)[
](deref/https://openreview.net/forum?id=1PL1NIMMrw)[
](deref/https://openreview.net/forum?id=1PL1NIMMrw)
SeanWelleck,AmandaBertsch,MatthewFinlayson,HaileySchoelkopf,AlexXie,GrahamNeubig,Ilia
Kulikov,andZaidHarchaoui.Fromdecodingtometa-generation:Inference-timealgorithmsforlarge
languagemodels.TransactionsonMachineLearningResearch,2024.ISSN2835-8856.URL
https:
//openreview.net/forum?id=eskQMcIbMS.SurveyCertiﬁcation.
HaibinWu,XuanjunChen,Yi-ChengLin,Kai-weiChang,Ho-LamChung,AlexanderHLiu,andHung-yiLee.
Towardsaudiolanguagemodeling-anoverview.arXivpreprintarXiv:2402.13236,2024.
JianXie,KaiZhang,JiangjieChen,TinghuiZhu,RenzeLou,YuandongTian,YanghuaXiao,andYuSu.
Travelplanner:Abenchmarkforreal-worldplanningwithlanguageagents.InForty-ﬁrstInternational
ConferenceonMachineLearning,2024.
ZhenjieYang,XiaosongJia,HongyangLi,andJunchiYan.Llm4drive:Asurveyoflargelanguagemodelsfor
autonomousdriving.InNeurIPS2024WorkshoponOpen-WorldAgents,2023.
ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamWCohen,RuslanSalakhutdinov,andChristo-
pherDManning.Hotpotqa:Adatasetfordiverse,explainablemulti-hopquestionanswering.arXivpreprint
arXiv:1809.09600,2018.
ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,TomGrifﬁths,YuanCao,andKarthikNarasimhan.Tree
ofthoughts:Deliberateproblemsolvingwithlargelanguagemodels.AdvancesinNeuralInformation
ProcessingSystems,36,2024.
YuZhao,HuifengYin,BoZeng,HaoWang,TianqiShi,ChenyangLyu,LongyueWang,WeihuaLuo,
andKaifuZhang.Marco-o1:Towardsopenreasoningmodelsforopen-endedsolutions.arXivpreprint
arXiv:2411.14405,2024a.
YuZhao,HuifengYin,BoZeng,HaoWang,TianqiShi,ChenyangLyu,LongyueWang,WeihuaLuo,and
KaifuZhang.Marco-o1:Towardsopenreasoningmodelsforopen-endedsolutions,2024b.URL
https:
//arxiv.org/abs/2411.14405.
HuaixiuStevenZheng,SwaroopMishra,HughZhang,XinyunChen,MinminChen,AzadeNova,LeHou,
Heng-TzeCheng,QuocVLe,EdHChi,etal.Naturalplan:Benchmarkingllmsonnaturallanguageplanning.
arXivpreprintarXiv:2406.04520,2024.
AndyZhou,KaiYan,MichalShlapentokh-Rothman,HaohanWang,andYu-XiongWang.Languageagent
treesearchuniﬁesreasoning,acting,andplanninginlanguagemodels.InICML,2024.URL
https:
//openreview.net/forum?id=njwv9BsGHF.
DennyZhou,NathanaelSchärli,LeHou,JasonWei,NathanScales,XuezhiWang,DaleSchuurmans,Claire
Cui,OlivierBousquet,QuocVLe,andEdH.Chi.Least-to-mostpromptingenablescomplexreasoningin
largelanguagemodels.InTheEleventhInternationalConferenceonLearningRepresentations,2023.URL
https://openreview.net/forum?id=WZH7099tgfM.
13
[
](deref/https://openreview.net/forum?id=eskQMcIbMS)[
](deref/https://openreview.net/forum?id=eskQMcIbMS)[
](deref/https://arxiv.org/abs/2411.14405)[
](deref/https://arxiv.org/abs/2411.14405)[
](deref/https://openreview.net/forum?id=njwv9BsGHF)[
](deref/https://openreview.net/forum?id=njwv9BsGHF)[
](deref/https://openreview.net/forum?id=WZH7099tgfM)
... It is crucial for assessing the suitability of the model for real-time or highvolume prediction tasks in practical insurance applications. Generally, lower inference times are preferable for operational efficiency[31]; -Train-Val Loss Diff: The train-val method splits the data into training and validation sets, ensuring that the expected loss matches the meta-test-time loss, making it an unbiased empirical risk minimization (ERM) procedure. In contrast, the train-train method uses all data for both training and evaluation, resulting in a biased expected loss. ...
[GraphSAGE optimization for insurance risk assessment: balancing performance and efficiency](publication/397658755_GraphSAGE_optimization_for_insurance_risk_assessment_balancing_performance_and_efficiency)
Article
* Nov 2025
* [Oleksandr Lutsenko](https://www.researchgate.net/scientific-contributions/Oleksandr-Lutsenko-2331156591)
* [Serhii Shcherbak](https://www.researchgate.net/scientific-contributions/Serhii-Shcherbak-2293898987)
The subject of this article is the application and optimization of Graph Neural Networks, specifically the GraphSAGE (Graph SAmple and aggreGatE) architecture, for insurance risk assessment in volatile environments. This study aims to develop a robust and efficient GraphSAGE-based framework for insurance risk assessment that balances predictive performance with computational efficiency. This is achieved by systematically exploring various GraphSAGE architectures, optimizing hyperparameters, and implementing regularization techniques to prevent overfitting. The effectiveness of different configurations is evaluated through empirical analysis to find the optimal balance between model performance (accuracy) and efficiency (computational speed and memory usage). The tasks to be accomplished in this study include: designing and implementing a synthetic graph generation process that accurately represents the complexities of insurance risk data; conducting a systematic exploration of GraphSAGE architectures, varying the number of layers (2, 3, 4) and hidden channels (64, 128, 256); investigating the impact of different learning rates (0.1, 0.01, 0.001) on model convergence and stability; analyzing the effectiveness of various regularization techniques, including dropout (0.1 to 0.5) and weight decay (1e-05 to 0.0001); evaluating different training strategies, including the optimal number of epochs (100 to 300) and the implementation of early stopping; assessing the performance of different loss functions in handling outliers common in insurance data; and developing a comparison framework to facilitate informed decision-making in model selection for insurance risk assessment tasks. The methods used in this study are: employing an experimental approach, utilizing the PyTorch Geometric library for implementing GraphSAGE models, deploying the models and testing them on the cloud infrastructure, developing a custom graph generation algorithm to create realistic insurance risk scenarios, incorporating factors such as health scores, smoking status, and regular check-ups, and a grid search strategy for hyperparameter optimization, combined with cross-validation, regularization techniques to prevent overfitting, and employment of early stopping mechanisms. The quantitative results were confirmed by generating synthetic graphs that simulate realistic insurance risk scenarios and by conducting experiments to test different model configurations. One key finding is that a 2-layer GraphSAGE model with 128 hidden channels achieved performance comparable to more complex architectures, demonstrating that simpler models can be effective for insurance risk assessment tasks. Conclusions. The novelty of the results is as follows: 1) the relatively simple GraphSAGE architectures, such as 2-layer models with 128 hidden channels, can achieve performance comparable to more complex models in insurance risk assessment tasks. This suggests that the inherent structure of insurance risk data may not always require deep, elaborate neural networks to capture essential patterns. 2) the research underscores the importance of tailored regularization strategies, with deeper models generally requiring stronger regularization to combat overfitting. The investigation into training dynamics reveals the role of learning rate selection and early stopping strategies, with shallower models benefiting from higher learning rates, whereas deeper architectures require more conservative learning rates for stable convergence. The consistent performance of the Smooth L1 loss function across various model architectures demonstrates its suitability for insurance risk assessment tasks. 3) a foundation for the effective application of GraphSAGE models in insurance risk assessment is established, emphasizing the importance of a balanced approach to model design that considers not only predictive performance but also computational efficiency and practical deployment considerations.
[View](publication/397658755_GraphSAGE_optimization_for_insurance_risk_assessment_balancing_performance_and_efficiency)
Show abstract
... It is crucial for assessing the suitability of the model for real-time or highvolume prediction tasks in practical insurance applications. Generally, lower inference times are preferable for operational efficiency[31]; -Train-Val Loss Diff: The train-val method splits the data into training and validation sets, ensuring that the expected loss matches the meta-test-time loss, making it an unbiased empirical risk minimization (ERM) procedure. In contrast, the train-train method uses all data for both training and evaluation, resulting in a biased expected loss. ...
[GraphSAGE optimization for insurance risk assessment: balancing performance and efficiency](publication/396906475_GraphSAGE_optimization_for_insurance_risk_assessment_balancing_performance_and_efficiency)
Article
* Sep 2025
* [Oleksandr Lutsenko](https://www.researchgate.net/profile/Oleksandr-Lutsenko)
* [Serhii Shcherbak](https://www.researchgate.net/scientific-contributions/Serhii-Shcherbak-2293898987)
The subject of this article is the application and optimization of Graph Neural Networks, specifically the GraphSAGE (Graph SAmple and aggreGatE) architecture, for insurance risk assessment in volatile environments. This study aims to develop a robust and efficient GraphSAGE-based framework for insurance risk assessment that balances predictive performance with computational efficiency. This is achieved by systematically exploring various GraphSAGE architectures, optimizing hyperparameters, and implementing regularization techniques to prevent overfitting. The effectiveness of different configurations is evaluated through empirical analysis to find the optimal balance between model performance (accuracy) and efficiency (computational speed and memory usage). The tasks to be accomplished in this study include: designing and implementing a synthetic graph generation process that accurately represents the complexities of insurance risk data; conducting a systematic exploration of GraphSAGE architectures, varying the number of layers (2, 3, 4) and hidden channels (64, 128, 256); investigating the impact of different learning rates (0.1, 0.01, 0.001) on model convergence and stability; analyzing the effectiveness of various regularization techniques, including dropout (0.1 to 0.5) and weight decay (1e-05 to 0.0001); evaluating different training strategies, including the optimal number of epochs (100 to 300) and the implementation of early stopping; assessing the performance of different loss functions in handling outliers common in insurance data; and developing a comparison framework to facilitate informed decision-making in model selection for insurance risk assessment tasks. The methods used in this study are: employing an experimental approach, utilizing the PyTorch Geometric library for implementing GraphSAGE models, deploying the models and testing them on the cloud infrastructure, developing a custom graph generation algorithm to create realistic insurance risk scenarios, incorporating factors such as health scores, smoking status, and regular check-ups, and a grid search strategy for hyperparameter optimization, combined with cross-validation, regularization techniques to prevent overfitting, and employment of early stopping mechanisms. The quantitative results were confirmed by generating synthetic graphs that simulate realistic insurance risk scenarios and by conducting experiments to test different model configurations. One key finding is that a 2-layer GraphSAGE model with 128 hidden channels achieved performance comparable to more complex architectures, demonstrating that simpler models can be effective for insurance risk assessment tasks. Conclusions. The novelty of the results is as follows: 1) the relatively simple GraphSAGE architectures, such as 2-layer models with 128 hidden channels, can achieve performance comparable to more complex models in insurance risk assessment tasks. This suggests that the inherent structure of insurance risk data may not always require deep, elaborate neural networks to capture essential patterns. 2) the research underscores the importance of tailored regularization strategies, with deeper models generally requiring stronger regularization to combat overfitting. The investigation into training dynamics reveals the role of learning rate selection and early stopping strategies, with shallower models benefiting from higher learning rates, whereas deeper architectures require more conservative learning rates for stable convergence. The consistent performance of the Smooth L1 loss function across various model architectures demonstrates its suitability for insurance risk assessment tasks. 3) a foundation for the effective application of GraphSAGE models in insurance risk assessment is established, emphasizing the importance of a balanced approach to model design that considers not only predictive performance but also computational efficiency and practical deployment considerations.
[View](publication/396906475_GraphSAGE_optimization_for_insurance_risk_assessment_balancing_performance_and_efficiency)
Show abstract
[DeepSeek-R1 vs OpenAI o1 for Ophthalmic Diagnoses and Management Plans](publication/395268634_DeepSeek-R1_vs_OpenAI_o1_for_Ophthalmic_Diagnoses_and_Management_Plans)
Article
Full-text available
* Oct 2025
* [David Mikhail](https://www.researchgate.net/profile/David-Mikhail-3)
* [Andrew Farah](https://www.researchgate.net/profile/Andrew-Farah-2)
* [Jason Milad](https://www.researchgate.net/profile/Jason-Milad)
* [Renaud Duval](https://www.researchgate.net/profile/Renaud-Duval)
Importance
Large language models (LLMs) are increasingly being explored in clinical decision-making, but few studies have evaluated their performance on complex ophthalmology cases from clinical practice settings. Understanding whether open-weight, reasoning-enhanced LLMs can outperform proprietary models has implications for clinical utility and accessibility.
Objective
To evaluate the diagnostic accuracy, management decision-making, and cost of DeepSeek-R1 vs OpenAI o1 across diverse ophthalmic subspecialties.
Design, Setting, and Participants
This was a cross-sectional evaluation conducted using standardized prompts and model configurations. Clinical cases were sourced from JAMA Ophthalmology’s Clinical Challenge articles, containing complex cases from clinical practice settings. Each case included an open-ended diagnostic question and a multiple-choice next-step decision. All cases were included without exclusions, and no human participants were involved. Data were analyzed from March 13 to March 30, 2025.
Exposures
DeepSeek-R1and OpenAI o1 were evaluated using the Plan-and-Solve Plus (PS+) prompt engineering method.
Main Outcomes and Measures
Primary outcomes were diagnostic accuracy and next-step decision-making accuracy, defined as the proportion of correct responses. Token cost analyses were performed to estimate expenses. Intermodel agreement was evaluated using Cohen κ, and McNemar test was used to compare performance.
Results
A total of 422 clinical cases were included, spanning 10 subspecialties. DeepSeek-R1 achieved a higher diagnostic accuracy of 70.4% (297 of 422 cases) compared with 63.0% (266 of 422 cases) for OpenAI o1, a 7.3% difference (95% CI, 1.0%-13.7%; P = .02). For next-step decisions, DeepSeek-R1 was correct in 82.7% of cases (349 of 422 cases) vs OpenAI o1’s accuracy of 75.8% (320 of 422 cases), a 6.9% difference (95% CI, 1.4%-12.3%; P = .01). Intermodel agreement was moderate (κ = 0.422; 95% CI, 0.375-0.469; P &lt; .001). DeepSeek-R1 offered lower costs per query than OpenAI o1, with savings exceeding 66-fold (up to 98.5%) during off-peak pricing.
Conclusions and Relevance
DeepSeek-R1 outperformed OpenAI o1 in diagnosis and management across subspecialties while lowering operating costs, supporting the potential of open-weight, reinforcement learning–augmented LLMs as scalable and cost-saving tools for clinical decision support. Further investigations should evaluate safety guardrails and assess performance of self-hosted adaptations of DeepSeek-R1 with domain-specific ophthalmic expertise to optimize clinical utility.
[View](publication/395268634_DeepSeek-R1_vs_OpenAI_o1_for_Ophthalmic_Diagnoses_and_Management_Plans)
Show abstract
[Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model](publication/380399749_Evolution_of_Heuristics_Towards_Efficient_Automatic_Algorithm_Design_Using_Large_Language_Model)
Conference Paper
Full-text available
* May 2024
* [Fei Liu](https://www.researchgate.net/profile/Fei-Liu-46)
* [Xialiang Tong](https://www.researchgate.net/scientific-contributions/Xialiang-Tong-2160455008)
* [Mingxuan Yuan](https://www.researchgate.net/scientific-contributions/Mingxuan-Yuan-34968069)
* [Qingfu Zhang](https://www.researchgate.net/profile/Qingfu-Zhang-2)
Heuristics are widely used for dealing with com-
plex search and optimization problems. How-
ever, manual design of heuristics can be often
very labour extensive and requires rich working
experience and knowledge. This paper proposes
Evolution of Heuristic (EoH), a novel evolution-
ary paradigm that leverages both Large Language
Models (LLMs) and Evolutionary Computation
(EC) methods for Automatic Heuristic Design
(AHD). EoH represents the ideas of heuristics in
natural language, termed thoughts. They are then
translated into executable codes by LLMs. The
evolution of both thoughts and codes in an evo-
lutionary search framework makes it very effec-
tive and efficient for generating high-performance
heuristics. Experiments on three widely studied
combinatorial optimization benchmark problems
demonstrate that EoH outperforms commonly
used handcrafted heuristics and other recent AHD
methods including FunSearch. Particularly, the
heuristic produced by EoH with a low computa-
tional budget (in terms of the number of queries
to LLMs) significantly outperforms widely-used
human hand-crafted baseline algorithms for the
online bin packing problem.
[View](publication/380399749_Evolution_of_Heuristics_Towards_Efficient_Automatic_Algorithm_Design_Using_Large_Language_Model)
Show abstract
[A survey on large language model based autonomous agents](publication/379217962_A_survey_on_large_language_model_based_autonomous_agents)
Article
Full-text available
* Mar 2024
* [Lei Wang](https://www.researchgate.net/profile/Lei-Wang-862)
* [Chen Ma](https://www.researchgate.net/scientific-contributions/Chen-Ma-2259250531)
* [Xueyang Feng](https://www.researchgate.net/profile/Xueyang-Feng-5)
* [Jirong Wen](https://www.researchgate.net/scientific-contributions/Jirong-Wen-2276998129)
Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.
[View](publication/379217962_A_survey_on_large_language_model_based_autonomous_agents)
Show abstract
[Mathematical discoveries from program search with large language models](publication/376546904_Mathematical_discoveries_from_program_search_with_large_language_models)
Article
Full-text available
* Dec 2023
* NATURE
* [Bernardino Romera-Paredes](https://www.researchgate.net/profile/Bernardino-Romera-Paredes)
* [Mohammadamin Barekatain](https://www.researchgate.net/scientific-contributions/Mohammadamin-Barekatain-2232593100)
* [Alexander Novikov](https://www.researchgate.net/scientific-contributions/Alexander-Novikov-2232555096)
* [Alhussein Fawzi](https://www.researchgate.net/scientific-contributions/Alhussein-Fawzi-2232567913)
Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements1,2. This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches³. Applying FunSearch to a central problem in extremal combinatorics—the cap set problem—we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.
[View](publication/376546904_Mathematical_discoveries_from_program_search_with_large_language_models)
Show abstract
[Reasoning with Language Model is Planning with World Model](publication/376401333_Reasoning_with_Language_Model_is_Planning_with_World_Model)
Conference Paper
Full-text available
* Jan 2023
* [Shibo Hao](https://www.researchgate.net/profile/Shibo-Hao)
* [Yi Gu](https://www.researchgate.net/profile/Yi-Gu-22)
* [Haodi Ma](https://www.researchgate.net/scientific-contributions/Haodi-Ma-2220567222)
* [Zhiting Hu](https://www.researchgate.net/profile/Zhiting-Hu)
[View](publication/376401333_Reasoning_with_Language_Model_is_Planning_with_World_Model)
[Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies](publication/351169132_Did_Aristotle_Use_a_Laptop_A_Question_Answering_Benchmark_with_Implicit_Reasoning_Strategies)
Article
Full-text available
* Apr 2021
* [Mor Geva](https://www.researchgate.net/profile/Mor-Geva)
* [Daniel Khashabi](https://www.researchgate.net/profile/Daniel-Khashabi)
* [Elad Segal](https://www.researchgate.net/scientific-contributions/Elad-Segal-2164325270)
* [Jonathan Berant](https://www.researchgate.net/profile/Jonathan-Berant)
A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼66%.
[View](publication/351169132_Did_Aristotle_Use_a_Laptop_A_Question_Answering_Benchmark_with_Implicit_Reasoning_Strategies)
Show abstract
[The Neglected Tails in Vision-Language Models](publication/384236904_The_Neglected_Tails_in_Vision-Language_Models)
Conference Paper
* Jun 2024
* [Shubham Parashar](https://www.researchgate.net/scientific-contributions/Shubham-Parashar-2268377793)
* [Zhiqiu Lin](https://www.researchgate.net/profile/Zhiqiu-Lin)
* [Tian Liu](https://www.researchgate.net/scientific-contributions/Tian-Liu-2254048677)
* [Shu Kong](https://www.researchgate.net/profile/Shu-Kong-2)
[View](publication/384236904_The_Neglected_Tails_in_Vision-Language_Models)
[Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation](publication/384213581_Everything_of_Thoughts_Defying_the_Law_of_Penrose_Triangle_for_Thought_Generation)
Conference Paper
* Jan 2024
* [Ruomeng Ding](https://www.researchgate.net/scientific-contributions/Ruomeng-Ding-2255329530)
* [Chaoyun Zhang](https://www.researchgate.net/profile/Chaoyun-Zhang)
* [Lu Wang](https://www.researchgate.net/scientific-contributions/Lu-Wang-2159801711)
* [Dongmei Zhang](https://www.researchgate.net/profile/Dongmei_Zhang13)
[View](publication/384213581_Everything_of_Thoughts_Defying_the_Law_of_Penrose_Triangle_for_Thought_Generation)
[From LSAT: The Progress and Challenges of Complex Reasoning](publication/359678019_From_LSAT_The_Progress_and_Challenges_of_Complex_Reasoning)
Article
* Jan 2022
* [Siyuan Wang](https://www.researchgate.net/profile/Siyuan-Wang-59)
* [Zhongkun Liu](https://www.researchgate.net/scientific-contributions/Zhongkun-Liu-2194908107)
* [Wanjun Zhong](https://www.researchgate.net/profile/Wanjun-Zhong-2)
* [Nan Duan](https://www.researchgate.net/profile/Nan-Duan-4)
Complex reasoning aims to draw a correct inference based on complex rules. As a hallmark of human intelligence, it involves a degree of explicit reading comprehension, interpretation of logical knowledge and complex rule application. In this paper, we take a step forward in complex reasoning by systematically studying the three challenging and domain-general tasks of the Law School Admission Test (LSAT), including analytical reasoning, logical reasoning and reading comprehension. We propose a hybrid reasoning system to integrate these three tasks and achieve impressive overall performance on the LSAT tests. The experimental results demonstrate that our system endows itself a certain complex reasoning ability, especially the fundamental reading comprehension and challenging logical reasoning capacities. Further analysis also shows the effectiveness of combining the pre-trained models with the task-specific reasoning module, and integrating symbolic knowledge into discrete interpretable reasoning steps in complex reasoning. We further shed a light on the potential future directions, like unsupervised symbolic knowledge extraction, model interpretability, few-shot learning and comprehensive benchmark for complex reasoning.
[View](publication/359678019_From_LSAT_The_Progress_and_Challenges_of_Complex_Reasoning)
Show abstract
[Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems](publication/318740342_Program_Induction_by_Rationale_Generation_Learning_to_Solve_and_Explain_Algebraic_Word_Problems)
Conference Paper
* Jan 2017
* [Wang Ling](https://www.researchgate.net/profile/Wang-Ling-16)
* [Dani Yogatama](https://www.researchgate.net/scientific-contributions/Dani-Yogatama-69685138)
* [Chris Dyer](https://www.researchgate.net/scientific-contributions/Chris-Dyer-70520053)
* [Phil Blunsom](https://www.researchgate.net/scientific-contributions/Phil-Blunsom-70806035)
[View](publication/318740342_Program_Induction_by_Rationale_Generation_Learning_to_Solve_and_Explain_Algebraic_Word_Problems)
[Attention Is All You Need](publication/317558625_Attention_Is_All_You_Need)
Article
* Jun 2017
* [Ashish Vaswani](https://www.researchgate.net/scientific-contributions/Ashish-Vaswani-2107454759)
* [Noam Shazeer](https://www.researchgate.net/profile/Noam-Shazeer)
* [Niki Parmar](https://www.researchgate.net/scientific-contributions/Niki-Parmar-2129043508)
* [Illia Polosukhin](https://www.researchgate.net/scientific-contributions/Illia-Polosukhin-2113887535)
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new
