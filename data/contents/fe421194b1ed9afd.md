# CMRAG: Co-modalityâ€“based document retrieval and visual question answering

**URL:** https://arxiv.org/html/2509.02123v1
**Published:** 2025-01-01T00:00:00.000Z

---

## Summary

The webpage describes **CMRAG (Co-modalityâ€“based document retrieval and visual question answering)**, a novel Retrieval-Augmented Generation (RAG) framework designed to handle multimodal documents by simultaneously leveraging both **text and image** information.

Key aspects related to your query:

*   **Multimodal RAG/Vision-Language Models (VLMs):** CMRAG is a multimodal RAG approach that addresses the limitations of pure text-based RAG (which ignores images) and pure vision-based RAG (which ignores precise text semantics). It uses VLMs (like Qwen2.5-VL-7B-Instruct) for document parsing and final answer generation.
*   **Document Understanding/PDF Parsing/Chart/Table Extraction:** The framework performs structured parsing on documents to obtain co-modality representations, including the entire page image, parsed sub-images (capturing localized elements like figures/tables), and extracted text.
*   **Report Generation with LLMs (VLM):** The retrieved co-modality evidence (text and image) is fed into a VLM to generate the final answer, enabling cross-modal reasoning.
*   **Performance:** Experiments show that CMRAG significantly outperforms pure-vision-based RAG, particularly when integrating parsed text with entire images, which provides complementary grounding for accurate reasoning.

The paper focuses on improving **Visual Document Question Answering (VQA)** by unifying text and image modalities within the RAG pipeline.

---

## Full Content

CMRAG: Co-modalityâ€“based document retrieval and visual question answering
# CMRAG: Co-modalityâ€“based document retrieval and visual question answering
Wang Chen
Baidu Inc.
The University of Hong Kong
wchen22@connect.hku.hk&amp;Guanqiang Qi
Baidu Inc.
qiguanqiang@baidu.com&amp;Weikang Li
Peking University
wavejkd@pku.edu.cn&amp;Yang Li
Baidu Inc.
liyang164@baidu.comCorresponding author
###### Abstract
Retrieval-Augmented Generation (RAG) has become a core paradigm in document question answering tasks. However, existing methods have limitations when dealing with multimodal documents: one category of methods relies on layout analysis and text extraction, which can only utilize explicit text information and struggle to capture images or unstructured content; the other category treats document segmentation as visual input and directly passes it to visual language models (VLMs) for processing, yet it ignores the semantic advantages of text, leading to suboptimal generation results.
This paper proposes co-modalityâ€“based RAG (CMRAG), which can simultaneously leverage text and images for efficient retrieval and generation. Specifically, we first perform structured parsing on documents to obtain co-modality representations of text segments and image regions. Subsequently, in response to user queries, we retrieve candidate evidence from text and image channels, respectively, and aggregate the results at the cross-modal retrieval level. Finally, we prompt the VLM to generate the final response based on the co-modality retrieval results.
Experiments demonstrate that our method significantly outperforms pure-visionâ€“based RAG in visual document question answering tasks. The findings of this paper show that integrating co-modality information into the RAG framework in a unified manner is an effective approach to improving the performance of complex document visual question-answering (VQA) systems.
## 1Introduction
![Refer to caption](x1.png)Figure 1:Comparison among (a) textâ€“based RAG, (b) imageâ€“based RAG, and (c) co-modalityâ€“based RAG.
Large language models (LLMs) have received extensive attention in recent years> (Touvron etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib60)> ; Achiam etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib1)> ; Guo etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib21)> ; Yang etÂ al., [> 2025a
](https://arxiv.org/html/2509.02123v1#bib.bib69)> )
, but they have inherent limitations in handling out-of-domain knowledge> (Ji etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib27)> )
. To address this issue, RAG integrates external knowledge retrieval with the generation process> (Lewis etÂ al., [> 2020
](https://arxiv.org/html/2509.02123v1#bib.bib32)> ; Guu etÂ al., [> 2020
](https://arxiv.org/html/2509.02123v1#bib.bib22)> ; Karpukhin etÂ al., [> 2020
](https://arxiv.org/html/2509.02123v1#bib.bib28)> ; Chen etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib10)> )
. RAG achieves wide success in open-domain question answering, knowledge retrieval, and dialogue systems, and becomes an effective means of extending the knowledge boundaries of LLMs> (Ram etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib50)> ; Gao etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib18)> )
. However, most external data sources (e.g., documents) are essentially multimodal> (Jeong etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib26)> ; Faysse etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib17)> )
, often containing natural language text, formulas, tables, images, and complex layout structures. How to effectively leverage such multimodal information in question answering remains a challenging problem that is not fully solved.
One line of approaches is text-based RAG, which typically relies on layout parsing and text extraction> (Xu etÂ al., [> 2020
](https://arxiv.org/html/2509.02123v1#bib.bib67)> ; Dong etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib15)> ; Yang etÂ al., [> 2025b
](https://arxiv.org/html/2509.02123v1#bib.bib70)> ; Perez &amp; Vizcaino, [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib46)> )
. These methods first detect document layouts and then extract textual information for subsequent retrieval and generation. While stable at the semantic level, they struggle to handle content such as images and tables.
Recently, VLMs> (Ghosh etÂ al., [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib20)> ; Radford etÂ al., [> 2021
](https://arxiv.org/html/2509.02123v1#bib.bib49)> ; Alayrac etÂ al., [> 2022
](https://arxiv.org/html/2509.02123v1#bib.bib3)> )
enable RAG systems to process documents directly as images> (Faysse etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib17)> ; Yu etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib72)> ; Qi etÂ al., [> 2024a
](https://arxiv.org/html/2509.02123v1#bib.bib47)> ; Wang etÂ al., [> 2025b
](https://arxiv.org/html/2509.02123v1#bib.bib63)> )
, giving rise to vision-based RAG> (Huang etÂ al., [> 2022
](https://arxiv.org/html/2509.02123v1#bib.bib24)> ; Kim etÂ al., [> 2022
](https://arxiv.org/html/2509.02123v1#bib.bib30)> ; Yu etÂ al., [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib71)> )
. Specifically, these methods divide document pages into image segments and perform retrieval and reasoning through visual understanding models. Although they capture non-textual information, they often overlook the precise information carried by text, leading to performance bottlenecks.
To overcome these limitations, we propose a novel co-modalityâ€“based RAG framework, which unifies text and image modalities, as illustrated in Fig.[1](https://arxiv.org/html/2509.02123v1#S1.F1).
In this framework, we first parse documents to extract structured text and image segments. For a given query, we then perform retrieval in both text and visual spaces, ensuring that semantic matching from text and perceptual grounding from images are simultaneously leveraged. Finally, we feed the co-modality evidence into a VLM to integrate information and generate answers. In this way, our method not only handles cross-modal reasoning that single-modal RAG cannot cover but also achieves significant performance improvements on standard visual document question answering benchmarks.
The main contributions of this paper are summarized as follows:
* â€¢We propose a novel RAG framework that jointly leverages text and image modalities to enhance performance on complex visual document QA tasks.
* â€¢We design a co-modalityâ€“based retrieval mechanism that enables complementary use of text and image evidence during retrieval, leading to more comprehensive knowledge grounding.
* â€¢We conduct extensive experiments on document VQA benchmarks, and the results demonstrate that our approach significantly outperforms existing single-modal RAG methods in both accuracy and robustness.
## 2Related work
In this section, we discuss the recent studies related to multi-modal RAG (MMRAG). Specifically, we primarily focus on three folds: (1) knowledgeâ€“based MMRAG, (2) videoâ€“based MMRAG, and (3) documentâ€“based MMRAG.
Knowledgeâ€“based MMRAGrefers to retrieving knowledge (text or image modality) from external sources such as Wikipedia articles and websites to answer textual or visual queries> (Talmor etÂ al., [> 2021
](https://arxiv.org/html/2509.02123v1#bib.bib56)> ; Marino etÂ al., [> 2019
](https://arxiv.org/html/2509.02123v1#bib.bib41)> ; Chang etÂ al., [> 2022
](https://arxiv.org/html/2509.02123v1#bib.bib9)> ; Schwenk etÂ al., [> 2022
](https://arxiv.org/html/2509.02123v1#bib.bib54)> ; Mensink etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib43)> ; Chen etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib12)> ; Ma etÂ al., [> 2024a
](https://arxiv.org/html/2509.02123v1#bib.bib38)> ; Hu etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib23)> )
. Although the external knowledge database can enhance the system performance> (Caffagni etÂ al., [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib8)> )
, the key issue of knowledgeâ€“based MMRAG is the inconsistency between textual and visual queries as well as the external knowledge database> (Chen etÂ al., [> 2022
](https://arxiv.org/html/2509.02123v1#bib.bib11)> ; Lin etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib36)> ; Zhang etÂ al., [> 2024b
](https://arxiv.org/html/2509.02123v1#bib.bib74)> )
. To address this issue,> Lin &amp; Byrne (
[> 2022
](https://arxiv.org/html/2509.02123v1#bib.bib35)> )
adopted multiple algorithms, including object detection, image captioning, and OCR, to transform visual queries into language space, and proposed a joint training scheme to optimize retrieval and generation simultaneously. A similar training strategy was also used by> (Adjali etÂ al., [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib2)> )
. Also,> Yan &amp; Xie (
[> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib68)> )
used a consistent modality for both retrieval and generation: visual modality for retrieval (visual queries and Wikipedia article pages) and textual modality for generation (textual queries and wiki articles). A similar strategy can also be found in RORA> (Qi etÂ al., [> 2024a
](https://arxiv.org/html/2509.02123v1#bib.bib47)> )
. In addition,> Tian etÂ al. (
[> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib58)> )
proposed cross-source knowledge reconciliation for MMRAG, which could address the inconsistency between textual and visual external knowledge.
Videoâ€“based MMRAGrefers to retrieving videos from the corpus to help answer given queries> (CabaÂ Heilbron etÂ al., [> 2015
](https://arxiv.org/html/2509.02123v1#bib.bib7)> ; Xu etÂ al., [> 2016
](https://arxiv.org/html/2509.02123v1#bib.bib66)> ; AnneÂ Hendricks etÂ al., [> 2017
](https://arxiv.org/html/2509.02123v1#bib.bib4)> ; Wang etÂ al., [> 2019
](https://arxiv.org/html/2509.02123v1#bib.bib64)> ; Kriz etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib31)> ; Wan etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib61)> )
. Since encoding videos may incur high computational costs, a few studies pre-processed videos using VLMs and converted videos to textual modality> (Zhang etÂ al., [> 2024a
](https://arxiv.org/html/2509.02123v1#bib.bib73)> ; Arefeen etÂ al., [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib5)> ; Ma etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib40)> )
. For example,> Zhang etÂ al. (
[> 2024a
](https://arxiv.org/html/2509.02123v1#bib.bib73)> )
first detected key information in videos such as human faces, based on which a VLM was prompted to generate scene captions for video frames. Consequently, the video modality can be converted to a text modality, which can significantly reduce computational costs and facilitate retrieval and generation. Furthermore, a few studies> (Luo etÂ al., [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib37)> ; Jeong etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib26)> ; Reddy etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib51)> )
processed videos by selecting or clustering representative frames, facilitating video retrieval and final generation.
Documentâ€“based MMRAGrefers to retrieving a few document pages from one or multiple documents to help generate answers for given questions> (Methani etÂ al., [> 2020
](https://arxiv.org/html/2509.02123v1#bib.bib44)> ; Masry etÂ al., [> 2022
](https://arxiv.org/html/2509.02123v1#bib.bib42)> ; Tanaka etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib57)> ; Tito etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib59)> ; Ma etÂ al., [> 2024b
](https://arxiv.org/html/2509.02123v1#bib.bib39)> ; Li etÂ al., [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib33)> ; Hui etÂ al., [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib25)> ; Qi etÂ al., [> 2024b
](https://arxiv.org/html/2509.02123v1#bib.bib48)> ; Cho etÂ al., [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib13)> ; Wang etÂ al., [> 2025a
](https://arxiv.org/html/2509.02123v1#bib.bib62)> ; Li etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib34)> ; Wasserman etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib65)> ; Faysse etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib17)> )
. Traditionally, documents were parsed using detection models> (Ge etÂ al., [> 2021
](https://arxiv.org/html/2509.02123v1#bib.bib19)> )
and OCR engines> (Smith, [> 2007
](https://arxiv.org/html/2509.02123v1#bib.bib55)> )
, and the extracted components (e.g., text) were input to LLMs to generate answers> (Riedler &amp; Langer, [> 2024
](https://arxiv.org/html/2509.02123v1#bib.bib52)> ; Faysse etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib17)> )
. With the proliferation of VLMs, a few studies> (Faysse etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib17)> ; Yu etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib72)> ; Wang etÂ al., [> 2025b
](https://arxiv.org/html/2509.02123v1#bib.bib63)> )
processed document pages as images directly. Specifically, they used VLMs to encode queries and document pages as text and images, respectively, based on which the similarity scores between queries and visual document pages can be calculated. This method paves the way for documentâ€“based MMRAG, as it does not need to parse documents and can retrieve document pages directly. However, this method overlooks text modality in documents, which may degrade the performance of RAG systems.
## 3Methodology
In this section, we introduce the problem definition of documentâ€“based VQA and our proposed CMRAG framework in detail.
### 3.1Problem definition
In a documentâ€“based VQA system, a collection of visual documents provides evidence for answering given queries. Formally, all pages are treated as candidate evidenceð’Ÿ={p1,p2,â€¦,pM}\\mathcal{D}=\\{p\_{1},p\_{2},\\dots,p\_{M}\\}, whereMMdenotes the total number of pages across the document collection. For a given queryqq, the retrieverâ„›\\mathcal{R}identifies the top-kkrelevant pagesPk={p1,p2,â€¦,pk}P\_{k}=\\{p\_{1},p\_{2},\\dots,p\_{k}\\}according to a similarity scoresâ€‹(q,pi)s(q,p\_{i})(âˆ€piâˆˆPk\\forall p\_{i}\\in P\_{k}). Once the top-kkevidence pages are retrieved, they are combined with the query into a promptð’«â€‹(q,Pk)\\mathcal{P}(q,P\_{k}), which is then passed to a generator modelð’¢\\mathcal{G}, often instantiated as a visionâ€“language model (VLM). The generator integrates the multimodal evidence and produces the final answera^=ð’¢â€‹(ð’«â€‹(q,Pk))\\hat{a}=\\mathcal{G}(\\mathcal{P}(q,P\_{k})).
### 3.2VLMâ€“based document parsing
Advanced visionâ€“language models (VLMs) can directly parse document pages, simultaneously detecting layouts, extracting text, and identifying localized visual regions. For each pagepip\_{i}, the parsing process produces three complementary components:the entire imageIiI\_{i}, which preserves global layout and contextual relationships;the parsed sub-imagesSâ€‹IiSI\_{i}, which capture localized elements such as figures, tables, and diagrams; andthe parsed textTiT\_{i}, which provides precise semantic grounding for fine-grained reasoning. Thus, each page can be represented aspi={Ii,Sâ€‹Ii,Ti}p\_{i}=\\{I\_{i},SI\_{i},T\_{i}\\}, ensuring that both global structure and detailed content are available for retrieval and downstream answer generation.
### 3.3Co-modality late interaction
Late interaction.Given a queryqqand document page imagepp, the encoderâ„°\\mathcal{E}project them into a high-dimension spaceâ„D\\mathbb{R}^{D}, i.e.,ðª=â„°â€‹(q)âˆˆâ„NqÃ—D\\mathbf{q}=\\mathcal{E}(q)\\in\\mathbb{R}^{N\_{q}\\times D}andð©=â„°â€‹(p)âˆˆâ„NpÃ—D\\mathbf{p}=\\mathcal{E}(p)\\in\\mathbb{R}^{N\_{p}\\times D}, whereNqN\_{q}andNpN\_{p}denote the number of vectors in the query and document embeddings, respectively. Hence, the late interaction (LI)> (Khattab &amp; Zaharia, [> 2020
](https://arxiv.org/html/2509.02123v1#bib.bib29)> ; Santhanam etÂ al., [> 2022
](https://arxiv.org/html/2509.02123v1#bib.bib53)> ; ClaviÃ©, [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib14)> ; Faysse etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib17)> )
can be calculated as follows:
|LIâ€‹(q,p)=âˆ‘i=1Nqmaxjâˆˆ{1,2,â€¦,Np}â¡âŸ¨ðªi,ð©jâŸ©,\\textbf{LI}(q,p)=\\sum\_{i=1}^{N\_{q}}\\max\_{j\\in\\{1,2,...,N\_{p}\\}}\\langle\\mathbf{q}^{i},\\mathbf{p}^{j}\\rangle,||(1)|
whereâŸ¨â‹…,â‹…âŸ©\\langle\\cdot,\\cdot\\rangledenote dot production, andðªi\\mathbf{q}^{i}andð©j\\mathbf{p}^{j}areithi^{\\text{th}}andjthj^{\\text{th}}vector in the query and page embeddings, respectively. Compared with the mean pooling method> (Karpukhin etÂ al., [> 2020
](https://arxiv.org/html/2509.02123v1#bib.bib28)> ; Yu etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib72)> )
, LI can consider token-level similarity for detailed matching> (Wan etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib61)> )
.
Co-modality late interaction.Furthermore, inspired by CLAMR> (Wan etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib61)> )
, we consider retrieving evidence based on similarity between the queryqqand the co-modality document contentIIandTT. Formally, we encode the entire page imageIIand extracted textTTasð©I=â„°â€‹(I)\\mathbf{p}^{I}=\\mathcal{E}(I)andð©T=â„°â€‹(T)\\mathbf{p}^{T}=\\mathcal{E}(T), respectively. Therefore, we adopt co-modality late interaction (CoLI) to calculate the similarity, as follows:
|CoLIâ€‹(q,p)=maxmâˆˆ{I,T}â€‹âˆ‘i=1Nqmaxjâˆˆ{1,2,â€¦,Np}â¡âŸ¨ðªi,ð©m,jâŸ©,\\textbf{CoLI}(q,p)=\\max\_{m\\in\\{I,T\\}}\\sum\_{i=1}^{N\_{q}}\\max\_{j\\in\\{1,2,...,N\_{p}\\}}\\langle\\mathbf{q}^{i},\\mathbf{p}^{m,j}\\rangle,||(2)|
whereð©m,j\\mathbf{p}^{m,j}isjthj^{\\text{th}}vector in themthm^{\\text{th}}modality document embedding.
Contrastive learning.Let{(qi,pi)}i=1b\\{(q\_{i},p\_{i})\\}\_{i=1}^{b}denote a data batch where each queryqiq\_{i}(âˆ€iâˆˆ{1,2,â€¦,b}\\forall i\\in\\{1,2,...,b\\}) corresponds to a document pagepip\_{i}andbbdenotes the batch size. The loss> (Oord etÂ al., [> 2018
](https://arxiv.org/html/2509.02123v1#bib.bib45)> )
can be represented as follows:
|â„’=âˆ’1bâ€‹âˆ‘i=1blogâ¡expâ¡(si,i/Ï„)âˆ‘j=1bexpâ¡(si,j/Ï„),\\mathcal{L}=-\\frac{1}{b}\\sum\_{i=1}^{b}\\log\\frac{\\exp(s\_{i,i}/\\tau)}{\\sum\_{j=1}^{b}\\exp(s\_{i,j}/\\tau)},||(3)|
wheresi,js\_{i,j}(âˆ€i,jâˆˆ{1,2,â€¦,b}\\forall i,j\\in\\{1,2,...,b\\}) denote the similarity score betweenithi^{\\text{th}}query andjthj^{\\text{th}}document page andÏ„\\tauthe temperature.
### 3.4Co-modalityâ€“based generation
Recent advances in VLMs> (Bai etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib6)> ; Du etÂ al., [> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib16)> )
enable them to directly process naive-resolution page images without the need for splitting each page into smaller sub-images, which often risks losing global context. In addition, modern VLMs can accept multiple images simultaneously, meaning that all retrieved pages can be jointly fed into the model. This design facilitates cross-page reasoning, where evidence from different parts of the document can be integrated to produce more accurate and comprehensive answers. Therefore, we combine the retrieved images and text into the prompt and leverage the VLM to generate the final answer based on the co-modality input.
## 4Experiments
### 4.1Experiment setup
Datasets.We consider two datasets: MMLongBench> (Ma etÂ al., [> 2024b
](https://arxiv.org/html/2509.02123v1#bib.bib39)> )
and SlideVQA> (Tanaka etÂ al., [> 2023
](https://arxiv.org/html/2509.02123v1#bib.bib57)> )
. These datasets comprise various query types (multi-hop and single-hop) and answer types (multi-span and single-span), which can be used to evaluate the capacities of VLMs. The statistics of the datasets are summarized in Tab.[1](https://arxiv.org/html/2509.02123v1#S4.T1). We follow the implementation of> (Wang etÂ al., [> 2025a
](https://arxiv.org/html/2509.02123v1#bib.bib62)> )
and use refined queries of SlideVQA.
|Method|#Files|#Images|#QAs|#Images/Q|
MMLongBench|135|6,529|1,082|1.00|
SlideVQA|400|8,000|2,020|1.26|
Table 1:Statistics of evaluation datasets.
Baselines.To evaluate the proposed method, we first consider three evaluation scenarios: (1)Oracle (Entire images), (2)Oracle (Sub-images + parsed text), and (3)Oracle (Entire images + parsed text). We consider using the ground truth document pages, with the entire images, parsed sub-images plus text, and entire images plus parsed text as the input of the VLM, respectively.
LLM-as-a-Judge.The gold answers in the benchmark datasets consist of multiple spans or exhibit diverse valid expressions. Traditional evaluation metrics, such as Exact Match (EM) and token-level F1, struggle to capture the semantic equivalence between generated responses and these multi-span gold answers. As a result, these metrics may underestimate the actual performance of models. To address this limitation, we employ a large language model (LLM) as an automatic judge to assess whether a generated answer is correct, providing a more flexible and reliable evaluation framework, enabling fairer comparisons across different models.
Implementation details.We implemented our CMRAG framework using Qwen2.5-VL-7B-Instruct> Bai etÂ al. (
[> 2025
](https://arxiv.org/html/2509.02123v1#bib.bib6)> )
as the backbone VLM for parsing documents and answering queries. In addition, we adopted Qwen2.5-7B-Instruct> (Yang etÂ al., [> 2025a
](https://arxiv.org/html/2509.02123v1#bib.bib69)> )
as the judge. To ensure deterministic outputs and eliminate variability due to random sampling, we set the temperature to 0.0 during decoding. We present the prompts used in this study in the appendix.
|Method|MMLongBench|SlideVQA|
Oracle (Entire images)|40.39|81.58|
Oracle (Sub-images + parsed text)|29.11|58.76|
Oracle (Entire images + parsed text)|46.86|81.68|
Table 2:Main results.Boldvalues represent the best scores.
### 4.2Main results
Tab.[2](https://arxiv.org/html/2509.02123v1#S4.T2)summarizes the performance of different oracle settings on MMLongBench and SlideVQA. The results reveal several key findings. First, incorporating parsed text consistently enhances generation accuracy, as shown by the improved scores of the oracle that integrates entire images with parsed text. For instance, on MMLongBench, the combined setting achieves the best score of 46.86, which significantly outperforms using only entire images (40.39). Similarly, on SlideVQA, the combination yields 81.68, slightly higher than the image-only oracle. Second, directly using sub-images with parsed text leads to degraded performance. This decline is likely due to incomplete visual information within sub-images, which may omit contextual elements crucial for accurate reasoning. On MMLongBench, the score drops to 29.11, while SlideVQA falls to 58.76â€”both substantially lower than their entire image counterparts. These results suggest that incomplete visual segmentation can hinder the integration of multimodal cues. Overall, the findings highlight that while parsed text provides a complementary signal that improves multimodal reasoning, extracting sub-images without preserving global context can be detrimental. The best results are achieved when both entire images and parsed text are jointly considered.
![Refer to caption](x2.png)Figure 2:Qualitative comparison among three baselines.
### 4.3Case study
To further illustrate the effects observed in Table[2](https://arxiv.org/html/2509.02123v1#S4.T2), we present a case study in Fig.[2](https://arxiv.org/html/2509.02123v1#S4.F2). This example highlights the strengths and limitations of different Oracle settings in handling complex queries. First, when relying solely on entire images, the VLM misinterprets the numbers, producing an incorrect output of 36 instead of 62. This demonstrates that VLMs may struggle to accurately ground numeric reasoning based on purely visual inputs. Similarly, when only sub-images plus parsed text are used, the model fails to capture the complete context, yielding partial and incomplete answers. The problem arises because the VLM failed to accurately parse the sub-image but extracted textual numbers only. However, incorporating entire images together with parsed text enables the model to generate the correct multi-span answer, as the parsed text provides a reliable textual grounding that compensates for the VLMâ€™s difficulty in interpreting fine-grained visual details. This shows that parsed text can serve as an essential complement, ensuring accurate reasoning across multiple evidence spans.
Another challenge revealed by this case is that the query requires multi-span answers, i.e., identifying two distinct values from different textual locations. Conventional automatic evaluation metrics, such as exact match (EM) or F1 score, cannot adequately capture the correctness of such answers, which further validates the necessity of LLM-as-a-Judge.
## 5Conclusion
In this paper, we introduced a co-modalityâ€“based RAG framework that unifies text and image modalities to address the limitations of existing single-modal approaches in visual document question answering. By jointly leveraging parsed text and entire images, our method enables complementary retrieval and reasoning, where textual evidence provides precise grounding and visual context preserves global completeness. Experimental results on MMLongBench and SlideVQA demonstrate that the proposed framework consistently outperforms single-modality baselines, and case studies further highlight its advantages in handling multi-span queries that traditional metrics cannot fully capture. Overall, this work establishes co-modality RAG as an effective and robust paradigm for document VQA, paving the way for future research on integrating structured parsing, fine-grained retrieval, and LLM-based evaluation.
## References
* Achiam etÂ al. (2023)Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
FlorenciaÂ Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, etÂ al.Gpt-4 technical report.*arXiv preprint arXiv:2303.08774*, 2023.
* Adjali etÂ al. (2024)Omar Adjali, Olivier Ferret, Sahar Ghannay, and HervÃ© LeÂ Borgne.Multi-level information retrieval augmented generation for
knowledge-based visual question answering.In*Proceedings of the 2024 Conference on Empirical Methods in
Natural Language Processing*, pp.Â  16499â€“16513. Association for
Computational Linguistics, 2024.
* Alayrac etÂ al. (2022)Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
etÂ al.Flamingo: a visual language model for few-shot learning.*Advances in neural information processing systems*,
35:23716â€“23736, 2022.
* AnneÂ Hendricks etÂ al. (2017)Lisa AnneÂ Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,
and Bryan Russell.Localizing moments in video with natural language.In*Proceedings of the IEEE international conference on computer
vision*, pp.Â  5803â€“5812, 2017.
* Arefeen etÂ al. (2024)MdÂ Adnan Arefeen, Biplob Debnath, MdÂ YusufÂ Sarwar Uddin, and Srimat Chakradhar.irag: Advancing rag for videos with an incremental approach.In*Proceedings of the 33rd ACM International Conference on
Information and Knowledge Management*, pp.Â  4341â€“4348, 2024.
* Bai etÂ al. (2025)Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai
Dang, Peng Wang, Shijie Wang, Jun Tang, etÂ al.Qwen2. 5-vl technical report.*arXiv preprint arXiv:2502.13923*, 2025.
* CabaÂ Heilbron etÂ al. (2015)Fabian CabaÂ Heilbron, Victor Escorcia, Bernard Ghanem, and Juan CarlosÂ Niebles.Activitynet: A large-scale video benchmark for human activity
understanding.In*Proceedings of the ieee conference on computer vision and
pattern recognition*, pp.Â  961â€“970, 2015.
* Caffagni etÂ al. (2024)Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella
Cornia, Lorenzo Baraldi, and Rita Cucchiara.Wiki-llava: Hierarchical retrieval-augmented generation for
multimodal llms.In*Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition*, pp.Â  1818â€“1826, 2024.
* Chang etÂ al. (2022)Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and
Yonatan Bisk.Webqa: Multihop and multimodal qa.In*Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition*, pp.Â  16495â€“16504, 2022.
* Chen etÂ al. (2025)Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Deguo Xia, and Jizhou Huang.Pairs: Parametric-verified adaptive information retrieval and
selection for efficient rag.*arXiv preprint arXiv:2508.04057*, 2025.
* Chen etÂ al. (2022)Wenhu Chen, Hexiang Hu, XiÂ Chen, Pat Verga, and William Cohen.Murag: Multimodal retrieval-augmented generator for open question
answering over images and text.In*Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing*, pp.Â  5558â€“5570, 2022.
* Chen etÂ al. (2023)Yang Chen, Hexiang Hu, YiÂ Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter,
and Ming-Wei Chang.Can pre-trained vision and language models answer visual
information-seeking questions?*arXiv preprint arXiv:2302.11713*, 2023.
* Cho etÂ al. (2024)Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal.M3docrag: Multi-modal retrieval is what you need for multi-page
multi-document understanding.*arXiv preprint arXiv:2411.04952*, 2024.
* ClaviÃ© (2025)Benjamin ClaviÃ©.Jacolbertv2. 5: Optimising multi-vector retrievers to create
state-of-the-art japanese retrievers with constrained resources.*Journal of Natural Language Processing*, 32(1):176â€“218, 2025.
* Dong etÂ al. (2025)Yuyang Dong, Nobuhiro Ueda, KrisztiÄ‚Ä„n Boros, Daiki Ito, Takuya
Sera, and Masafumi Oyamada.Scan: Semantic document layout analysis for textual and visual
retrieval-augmented generation.*arXiv preprint arXiv:2505.14381*, 2025.
* Du etÂ al. (2025)Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin
Zhang, Chenzhuang Du, Chu Wei, etÂ al.Kimi-vl technical report.*arXiv preprint arXiv:2504.07491*, 2025.
* Faysse etÂ al. (2025)Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, CELINE
HUDELOT, and Pierre Colombo.Colpali: Efficient document retrieval with vision language models.In*The Thirteenth International Conference on Learning
Representations*, 2025.
* Gao etÂ al. (2023)Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin
Dai, Jiawei Sun, Haofen Wang, and Haofen Wang.Retrieval-augmented generation for large language models: A survey.*arXiv preprint arXiv:2312.10997*, 2(1), 2023.
* Ge etÂ al. (2021)Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun.Yolox: Exceeding yolo series in 2021.*arXiv preprint arXiv:2107.08430*, 2021.
* Ghosh etÂ al. (2024)Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, and Aman Chadha.Exploring the frontier of vision-language models: A survey of current
methodologies and future directions.*arXiv preprint arXiv:2404.07214*, 2024.
* Guo etÂ al. (2025)Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu,
Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, etÂ al.Deepseek-r1: Incentivizing reasoning capability in llms via
reinforcement learning.*arXiv preprint arXiv:2501.12948*, 2025.
* Guu etÂ al. (2020)Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.Retrieval augmented language model pre-training.In*International conference on machine learning*, pp.Â 3929â€“3938. PMLR, 2020.
* Hu etÂ al. (2025)Wenbo Hu, Jia-Chen Gu, Zi-Yi Dou, Mohsen Fayyaz, Pan Lu, Kai-Wei Chang, and
Nanyun Peng.Mrag-bench: Vision-centric evaluation for retrieval-augmented
multimodal models.In*The Thirteenth International Conference on Learning
Representations*, 2025.
* Huang etÂ al. (2022)Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei.Layoutlmv3: Pre-training for document ai with unified text and image
masking.In*Proceedings of the 30th ACM international conference on
multimedia*, pp.Â  4083â€“4091, 2022.
* Hui etÂ al. (2024)Yulong Hui, Yao Lu, and Huanchen Zhang.Uda: A benchmark suite for retrieval augmented generation in
real-world document analysis.*Advances in Neural Information Processing Systems*,
37:67200â€“67217, 2024.
* Jeong etÂ al. (2025)Soyeong Jeong, Kangsan Kim, Jinheon Baek, and SungÂ Ju Hwang.Videorag: Retrieval-augmented generation over video corpus.In*Findings of the Association for Computational Linguistics:
ACL 2025*, pp.Â  21278â€“21298, 2025.
* Ji etÂ al. (2023)Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
YeÂ Jin Bang, Andrea Madotto, and Pascale Fung.Survey of hallucination in natural language generation.*ACM computing surveys*, 55(12):1â€“38, 2023.
* Karpukhin etÂ al. (2020)Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickÂ SH Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih.Dense passage retrieval for open-domain question answering.In*EMNLP (1)*, pp.Â  6769â€“6781, 2020.
* Khattab &amp; Zaharia (2020)Omar Khattab and Matei Zaharia.Colbert: Efficient and effective passage search via contextualized
late interaction over bert.In*Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval*, pp.Â  39â€“48, 2020.
* Kim etÂ al. (2022)Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong
Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.Ocr-free document understanding transformer.In*European Conference on Computer Vision*, pp.Â  498â€“517.
Springer, 2022.
* Kriz etÂ al. (2025)Reno Kriz, Kate Sanders, David Etter, Kenton Murray, Cameron Carpenter, Hannah
Recknor, Jimena Guallar-Blasco, Alexander Martin, Eugene Yang, and Benjamin
VanÂ Durme.Multivent 2.0: A massive multilingual benchmark for event-centric
video retrieval.In*Proceedings of the Computer Vision and Pattern Recognition
Conference*, pp.Â  24149â€“24158, 2025.
* Lewis etÂ al. (2020)Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim
RocktÃ¤schel, etÂ al.Retrieval-augmented generation for knowledge-intensive nlp tasks.*Advances in neural information processing systems*,
33:9459â€“9474, 2020.
* Li etÂ al. (2024)Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and
QiÂ Liu.Multimodal arxiv: A dataset for improving scientific comprehension of
large vision-language models.In*Proceedings of the 62nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)*, pp.Â  14369â€“14387,
2024.
* Li etÂ al. (2025)Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui
Wang, Hai-Tao Zheng, Fei Huang, Jingren Zhou, etÂ al.Benchmarking multimodal retrieval augmented generation with dynamic
vqa dataset and self-adaptive planning agent.In*The Thirteenth International Conference on Learning
Representations*, 2025.
* Lin &amp; Byrne (2022)Weizhe Lin and Bill Byrne.Retrieval augmented visual question answering with outside knowledge.In*Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing*, pp.Â  11238â€“11254, 2022.
* Lin etÂ al. (2023)Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne.Fine-grained late-interaction multi-modal retrieval for retrieval
augmented visual question answering.*Advances in Neural Information Processing Systems*,
36:22820â€“22840, 2023.
* Luo etÂ al. (2024)Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi
Ji, Fei Chao, Jiebo Luo, and Rongrong Ji.Video-rag: Visually-aligned retrieval-augmented long video
comprehension.*arXiv preprint arXiv:2411.13093*, 2024.
* Ma etÂ al. (2024a)Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin.Unifying multimodal retrieval via document screenshot embedding.In*Proceedings of the 2024 Conference on Empirical Methods in
Natural Language Processing*, pp.Â  6492â€“6505, 2024a.
* Ma etÂ al. (2024b)Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan
Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, etÂ al.Mmlongbench-doc: Benchmarking long-context document understanding
with visualizations.*Advances in Neural Information Processing Systems*,
37:95963â€“96010, 2024b.
* Ma etÂ al. (2025)Ziyu Ma, Chenhui Gou, Hengcan Shi, Bin Sun, Shutao Li, Hamid Rezatofighi, and
Jianfei Cai.Drvideo: Document retrieval based long video understanding.In*Proceedings of the Computer Vision and Pattern Recognition
Conference*, pp.Â  18936â€“18946, 2025.
* Marino etÂ al. (2019)Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.Ok-vqa: A visual question answering benchmark requiring external
knowledge.In*Proceedings of the IEEE/cvf conference on computer vision
and pattern recognition*, pp.Â  3195â€“3204, 2019.
* Masry etÂ al. (2022)Ahmed Masry, XuanÂ Long Do, JiaÂ Qing Tan, Shafiq Joty, and Enamul Hoque.Chartqa: A benchmark for question answering about charts with visual
and logical reasoning.In*Findings of the Association for Computational Linguistics:
ACL 2022*, pp.Â  2263â€“2279, 2022.
* Mensink etÂ al. (2023)Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar,
Howard Zhou, Fei Sha, AndrÃ© Araujo, and Vittorio Ferrari.Encyclopedic vqa: Visual questions about detailed properties of
fine-grained categories.In*Proceedings of the IEEE/CVF International Conference on
Computer Vision*, pp.Â  3113â€“3124, 2023.
* Methani etÂ al. (2020)Nitesh Methani, Pritha Ganguly, MiteshÂ M Khapra, and Pratyush Kumar.Plotqa: Reasoning over scientific plots.In*Proceedings of the ieee/cvf winter conference on
applications of computer vision*, pp.Â  1527â€“1536, 2020.
* Oord etÂ al. (2018)Aaron vanÂ den Oord, Yazhe Li, and Oriol Vinyals.Representation learning with contrastive predictive coding.*arXiv preprint arXiv:1807.03748*, 2018.
* Perez &amp; Vizcaino (2024)Arnau Perez and Xavier Vizcaino.Advanced ingestion process powered by llm parsing for rag system.*arXiv preprint arXiv:2412.15262*, 2024.
* Qi etÂ al. (2024a)Jingyuan Qi, Zhiyang Xu, Rulin Shao, Yang Chen, Jin Di, YuÂ Cheng, Qifan Wang,
and Lifu Huang.Rora-vlm: Robust retrieval-augmented vision language models.*arXiv preprint arXiv:2410.08876*, 2024a.
* Qi etÂ al. (2024b)Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, and Wei Xu.Long2rag: Evaluating long-context &amp; long-form retrieval-augmented
generation with key point recall.In*Findings of the Association for Computational Linguistics:
EMNLP 2024*, pp.Â  4852â€“4872, 2024b.
* Radford etÂ al. (2021)Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
etÂ al.Learning transferable visual models from natural language
supervision.In*International conference on machine learning*, pp.Â 8748â€“8763. PmLR, 2021.
* Ram etÂ al. (2023)Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin
Leyton-Brown, and Yoav Shoham.In-context retrieval-augmented language models.*Transactions of the Association for Computational Linguistics*,
11:1316â€“1331, 2023.
* Reddy etÂ al. (2025)Arun Reddy, Alexander Martin, Eugene Yang, Andrew Yates, Kate Sanders, Kenton
Murray, Reno Kriz, CelsoÂ M deÂ Melo, Benjamin VanÂ Durme, and Rama Chellappa.Video-colbert: Contextualized late interaction for text-to-video
retrieval.In*Proceedings of the Computer Vision and Pattern Recognition
Conference*, pp.Â  19691â€“19701, 2025.
* Riedler &amp; Langer (2024)Monica Riedler and Stefan Langer.Beyond text: Optimizing rag with multimodal inputs for industrial
applications.*arXiv preprint arXiv:2410.21943*, 2024.
* Santhanam etÂ al. (2022)Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei
Zaharia.Colbertv2: Effective and efficient retrieval via lightweight late
interaction.In*Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies*, pp.Â  3715â€“3734, 2022.
* Schwenk etÂ al. (2022)Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and
Roozbeh Mottaghi.A-okvqa: A benchmark for visual question answering using world
knowledge.In*European conference on computer vision*, pp.Â  146â€“162.
Springer, 2022.
* Smith (2007)Ray Smith.An overview of the tesseract ocr engine.In*Ninth international conference on document analysis and
recognition (ICDAR 2007)*, volumeÂ 2, pp.Â  629â€“633. IEEE, 2007.
* Talmor etÂ al. (2021)Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai,
Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant.Multimodalqa: complex question answering over text, tables and
images.In*International Conference on Learning Representations*, 2021.
* Tanaka etÂ al. (2023)Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and
Kuniko Saito.Slidevqa: A dataset for document visual question answering on
multiple images.In*Proceedings of the AAAI Conference on Artificial
Intelligence*, volumeÂ 37, pp.Â  13636â€“13645, 2023.
* Tian etÂ al. (2025)Yang Tian, Fan Liu, Jingyuan Zhang, V.Â W., Yupeng Hu, and Liqiang Nie.CoRe-MMRAG: Cross-source knowledge reconciliation for
multimodal RAG.In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and MohammadÂ Taher
Pilehvar (eds.),*Proceedings of the 63rd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)*, pp.Â 32967â€“32982, Vienna, Austria, July 2025. Association for Computational
Linguistics.doi:10.18653/v1/2025.acl-long.1583.URL[https://aclanthology.org/2025.acl-long.1583/](https://aclanthology.org/2025.acl-long.1583/).
* Tito etÂ al. (2023)RubÃ¨n Tito, Dimosthenis Karatzas, and Ernest Valveny.Hierarchical multimodal transformers for multipage docvqa.*Pattern Recognition*, 144:109834, 2023.
* Touvron etÂ al. (2023)Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric
Hambro, Faisal Azhar, etÂ al.Llama: Open and efficient foundation language models.*arXiv preprint arXiv:2302.13971*, 2023.
* Wan etÂ al. (2025)David Wan, Han Wang, Elias Stengel-Eskin, Jaemin Cho, and Mohit Bansal.Clamr: Contextualized late-interaction for multimodal content
retrieval.*arXiv preprint arXiv:2506.06144*, 2025.
* Wang etÂ al. (2025a)Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, and
Feng Zhao.Vidorag: Visual document retrieval-augmented generation via dynamic
iterative reasoning agents.*arXiv preprint arXiv:2502.18017*, 2025a.
* Wang etÂ al. (2025b)Qiuchen Wang, Ruixue Ding, YuÂ Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun
Xie, Fei Huang, and Feng Zhao.Vrag-rl: Empower vision-perception-based rag for visually rich
information understanding via iterative reasoning with reinforcement
learning.*arXiv preprint arXiv:2505.22019*, 2025b.
* Wang etÂ al. (2019)Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and WilliamÂ Yang
Wang.Vatex: A large-scale, high-quality multilingual dataset for
video-and-language research.In*Proceedings of the IEEE/CVF international conference on
computer vision*, pp.Â  4581â€“4591, 2019.
* Wasserman etÂ al. (2025)Navve Wasserman, Roi Pony, Oshri Naparstek, AdiÂ Raz Goldfarb, Eli Schwartz, Udi
Barzelay, and Leonid Karlinsky.Real-mm-rag: A real-world multi-modal retrieval benchmark.*arXiv preprint arXiv:2502.12342*, 2025.
* Xu etÂ al. (2016)Jun Xu, Tao Mei, Ting Yao, and Yong Rui.Msr-vtt: A large video description dataset for bridging video and
language.In*Proceedings of the IEEE conference on computer vision and
pattern recognition*, pp.Â  5288â€“5296, 2016.
* Xu etÂ al. (2020)Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou.Layoutlm: Pre-training of text and layout for document image
understanding.In*Proceedings of the 26th ACM SIGKDD international conference
on knowledge discovery &amp; data mining*, pp.Â  1192â€“1200, 2020.
* Yan &amp; Xie (2024)Yibin Yan and Weidi Xie.Echosight: Advancing visual-language models with wiki knowledge.In*Findings of the Association for Computational Linguistics:
EMNLP 2024*, pp.Â  1538â€“1551, 2024.
* Yang etÂ al. (2025a)AnÂ Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, BoÂ Zheng, Bowen
Yu, Chang Gao, Chengen Huang, Chenxu Lv, etÂ al.Qwen3 technical report.*arXiv preprint arXiv:2505.09388*, 2025a.
* Yang etÂ al. (2025b)Jeff Yang, Duy-Khanh Vu, Minh-Tien Nguyen, Xuan-Quang Nguyen, Linh Nguyen, and
Hung Le.Superrag: Beyond rag with layout-aware graph modeling.*arXiv preprint arXiv:2503.04790*, 2025b.
* Yu etÂ al. (2024)Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu,
Shuo Wang, XuÂ Han, Zhiyuan Liu, etÂ al.Visrag: Vision-based retrieval-augmented generation on multi-modality
documents.*arXiv preprint arXiv:2410.10594*, 2024.
* Yu etÂ al. (2025)Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu,
Shuo Wang, XuÂ Han, Zhiyuan Liu, etÂ al.Visrag: Vision-based retrieval-augmented generation on multi-modality
documents.In*The Thirteenth International Conference on Learning
Representations*, 2025.
* Zhang etÂ al. (2024a)LuÂ Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, and Kyusong Lee.Omagent: A multi-modal agent framework for complex video
understanding with task divide-and-conquer.In*Proceedings of the 2024 Conference on Empirical Methods in
Natural Language Processing*, pp.Â  10031â€“10045, 2024a.
* Zhang etÂ al. (2024b)Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfeng Yuan,
Bing Li, Junfu Pu, Yuxuan Zhao, Zehua Xie, etÂ al.mr2ag: Multimodal retrieval-reflection-augmented generation
for knowledge-based vqa.*arXiv preprint arXiv:2411.15041*, 2024b.
## Appendix APrompt template
![Refer to caption](x3.png)Figure 3:Prompt templates for (a) parsing images, (b) generating answers based on entire images, (c) generating answers based on sub-images and text, generating answers based on entire images and text, and (e) judging generated answers. The first template can be found at[https://github.com/QwenLM/Qwen2.5VL/blob/main/cookbooks/document\_parsing.ipynb](https://github.com/QwenLM/Qwen2.5VL/blob/main/cookbooks/document_parsing.ipynb)and the rest can be referred to> (Wang etÂ al., [> 2025b
](https://arxiv.org/html/2509.02123v1#bib.bib63)> )
.
