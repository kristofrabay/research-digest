# 

**URL:** https://arxiv.org/pdf/2502.11116
**Published:** None

---

## Summary

This page describes **Gumbel Reranking (G-Rerank)**, an end-to-end training framework designed to optimize **rerankers** within **Retrieval-Augmented Generation (RAG) architectures**.

Key aspects covered in relation to your query:

*   **Rerankers:** The core focus is on improving reranker optimization, especially when labeled query-document pairs are scarce. G-Rerank reframes reranking as learning an optimal document-wise **attention mask**.
*   **RAG Architectures:** The method is specifically designed for RAG systems, aiming to minimize the final language modeling loss, thus aligning training and inference objectives.
*   **RAG Alternatives/Hybrid Search/Chunking Strategies:** These specific topics are **not** directly discussed in detail. The paper focuses on the reranking component, assuming documents have already been retrieved.
*   **Embeddings (new efficient models):** The paper mentions that retrieval models often leverage dense vectors and transformer architectures, but it does not introduce or detail new embedding models.
*   **Differentiable Optimization:** It uses the **Gumbel Trick** and **Relaxed Top-k sampling** to create a Differentiable Masked Attention (DMA) mechanism, allowing the reranker to be optimized end-to-end using backpropagation based on the language model loss.
*   **Performance:** Experiments show G-Rerank consistently improves performance over existing LLM-supervised fine-tuning methods, particularly excelling in identifying **indirectly relevant documents** in multi-hop QA tasks (e.g., 10.4% improvement in Recall@5 on HotpotQA).

In summary, the page details a novel training method for **rerankers** within **RAG architectures** that uses differentiable masked attention to optimize selection based on final generation loss.

The provided text discusses various aspects of **Retrieval-Augmented Generation (RAG)**, particularly focusing on **reranker training strategies** and **multi-hop Question Answering (QA)**.

Here is a summary of the topics mentioned that align with your query:

*   **RAG Architectures/Concepts:** The text extensively covers **Retrieval-Augmented Generation (RAG)**, mentioning specific architectures like **Fusion-in-Decoder (FiD)** and its variants (KG-FiD, FiDO, FiD-Light, RFiD, MG-FiD). It also discusses **multi-hop QA** which relies on RAG principles.
*   **Rerankers:** There is a significant focus on **reranker training methods** supervised by Large Language Models (LLMs), including **Attention Distillation (ADist)**, **Perplexity Distillation (PDist)**, **Leave-one-out Perplexity Distillation (LOOP)**, and **EMDR2**. The text also introduces a novel approach using **Learnable Sampling Weights** which bypasses a dedicated reranker by optimizing document weights directly via the language modeling loss.
*   **Embeddings/Retrieval:** While the text doesn't detail "new efficient models" for embeddings, it references several retrieval methods and models used in QA, such as **Dense Passage Retrieval (DPR)** (implied by context), **ColBERT**, and **Beam Dense Retrieval (BeamDR)**. It also mentions **hybrid search** implicitly through the discussion of different retrieval strategies.
*   **Chunking Strategies:** The text does not explicitly detail "chunking strategies," although the concept is fundamental to retrieval systems that process passages.
*   **Vector Databases:** **Vector databases** are not explicitly mentioned, but the context of dense retrieval implies their use.

In essence, the page provides a detailed overview of **reranker training** within **RAG architectures** (especially for multi-hop QA) and proposes a novel method to learn document relevance weights directly, touching upon concepts related to retrieval and ranking.

---

## Full Content

arXiv:2502.11116v2 [cs.CL] 8 Jun 2025
Gumbel Reranking: Differentiable End-to-End Reranker Optimization
Siyuan Huang1,3,
*
, Zhiyuan Ma
2
, Jintao Du
2
, Changhua Meng
2
, Weiqiang Wang
2
,
Jingwen Leng
4
, Minyi Guo
4
, Zhouhan Lin 1,
†
1LUMIA Lab, Shanghai Jiao Tong University, 2Tiansuan Lab, Ant Group Co., Ltd. 3SJTU Paris Elite Institute of Technology 4Shanghai Jiao Tong University
siyuan_huang_sjtu@outlook.com, lin.zhouhan@gmail.com
{mazhiyuan.mzy,lingke.djt,changhua.mch,weiqiang.wwq}@antgroup.com
Abstract
RAG systems rely on rerankers to identify relevant documents. However, fine-tuning these
models remains challenging due to the scarcity
of annotated query-document pairs. Existing distillation-based approaches suffer from
training-inference misalignment and fail to capture interdependencies among candidate documents. To overcome these limitations, we
reframe the reranking process as an attentionmask problem and propose Gumbel Reranking,
an end-to-end training framework for rerankers
aimed at minimizing the training-inference gap.
In our approach, reranker optimization is reformulated as learning a stochastic, documentwise Top-k attention mask using the Gumbel trick and relaxed top-k sampling. This
formulation enables end-to-end optimization
by minimizing the overall language loss. Experiments across various settings consistently
demonstrate performance gains, including a
10.4% improvement in recall on HotpotQA for
distinguishing indirectly relevant documents.
1 Introduction
Retrieval-Augmented Generation (RAG) has
shown great potential in natural language processing tasks (Lewis et al., 2020; Guu et al., 2020;
Izacard and Grave, 2021b; Borgeaud et al., 2022).
Despite their remarkable progress, retrieval models
in RAG systems—comprising both the retriever
and reranker—are typically trained on publicly
available datasets and often struggle with longtail queries requiring domain-specific knowledge.
As a result, they necessitate further fine-tuning for
specific downstream tasks (Glass et al., 2022; Shi
et al., 2024). A key challenge in this context is
the scarcity of labeled query-document pairs (Lee
et al., 2019; Sachan et al., 2023). Therefore, a critical research question is how to end-to-end optimize *Work done during an internship at Ant Group. †Corresponding Author. ‡Code is available at this url.
the retrieval models of RAG systems solely relying
on the system’s final language modeling loss.
Recent efforts to improve retriever or reranker
in RAG systems have explored distilling knowledge from LLMs into retrieval components. Techniques such as attention-based distillation (Izacard
and Grave, 2021a) and perplexity-based distillation (Sachan et al., 2021; Shi et al., 2024; Lin et al.,
2024; Izacard et al., 2023; Glass et al., 2022) have
yielded notable performance gains. However, these
methods still exhibit critical limitations. First, although these methods claim to be end-to-end optimized, they focus on LLM-supervised losses like
KL divergence (Izacard et al., 2023; Glass et al.,
2022) or marginalization (Sachan et al., 2021; Shi
et al., 2024; Lin et al., 2024), which do not directly
minimize the RAG system’s final generation loss,
leading to potential misalignment between training
and evaluation objectives. Additionally, attentionbased distillation suffers from the distraction problem, where accumulated attention scores do not
always reflect document relevance (Ke et al., 2024;
Li et al., 2024). While perplexity-based distillation methods evaluate each candidate document in
isolation, neglecting the interdependencies among
retrieved documents. This oversight is particularly
detrimental in multi-hop reasoning tasks requiring coherent logical relationships between documents (Trivedi et al., 2022; Ho et al., 2020).
In this work, we propose a novel end-to-end
strategy for training rerankers in RAG systems. We
reformulate the reranking task through the lens of
attention masks, where selecting the topk subset
from the retrieved candidate documents is viewed
as the application of a document-wise topk attention mask during attention computation. This
perspective leads to a shift in the problem formulation: instead of directly learning a more effective reranker, we focus on learning the optimal
document-wise topk attention mask.
However, since the hard attention mask is dis-
Retrieved Docs Top Ranking Docs Language Loss
Language Loss
1
0
1
Hard Attention Mask
Equivalent
Retrieved Docs Language Loss
0.87
0.15
0.98
Soft Attention Mask
Retrieved Docs
Objective: Reranker optimization for RAG in the absence of relevance-labeled documents
Differentiable Masked Attention
Teacher
LLM Reader
Relevance
Annotation
Reranker
Training Reranker
Ranking as Sampling Student
Training-Inference Objective Gap
Heavy Cost
Minimizing Training-Inference Gap
Reranker
Reranker
Reranker
Reader
Reader
Reader
subsection 3.2
subsection 3.3
Existing
Methods
Viewing Reranker as Attention Mask
A Typical Reranking Scenario
Human /
Commercial LLM
The gradient cannot be backpropagated to
Retrieved Docs
Gumbel Trick
The gradient cannot be backpropagated to
LLM-Supervised
Finetuning
(Using Parallel Context Pre-Filling)
Figure 1: Vanilla reranker training methods for RAG systems typically rely on supervised learning of querydocument pairs, which is limited by the scarcity of labeled data. To address this issue, existing methods leverage
various LLM-supervised losses. However, this can lead to potential gaps between training and inference. In contrast,
G-Rerank frames reranker training as learning a stochastic, document-wise top-k attention mask. This enables
end-to-end optimization by minimizing language loss, ensuring better alignment between training and inference.
crete, it can not be directly optimized via gradient
descent. To overcome this challenge, we introduce
a solution based on the Gumbel Trick (Jang et al.,
2017) and Relaxed Top-k techniques (Chen et al.,
2018). This enables us to design a stochastic, top-k
attention mask that is fully differentiable, allowing
for end-to-end optimization. We note this approach
as Differentiable Masked Attention (DMA).
With DMA in place, we reformulate the reranking problem as learning the optimal sampling
weight for the corresponding attention mask. This
leads to our end-to-end training framework, which
we refer to as Gumbel Reranking (G-Rerank). Unlike previous methods that rely on LLM-supervised
losses, G-Rerank directly optimizes the reranker by
minimizing the overall language modeling loss of
the RAG system, thereby ensuring that the training
objective closely aligns with the inference process.
Additionally, G-Rerank accounts for interdependencies between retrieved candidate documents,
making it suitable for multi-hop QA tasks.
We evaluate our training approach across various
architectures. Specifically, we conduct experiments
using two language models—FiD (Izacard and
Grave, 2021b) and CEPE-Llama2-7B (Yen et al.,
2024)—as well as two rerankers—BGE-RerankerBase (Xiao and Liu, 2023) and RankT5 (Zhuang
et al., 2023). Our method is tested on five benchmark datasets, covering both single-hop and multihop QA tasks. To comprehensively assess the effectiveness of our approach, we consider three different evaluation settings: mining, reranking, and generation. Our proposed training strategy achieves
consistent improvements across all these settings.
Furthermore, compared to distillation-based methods, our training approach significantly improves
the reranker’s ability to distinguish indirectly relevant documents, leading to a 10.4% improvement
in the Recall@5 metric on HotpotQA. Finally, we
analyze the necessity of the Gumbel trick and the
impact of prior knowledge in rerankers.
2 Related Works
Training the Reranking Module in RAG Systems The effectiveness of RAG systems relies
heavily on the quality of retrieval and reranking
(Glass et al., 2022; Dong et al., 2024). Traditional
retrieval methods are based on lexical similarity
(Robertson and Zaragoza, 2009), while recent advances leverage dense vectors and transformer ar-
chitectures (Karpukhin et al., 2020; Khattab and
Zaharia, 2020). However, retrieval modules finetuned on public datasets often require additional
adaptation for specific downstream tasks (Izacard
et al., 2023; Salemi and Zamani, 2024).
To bridge this gap, recent works explore finetuning retrieval and reranking modules for tasks
such as open-domain question answering (ODQA).
One common strategy distills knowledge from
LLMs into retrievers by ranking candidate documents based on generated answer perplexity (Shi
et al., 2024; Glass et al., 2022; Lin et al., 2024).
However, such methods overlook inter-document
dependencies, crucial for multi-hop reasoning
tasks (Trivedi et al., 2022; Ho et al., 2020). Alternative approaches use attention scores (Izacard
and Grave, 2021a) or leave-one-out methods (Izacard et al., 2023; Asai et al., 2022), but these are not
end-to-end optimized for generation quality, leading to a retriever-generation gap (Ke et al., 2024).
Stochastic k-Subset Selection and Masked Attention Top-k relaxation has been widely studied
for differentiable subset sampling, extending the
Gumbel-Softmax trick (Jang et al., 2017; Xie and
Ermon, 2019; Xie et al., 2020), with important applications in semi-structured pruning (Fang et al.,
2024), model interpretability (Chen et al., 2018)
and point clouds analysis (Yang et al., 2019).
The reranking process can also be modeled as
a subset sampling problem. However, since retrieved documents influence LLM outputs through
attention, a key challenge lies in introducing sparsity into the attention computation. Existing approaches employ soft attention masks to model
discrete selections (Fan et al., 2021; Yang et al.,
2019). Inspired by these methods, we model RAG
reranking as a subset sampling process with soft
masks, facilitating end-to-end optimization.
3 Methodology
3.1 Problem Setting
For common downstream tasks, such as OpenDomain QA (Zhu et al., 2021), the training data
typically consists of an input query q and the corresponding ground-truth answer a. During the
retrieval process, a set of candidate documents
d1, . . . , dn is retrieved based on q. The reranker R
is then applied to these candidate documents, generating a set of candidate scores. We retain only the
top-k scored documents for further computation:
wi = R(Concatenate(q, di)), ∀i ∈ [n]
Ik = {i | wi ∈ top-k ({wi}
n
i=1)}
(1)
where [n] ≜ {1, 2, . . . , n}. The top-k documents,
Dk = {di| i ∈ Ik}, are selected as input to
the LLM, which then computes the corresponding
logits and language loss LLM . In this work, we
focus on training the reranker in the RAG system.
A key challenge is that the candidate documents
d1, . . . , dn lack relevance annotations, making it
infeasible to directly fine-tune the reranker. Additionally, although we have access to the language
loss LLM , the top-k operation in Equation 1 is nondifferentiable, preventing gradient propagation to
the reranker and thus hindering end-to-end training.
3.2 Viewing Reranker as Attention Mask
We reinterpret the reranking process from the perspective of attention masks. Let Ki,t and Vi,t denote the key and value embeddings of the t-th token
in the i-th candidate document, respectively. And
let Qm denote the query embedding for the m-th
token in the decoding phase of LLM, the standard
attention computation is defined as:
A(Qm, Ki,t) =
exp 
QmKT
√
i,t
dk

P
i
′
P
t
′ exp 
QmKT
i
′
,t′ √
dk
 (2)
where A(Qm, Ki,t) represents the attention score
of the m-th token in the decoding process to the
t-th token in the i-th candidate document.
The reranker retains only the top-k documents
for attention computation, and these top-k documents, as a set, are used as part of the prompt. The
order of these documents is no longer important.
Therefore, we can use a corresponding hard attention mask MR to simulate the reranking process.
MR
i =
(
1, if i ∈ Ik
0, otherwise
(3)
MA(Qm, Ki,t) =
MR
i
exp 
QmKT
√
i,t
dk

P
i
′ MR
i
′
P
t
′ exp 
QmKT
i
′
,t′ √
dk

(4)
This formulation of masked attention is mathematically equivalent to reranking. If document i is
Algorithm 1 Gumbel Reranking: Training Reranker via Differentiable Masked Attention
1: procedure STOCHASTICSUBSETMASK(reranker R, documents d1, . . . , dn, query q, temperature τ ,
scale factor κ, subset size k)
2: wi = R(Concatenate(q, di)) ∀i ∈ [n] ≜ {1, 2, . . . , n} ▷ Apply Reranker
3: for j = 1 to k do ▷ Stochastic Top-k Sampling
4: w˜i = − log(− log(ui)) + κ · wi, ui ∼ U(0, 1) ∀i ∈ [n]
5: Mˆ R,j = softmax w˜
τ

, w˜ = ( ˜w1, w˜2, . . . , w˜n)
6: end for
7: return max(Mˆ R,1, . . . ,Mˆ R,k) ▷ Return Relaxed Top-k Mask
8: end procedure
9:
10: for each (query q, answer a) in training data do ▷ Training Loop
11: Retrieve n documents d1, . . . , dn using q
12: Mˆ R = StochasticSubsetMask(R, d1, . . . , dn, q, τ, κ, k)
13: Apply DMA(Mˆ R) to obtain logits and language loss LLM ▷ subsection 3.3
14: Update reranker R with ∇RLLM ▷ Reranker Optimization
15: end for
not selected by the reranker, i.e., MR
i = 0, then
all tokens within document i receive an attention
score of zero, i.e., MA(Qm, Ki,t) = 0, ∀t.
Independence Requirements in Pre-Filling To
effectively simulate reranking via MA, it is crucial to ensure the independence of candidate documents. First, all candidate documents should use
the same positional encoding to eliminate position
bias. Second, each document should be encoded
independently during pre-filling to prevent information leakage across documents. To enforce these
constraints, we adopt the parallel pre-filling architecture, as seen in models like FiD (Izacard and
Grave, 2021b), CEPE (Yen et al., 2024), and Parallel Windows (Ratner et al., 2023), where retrieved
documents are encoded separately with independent position encodings during the pre-filling stage.
3.3 Differentiable Masked Attention
The problem of learning a more effective reranker
is thus reformulated as learning a better attention
mask MR. However, the hard attention mask MR
defined in Equation 3 remains non-differentiable,
preventing end-to-end optimization based on the
final language loss. To solve this problem, we leverage the Gumbel-Softmax technique (Jang et al.,
2017) to convert discrete sampling into a differentiable process. Specifically, we transform the
reranker’s output w1, w2, . . . , wn into a probability
distribution for sampling an attention mask:
Gi = − log− log(ui)

, ui ∼ U(0, 1),
Mˆ R =
exp
w˜i
τ

Pn
j=1 exp
w˜j
τ
, w˜i = Gi + κ · wi
(5)
where Mˆ R
i
represents the probability of selecting
the i-th document. τ and κ are hyperparameters
in the Gumbel training process. We discuss their
effects in detail in Appendix A. To approximate
Top-k reranking, we perform independent sampling
k times and compute the element-wise maximum:
Mˆ R = max Mˆ R,1, . . . ,Mˆ R,k(6)
This results in a soft attention mask representing the sampled subset, leading to Differentiable
Masked Attention:
DMA(Qm, Ki,t) =
Mˆ R
i
exp 
QmKT
√
i,t
dk

P
i
′ Mˆ R
i
′
P
t
′ exp 
QmKT
i
′
,t′ √
dk

(7)
This formulation allows end-to-end optimization
of the reranker R based on final language model
loss, improving overall RAG system performance.
3.4 Gumbel Reranking Pipeline
In this section, we introduce Gumbel Reranking, an
end-to-end reranker optimization framework leveraging the previously introduced DMA. The overall pipeline is outlined in Algorithm 1.
Parallel Context Pre-Filling LLM Decoder
Query Doc 3
Query Doc 2
Query Doc 1
Doc 1 Doc 2 Doc 3 Query
Any Pretrained Parallel Context LLM
e.g. FiD (T5-based), CEPE (Llama2-7B-based)
Any Reranker
e.g. RankT5, BGE-Reranker
Reranker
Gumbel
Subset
Sampling
-2.6
1.5
1.3
Candidates Score
0.05
0.97
0.98
Relaxed Top-K
Attention Mask
Encoded Passages
w/ Relaxed Sampling
e.g.
What is the capital of Turkey?
Generation
e.g. Istanbul
e.g. Ankara
Truth
Language
Loss
Trainable
Reranker
Fixed Pretrained
Language Model
Adding
Gumbel Noise
Performing
Relaxed Top-K
An End-to-End Pipeline
for Training Reranker
Maintain Independence
during Pre-Filling
for Exploration for Differentiable
Forward Propagation
Backward Propagation
Figure 2: Implementation workflow of G-Rerank. We focus on fine-tuning the reranker while keeping LLM
parameters fixed. However, given sufficient computational resources, joint fine-tuning of both the LLM and the
reranker is feasible. In the Pre-Filling phase, it is essential to maintain the independence of candidate documents.
Training Process Given a query q and a set of
candidate documents d1, . . . , dn, the reranker first
computes a relevance score for each document. The
Stochastic Subset Mask algorithm then generates a
Top-k attention mask Mˆ R,k, which represents the
probability of selecting each candidate document.
The selected documents are subsequently used in
the generation process, where the attention mechanism follows Equation 7 to compute logits and
the language modeling loss. Finally, the reranker is
optimized by minimizing the language loss LLM .
Since our proposed framework primarily focuses
on enhancing the reranking module, we fix the parameters of the LLM in our experimental setup,
as shown in Figure 2. This also facilitates a fairer
comparison of different reranker training strategies.
Key Advantages This framework facilitates endto-end optimization of the reranker via backpropagation, offering two primary advantages. First,
by modeling reranking as applying a documentwise attention mask, it mitigates the discrepancy between training and inference, guiding the reranker
to prioritize documents that minimize the final generation loss. Second, our approach leverages gumbel subset sampling, enabling the model to identify the complete evidence subset during training,
rather than analyzing each candidate document independently. This advantage makes our method
well-suited for multi-hop QA scenarios and sets
it apart from existing perplexity-based distillation
techniques, as discussed in Appendix D.
4 Experiments
In subsection 4.2, we first validate the effectiveness
of our approach under three different experimental
settings. Then, in subsection 4.3, we focus on
whether the reranker can learn to prioritize indirect
evidence in multi-hop question answering. Next,
in subsection 4.4, we conduct an ablation study
on the Gumbel trick and demonstrate its necessity.
Finally, in subsection 4.5, we remove the reranker
and assign each document a learnable weight to
further verify the efficacy of our training objective
in capturing the relative importance of documents.
4.1 Experimental Setup
Language Models We experiment with two different language models as the generation module in
our RAG system: Fusion-in-Decoder (FiD) (Izacard and Grave, 2021b) and CEPE-Llama2-7B (Yen
et al., 2024). FiD (Raffel et al., 2020), built upon
the T5 architecture, is specifically designed for
knowledge-intensive QA and is fine-tuned for each
task. CEPE-Llama2-7B segments long documents
with a lightweight encoder and employs crossattention for effective context utilization, operating
in a zero-shot manner.
Reranker We experiment with RankT5-
Base (Zhuang et al., 2023) and BGE-BaseReranker (Xiao and Liu, 2023) as the reranking
module in the RAG system. RankT5-Base is
fine-tuned in an encoder-decoder setup to perform
reranking, while BGE-Base-Reranker is an
encoder-only model based on BERT.
Query Retrieved Docs Answer
Input Data
Training Set
Query Retrieved Docs
Mining Setting
Query Retrieved Docs Answer
Training Set
Target
Three evaluation perspectives
Reranker Setting
Query Retrieved Docs
Target
Test Set
Generator Setting
Query Retrieved Docs
Target
Answer
Test Set Test Set
Figure 3: Comparison of three different experimental settings. In addition to common evaluation metrics on the test
set, we also assess the reranker’s ability to identify relevant documents from the training set.
Mining Setting Reranker Setting Generator Setting
Training Methods Recall@5 NDCG@5 Recall@5 NDCG@5 MRR EM SubEM F1
Dataset: Hotpotqa
Reranker: RankT5
- EMDR (Lin et al., 2024) 78.0 80.5 78.7 80.6 95.9 60.8 66.1 75.8
- PDist (Glass et al., 2022) 76.8 79.5 78.1 80.0 95.7 60.8 66.0 75.8
- LOOP (Izacard et al., 2023) 71.7 74.7 72.5 74.9 93.0 60.0 65.1 75.0
- ADist (Izacard and Grave, 2021a) 71.3 72.1 71.3 71.9 88.4 57.0 61.9 71.5
- G-Rerank 83.3 84.7 84.4 84.9 95.9 61.1 66.5 76.3
Reranker: BGE-Base
- EMDR (Lin et al., 2024) 81.1 83.2 81.8 83.1 96.3 60.8 66.0 75.8
- PDist (Glass et al., 2022) 79.1 81.6 81.2 82.6 96.2 60.9 66.1 75.7
- LOOP (Izacard et al., 2023) 79.1 81.1 80.4 81.7 95.3 60.3 65.4 75.2
- ADist (Izacard and Grave, 2021a) 77.7 79.5 78.1 79.5 93.7 59.8 65.0 74.7
- G-Rerank 81.6 83.3 81.1 82.9 95.8 60.9 66.1 75.7
Dataset: Musique
Reranker: RankT5
- EMDR (Lin et al., 2024) 56.6 65.8 55.0 58.1 82.0 39.6 42.1 48.6
- PDist (Glass et al., 2022) 57.3 65.3 52.7 55.0 79.5 39.6 42.2 48.3
- LOOP (Izacard et al., 2023) 56.3 64.9 53.3 55.6 79.6 39.2 41.7 48.0
- ADist (Izacard and Grave, 2021a) 53.8 55.3 47.7 47.3 66.4 35.4 37.9 44.1
- G-Rerank 60.7 67.8 57.9 59.7 81.5 40.0 42.4 49.1
Reranker: BGE-Base
- EMDR (Lin et al., 2024) 56.6 65.7 53.6 57.1 81.5 39.7 42.4 48.8
- PDist (Glass et al., 2022) 60.3 66.1 58.2 59.6 80.5 39.4 42.3 48.6
- LOOP (Izacard et al., 2023) 58.7 65.6 57.2 59.3 81.8 39.7 42.2 48.8
- ADist (Izacard and Grave, 2021a) 57.9 64.5 46.0 45.3 64.7 34.8 37.3 43.4
- G-Rerank 60.9 66.6 57.6 59.7 81.5 39.9 42.7 49.1
Dataset: 2wikihop
Reranker: RankT5
- EMDR (Lin et al., 2024) 58.6 63.4 62.9 68.7 88.7 67.2 69.9 72.5
- PDist (Glass et al., 2022) 72.6 76.5 77.2 81.9 94.1 70.2 73.0 75.5
- LOOP (Izacard et al., 2023) 80.4 87.1 79.2 85.4 97.5 71.6 74.4 76.9
- ADist (Izacard and Grave, 2021a) 74.7 79.2 72.4 76.6 90.1 64.1 66.5 69.6
- G-Rerank 80.8 86.9 82.7 88.4 97.8 71.8 74.7 77.2
Reranker: BGE-Base
- EMDR (Lin et al., 2024) 61.8 67.3 71.0 77.1 93.8 68.9 71.8 74.3
- PDist (Glass et al., 2022) 74.0 76.8 76.6 82.2 94.5 69.1 71.9 74.4
- LOOP (Izacard et al., 2023) 77.3 85.0 76.0 83.3 98.5 71.2 73.9 76.3
- ADist (Izacard and Grave, 2021a) 81.4 87.7 80.5 86.4 97.1 70.7 73.5 76.1
- G-Rerank 79.6 86.4 81.4 86.5 97.5 70.9 73.7 76.2
Table 1: Experiments on 2WikiHop, Musique, and HotpotQA using FiD-Large as reader. We consider the settings
illustrated in Figure 3. The best performance is highlighted in bold, while the second-best performance is underlined.
Datasets We evaluate on five QA datasets: multihop (2WikiHop (Ho et al., 2020), HotpotQA (Yang
et al., 2018), Musique (Trivedi et al., 2022))
and single-hop (NQ (Kwiatkowski et al., 2019),
TQA (Kim et al., 2019)). Details are in subsection C.3. For NQ and TQA, we retrieve 20 candidate documents per query using DPR (Karpukhin
et al., 2020). For multi-hop datasets, we apply
the distraction setting to ensure ground-truth documents are included, adding 10 random candidates
in Musique to maintain 20 candidates per query.
Baselines We compare against four LLMsupervised reranker training methods:
EMDR (Sachan et al., 2021; Shi et al., 2024; Lin
et al., 2024), PDist (Izacard et al., 2023; Glass
et al., 2022), LOOP (Izacard et al., 2023), and
ADist (Izacard and Grave, 2021a), which employ
different LLM-supervised losses. Details about
these baselines can be found in Appendix B.
Reranker Setting Mining Setting
FiD-Base FiD-Large FiD-Base FiD-Large
Training Methods Recall MRR NDCG Recall MRR NDCG Recall MRR NDCG Recall MRR NDCG
- EMDR (Lin et al., 2024) 63.0 45.6 45.2 61.8 45.2 44.4 60.3 42.9 42.3 59.0 42.7 41.6
- PDist (Glass et al., 2022) 50.5 39.8 36.2 60.2 44.4 43.4 47.6 37.5 33.5 56.3 41.3 39.7
- LOOP (Izacard et al., 2023) 53.1 40.7 38.1 52.5 40.2 37.3 49.6 38.2 34.9 49.8 37.6 34.5
- ADist (Izacard and Grave, 2021a) 55.2 43.4 40.8 56.3 44.5 41.9 52.8 41.5 38.5 54.2 42.3 39.5
- G-Rerank 69.3 48.2 49.6 72.2 49.5 51.5 65.5 45.0 45.8 68.4 46.4 47.8
Table 2: Results on HotpotQA using FiD as reader for identifying indirectly relevant documents, which are part of
the evidence chain but do not directly contain the answer. Details can be found in Appendix E.
RankT5 BGE-Base
Training Methods NQ TQA NQ TQA
- EMDR (Lin et al., 2024) 33.4 62.4 33.7 62.5
- PDist (Glass et al., 2022) 32.9 61.8 33.9 61.7
- LOOP (Izacard et al., 2023) 33.7 62.1 33.5 62.2
- ADist (Izacard and Grave, 2021a) 33.1 61.6 33.2 62.0
- G-Rerank 34.3 62.8 34.5 63.1
Table 3: Experimental results on NQ and TQA datasets
using CEPE-Llama2-7B as the reader. We employ
SubEM as the evaluation metric.
4.2 Main Experiments
Task Definition We consider the QA task where
the model is trained on question-answer pairs along
with retrieved documents, but at test time, it only
receives the question and the retrieved documents.
We define three evaluation settings, with their respective distinctions illustrated in Figure 3:
1. Mining Setting: During training, given a
question-answer pair, can the reranker effectively identify relevant documents?
2. Reranker Setting: At test time, given a question, can the reranker effectively identify relevant documents?
3. Generator Setting: At test time, given a question, can the model generate correct answers?
Experimental Results Table 1 presents the experimental results using FiD-Large as the generator
model. Our method, G-Rerank, achieves the best
or second-best performance across most datasets.
In the Mining Setting, G-Rerank significantly improves the ability to identify relevant documents
during training, given question-answer pairs. For
instance, it achieves a 5.3% improvement on the
HotpotQA when using RankT5. In the Reranker
Setting, G-Rerank demonstrates a notable improvement over other LLM-supervised loss-based training methods, with a 5.7% Recall improvement on
HotpotQA when using RankT5. Furthermore, in the
Generator Setting, G-Rerank shows consistent performance gains in generation quality, as G-Rerank
directly takes the minimization of the final generation loss as the training objective.
Table 3 presents the SubEM results using CEPELlama2-7B as the generator model. We do not
fine-tune CEPE-Llama2-7B on the downstream
datasets; instead, we leverage its zero-shot capabilities. On both NQ and TQA, the G-Rerank training
strategy leads to the best generation performance.
Notably, these improvements are achieved solely
by fine-tuning the retrieval module while keeping
the language model parameters fixed.
4.3 Identifying Indirectly Relevant
Documents
In multi-hop question answering, a RAG system
is required to retrieve a complete evidence chain
comprising multiple documents to support its final answer. In such scenarios, the reranker should
be able to identify indirectly relevant documents,
which are relevant to the query but do not directly
contain the final answer. The challenge, however,
lies in the fact that these documents often serve
as ‘partial’ evidence, and their relevance is not immediately apparent without being combined with
other documents. Existing perplexity-based training methods commonly used in the literature distill
independent relevance scores for each document,
which fail to capture the inter-document dependencies that are essential for identifying indirectly
relevant documents, as discussed in Appendix D.
We evaluate various reranker training methods
on HotpotQA to assess their ability to identify indirectly relevant documents. To obtain the data,
we employ a straightforward rule-based method to
extract such documents: any document labeled as
relevant in the dataset but not directly containing
the final answer is considered an indirectly relevant
document. Further discussion about this rule can
be found in Appendix E.
0 10000 20000 30000 40000 50000
Training Step
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Max Sampling Weight
w/o Gumbel Noise
w/ Gumbel Noise
Figure 4: Comparison of Max Sampling
Weight (indicating the reranker’s ability to distinguish between candidate documents) with
and without Gumbel noise on the NQ dataset.
0.48
0.50
0.52
0.54
Recall@5
0.86
0.88
0.90
0.92
0.94
0.96
0.98
MRR
0.54
0.56
0.58
0.60
0.62
0.64
NDCG@5
Lowest Perplexity
Highest Leave-one-out Perplexity
Highest Attention Score
Highest Sampling Weight
Figure 5: Performance comparison of different scalar metrics for
assessing candidate document relevance in the Mining Setting.
Our method is illustrated in Figure 8 and Algorithm 2, while
other baseline methods are described in detail in Appendix F.
The experimental results are summarized in Table 2. Our method, G-Rerank, demonstrates a significant improvement in identifying indirectly relevant documents. Specifically, when FiD-Large is
used as the generator model, G-Rerank achieves a
recall improvement of 10.4%. These results suggest that our approach, which views reranking as
a subset sampling problem, allows the model to
better capture inter-document relationships and effectively recognize complete evidence chains.
4.4 Necessity of Gumbel Trick
We leverage the Gumbel trick to transform the output weights of the reranker into an approximately
discrete attention mask, where values tend to converge to either 0 or 1. A natural question arises:
Is the introduction of Gumbel noise essential? We
conduct an ablation study by removing the Gumbel
noise and directly utilizing the reranker’s output
weights as the attention mask while maintaining
the same end-to-end optimization process.
Our experiments reveal a substantial drop in performance when Gumbel noise is omitted. Specifically, the EM metric on the NQ dataset decreases
drastically from 46.2 (with Gumbel) to 12.7 (without Gumbel). To gain further insight, we visualize
the reranker’s output weights during training.
Figure 4 presents the average maximum normalized document weight assigned by the reranker.
With the Gumbel noise applied, we observe a clear
upward trend in the maximum document weight,
indicating that the reranker progressively enhances
the differentiation between candidate documents,
which ultimately leads to convergence. In contrast,
when Gumbel noise is removed, the maximum document weight decreases over time, eventually stabilizing at 0.05, signaling a diminished ability to
distinguish between candidates. This degradation
occurs because, in the absence of the discretization constraint introduced by the Gumbel trick, the
model tends to preserve the original attention distribution, thus treating the removal of the attention
mask as its objective. Consequently, the reranker
learns to assign uniform soft mask across all candidates, i.e., Mˆ R
i,w/o Gumbel =
1
N = 0.05, ∀i, thereby
reverting the masked attention mechanism Equation 7 to its original form as defined in Equation 2.
These findings underscore the importance of the
discretization constraint imposed by the Gumbel
trick for learning an effective attention mask.
4.5 Learnable Sampling Weights
The presence of the reranker can be viewed as incorporating text-based prior knowledge into the
document relevance learning process. However,
even in the absence of text-based priors, our training methodology can still effectively identify the
relevant documents. To verify this, we focus on the
Mining Setting and investigate whether the model
is capable of learning meaningful document relevance scores without the use of a reranker.
Our method is illustrated in Figure 8 and Algorithm 2, while the experimental setup and baseline
methods are explained in Appendix F. Specifically,
we remove the reranker component and instead assign each candidate document a learnable sampling
weight, initializing all weights to zero. The results,
presented in Figure 5, show that even without the
reranker (i.e., without prior knowledge of the text),
our approach is still able to learn reliable relevance
scores for each document. Moreover, it significantly outperforms other scalar metrics based on
perplexity or attention scores, further confirming
the effectiveness of our training objective.
5 Conclusion
In this work, we introduce G-Rerank, an end-toend optimization framework for training rerankers
in RAG systems. By reinterpreting the reranking
process as masked attention, we leverage the Gumbel Trick and Relaxed Top-k to enable direct optimization of the document-wise attention mask.
Our method effectively captures document interdependencies and aligns retrieval and generation
objectives. Experiments across different settings
show that G-Rerank notably improves reranker performance, especially in multi-hop QA tasks.
6 Limitations
Our method imposes certain constraints on its applicability to existing decoder-only LLMs due to its
reliance on parallel encoding/decoding capabilities
during the pre-filling stage. This requirement limits
its direct adoption in conventional autoregressive
LLMs. However, it is worth noting that many highperformance language models with parallel encoding/decoding capabilities have already become
standard choices in various Retrieval-Augmented
Generation (RAG) systems, such as FiD (Izacard
and Grave, 2021b), CEPE (Yen et al., 2024), and
Parallel Windows (Ratner et al., 2023). Furthermore, our approach requires such models only during the reranker training phase; once trained, the
reranker itself is independent of any specific LLM
and can be flexibly adapted to other decoder-only
models. Therefore, our method primarily serves
as a general training framework rather than imposing architectural constraints on the final inference
model. Additionally, our approach introduces extra
hyperparameters in the Gumbel-Softmax process,
including the temperature parameter τ and the scaling factor κ, which require tuning to achieve optimal performance. However, through empirical
studies, we find that τ = 0.5 and κ = 1.0 provide robust and stable performance across different
model architectures and datasets. We provide a
further discussion on the effect of τ and κ in Appendix A.
7 Ethical Considerations
While our method aims to improve the accuracy of
the RAG system, it does not eliminate the inherent
risks of biased data or model outputs, as the performance of RAG systems still heavily depends on
the quality of training data and underlying models.
The potential for bias in the training data, particularly for domain-specific queries, can lead to the
amplification of these biases in the retrieved results,
which can impact downstream applications.
Acknowledgement
This research work has been sponsored by Ant
Group Security and Risk Management Fund, the
Shanghai Science and Technology Commission
Blockchain Special Project (No. 24BC3200100)
and the National Key Research and Development
Program of China (No. 2023ZD0121402).
References
Akari Asai, Matt Gardner, and Hannaneh Hajishirzi. 2022. Evidentiality-guided generation for
knowledge-intensive NLP tasks. In Proceedings of
the 2022 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle,
WA, United States, July 10-15, 2022, pages 2226–
2243. Association for Computational Linguistics.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,
Trevor Cai, Eliza Rutherford, Katie Millican, George
van den Driessche, Jean-Baptiste Lespiau, Bogdan
Damoc, Aidan Clark, Diego de Las Casas, Aurelia
Guy, Jacob Menick, Roman Ring, Tom Hennigan,
Saffron Huang, Loren Maggiore, Chris Jones, Albin
Cassirer, Andy Brock, Michela Paganini, Geoffrey
Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.
2022. Improving language models by retrieving from
trillions of tokens. In International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings
of Machine Learning Research, pages 2206–2240.
PMLR.
Jianbo Chen, Le Song, Martin J. Wainwright, and
Michael I. Jordan. 2018. Learning to explain: An
information-theoretic perspective on model interpretation. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018,
volume 80 of Proceedings of Machine Learning Research, pages 882–891. PMLR.
Eunseong Choi, Hyeri Lee, and Jongwuk Lee. 2024.
Multi-granularity guided fusion-in-decoder. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21,
2024, pages 2201–2212. Association for Computational Linguistics.
Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie,
Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and
William W. Cohen. 2023. Fido: Fusion-in-decoder
optimized for stronger performance and faster inference. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada,
July 9-14, 2023, pages 11534–11547. Association for
Computational Linguistics.
Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang,
and Anton Tsitsulin. 2024. Don’t forget to connect!
improving RAG with graph-based reranking. CoRR,
abs/2405.18414.
Zhihao Fan, Yeyun Gong, Dayiheng Liu, Zhongyu Wei,
Siyuan Wang, Jian Jiao, Nan Duan, Ruofei Zhang,
and Xuanjing Huang. 2021. Mask attention networks:
Rethinking and strengthen transformer. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2021, Online, June 6-11, 2021, pages 1692–
1701. Association for Computational Linguistics.
Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg
Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov,
and Xinchao Wang. 2024. Maskllm: Learnable semistructured sparsity for large language models. In
Advances in Neural Information Processing Systems
38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC,
Canada, December 10 - 15, 2024.
Michael R. Glass, Gaetano Rossiello, Md. Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and
Alfio Gliozzo. 2022. Re2g: Retrieve, rerank, generate. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States,
July 10-15, 2022, pages 2701–2715. Association for
Computational Linguistics.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,
and Ming-Wei Chang. 2020. Retrieval augmented
language model pre-training. In Proceedings of the
37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, volume
119 of Proceedings of Machine Learning Research,
pages 3929–3938. PMLR.
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,
and Akiko Aizawa. 2020. Constructing A multi-hop
QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International
Conference on Computational Linguistics, COLING
2020, Barcelona, Spain (Online), December 8-13,
2020, pages 6609–6625. International Committee on
Computational Linguistics.
Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and
Hamed Zamani. 2023. Fid-light: Efficient and effective retrieval-augmented text generation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27,
2023, pages 1437–1447. ACM.
Gautier Izacard and Edouard Grave. 2021a. Distilling
knowledge from reader to retriever for question answering. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
Gautier Izacard and Edouard Grave. 2021b. Leveraging
passage retrieval with generative models for open domain question answering. In Proceedings of the 16th
Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,
EACL 2021, Online, April 19 - 23, 2021, pages 874–
880. Association for Computational Linguistics.
Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli,
Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2023. Atlas: Few-shot learning
with retrieval augmented language models. J. Mach.
Learn. Res., 24:251:1–251:43.
Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization with gumbel-softmax. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings. OpenReview.net.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,
and Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,
November 16-20, 2020, pages 6769–6781. Association for Computational Linguistics.
Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang,
Qiaozhu Mei, and Michael Bendersky. 2024. Bridging the preference gap between retrievers and llms.
In Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), ACL 2024, Bangkok, Thailand, August
11-16, 2024, pages 10438–10451. Association for
Computational Linguistics.
Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over BERT. In Proceedings of
the 43rd International ACM SIGIR conference on
research and development in Information Retrieval,
SIGIR 2020, Virtual Event, China, July 25-30, 2020,
pages 39–48. ACM.
Daesik Kim, Seonhoon Kim, and Nojun Kwak. 2019.
Textbook question answering with multi-modal context graph understanding and self-supervised openset comprehension. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August
2, 2019, Volume 1: Long Papers, pages 3568–3584.
Association for Computational Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering
research. Trans. Assoc. Comput. Linguistics, 7:452–
466.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th
Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August
2, 2019, Volume 1: Long Papers, pages 6086–6096.
Association for Computational Linguistics.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual.
Xin-Yi Li, Wei-Jun Lei, and Yu-Bin Yang. 2023. From
easy to hard: Two-stage selector and reader for multihop question answering. In IEEE International Conference on Acoustics, Speech and Signal Processing
ICASSP 2023, Rhodes Island, Greece, June 4-10,
2023, pages 1–5. IEEE.
Zhenyu Li, Yike Zhang, Tengyu Pan, Yutao Sun,
Zhichao Duan, Junjie Fang, Rong Han, Zixuan
Wang, and Jianyong Wang. 2024. Focusllm: Scaling llm’s context by parallel decoding. CoRR,
abs/2408.11745.
Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia
Shi, Maria Lomeli, Richard James, Pedro Rodriguez,
Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke
Zettlemoyer, and Wen-tau Yih. 2024. RA-DIT:
retrieval-augmented dual instruction tuning. In The
Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11,
2024. OpenReview.net.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67.
Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram,
Inbal Magar, Omri Abend, Ehud Karpas, Amnon
Shashua, Kevin Leyton-Brown, and Yoav Shoham.
2023. Parallel context windows for large language
models. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023, pages 6383–6402. Association for
Computational Linguistics.
Stephen E. Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333–389.
Devendra Singh Sachan, Mike Lewis, Dani Yogatama,
Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer.
2023. Questions are all you need to train a dense
passage retriever. Trans. Assoc. Comput. Linguistics,
11:600–616.
Devendra Singh Sachan, Siva Reddy, William L. Hamilton, Chris Dyer, and Dani Yogatama. 2021. End-toend training of multi-document reader and retriever
for open-domain question answering. In Advances
in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing
Systems 2021, NeurIPS 2021, December 6-14, 2021,
virtual, pages 25968–25981.
Alireza Salemi and Hamed Zamani. 2024. Evaluating
retrieval quality in retrieval-augmented generation.
In Proceedings of the 47th International ACM SIGIR
Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA,
July 14-18, 2024, pages 2395–2400. ACM.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. REPLUG: retrievalaugmented black-box language models. In Proceedings of the 2024 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1:
Long Papers), NAACL 2024, Mexico City, Mexico,
June 16-21, 2024, pages 8371–8384. Association for
Computational Linguistics.
Yixuan Tang and Yi Yang. 2024. Multihop-rag: Benchmarking retrieval-augmented generation for multihop queries. CoRR, abs/2401.15391.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition.
Trans. Assoc. Comput. Linguistics, 10:539–554.
Ming Tu, Kevin Huang, Guangtao Wang, Jing Huang,
Xiaodong He, and Bowen Zhou. 2020. Select, answer and explain: Interpretable multi-hop reading
comprehension over multiple documents. In The
Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational
Advances in Artificial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020, pages 9073–
9080. AAAI Press.
Cunxiang Wang, Haofei Yu, and Yue Zhang. 2023. Rfid:
Towards rational fusion-in-decoder for open-domain
question answering. In Findings of the Association
for Computational Linguistics: ACL 2023, Toronto,
Canada, July 9-14, 2023, pages 2473–2481. Association for Computational Linguistics.
Bohong Wu, Zhuosheng Zhang, and Hai Zhao. 2021.
Graph-free multi-hop reading comprehension: A
select-to-guide strategy. CoRR, abs/2107.11823.
Shitao Xiao and Zheng Liu. 2023. Baai general embedding.
Sang Michael Xie and Stefano Ermon. 2019. Reparameterizable subset sampling via continuous relaxations. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages
3919–3925. ijcai.org.
Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo
Zhao, Hongyuan Zha, Wei Wei, and Tomas Pfister.
2020. Differentiable top-k with optimal transport.
In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual.
Wenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei
Du, Patrick S. H. Lewis, William Yang Wang, Yashar
Mehdad, Scott Yih, Sebastian Riedel, Douwe Kiela,
and Barlas Oguz. 2021. Answering complex opendomain questions with multi-hop dense retrieval. In
9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,
2021. OpenReview.net.
Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo
Li, Jinxian Liu, Mengdie Zhou, and Qi Tian. 2019.
Modeling point clouds with self-attention and gumbel subset sampling. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019,
Long Beach, CA, USA, June 16-20, 2019, pages 3323–
3332. Computer Vision Foundation / IEEE.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,
Brussels, Belgium, October 31 - November 4, 2018,
pages 2369–2380. Association for Computational
Linguistics.
Howard Yen, Tianyu Gao, and Danqi Chen. 2024. Longcontext language modeling with parallel context encoding. In Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand,
August 11-16, 2024, pages 2588–2610. Association
for Computational Linguistics.
Zhangyue Yin, Yuxin Wang, Xiannian Hu, Yiguang Wu,
Hang Yan, Xinyu Zhang, Zhao Cao, Xuanjing Huang,
and Xipeng Qiu. 2023. Rethinking label smoothing on multi-hop question answering. In Chinese
Computational Linguistics - 22nd China National
Conference, CCL 2023, Harbin, China, August 3-5,
2023, Proceedings, volume 14232 of Lecture Notes
in Computer Science, pages 72–87. Springer.
Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao
Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng. 2022. Kg-fid: Infusing knowledge graph in fusion-in-decoder for opendomain question answering. In Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL
2022, Dublin, Ireland, May 22-27, 2022, pages 4961–
4974. Association for Computational Linguistics.
Jiahao Zhang, Haiyang Zhang, Dongmei Zhang, Yong
Liu, and Shen Huang. 2024. End-to-end beam retrieval for multi-hop question answering. In Proceedings of the 2024 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1:
Long Papers), NAACL 2024, Mexico City, Mexico,
June 16-21, 2024, pages 1718–1731. Association for
Computational Linguistics.
Chen Zhao, Chenyan Xiong, Jordan L. Boyd-Graber,
and Hal Daumé III. 2021. Multi-step reasoning over
unstructured text with beam dense retrieval. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACLHLT 2021, Online, June 6-11, 2021, pages 4635–
4641. Association for Computational Linguistics.
Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming
Zheng, Soujanya Poria, and Tat-Seng Chua. 2021.
Retrieving and reading: A comprehensive survey on open-domain question answering. CoRR,
abs/2101.00774.
Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui,
Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and
Michael Bendersky. 2023. Rankt5: Fine-tuning T5
for text ranking with ranking losses. In Proceedings
of the 46th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, pages
2308–2313. ACM.
A Effect of hyper-parameters on the
Training Process
The temperature parameter τ controls the sharpness of the softmax distribution used in the selection process of documents. We conduct sampling
weight learning for 20 candidate documents based
on RankT5 on the NQ dataset, and tested the impact of different τ values on the sampling weights
during the training process. The experimental results are shown in Figure 6. Specifically, as τ approaches zero, the softmax distribution becomes
increasingly sharp, leading to a hard selection process where the model heavily favors the document
with the highest score. This results in a deterministic decision-making process, where the model’s
focus is on exploitation, quickly converging to a
particular document. On the other hand, when τ increases, the distribution becomes smoother, allowing for a more stochastic sampling process. This
introduces more exploration, as the model is less
likely to fixate on a single document, encouraging
the exploration of other potential candidates. A
larger τ thus promotes diversity in the selection
process, which can be beneficial for avoiding local optima and improving generalization during
training.
The scaling factor κ plays a critical role in controlling the relative influence of the Reranker scores
on the overall document selection process. We test
the impact of different κ values on the sampling
weights during the training process. The experimental results are shown in Figure 7. Specifically,
κ modulates the contribution of the Reranker score
wito the final selection probability. When κ is
small, the contribution of the original Gumbel noise
term Gi dominates the selection process. This introduces significant randomness, increasing the exploration rate during training. A small κ value
results in noisy selection, encouraging the model to
explore various documents and learn more diverse
representations. Conversely, when κ is large, the
Reranker score wi has a stronger influence, and the
model’s selection becomes more deterministic. In
this case, the Reranker score dominates the sampling process, leading to faster convergence as the
model focuses on selecting the most highly scored
documents. However, an overly large κ may limit
the exploration of alternative options, potentially
leading to overfitting and reduced generalization.
B LLM-Supervised Baselines
Baselines. We compare our approach against four
LLM-supervised reranker training methods that
leverage generative language model signals to supervise retriever learning without requiring additional document annotations. In particular, we consider the following methods:
Attention Distillation (ADist) (Izacard
and Grave, 2021a): This method utilizes
the cross-attention scores from the language
model—augmented by the norms of the corresponding value vectors—to compute a target
relevance distribution over retrieved documents.
The reranker R is trained by minimizing the
KL-divergence between its own distribution over
the top-k documents and the attention-based target
distribution. The target distribution for the reranker
is defined as:
pATTN(pk) = exp(αk∥vk∥2)
PK
i=1 exp(αi∥vi∥2)
where αk is the attention score for document pk
and ∥vk∥2 is the L2 norm of the corresponding
value vector. The loss function minimizes the KLdivergence between the reranker’s distribution pR
and the target distribution pATTN:
KL(pATTN ∥ pR) = X
K
k=1
pATTN(pk) log pATTN(pk)
pR(pk)
End-to-end Multi-Document Reader and
Reranker (EMDR2) (Sachan et al., 2021; Shi
et al., 2024; Lin et al., 2024): EMDR2adopts an
expectation-maximization approach, treating the
retrieved documents as latent variables. Given a
query q and a corresponding answer a, along with
the top-k retrieved documents, the loss is designed
to maximize the log-likelihood of the output given
these documents. The objective function is:
LEMDR2 = log "X
K
k=1
pLM(a | q, pk)pR(pk | q)
#
where pLM(a | q, pk) is the language model’s probability of generating the answer a conditioned on
the query q and document pk, and pR(pk | q)
is the reranker’s distribution over the top-k documents.
Perplexity Distillation (PDist) (Izacard et al.,
2023; Glass et al., 2022): In this approach, the
reranker is trained to predict the improvement in
0 20000 40000
0.2
0.4
0.6
0.8
1.0
1st Sampling Weight
0 20000 40000
0.0
0.1
0.2
0.3
0.4
0.5
2nd Sampling Weight
0 20000 40000
0.0
0.1
0.2
0.3
3rd Sampling Weight
0 20000 40000
0.00
0.05
0.10
0.15
0.20
4th Sampling Weight
0 20000 40000
0.000
0.025
0.050
0.075
0.100
0.125
5th Sampling Weight
0 20000 40000
0.000
0.025
0.050
0.075
0.100
6th Sampling Weight
0 20000 40000
0.00
0.02
0.04
0.06
0.08
7th Sampling Weight
0 20000 40000
0.00
0.02
0.04
0.06
0.08
8th Sampling Weight
0 20000 40000
0.00
0.02
0.04
0.06
9th Sampling Weight
0 20000 40000
0.00
0.02
0.04
0.06
10th Sampling Weight
0 20000 40000
0.00
0.02
0.04
11th Sampling Weight
0 20000 40000
0.00
0.02
0.04
12th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
0.05
13th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
0.05
14th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
0.05
15th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
16th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
17th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
18th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
19th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
20th Sampling Weight
Tau Values
Tau=0.1 Tau=0.3 Tau=0.7 Tau=1.0
Figure 6: The impact of different τ values on the training process. We conduct with 20 candidate documents and
RankT5 on the NQ dataset. The solid line in the figure represents the moving average. The differences in sampling
weights indicate the Reranker’s ability to distinguish between candidate documents.
the language model’s perplexity when each document is used to condition the model’s output. The
KL divergence is minimized between the reranker’s
distribution over documents and the posterior distribution derived from the language model, which
provides a direct measure of how much a document
contributes to the model’s performance. The target
distribution for the reranker is computed as:
pk =
exp(log pLM(a | pk, q))
PK
i=1 exp(log pLM(a | pi
, q))
The reranker is trained to minimize the KLdivergence between its predicted distribution over
the documents and this target distribution:
LPDist =
X
K
k=1
pR(pk | q) log pR(pk | q)
pk
Here, pk is the distribution over documents that
the language model would prefer, and the reranker
is trained to match this distribution to improve the
language model’s perplexity. The KL-divergence
loss encourages the reranker to select documents
0 20000 40000
0.2
0.4
0.6
0.8
1.0
1st Sampling Weight
0 20000 40000
0.0
0.1
0.2
0.3
0.4
0.5
2nd Sampling Weight
0 20000 40000
0.0
0.1
0.2
3rd Sampling Weight
0 20000 40000
0.00
0.05
0.10
0.15
0.20
4th Sampling Weight
0 20000 40000
0.000
0.025
0.050
0.075
0.100
0.125
5th Sampling Weight
0 20000 40000
0.000
0.025
0.050
0.075
0.100
0.125
6th Sampling Weight
0 20000 40000
0.00
0.02
0.04
0.06
0.08
0.10
7th Sampling Weight
0 20000 40000
0.00
0.02
0.04
0.06
0.08
8th Sampling Weight
0 20000 40000
0.00
0.02
0.04
0.06
9th Sampling Weight
0 20000 40000
0.00
0.02
0.04
0.06
10th Sampling Weight
0 20000 40000
0.00
0.02
0.04
0.06
11th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
0.05
12th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
0.05
13th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
14th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
15th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
16th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
17th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
0.04
18th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
19th Sampling Weight
0 20000 40000
0.00
0.01
0.02
0.03
20th Sampling Weight
Kappa Values
Kappa=0.1 Kappa=0.5 Kappa=1.0 Kappa=3.0
Figure 7: The impact of different κ values on the training process. We conduct with 20 candidate documents and
RankT5 on the NQ dataset. The solid line in the figure represents the moving average. The differences in sampling
weights indicate the Reranker’s ability to distinguish between candidate documents.
that enhance the model’s ability to generate the
correct answer.
The objective function in EMDR2is based on
maximizing the log-likelihood of the correct answer, given the query and documents. This method
treats the documents as latent variables and aims
to optimize the likelihood of generating the correct
answer based on the combination of the language
model and reranker’s distributions. On the other
hand, PDist focuses on optimizing the reranker’s
distribution by minimizing the KL-divergence between its predictions and the target distribution,
which is derived from the language model’s perplexity.
Leave-one-out Perplexity Distillation
(LOOP) (Izacard et al., 2023): LOOP refines
the PDist approach by considering the impact
of each document in the context of all other
documents in the top-k set. For each document,
the log-likelihood of the output is computed by
excluding the document from the retrieval set, and
the negative of this value is used as a relevance
score. The target distribution is:
pLOOP(pk) =
exp(− log pLM(a | DK \ {pk}, q))
PK
i=1 exp(− log pLM(a | DK \ {pi}, q))
The reranker is trained to minimize the KLdivergence between this distribution and the one
obtained from the reranker.
C More Details
C.1 Variants of FiD
Recent advancements in Open-Domain Question
Answering have led to the development of several
enhanced Fusion-in-Decoder models. KG-FiD (Yu
et al., 2022) enhances the traditional FiD framework by integrating knowledge graphs to establish
structural relationships among retrieved passages.
This integration employs graph neural networks to
re-rank passages, selecting the most pertinent ones
for answer generation, thereby improving both effectiveness and efficiency. FiDO (de Jong et al.,
2023) addresses memory bandwidth constraints inherent in the FiD architecture by reallocating computational resources. This optimization results in a
significant increase in inference speed without compromising performance, making it more suitable for
real-time applications. FiD-Light (Hofstätter et al.,
2023) focuses on efficient retrieval-augmented text
generation by optimizing the balance between retrieval and generation components. This approach
reduces computational overhead while maintaining answer accuracy, offering a more resourceefficient alternative. RFiD (Wang et al., 2023)
introduces a multi-task learning approach to discern evidentiality, combining passage re-ranking
with sentence classification. This method enhances
the model’s ability to identify causal relationships
between questions and passages, leading to improved answer accuracy. Multi-Granularity Guided
Fusion-in-Decoder (MG-FiD) (Choi et al., 2024)
further refines the FiD approach by aggregating
evidence across multiple levels of granularity. It
harmonizes passage re-ranking with sentence-level
classification, enhancing both accuracy and decoding efficiency.
In our experiments, since we are mainly focusing
on reranker training strategies rather than reader,
we utilize the classical Fusion-in-Decoder model architecture. Building upon this foundation, we compare our approach with various LLM-supervised
reranker training strategies to assess their impact
on ODQA performance.
C.2 Multi-Hop Question Answering
Multi-hop Question Answering (QA) systems typically follow a two-phase process: first, retrieving
relevant passages, and then using these passages to
answer the question. 2WikiHop (Ho et al., 2020),
HotpotQA (Yang et al., 2018), Musique (Trivedi
et al., 2022), and MultiHop-RAG (Tang and Yang,
2024) are widely used benchmarks for evaluating
and improving RAG systems in handling complex
multi-hop reasoning tasks. The retrieval strategies
can differ depending on the QA setting, which may
either be open-domain or reading comprehension.
In open-domain QA, the focus is on retrieving
relevant passages from a large corpus. Methods
like MDR (Xiong et al., 2021) and BeamDR (Zhao
et al., 2021) are commonly used in this context. In
the case of reading comprehension, retrieval methods are generally categorized into one-step and
two-step approaches. One-step methods, such as
SAE (Tu et al., 2020), rank passages by concatenating the question with each candidate passage.
Two-step methods, including S2G (Wu et al., 2021)
and FE2H (Li et al., 2023), start by selecting an
initial hop passage and then refine the search by
pairing it with additional candidates. The R3 model
(Yin et al., 2023) enhances this approach by selecting multiple passages at the outset and combining
them to find the correct answer. Beam Retrieval
(Zhang et al., 2024) further extends the process by
using a beam search, enabling it to handle more
complex multi-hop retrieval tasks that go beyond
just two hops.
Our work focuses on a different scenario: we
analyze the limitations of the LLM-Supervised
Reranker Training strategy in widely used RAG
systems for multi-hop question answering tasks (as
detailed in Appendix D) and propose an end-to-end
reranker training strategy based on Gumbel Subset Sampling, which is well-suited for multi-hop
question answering tasks.
C.3 Dataset Description
The datasets in our study encompass a variety
of challenges designed to assess different facets
of question answering. HotpotQA (Yang et al.,
2018) is a multi-hop QA dataset that requires reasoning over multiple Wikipedia articles to derive
answers, emphasizing both factual retrieval and
reasoning capabilities. Similarly, 2WikiHop (Ho
Mining Setting Reranker Setting Generator Setting
Recall@5 NDCG@5 Recall@5 NDCG@5 MRR EM SubEM F1
Dataset: Hotpotqa
Reranker: Rank-T5
- EMDR (Lin et al., 2024) 78.8 81.1 79.5 81.1 95.8 58.3 64.6 73.1
- PDist (Glass et al., 2022) 70.9 73.9 71.4 73.6 92.6 57.8 64.0 72.5
- LOOP (Izacard et al., 2023) 72.2 75.4 73.4 75.7 93.7 58.0 64.2 72.7
- ADist (Izacard and Grave, 2021a) 72.2 74.1 72.5 73.8 90.5 56.5 62.6 71.1
- G-Rerank 81.9 83.7 83.1 84.0 95.9 58.8 65.1 73.5
Reranker: BGE-Base
- EMDR (Lin et al., 2024) 78.4 81.1 78.8 80.7 95.9 58.6 64.8 73.4
- PDist (Glass et al., 2022) 76.7 79.8 78.5 80.7 96.1 58.7 64.9 73.4
- LOOP (Izacard et al., 2023) 76.0 79.0 77.1 79.2 95.3 58.3 64.5 73.0
- ADist (Izacard and Grave, 2021a) 78.5 80.7 79.3 80.9 95.1 58.2 64.4 73.0
- G-Rerank 81.6 83.3 82.6 83.3 95.8 58.8 65.0 73.5
Table 4: Experiments on HotpotQA using FiD-Base as reader. We consider the settings illustrated in Figure 3. The
best performance is highlighted in bold, while the second-best performance is underlined.
et al., 2020) extends the complexity of multi-hop
reasoning by introducing questions that necessitate navigating a knowledge graph, enhancing the
evaluation of entity-based information retrieval.
Musique (Trivedi et al., 2022) is designed to assess
compositional reasoning by decomposing complex
questions into a sequence of simpler sub-questions,
providing a structured approach to multi-step reasoning evaluation. Meanwhile, Natural Questions
(NQ) (Kwiatkowski et al., 2019) presents realworld search queries answered using long-form
documents, challenging models to extract and summarize information from extensive contexts. Lastly,
TextbookQA (TQA) (Kim et al., 2019) focuses on
domain-specific comprehension, where questions
require understanding of textbook-style knowledge,
integrating both textual and diagrammatic content
for a holistic assessment of contextual understanding and inferential capabilities.
C.4 Additional Results with FiD-Base
We present additional results on the HotpotQA
dataset using FiD-Base as the reader, as shown
in Table 4. Our method outperforms others across
most metrics, demonstrating its efficacy.
D Challenges in Handling Indirectly
Relevant Documents with EMDR/PDist
Both EMDR2(Sachan et al., 2021; Shi et al., 2024;
Lin et al., 2024) and PDist (Izacard et al., 2023;
Glass et al., 2022) are based on the premise of distilling the importance of individual documents in
a multi-document retrieval and generation process.
While effective for ranking directly relevant documents, both methods encounter challenges when
dealing with indirectly relevant documents, which
provide context but do not directly contain the answer.
In EMDR2, the objective is to maximize the loglikelihood of generating the answer given the query
and individual documents. The loss function is
given by:
LEMDR2 = log "X
K
k=1
pLM(a | q, pk)pR(pk | q)
#
where pLM(a | q, pk) represents the language
model’s probability of generating the answer conditioned on the query and document, and pR(pk | q)
is the reranker’s preference for the k-th document.
However, this approach assumes the independence
of documents when generating the answer. Indirectly relevant documents, while crucial in providing context, do not appear to contribute meaningfully when evaluated independently. The importance of such documents can only be assessed when
they interact with other documents in the evidence
chain, making their relevance difficult to capture in
this formulation.
Similarly, PDist minimizes the KL divergence
between the reranker’s distribution pR(pk | q)
and the distribution pk derived from the language
model’s perplexity. pk represents the distribution
over documents that the language model prefers
based on its perplexity improvement:
pk =
exp(log pLM(a | pk, q))
PK
i=1 exp(log pLM(a | pi
, q))
In both methods, the language model
pLM(a | q, pk) computes the probability of
generating the answer based on the query and
a single document pk, treating the document
in isolation. This formulation assumes that
each document, independently, provides enough
information to determine the relevance to the query
and the answer. However, in the case of indirectly
relevant documents, this assumption breaks down.
Indirectly relevant documents do not contain
the answer directly but instead contribute to the
reasoning process by supporting or contextualizing
other documents. When evaluated alone, these
documents may appear less relevant or even
irrelevant, which undermines the effectiveness of
both methods.
E Irrelevant Document Setting
Multi-hop question answering in HotpotQA involves synthesizing information from multiple documents to resolve a single query. Although these
questions can be categorized into four major reasoning types—such as bridging intermediate information, comparing entities, verifying multiple properties, or inferring properties through a bridge—they
all share the common requirement of gathering evidence across several sources. In this setting, identifying and assessing indirectly relevant documents
can be instrumental for measuring how effectively
a model captures the full chain of reasoning. Our
approach defines an indirectly relevant document
as any document labeled as relevant in the dataset
yet not explicitly containing the final answer. This
rule is rational in that it highlights the documents
that contribute background or bridging information.
However, this simple rule sometimes blurs the distinction between direct and indirect relevance. For
instance, when a document only partially contains
the answer, or when multiple sources each provide
different fragments of a single reasoning chain (especially in question types like comparing entities
or verifying multiple properties), all supporting
documents will be classified as indirectly relevant
by this rule. The concept of “partial” evidence is
inherently difficult to categorize as either direct
or indirect, and our rule consequently treats such
“partial” evidence as indirectly relevant documents.
After processing the dataset, we observe that in
the training set, the ratio of total queries to data entries that do contain indirectly relevant documents
is 90,447 to 664,247. In the development set, this
ratio is 7,405 to 5,966. The relatively large number of data entries that do contain indirectly relevant documents allows for a robust evaluation of
a model’s ability to retrieve and utilize such supporting evidence. Thus, this formulation not only
aligns well with the structure of HotpotQA but also
provides a meaningful benchmark for analyzing
the effectiveness of different methods in capturing multi-hop dependencies beyond direct answer
retrieval.
F Learnable Sampling Weights Setting
F.1 Scalar Relevance Baselines
In our thesis, we employ different scalar metrics
to quantify the relevance of each candidate document in the RAG system. These metrics correspond to the different LLM-supervised reranker
training methods, and they serve as proxies for the
document’s contribution to generating the correct
answer. In particular, we consider the following
three metrics:
Lowest Perplexity For methods such as EMDR2
and Perplexity Distillation (PDist), each candidate
document pk is evaluated by combining it with
the query q and computing the language model’s
negative log-likelihood of generating the groundtruth answer a. Formally, the scalar relevance score
is defined as:
sperplexity(pk) = − log pLM(a | q, pk).
In this setting, a lower perplexity (i.e., a higher
value of sperplexity) indicates that the document better facilitates the generation of the correct answer,
and is therefore considered more relevant.
Highest Attention Score The Attention Distillation (ADist) method evaluates relevance by feeding
all candidate documents to the language model simultaneously and aggregating the cross-attention
scores. For each document pk, the relevance score
is computed by weighting the attention score αk
with the L2 norm of its corresponding value vector
vk:
sattn(pk) = αk ∥vk∥2.
Here, a higher attention score signifies that the language model assigns more importance to the document during answer generation, thereby indicating
greater relevance.
Highest Leave-one-out Perplexity The Leaveone-out Perplexity Distillation (LOOP) method assesses the impact of each candidate document by
measuring the degradation in the language model’s
performance when that document is excluded from
Algorithm 2 Learnable Sampling Weights Setting
1: procedure STOCHASTICSUBSETMASK(document weights w1, . . . , wn, temperature τ , scale factor
κ, subset size k)
2: for j = 1 to k do
3: w˜i = − log(− log(ui)) + κ · wi, ui ∼ U(0, 1) ∀i ∈ [n]
4: Mˆ j = max(Mˆ j−1,softmax 
( ˜w1,...,w˜n)
τ

) # Mˆ 0 = [0, . . . , 0]
5: end for
6: return Mˆ k ▷ Return Relaxed top-k Mask
7: end procedure
8:
9: Given a query q, answer a, and n retrieved passages p1, . . . , pn
10: Initialization: Initialize learnable document weights w1 = 0, w2 = 0, . . . , wn = 0
11: for each training step do
12: Mˆ = StochasticSubsetMask(w1, . . . , wn, τ, κ, k)
13: Apply DMA(Mˆ ) to obtain logits and language loss LLM ▷ subsection 3.3
14: Update document weights w1, . . . , wn with ∇w1,...,wn LLM ▷ Gradient-based update
15: end for
Query Retrieved Docs Answer
Input Data
Training Set
Query Retrieved Docs
Mining Setting
Query Retrieved Docs Answer
Training Set
Target
Three evaluation perspectives
Reranker Setting
Query Retrieved Docs
Target
Test Set
Generator Setting
Query Retrieved Docs
Target
Answer
Test Set Test Set
Parallel Context Pre-Filling LLM Decoder
Doc 1 Doc 2 Doc 3 Query
Any Pretrained Parallel Context LLM
e.g. FiD (T5-based), CEPE (Llama2-7B-based)
Gumbel
Subset
Sampling
0.05
0.97
0.98
Relaxed Top-K
Attention Mask
Encoded Passages
w/ Relaxed Sampling
e.g.
What is the capital of Turkey?
Generation
e.g. Istanbul
e.g. Ankara
Truth
Language
Loss
Learnable
Sampling
Weight
Fixed Pretrained
Language Model
Adding
Gumbel Noise
Performing
Relaxed Top-K
Maintain Independence
during Pre-Filling
for Exploration for Differentiable
Forward Propagation
Backward Propagation
Figure 8: Setting of Learnable Sampling Weight. Optimizing candidate document sampling weights directly without
leveraging reranker’s prior textual knowledge.
the candidate set. For each document pk, the relevance score is defined as:
sloop(pk) = − log pLMa | DK \ {pk}, q

,
where DK denotes the set of top-k candidate documents. A higher leave-one-out score implies that
the removal of pk leads to a significant deterioration in the language model’s ability to generate the
answer, marking it as highly relevant.
F.2 Our method
In our proposed approach, we eliminate the dependency on a dedicated reranker by replacing it
with a set of learnable scalar weights—one per
document—that are initialized at zero and updated
directly via gradients from the language modeling loss computed by the differentiable masked
attention module. As detailed in Algorithm 2
and Figure 8, the algorithm employs stochastic
Gumbel noise to perform a relaxed top-k selection
over these weights, ensuring that the entire process
remains fully differentiable. This method iteratively refines the document weights over multiple
steps on a single query–answer pair and its corresponding documents, thereby enabling the model
to learn which documents are most informative for
the downstream language modeling task.
Dataset URL License
Multi-hop QA
2WikiMultiHopQA (Ho et al., 2020) https://github.com/Alab-NII/
2wikimultihop
Apache License 2.0:
https://github.com/
Alab-NII/2wikimultihop/
blob/master/LICENSE
HotpotQA (Yang et al., 2018) https://hotpotqa.github.io/ CC BY-SA 4.0: https://
hotpotqa.github.io/
MuSiQue (Trivedi et al., 2022) https://github.com/
stonybrooknlp/musique
CC BY 4.0: https://
github.com/stonybrooknlp/
musique/blob/main/LICENSE
Single-hop QA
Natural Questions (NQ) (Kwiatkowski
et al., 2019)
https://ai.google.com/research/
NaturalQuestions
CC BY-SA 3.0: https:
//ai.google.com/research/
NaturalQuestions/download
Textbook Question Answering
(TQA) (Kim et al., 2019)
https://prior.allenai.org/
projects/tqa
CC BY-NC 3.0: https:
//prior.allenai.org/
projects/tqa
Table 5: Summary of URLs and Licenses for Datasets
G URLs and Licenses
Table 5 provides license information for the
datasets we utilize in our experiments. We employ
all the above datasets solely for research purposes,
in accordance with their designated uses.
