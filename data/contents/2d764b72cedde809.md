# Beyond vibe-coding of Agentic AI — understanding LangChain, LangGraph and LlamaIndex

**URL:** https://medium.com/mitb-for-all/beyond-vibe-coding-of-agentic-ai-understanding-langchain-langgraph-and-llamaindex-5d470fb51e29
**Published:** 2025-09-16T00:00:00.000Z

---

## Summary

The webpage provides a detailed guide on understanding and implementing agentic AI concepts using the **LangChain**, **LangGraph**, and **LlamaIndex** frameworks.

The summary relevant to your query is as follows:

*   **Agent Frameworks (LangChain, LangGraph, LlamaIndex):** The article extensively covers the foundational building blocks of these three major frameworks, showing how to create sequential chains (LangChain), stateful graphs (LangGraph), and RAG-focused pipelines (LlamaIndex).
*   **Tool Use:** The concept of tool use is demonstrated in both LangChain and LangGraph.
    *   In **LangChain**, it shows how to explicitly use a tool (Tavily search) in a chain and how to implement **Tool Calling** where the LLM decides which of the defined tools (`tavily_search` or `internal_lookup`) to invoke based on the query.
    *   In **LangGraph**, it shows how to build a graph structure that incorporates tool selection and execution using conditional edges, analogous to the ReAct pattern.
*   **Agent Orchestration:** LangChain's `initialize_agent` and LangGraph's `create_react_agent` are shown as methods for orchestrating tool use and reasoning steps. LlamaIndex's `ReActAgent` is also covered for similar autonomous decision-making.
*   **Agent Memory/Agentic Memory:** Memory is demonstrated in LangChain using `RunnableWithMessageHistory` and in LangGraph using `MemorySaver` within the state definition, allowing the agent to retain context across conversational turns.
*   **Function Calling & Structured Outputs:** While the article focuses more on general tool use, the concept of defining functions (`@tool` decorator in LangChain) that the agent can call is central to the tool-use sections. Structured outputs are implicitly handled by parsers in LangChain chains and by defined `TypedDict` states in LangGraph.
*   **MCP Servers, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** These specific infrastructure components or SDKs are **not mentioned** in the provided text. The examples primarily use OpenAI's models (`ChatOpenAI`, `OpenAI`) and the Tavily search API.

**In conclusion, the page covers agent frameworks, tool use, agent orchestration, and agent memory, but does not discuss MCP servers or the specific SDKs for Anthropic

---

## Full Content

Beyond vibe-coding of Agentic AI —understanding LangChain, LangGraph and LlamaIndex | by James Koh, PhD | MITB For All | Medium
[Sitemap](https://medium.com/sitemap/sitemap.xml)
[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&amp;referrer=utm_source=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------)
Sign up
[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/mitb-for-all/beyond-vibe-coding-of-agentic-ai-understanding-langchain-langgraph-and-llamaindex-5d470fb51e29&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)
[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)
[
Write
](https://medium.com/m/signin?operation=register&amp;redirect=https://medium.com/new-story&amp;source=---top_nav_layout_nav-----------------------new_post_topnav------------------)
[
Search
](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)
Sign up
[Sign in](https://medium.com/m/signin?operation=login&amp;redirect=https://medium.com/mitb-for-all/beyond-vibe-coding-of-agentic-ai-understanding-langchain-langgraph-and-llamaindex-5d470fb51e29&amp;source=post_page---top_nav_layout_nav-----------------------global_nav------------------)
![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)
[## MITB For All
](https://medium.com/mitb-for-all?source=post_page---publication_nav-825f074cc537-5d470fb51e29---------------------------------------)
·[
![MITB For All](https://miro.medium.com/v2/resize:fill:76:76/1*Yq5V5laVKBeLfIVDQi8Feg.png)
](https://medium.com/mitb-for-all?source=post_page---post_publication_sidebar-825f074cc537-5d470fb51e29---------------------------------------)
Tech contents related to AI, Analytics, Fintech, and Digital Transformation. Written by MITB Alumni; open-access for everyone.
Featured
# Beyond vibe-coding of Agentic AI —understanding LangChain, LangGraph and LlamaIndex
## Zero to (aspiring) Hero
[
![James Koh, PhD](https://miro.medium.com/v2/resize:fill:64:64/1*rGYsYYA8SAiY3-6gN2xkQw.jpeg)
](https://medium.com/@byjameskoh?source=post_page---byline--5d470fb51e29---------------------------------------)
[James Koh, PhD](https://medium.com/@byjameskoh?source=post_page---byline--5d470fb51e29---------------------------------------)
33 min read
·Sep 17, 2025
[
](https://medium.com/m/signin?actionUrl=https://medium.com/_/vote/mitb-for-all/5d470fb51e29&amp;operation=register&amp;redirect=https://medium.com/mitb-for-all/beyond-vibe-coding-of-agentic-ai-understanding-langchain-langgraph-and-llamaindex-5d470fb51e29&amp;user=James+Koh,+PhD&amp;userId=780706b02d58&amp;source=---header_actions--5d470fb51e29---------------------clap_footer------------------)
--
[](https://medium.com/m/signin?actionUrl=https://medium.com/_/bookmark/p/5d470fb51e29&amp;operation=register&amp;redirect=https://medium.com/mitb-for-all/beyond-vibe-coding-of-agentic-ai-understanding-langchain-langgraph-and-llamaindex-5d470fb51e29&amp;source=---header_actions--5d470fb51e29---------------------bookmark_footer------------------)
Listen
Share
This article is written for participants of my Masterclass held for MITB and SCIS alumni.
As things move fast in this domain, we have invited alumni to return and keep stay abreast with new developments and network with fellow graduates at the same time.
1 hour is nowhere near sufficient to master this domain, nor even have a strong grasp of it. But it was never meant to be —anyone who claims you can go from ‘Zero to Hero’ in a couple of hours is either delusional or a fraud.
The purpose of this session is to give participants a condensed explanation of the foundational building blocks, so that they have enough (1) knowledge, (2) interest, and (3) confidence to dive further and continue to learning journey on their own.
This is something you will***not*****get by attending conferences, nor reading journal papers, even the ‘survey papers’.
No more[task paralysis](https://www.nytimes.com/2022/12/16/learning/task-paralysis.html), because everything you need to get started has been compiled into this single article, explained snippet by snippet, and is guaranteed to work.
## Contents
[&lt; 1 &gt; Setting Up](#57a4)
[1.1] Managing packages
[1.2] Setting up tokens
[1.3] Quick check —Using the LLM alone
[&lt; 2 &gt; LangChain](#e3c1)
[2.1] Simple Chain
[2.2] Multiple key-val input
[2.3] Chain with memory
[2.4] Customization through RunnableLambda
(2.4.1) Using just the output received
(2.4.2) Passing additional arguments
[2.5] Tools
(2.5.1) Tavily in a chain
(2.5.2) Tool Calling
[&lt; 3 &gt; LangGraph](#c9d4)
[3.1] Simple Graph
[3.2] Multiple key-val input
[3.3] Graph with memory
[3.4] Using your own function in a node
[3.5] Tools
(3.5.1) Tavily in a graph
(3.5.2) Tool Calling
[3.6] LangSmith
[&lt; 4 &gt; LlamaIndex](#8a9e)
[4.1] The Basics
[4.2] Multiple key-val input
[4.3] Adding Memory
[4.4] Workflow
[4.5] Tools
[4.6] Adapters
*Disclaimer —All the steps here works exactly as-is, at the time of writing (Aug 2025). Things may change. If you find discrepancies in future, please leave a note and I will update.*
## 1 Setting Up
### 1.1 Managing packages
We will be using`uv`, because it is an “[extremely fast Python package installer and resolver](https://astral.sh/blog/uv)”. And by*extremely*, we mean[10–100x faster than pip](https://github.com/astral-sh/uv), according to the official sources.
Try it yourself and you should be convinced.
```
pip install uv
uv venv
.venv\\Scripts\\activate # For Windows
uv pip install requirements.txt
.venv\\Scripts\\python.exe -m ipykernel install --user --name uv-venv --display-name &quot;&quot;Py (uv venv)&quot;&quot;
```
where`requirements.txt`is as follows:
```
langchain-openai==0.3.28
langchain-core==0.3.70
langchain-mistralai==0.2.11
langchain==0.3.26
langgraph==0.5.4
typing-extensions==4.14.1
requests==2.32.4
llama-index==0.13.2
llama-index-llms-openai==0.5.4
llama-index-tools-tavily-research==0.4.0
```
For myself (since no one gave me this`requirements.txt`, I started out with just the basic libraries like langchain, and perform`uv pip install &lt;&lt;whatever\_library\_name&gt;&gt;`along with way as and when needed.
If you want to replicate this tutorial without creating (yet another) virtual environment on your computer, go to Google Colab and do:
```
from google.colab import files
uploaded = files.upload()
## upload requirements.txt
!pip install -r requirements.txt
```
You should be able to import all the following, and that means you’re good to go!
```
from langchain\_openai import ChatOpenAI
from langchain\_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain\_mistralai import ChatMistralAI
from langchain\_core.prompts import ChatPromptTemplate
from langchain\_core.output\_parsers import StrOutputParser
from langchain\_core.runnables.history import RunnableWithMessageHistory
from langchain\_core.chat\_history import InMemoryChatMessageHistory
from langchain\_core.runnables import RunnableLambda, RunnableBranch, RunnablePassthrough
from langchain\_core.tools import tool
from langchain\_core.messages import HumanMessage, ToolMessage
from langchain.agents import initialize\_agent
from langchain.agents.agent\_types import AgentType
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langchain.callbacks.tracers.langchain import LangChainTracer
from llama\_index.llms.openai import OpenAI
from llama\_index.core.prompts import ChatPromptTemplate
from llama\_index.core.llms import ChatMessage, MessageRole
from llama\_index.core.agent.workflow import FunctionAgent, ToolCall, ToolCallResult
from llama\_index.core.workflow import Workflow, step, Context, Event, StartEvent, StopEvent
from llama\_index.tools.tavily\_research.base import TavilyToolSpec
from llama\_index.core import VectorStoreIndex,SimpleDirectoryReader, Document, StorageContext, load\_index\_from\_storage
from llama\_index.core.node\_parser import SentenceSplitter
from llama\_index.core.tools import FunctionTool
```
### 1.2 Setting up tokens
Next, we need to set up our API tokens. You can proceed using just a free account, but “*time is money*” and I believe anyone who is serious about developing their capabilities in this domain ought to be willing to at least spend the equivalent of a latte.
It does not matter which company or organization produces the ‘best’ LLM in future, because the pipeline would be compatible with major leading models. I chose to use OpenAI for convenience, since I already purchased API tokens for previous projects.
![]()
Go to[OpenAI platform](https://platform.openai.com/api-keys), and under the left sidebar, look for ‘API Keys’, create a new secret key, and save the value in a`.env`file.
Press enter or click to view image in full size
![]()
Unless otherwise stated, all screenshots are by the author.
Press enter or click to view image in full size
![]()
Example of .env file. Do not need to worry about the other things like ‘LANGCHAIN\_ENDPOINT’ here.
To demonstrate the model-agnostic nature of this pipeline, let’s also use the Mistral model. Follow the process like above. You can create a free plan without requiring any credit card information.
![]()
If you’re wondering why ‘La Plateforme’, that is because Mistral is a French company!### 1.3 Quick check —Using the LLM alone
The LLM is a (core) component of an Agentic AI system. Let’s start by checking that we are able to run some LLM via APIs.
First, we need to read the API keys created in Section 1.2. This is done using`load\_dotenv`.
```
import os
from dotenv import load\_dotenv, find\_dotenv
\_ = load\_dotenv(find\_dotenv())
```
You can check that your API key is recognized by using`os.getenv(&#x27;&#x27;OPENAI\_API\_KEY&#x27;&#x27;),`or by verifying that you get a logical output from below.
```
from langchain\_openai import ChatOpenAI
from langchain\_core.messages import HumanMessage
llm = ChatOpenAI(
model=&quot;gpt-3.5-turbo&quot;,
# temperature=0,
)
response = llm.invoke([
HumanMessage(content=&quot;What century are we in?&quot;)
])
print(response.content)
```
Let’s try using Mistral here.
```
from langchain\_mistralai import ChatMistralAI
llm = ChatMistralAI(
model=&quot;mistral-small-latest&quot;,
# temperature=0,
)
response = llm.invoke(&quot;What century are we in?&quot;)
print(response.content)
```
Notice that in both cases, we perform the same`llm.invoke`. We can directly pass in a string (*see Mistral example*) and it will be implicitly wrapped into a list of`HumanMessage`. This is also true for other subclasses that inherits from OpenAI’s`BaseChatModel`.
As a reminder, you can perform`issubclass(ChatOpenAI, BaseChatModel)`to check.
For completeness, recall that temperature*T*influences the probability of the next token selected during inference, according to the following equation (where*zⱼ*is the logits for some token*j*). When*T*=1, it is a standard softmax. When*T*approaches zero, it becomes an argmax.
![]()
## 2. LangChain
Now that we’re well-positioned to proceed, let’s get into the Chain aspect of things. As the name suggests, the objective is to link things together sequentially.
### 2.1 Simple Chain
Consider the following chain below.
```
from langchain\_core.prompts import ChatPromptTemplate
from langchain\_core.output\_parsers import StrOutputParser
prompt = ChatPromptTemplate.from\_messages(
[
(&quot;system&quot;, &quot;You are a helpful assistant.&quot;),
(&quot;human&quot;, &quot;{input}&quot;)
]
)
llm = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)
parser = StrOutputParser()
chain = prompt | llm | parser
response = chain.invoke(
{&quot;input&quot;: &quot;Is the Earth flat?&quot;}
)
response
```
The output is as follows.
Press enter or click to view image in full size
![]()
Rather than trying to explain the chaining in words, I will break everything down and*show*it to you.
```
in\_1 = {&quot;&quot;input&quot;&quot;: &quot;&quot;Is the Earth flat?&quot;&quot;}
out\_1 = in\_2 = prompt.invoke(in\_1)
print(&quot;=== Prompt Output ===&quot;)
print(out\_1)
out\_2 = in\_3 = llm.invoke(in\_2)
print(&quot;&quot;\\n=== LLM Output ===&quot;&quot;)
print(out\_2)
out\_3 = parser.invoke(in\_3)
print(&quot;&quot;\\n=== Parsed Output ===&quot;&quot;)
print(out\_3)
```
Press enter or click to view image in full size
![]()
With this, it becomes very clear what really goes on within`chain = prompt | llm | parser`followed by`chain.invoke({&quot;input&quot;: &quot;Is the Earth flat?&quot;})`.
The input, which is a dictionary whose key is`input`and value is the question, goes into`prompt`, of which`prompt`’s output goes into`llm`, after which`llm`’s output goes into`parser`.
### 2.2 Multiple key-val input
Suppose we want a more complex`ChatPromptTemplate`that takes in multiple inputs.
This can be done. All that is required is to modify the dictionary which you pass into`.invoke()`.
```
prompt\_with\_character = ChatPromptTemplate.from\_messages(
[
(&quot;system&quot;, &quot;You are a {character}. If a trickster, deliberate give the wrong answer. If a sage, give a long philosophical answer.&quot;),
(&quot;human&quot;, &quot;{input}&quot;)
]
)
chain\_with\_character = prompt\_with\_character | llm | parser
response = chain\_with\_character.invoke(
{
&quot;input&quot;: &quot;Is the Earth flat?&quot;,
&quot;character&quot;: &quot;trickster&quot;
}
)
response
```
The first component of our new chain is`prompt\_with\_character`. It is a template with two placeholders,`input`and`character`— both of which are required when invoking the chain.
Due to the system being instructed to deliberately give the wrong answer, we will get something like the following.
Press enter or click to view image in full size
![]()
You might be thinking that the above is nothing more than ‘for fun’ and irrelevant in real-life, or that that we can always achieve a similar output by passing everything into the human input.
Is it?
What if we want to have different level of controls? For example, the above can be a quick and simple way if you want an application in which different categories (or ‘ranks’) of people receive different responses. Either due to varying the access to sensitive information, or to give a high-level non-technical explanation to the CEO while going into specific technical details when responding to a developer.
### 2.3 Chain with memory
Now that we are clear about how a chain actually works, let’s do away with breaking things down into`out1, out2, out3`and go back to using a chain for conciseness.
What happens if we use`chain.invoke`twice?
```
chain = prompt | llm | parser
old\_response = chain.invoke(
{&quot;input&quot;: &quot;Is the Earth flat?&quot;}
)
new\_response = chain.invoke(
{&quot;input&quot;: &quot;Do you remember the previous question I asked?&quot;}
)
new\_response
```
You will see that it doesn’t have memory.
Press enter or click to view image in full size
![]()
To retain past context, all we need to do is wrap`chain`within`RunnableWithMessageHistory`.
```
store = {}
def get\_session\_history(session\_id: str):
if session\_id not in store:
store[session\_id] = InMemoryChatMessageHistory()
return store[session\_id]
chain\_with\_memory = RunnableWithMessageHistory(
chain,
get\_session\_history,
input\_messages\_key=&quot;&quot;input&quot;&quot;
)
response = chain\_with\_memory.invoke(
{&quot;input&quot;: &quot;Is the Earth flat?&quot;},
config = {&quot;&quot;configurable&quot;&quot;: {&quot;&quot;session\_id&quot;&quot;: &quot;&quot;123&quot;&quot;}}
)
print(response)
response = chain\_with\_memory.invoke(
{&quot;input&quot;: &quot;What was my previous question?&quot;},
config = {&quot;&quot;configurable&quot;&quot;: {&quot;&quot;session\_id&quot;&quot;: &quot;&quot;123&quot;&quot;}}
)
response
```
By using`chain\_with\_memory`instead of`chain`, it can be seen that the agent remembers what I just said to it. (*And thereby already outperforming some people..*!)
Press enter or click to view image in full size
![]()
This allows you to build a conversation by asking follow-up questions.
### 2.4 Customization through RunnableLambda
So far, our chain is formed by three component;`chain = prompt | llm | parser`, where all are objects (`ChatPromptTemplate.from\_messages / ChatOpenAI / StrOutputParser`) which we had imported from a langchain library.
### 2.4.1 Using just the output received
Let’s extend the chain with`post\_processor`, that performs a custom python function. To keep the example simple, let’s assume our objective is to convert any mention of ‘earth’ into all capital letters, ie. perform the following on the output before returning to the user.
```
def emphasize\_earth(string):
## This comes after `parser`, therefore input is a string
return string.replace(&quot;earth&quot;, &quot;EARTH&quot;).replace(&quot;Earth&quot;, &quot;EARTH&quot;)
```
In order to run this as a part of the chain, we wrap`emphasize\_earth`(or whatever you name the function) into`RunnableLambda`, and make it a part of the chain. Since this function is to be executed*after*the llm gives an output, and*after*parsing the output, we put it at the*end*of the chain, ie.`chain | post\_processor`.
```
from langchain\_core.runnables import RunnableLambda, RunnableBranch, RunnablePassthrough
post\_processor = RunnableLambda(modify\_earth)
composed\_chain = chain | post\_processor # or chain = prompt | llm | parser | post\_processor
new\_chain\_with\_history = RunnableWithMessageHistory(
composed\_chain,
get\_session\_history,
input\_messages\_key=&quot;&quot;input&quot;&quot;
)
response = new\_chain\_with\_history.invoke(
{&quot;input&quot;: &quot;Is the Earth flat?&quot;},
config = {&quot;&quot;configurable&quot;&quot;: {&quot;&quot;session\_id&quot;&quot;: &quot;&quot;456&quot;&quot;}}
)
response
```
Press enter or click to view image in full size
![]()
Pay attention to the data type and ensure that they are consistent.`RunnableLambda`, which comes directly from the`langchain\_core`library, wraps the defined python function into an object that is compatible with joining the chain. Since`parser`returns a string, our`modify\_earth`function takes in that string as its input.
While this is a seemingly trivial example, I hope you are convinced that we can write the function to do pretty-much anything that you are able to do in Python.
### 2.4.2 Passing additional arguments
Let’s extend this learning point. What happens if we want to have a more complex custom function that is linked to`parser`which returns only a string?
The solution is to make use of the`config`arg when calling`.invoke()`.
Why would we want to do such that? Let’s pretend Earth is top-secret and cannot be revealed to the public. We want to censor all such confidential information to anyone with a clearance of Level 4 and below.
```
def modify\_earth(string, config):
if config[&quot;configurable&quot;][&quot;clearance&quot;] &lt;= 4:
return string.replace(&quot;earth&quot;, &quot;&lt;redacted&gt;&quot;).replace(&quot;Earth&quot;, &quot;&lt;redacted&gt;&quot;)
else:
return string.replace(&quot;earth&quot;, &quot;EARTH&quot;).replace(&quot;Earth&quot;, &quot;EARTH&quot;)
post\_processor = RunnableLambda(modify\_earth)
composed\_chain = prompt | llm | parser | post\_processor
response = composed\_chain.invoke(
{&quot;input&quot;: &quot;Is the Earth flat?&quot;},
config = {
&quot;&quot;configurable&quot;&quot;: {&quot;&quot;session\_id&quot;&quot;: &quot;&quot;456&quot;&quot;, &quot;&quot;clearance&quot;&quot;: 4}
}
)
response
```
Notice that the key-value pairs in`config`can be accessed even though`parser`only returns a string. The above will ensure that the word ‘earth’ never appears to someone of a low clearance level.
Press enter or click to view image in full size
![]()
Change ‘clearance’ to 5 and see what happens for yourself.
### 2.5 Tools
Until now, we haven’t really gotten to the*agentic*aspects yet. But stay with me; it is important to be clear of the fundamentals first before taking a step further.
When I learnt about tools, it was using math operations (add, multiply etc.) as examples, even though LLMs are already to perform such operations themselves. Personally, I felt that better examples could be used to illustrate the value of tool calling, even while keeping things simple. Hence, adding search functionalities is the best in my opinion.
### 2.5.1 Tavily
People may take search capabilities for granted, since the current ChatGPT incorporates search results under the hood. ChatGPT*is*actually an agent. Let’s see what happens when we try to obtain ‘real-time’ information using GPT-4o.
```
llm = ChatOpenAI(model=&quot;gpt-4o&quot;)
response = llm.invoke([
HumanMessage(content=&quot;Search the internet to find out the weather in Singapore today&quot;)
])
print(response.content)
```
Press enter or click to view image in full size
![]()
The good thing is that the model is smart and honest enough to acknowledge its limitations.
This is where Tavily comes into the picture —it is a real-time web search API. Let’s try using it to search the internet and check today’s weather. Note that you need to add your`TAVILY\_API\_KEY`to the`.env`file, alongside your other API tokens (see section 1.2).
```
import requests
llm = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)
search\_response = requests.post(
&quot;https://api.tavily.com/search&quot;,
headers={&quot;&quot;Authorization&quot;&quot;: f&quot;&quot;Bearer {os.getenv(&#x27;&#x27;TAVILY\_API\_KEY&#x27;&#x27;)}&quot;&quot;},
json={
&quot;query&quot;: &quot;weather in Singapore today&quot;, # query,
&quot;&quot;search\_depth&quot;&quot;: &quot;&quot;basic&quot;&quot;,
&quot;&quot;include\_answer&quot;&quot;: True
}
)
data = search\_response.json()
web\_answer = data.get(&quot;&quot;answer&quot;&quot;, &quot;&quot;No real-time info found.&quot;&quot;)
query = &quot;What is the weather in Singapore today? Is it good to go hiking?&quot;
prompt = f&quot;&quot;{query}.\\n Answer based on the following information: {web\_answer}&quot;&quot;
response = llm.invoke([
HumanMessage(content=prompt)
])
print(response.content)
```
In the above,`data`is a dictionary. We extract the value corresponding to ‘answer’, and define`prompt`with both the original query and the answer obtained from tavily, and pass it into`llm.invoke`. The input received by the LLM is something like:
Press enter or click to view image in full size
![]()
Now, we will package things nicely, by wrapping the post request into`tavily\_search`function, and linking things together as a chain.
```
def tavily\_search(input: str) -&gt;&gt; str:
response = requests.post(
&quot;https://api.tavily.com/search&quot;,
headers={&quot;&quot;Authorization&quot;&quot;: f&quot;&quot;Bearer {os.getenv(&#x27;&#x27;TAVILY\_API\_KEY&#x27;&#x27;)}&quot;&quot;},
json={
&quot;query&quot;: input[&#x27;input&#x27;],
&quot;&quot;search\_depth&quot;&quot;: &quot;&quot;basic&quot;&quot;,
&quot;&quot;include\_answer&quot;&quot;: True
}
)
data = response.json()
return data.get(&quot;answer&quot;, &quot;No real-time info found.&quot;)
def format\_prompt(original\_query: str, search\_result: str) -&gt;&gt; str:
prompt = f&quot;&quot;{original\_query}.\\n Answer based on the following information: {search\_result}&quot;&quot;
return prompt
search\_chain = RunnableLambda(tavily\_search)
format\_chain = RunnableLambda(lambda x: format\_prompt(x[&quot;&quot;input&quot;&quot;], x[&quot;&quot;search\_result&quot;&quot;]))
chain = (
RunnablePassthrough.assign(search\_result=search\_chain) # adds search\_result, while retaining original key-val
| format\_chain
| llm
| parser
)
response = chain.invoke(
{&quot;input&quot;: &quot;What is the weather in Singapore today? Is it good to go hiking?&quot;},
)
print(response)
```
The use of`RunnableLambda`had already been covered in section 2.4 above. Notice that the first component of our chain is`search\_chain`wrapped in`RunnablePassthrough`. This is because if you were to create your chain as`chain = search\_chain | format\_chain | ...`, then`format\_chain`would receive only the search results, with the original input lost.
### 2.5.2 Tool Calling
The idea of an agentic AI is when the system is able to decide that it needs external help to complete a task, and if so, what tools to use.
Although we just used a tool above, that’s kind of like cheating because we explicitly programmed the steps as such. Even if we asked something that ought to be basic common knowledge, like ‘*what is the past tense of eat*?’ the web search would still be performed, which is redundant.
Now let’s consider two tools —[1] to search the internet (using Tavily), and [2] to refer to a knowledge base that is not ‘common knowledge’.
In addition to`tavily\_search`, we define another function named`internal\_lookup`, which looks for the presence of keywords in each line, and return that entire line if found. Both functions are decorated with`@tool`. Note that docstrings here are necessary in order for the appropriate tool to be selected, and therefore should be a concise and accurate description of what it serves to perform.
```
@tool
def tavily\_search(query: str) -&gt;&gt; str:
&quot;&quot;&quot;Search the internet using Tavily to get up-to-date external information.&quot;&quot;&quot;
response = requests.post(
&quot;https://api.tavily.com/search&quot;,
headers={&quot;&quot;Authorization&quot;&quot;: f&quot;&quot;Bearer {os.getenv(&#x27;&#x27;TAVILY\_API\_KEY&#x27;&#x27;)}&quot;&quot;},
json={
&quot;query&quot;: query,
&quot;&quot;search\_depth&quot;&quot;: &quot;&quot;basic&quot;&quot;,
&quot;&quot;include\_answer&quot;&quot;: True
}
)
return response.json().get(&quot;answer&quot;, &quot;No relevant online info found.&quot;)
@tool
def internal\_lookup(query: str) -&gt;&gt; str:
&quot;&quot;&quot;&quot;&quot;&quot;Look up info from local file &#x27;&#x27;knowledge\_base.txt&#x27;&#x27;.&quot;&quot;&quot;&quot;&quot;&quot;
with open(&quot;&quot;knowledge\_base.txt&quot;&quot;, &quot;&quot;r&quot;&quot;) as f:
for line in f:
if query.lower() in line.lower():
return line.strip()
return &quot;No matching internal note found.&quot;
tools = [tavily\_search, internal\_lookup]
llm\_with\_tools = llm.bind\_tools(tools)
query = &quot;Why does my kid like to visit the puka puka on the way to kapu kapu?&quot;
messages = [HumanMessage(content=query)]
response = llm\_with\_tools.invoke(messages)
if response.tool\_calls:
for tool\_call in response.tool\_calls:
print(tool\_call)
```
Press enter or click to view image in full size
![]()
These are the tool\_calls stored in the object returned by llm\_with\_tools.invoke().
In the above example, the LLM identifies`puka puka`and`kapu kapu`to be a specific entity that is not common knowledge, and decides that it needs help to decipher these. The`internal\_lookup`tool was hence appropriately called, and that was a good call, since they are stored in the text file.
![]()
*Disclaimer: As this article is already very long, I used a naive implementation for*`*internal\_lookup*`*, where*`*knowledge\_base.txt*`*comprises just 2 key-val pairs. If you want to incorporate RAG with LangChain, refer to `Building a basic RAG application` section of*[*Langchain —Part 1 of LLM series*](https://medium.com/mitb-for-all/a-gentle-introduction-to-the-llm-multiverse-part-1-langchain-023a899d294e)*by*[*Titus*](https://medium.com/@tituslhy)*.*
Note that the above only decides on the tool to call, but did not perform the calling. To complete everything, we continue with the following.
```
messages.append(response)
if response.tool\_calls:
for tool\_call in response.tool\_calls:
selected\_tool = {
&quot;&quot;tavily\_search&quot;&quot;: tavily\_search,
&quot;&quot;internal\_lookup&quot;&quot;: internal\_lookup,
}[tool\_call[&quot;&quot;name&quot;&quot;]]
# print(f&quot;&quot;Invoking {tool\_call[&quot;&quot;args&quot;&quot;]} wiith {tool\_call[&quot;&quot;name&quot;&quot;]}&quot;&quot;)
tool\_output = selected\_tool.invoke(tool\_call[&quot;&quot;args&quot;&quot;])
messages.append(ToolMessage(content=tool\_output, tool\_call\_id=tool\_call[&quot;&quot;id&quot;&quot;]))
# Final step with outputs from all tool calls incorporated
final\_response = llm\_with\_tools.invoke(messages)
print(&quot;&quot;Final answer:&quot;&quot;, final\_response.content)
else:
print(&quot;No tools needed. Answer:&quot;, response.content)
```
I will also show the output of the print statements so that what goes on speaks for itself —we see that`internal\_lookup`is invoked twice, and the respective output is appended to the`messages`list, which is then passed into`llm\_with\_tools`.
Press enter or click to view image in full size
![]()
Will see similar printed logs if we uncomment the print statements above.
Now, let’s package the everything nicely. LangChain allows us to do this in a compact manner as follows:
```
from langchain.agents import initialize\_agent
from langchain.agents.agent\_types import AgentType
agent = initialize\_agent(
tools=[tavily\_search, internal\_lookup],
llm=ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;, temperature=0),
agent=AgentType.OPENAI\_FUNCTIONS,
verbose=True
)
response = agent.invoke(&quot;Why does my kid like to visit the puka puka on the way to kapu kapu?&quot;)
```
Press enter or click to view image in full size
![]()
Since ‘verbose=True’ was set in langchain.agents.initialize\_agent, the intermediate steps are printed.## 3. LangGraph
LangChain Academy offers a useful[Introduction to LangGraph](https://academy.langchain.com/courses/take/intro-to-langgraph)course. This section should be seen as complimentary to that; it is neither a summary, nor prequel, nor sequel of that course. Instead, I will make reference to the LangChain examples in Section 2 and rewrite them using LangGraph, illustrating the building blocks in the process and keeping this guide self-sufficient in its own right.
### 3.1 Simple Graph
There are (at least) two ways of building a graph —(1) freeform where we can freely propagate any key-value pairs across nodes, and (2) using`TypeDict`where your state must strictly follow a defined structure. We will go with the latter, following best practices, hence we define`StareGraph(State)`with`State(TypedDict)`.
The code and output will be shown first, followed by explanations.
```
from IPython.display import Image, display
from langgraph.graph import StateGraph, START, END
from langchain\_core.prompts import ChatPromptTemplate
from langchain\_core.output\_parsers import StrOutputParser
from langchain\_openai import ChatOpenAI
from typing\_extensions import TypedDict
class State(TypedDict):
input: str
message: str
response: str
output: str
prompt = ChatPromptTemplate.from\_messages(
[
(&quot;system&quot;, &quot;You are a helpful assistant.&quot;),
(&quot;human&quot;, &quot;{input}&quot;)
]
)
llm = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)
parser = StrOutputParser()
def prompt\_node(state):
messages = prompt.invoke(
{&quot;input&quot;: state[&quot;input&quot;]}
)
return {&quot;message&quot;: messages}
def llm\_node(state):
response = llm.invoke(state[&quot;message&quot;])
return {&quot;response&quot;: response}
def parser\_node(state):
parsed = parser.invoke(state[&quot;response&quot;])
return {&quot;output&quot;: parsed}
builder = StateGraph(State)
builder.add\_node(&quot;&quot;prompt&quot;&quot;, prompt\_node)
builder.add\_node(&quot;&quot;llm&quot;&quot;, llm\_node)
builder.add\_node(&quot;&quot;parser&quot;&quot;, parser\_node)
builder.add\_edge(START, &quot;&quot;prompt&quot;&quot;)
builder.add\_edge(&quot;&quot;prompt&quot;&quot;, &quot;&quot;llm&quot;&quot;)
builder.add\_edge(&quot;&quot;llm&quot;&quot;, &quot;&quot;parser&quot;&quot;)
builder.add\_edge(&quot;&quot;parser&quot;&quot;, END)
graph = builder.compile()
# display(Image(graph.get\_graph().draw\_mermaid\_png()))
response = graph.invoke({&quot;input&quot;: &quot;Is the Earth flat?&quot;})
response[&quot;output&quot;]
```
Press enter or click to view image in full size
![]()
Object returned by graph.invoke() is a dictionary with four key-val pairs, corresponding to ‘input’, ‘message’, ‘response’ and ‘output’.
The structure is self-explanatory from the names itself. A`StateGraph`object is created, and we build the graph one node and one edge at a time, using`.add\_node()`and`.add\_edge()`respectively. If you uncomment the`display(Imgae(...))`line, you will see the associated graph.
Press enter or click to view image in full size
![]()
Image rotated 90 degrees for compact display. The names of each node are defined in the first arg of .add\_node()
In a linear structure, you may say that we’re better off using LangChain and expressing things in a more compact manner (see Section 2.1). And I think so too, but the point here is to learn the building blocks for creating more complex graphs. Also, the use of`prompt\_node`is actually redundant, given that we are passing in the entire`input`as-is into`ChatPromptTemplate`, but it’s good for learning.
Notice that`response`is a dictionary with four key-value pairs. The first is defined during`graph.invoke()`, while the other three pairs are created in the`prompt\_node`,`llm\_node`,`parser\_node`.
If you are want to keep the state to a minimum, and do not need to access earlier values, you can simply set everything to, say, a single`state\_graph`. The code then becomes:
```
class State(TypedDict):
graph\_state: str
def prompt\_node(state):
# print(&quot;Input to 1st node: &quot;, state)
messages = prompt.invoke(
{&quot;&quot;input&quot;&quot;: state[&quot;&quot;graph\_state&quot;&quot;]}
)
return {&quot;&quot;graph\_state&quot;&quot;: messages}
def llm\_node(state):
# print(&quot;Input to 2nd node: &quot;, state)
response = llm.invoke(state[&quot;&quot;graph\_state&quot;&quot;])
return {&quot;&quot;graph\_state&quot;&quot;: response}
def parser\_node(state):
# print(&quot;Input to 3rd node: &quot;, state)
parsed = parser.invoke(state[&quot;&quot;graph\_state&quot;&quot;])
return {&quot;&quot;graph\_state&quot;&quot;: parsed}
builder = StateGraph(State)
## ... Add nodes and edges in the same way ...
graph = builder.compile()
response = graph.invoke({&quot;&quot;graph\_state&quot;&quot;: &quot;&quot;Is the Earth flat?&quot;&quot;})
response[&quot;&quot;graph\_state&quot;&quot;]
```
Note that if`graph\_state`is the only key that had been defined, and if you return a key-value pair that does not confirm to that (for example,`return {&quot;messages&quot;: messages}`), then there will obviously be an error.
In my opinion, the first approach, where the state holds multiple keys, will be clearer when you or another developer reads it, and helps in the debugging process as well.
### 3.2 Multiple key-val input
When we moved from Section 2.1 to Section 2.2, the chain was modified to take in an input with two key-val pairs.
The same will be done for our graph now. The only changes required are shown below. The rest of the code remains unchanged.
```
class State(TypedDict):
input: str
character: str
message: str
response: str
output: str
prompt = ChatPromptTemplate.from\_messages(
[
(&quot;system&quot;, &quot;You are a {character}. If a trickster, deliberate give the wrong answer. If a sage, give a long philosophical answer.&quot;),
(&quot;human&quot;, &quot;{input}&quot;)
]
)
## ...
def prompt\_node(state):
messages = prompt.invoke(
{
&quot;input&quot;: state[&quot;input&quot;],
&quot;character&quot;: state[&quot;character&quot;],
}
)
return {&quot;message&quot;: messages}
## ...
response = graph.invoke(
{
&quot;input&quot;: &quot;Is the Earth flat?&quot;,
&quot;character&quot;: &quot;trickster&quot;
}
)
```
Press enter or click to view image in full size
![]()
Due to the system prompt, the LLM is being instructed to act as a trickster and deliberately give the wrong answer.
We are still invoking the graph with a dictionary, except that now there are two key-val pairs instead of one.`prompt\_node`still takes in just`state`, although we need to extract the relevant information when calling`prompt.invoke()`.
### 3.3 Graph with memory
In its vanilla form, a graph also lacks memory. To illustrate this, let’s reuse the previous graph from Section 3.1. (Since I did not rename any variable, the prompt now includes`character`).
```
response = graph.invoke(
{
&quot;input&quot;: &quot;Is the Earth flat?&quot;,
&quot;character&quot;: &quot;sage&quot;
}
)
response = graph.invoke(
{
&quot;input&quot;: &quot;What was the previous question I asked you?&quot;,
&quot;character&quot;: &quot;sage&quot;
}
)
response[&quot;output&quot;]
```
Press enter or click to view image in full size
![]()
The AI said the previous question I asked was: “What was the previous question I asked”. To make it clear that it was not just a cheeky answer, let us try a question that is less susceptible to ambiguity.
```
response = graph.invoke(
{
&quot;input&quot;: &quot;Add 3 and 4.&quot;,
&quot;character&quot;: &quot;sage&quot;
}
)
response = graph.invoke(
{
&quot;input&quot;: &quot;Multiple that by 2.&quot;,
&quot;character&quot;: &quot;sage&quot;
}
)
response[&quot;output&quot;]
```
The response would be something like ‘*if you multiply nothing by 2, you still have nothing*’, which proves that there was no memory.
In my Masterclass, I will demonstrate how we modify the code to go from the chain in Section 2.3 into the graph below, step-by-step. If you simply do a naive conversion, you will notice that the memory would contain repeated and redundant information, which would not only lead to increased costs (due to more input tokens), but also risk diluting what’s really important.
In here, I will skip those and go straight into the final code.
```
import operator
from typing import Annotated, Sequence
from typing\_extensions import TypedDict, NotRequired
from langchain\_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langgraph.checkpoint.memory import MemorySaver
class State(TypedDict):
messages: Annotated[Sequence[BaseMessage], operator.add]
output: NotRequired[str]
llm = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)
def prompt\_node(state):
if not any(isinstance(msg, SystemMessage) for msg in state[&quot;messages&quot;]):
return {&quot;messages&quot;: [SystemMessage(content=&quot;You are a helpful assistant.&quot;)]}
return {}
def llm\_node(state):
clean\_messages = []
for msg in state[&quot;messages&quot;]:
if msg.\_\_class\_\_.\_\_name\_\_ == &quot;&quot;HumanMessage&quot;&quot;:
clean\_messages.append(HumanMessage(content=msg.content))
response = llm.invoke(clean\_messages)
return {&quot;messages&quot;: [AIMessage(response.content)]}
def parse\_output\_node(state):
last\_msg = state[&quot;&quot;messages&quot;&quot;][-1]
return {&quot;&quot;output&quot;&quot;: last\_msg.content}
builder = StateGraph(State)
builder.add\_node(&quot;&quot;prompt&quot;&quot;, prompt\_node)
builder.add\_node(&quot;&quot;llm&quot;&quot;, llm\_node)
builder.add\_node(&quot;&quot;parse\_output&quot;&quot;, parse\_output\_node)
builder.add\_edge(START, &quot;&quot;prompt&quot;&quot;)
builder.add\_edge(&quot;&quot;prompt&quot;&quot;, &quot;&quot;llm&quot;&quot;)
builder.add\_edge(&quot;&quot;llm&quot;&quot;, &quot;&quot;parse\_output&quot;&quot;)
builder.add\_edge(&quot;&quot;llm&quot;&quot;, END)
memory = MemorySaver()
graph\_with\_memory = builder.compile(checkpointer=memory)
config = {&quot;&quot;configurable&quot;&quot;: {&quot;&quot;thread\_id&quot;&quot;: &quot;&quot;1&quot;&quot;}}
messages = graph\_with\_memory.invoke(
{&quot;messages&quot;: [HumanMessage(content=&quot;Add 3 and 4&quot;)]},
config
)
messages = graph\_with\_memory.invoke(
{&quot;messages&quot;: [HumanMessage(content=&quot;Multiply that by 2&quot;)]},
config
)
messages = graph\_with\_memory.invoke(
{&quot;messages&quot;: [HumanMessage(content=&quot;What is thrice of that?&quot;)]},
config
)
print(&quot;Final result:&quot;, messages[&#x27;output&#x27;])
```
The final result is nicely captured in the`output`component of`state`.
Press enter or click to view image in full size
![]()
The value corresponding to the ‘output’ key of messages is printed here.
In addition, we are also able to investigate all the information stored in the`messages`component of`state`. This is made possible because we had defined`Annotated[Sequence[BaseMessage, operator.add]`within`State(TypedDict)`.
Press enter or click to view image in full size
![]()
Note that*m*here would be objects of type HumanMessage or SystemMessage or AIMessage.### 3.4**Using your own function in a node**
In section 2.4.2, we demonstrated post-processing the LLM’s output, such that`Earth`appears as`&lt;redacted&gt;`if the user’s clearance level is insufficient. This information is passed in within the config.
Press enter or click to view image in full size
![]()
Recap (from section 2.4) of how our custom function modify\_earth was incorporated.
Let’s try achieving something similar using LangGraph.
Each node can actually take in either one or two arguments. The first argument is the state, and the second argument (if we define it) will be the config. The config is defined when the graph is invoked, and can be accessed by all nodes (if they are written to take two arguments). Unlike`state`, the`config`cannot be mutated inside a node.
Each node can take either one argument (`state`) or two arguments (`state`and`config`). The`config`is supplied when the graph is invoked, and is available to all nodes that define a second argument.`config`should not be mutated within any node, but if mutated, it will only affect the current execution context and not persist across separate`graph.invoke()`calls.
```
def emphasize\_earth(string, config):
if config[&quot;configurable&quot;][&quot;clearance&quot;] &lt;= 4:
return string.replace(&quot;earth&quot;, &quot;&lt;redacted&gt;&quot;).replace(&quot;Earth&quot;, &quot;&lt;redacted&gt;&quot;)
else:
return string.replace(&quot;earth&quot;, &quot;EARTH&quot;).replace(&quot;Earth&quot;, &quot;EARTH&quot;)
def parse\_output\_node(state, config):
last\_msg = state[&quot;&quot;messages&quot;&quot;][-1]
string = last\_msg.content
return {&quot;&quot;output&quot;&quot;: emphasize\_earth(string, config)}
builder = StateGraph(State)
builder.add\_node(&quot;&quot;prompt&quot;&quot;, prompt\_node)
builder.add\_node(&quot;&quot;llm&quot;&quot;, llm\_node)
builder.add\_node(&quot;&quot;parse\_output&quot;&quot;, parse\_output\_node)
builder.add\_edge(START, &quot;&quot;prompt&quot;&quot;)
builder.add\_edge(&quot;&quot;prompt&quot;&quot;, &quot;&quot;llm&quot;&quot;)
builder.add\_edge(&quot;&quot;llm&quot;&quot;, &quot;&quot;parse\_output&quot;&quot;)
builder.add\_edge(&quot;&quot;llm&quot;&quot;, END)
memory = MemorySaver()
graph\_with\_memory = builder.compile(checkpointer=memory)
config = {&quot;&quot;configurable&quot;&quot;: {&quot;&quot;thread\_id&quot;&quot;: &quot;&quot;1&quot;&quot;, &quot;&quot;clearance&quot;&quot;: 4}}
result = graph\_with\_memory.invoke(
{&quot;messages&quot;: [HumanMessage(content=&quot;Is the Earth flat?&quot;)]},
config=config,
)
print(&quot;Final result:&quot;, result[&quot;output&quot;])
```
We can see that the output is just as expected. You may change the`clearance`within`config`and see what happens if the number if 5 or larger.
Press enter or click to view image in full size
![]()
### 3.5 Tools
### 3.5.1 Tavily in a graph
Recall that in section 2.5.1, we used tavily by writing`search\_chain = RunnableLmabda(tavily\_search)`, and using it within a chain.
To keep the code concise, and focus on the addition of the new feature, we will not be implementing memory here. It is trivial to incorporate the contents learnt from Section 3.3 if required. Just remember that if`state[&quot;messages&quot;]`is a list with many messages, you need to extract the relevant portion when performing the search as well as formatting the prompt.
```
class State(TypedDict):
messages: BaseMessage
search\_result: NotRequired[str]
output: NotRequired[str]
llm = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)
def tavily\_search\_node(state):
&quot;&quot;&quot;Call Tavily API with the last human message.&quot;&quot;&quot;
query = state[&quot;messages&quot;].content
response = requests.post(
&quot;https://api.tavily.com/search&quot;,
headers={&quot;&quot;Authorization&quot;&quot;: f&quot;&quot;Bearer {os.getenv(&#x27;&#x27;TAVILY\_API\_KEY&#x27;&#x27;)}&quot;&quot;},
json={
&quot;query&quot;: query,
&quot;&quot;search\_depth&quot;&quot;: &quot;&quot;basic&quot;&quot;,
&quot;&quot;include\_answer&quot;&quot;: True
}
)
data = response.json()
return {&quot;&quot;search\_result&quot;&quot;: data.get(&quot;&quot;answer&quot;&quot;, &quot;&quot;No real-time info found.&quot;&quot;)}
def format\_prompt\_node(state):
&quot;&quot;&quot;Format prompt using last query and search results.&quot;&quot;&quot;
query = query = state[&quot;messages&quot;].content
search\_result = state[&quot;&quot;search\_result&quot;&quot;]
prompt\_text = f&quot;&quot;{query}.\\n \\
Answer based on the following information: {search\_result}&quot;&quot;
return {&quot;&quot;messages&quot;&quot;: HumanMessage(content=prompt\_text)}
def llm\_node(state):
response = llm.invoke([state[&quot;messages&quot;]]) # ChatOpenAI.invoke() accepts either str, or PromptValue, or a list of BaseMessages.
return {&quot;messages&quot;: AIMessage(content=response.content)} # use {&quot;messages&quot;: response} if you want to keep all the metadata
def parse\_output\_node(state):
last\_msg = state[&quot;&quot;messages&quot;&quot;]
return {&quot;&quot;output&quot;&quot;: last\_msg.content}
builder = StateGraph(State)
builder.add\_node(&quot;&quot;search&quot;&quot;, tavily\_search\_node)
builder.add\_node(&quot;&quot;format\_prompt&quot;&quot;, format\_prompt\_node)
builder.add\_node(&quot;&quot;llm&quot;&quot;, llm\_node)
builder.add\_node(&quot;&quot;parse\_output&quot;&quot;, parse\_output\_node)
builder.add\_edge(START, &quot;&quot;search&quot;&quot;)
builder.add\_edge(&quot;&quot;search&quot;&quot;, &quot;&quot;format\_prompt&quot;&quot;)
builder.add\_edge(&quot;&quot;format\_prompt&quot;&quot;, &quot;&quot;llm&quot;&quot;)
builder.add\_edge(&quot;&quot;llm&quot;&quot;, &quot;&quot;parse\_output&quot;&quot;)
builder.add\_edge(&quot;&quot;parse\_output&quot;&quot;, END)
graph = builder.compile()
result = graph.invoke(
{&quot;messages&quot;: HumanMessage(content=&quot;What is the weather in Singapore today? Is it good to go hiking?&quot;)},
)
print(&quot;Final result:&quot;, result[&quot;output&quot;])
```
Notice that we have re-defined`State`such that its`messages`is now just a single message rather than a list of messages (see Section 3.3 on why we needed a list back then).
The most notable point to mention here is that the result from the tavily search is stored within the`search\_result`attribute of`state`, such that the`state`input to the`format\_prompt\_node`has two attributes (`messages`and`search\_result`). The`format\_prompt\_node`does not simply return the prompt as output. Instead, it modifies the`messages`attribute of`state`(based on our definition`messages: BaseMessage`, since we did not use the`operator.add`reducer, the value corresponding to`messages`key is replaced and overwritten).
### 3.5.2 Tool Selection and Calling with LangGraph
Earlier in Section 2.5.2, we defined the following two tools (repeated here for your convenience).
```
@tool
def tavily\_search(query: str) -&gt;&gt; str:
&quot;&quot;&quot;Search the internet using Tavily to get up-to-date external information.&quot;&quot;&quot;
response = requests.post(
&quot;https://api.tavily.com/search&quot;,
headers={&quot;&quot;Authorization&quot;&quot;: f&quot;&quot;Bearer {os.getenv(&#x27;&#x27;TAVILY\_API\_KEY&#x27;&#x27;)}&quot;&quot;},
json={
&quot;query&quot;: query,
&quot;&quot;search\_depth&quot;&quot;: &quot;&quot;basic&quot;&quot;,
&quot;&quot;include\_answer&quot;&quot;: True
}
)
return response.json().get(&quot;answer&quot;, &quot;No relevant online info found.&quot;)
@tool
def internal\_lookup(query: str) -&gt;&gt; str:
&quot;&quot;&quot;&quot;&quot;&quot;Look up info from local file &#x27;&#x27;knowledge\_base.txt&#x27;&#x27;.&quot;&quot;&quot;&quot;&quot;&quot;
with open(&quot;&quot;knowledge\_base.txt&quot;&quot;, &quot;&quot;r&quot;&quot;) as f:
for line in f:
if query.lower() in line.lower():
return line.strip()
return &quot;No matching internal note found.&quot;
```
To allow an apple-to-apple comparison, we will make use of these two tools just the way they are, but this time, using LangGraph. After defining the nodes as functions, we run`add\_node`and`add\_edge`, just like in Section 3.1. What’s different here is the`add\_conditional\_edge`.
```
class State(TypedDict):
messages: Annotated[Sequence[BaseMessage], operator.add]
llm\_with\_tools = llm.bind\_tools([tavily\_search, internal\_lookup])
def agent\_node(state: State) -&gt;&gt; State:
response = llm\_with\_tools.invoke(state[&quot;&quot;messages&quot;&quot;])
return {&quot;messages&quot;: [response]}
def tool\_node(state: State) -&gt;&gt; State:
last\_message = state[&quot;&quot;messages&quot;&quot;][-1]
tool\_messages = []
if hasattr(last\_message, &#x27;&#x27;tool\_calls&#x27;&#x27;) and last\_message.tool\_calls:
tool\_map = {
&quot;&quot;tavily\_search&quot;&quot;: tavily\_search,
&quot;&quot;internal\_lookup&quot;&quot;: internal\_lookup
}
for tool\_call in last\_message.tool\_calls:
selected\_tool = tool\_map[tool\_call[&quot;&quot;name&quot;&quot;]]
tool\_output = selected\_tool.invoke(tool\_call[&quot;&quot;args&quot;&quot;])
tool\_messages.append(ToolMessage(content=tool\_output, tool\_call\_id=tool\_call[&quot;&quot;id&quot;&quot;]))
return {&quot;&quot;messages&quot;&quot;: tool\_messages}
def should\_continue(state: State) -&gt;&gt; str:
last\_message = state[&quot;&quot;messages&quot;&quot;][-1]
return &quot;&quot;tools&quot;&quot; if hasattr(last\_message, &#x27;&#x27;tool\_calls&#x27;&#x27;) and last\_message.tool\_calls else END
builder = StateGraph(State)
builder.add\_node(&quot;&quot;agent&quot;&quot;, agent\_node)
builder.add\_node(&quot;&quot;tools&quot;&quot;, tool\_node)
builder.add\_edge(START, &quot;&quot;agent&quot;&quot;)
builder.add\_conditional\_edges(
&quot;agent&quot;,
should\_continue,
{&quot;tools&quot;: &quot;tools&quot;, END: END}
)
builder.add\_edge(&quot;&quot;tools&quot;&quot;, &quot;&quot;agent&quot;&quot;)
graph = builder.compile()
```
The first arg is`&quot;agent&quot;`, which is the node where our conditional edge starts. The second arg is the`should\_continue`function, which takes in the state and determines which node to go to next, either`&quot;tools&quot;`or`END`. The third arg is a dictionary, which says maps the output of the conditional to the node that be visited next.
Using a single line below, as introduced in Section 3.1, we can visualize the graph. Notice that the conditional edge is represented by the dotted rather than solid arrows.
```
display(Image(graph.get\_graph().draw\_mermaid\_png()))
```
![]()
There is also a more compact approach. In Section 2.5.2, we saw that LangChain allow us to create an agent using just a few lines of code, by importing`initialize\_agent`from the`langchain.agents`library. We can do the same here using`create\_react\_agent`, where ‘react’ stands for[**RE**asoning and**ACT**ing](https://par.nsf.gov/servlets/purl/10451467), first presented in a widely-cited ICLR 2023 work.
```
from langgraph.prebuilt import create\_react\_agent
agent = create\_react\_agent(
llm,
[tavily\_search, internal\_lookup],
prompt = &quot;&quot;You are a precise assistant that takes initiative. First try internal\_lookup for unfamiliar terms. If there is a new to check for up-to-date information, use tavily\_search. &quot;&quot;
)
tracer = LangChainTracer(project\_name=&quot;&quot;masterclass&quot;&quot;)
result = agent.invoke(
{&quot;messages&quot;: [HumanMessage(
content=&quot;Why does my kid like to visit the puka puka on the way home? Do you think it is related to Singapore&#x27;s weather right now?&quot;
)]},
config = {&quot;&quot;callbacks&quot;&quot;: [tracer], &quot;&quot;run\_name&quot;&quot;: &quot;&quot;run\_3&quot;&quot;}
)
print(&quot;Final answer:&quot;, result[&quot;messages&quot;][-1].content)
```
### 3.6 LangSmith
When you’re exploring something new, it is almost guaranteed that you will be encountering errors. The good thing is that you can use LangSmith to understand the flow between each node in your graph. It offers a user-friendly UI for you to see the input and output of each node (or the error if encountered).
And the coding overheads? Effectively zero. You can use your existing graph exactly as-is without any changes. All that is required is to define the`LangChainTracer`object, and include it in`config = {&quot;callbacks&quot;: [tracer]}`when invoking the graph.
```
tracer = LangChainTracer(project\_name=&quot;&quot;masterclass&quot;&quot;)
result = agent.invoke(
{&quot;messages&quot;: HumanMessage(content=&quot;Why does my kid like to visit the puka puka on the way home? Do you think it is related to Singapore&#x27;s weather right now?&quot;)},
config={&quot;&quot;callbacks&quot;&quot;: [tracer], &quot;&quot;run\_name&quot;&quot;: &quot;&quot;run\_3&quot;&quot;}
)
```
The`run\_name`is optional but will be helpful in identification.
*P.S. Note that you will need to log in to LangSmith (see screenshot below) and have a LangChain API key, along with*`*LANGCHAIN\_TRACING\_V2*`*and*`*LANGCHAIN\_ENDPOINT*`*defined properly. We’ve already included these in our*`*.env*`*file —see Section 1.2*.
Press enter or click to view image in full size
![]()
You will see a list of projects. Here, we have a project named ‘masterclass’ because it was defined earlier in`LangChainTracer(project\_name=&quot;&quot;masterclass&quot;&quot;).`
![]()
Inside, you will see a list of runs. We see ‘run\_3’ below, because of`config={&quot;&quot;callbacks&quot;&quot;:[tracer], &quot;&quot;run\_name&quot;&quot;:&quot;&quot;run\_3&quot;&quot;}`.
Press enter or click to view image in full size
![]()
Upon clicking the relevant run, you will see the associated information, such as the input and output from one node to another, as well as the tokens used. For small experiments, it is effectively as good as free-to-use.
Press enter or click to view image in full size
![]()
## 4. LlamaIndex
LlamaIndex is great when it comes to RAG applications. While you could still use RAG smoothly with LangChain or LangGraph, LlamaIndex excels at data indexing and retrieval.
A good place to get started (and which I used personally) is the[official documentations](https://docs.llamaindex.ai/en/stable/workflows/v1/). There is a lot of helpful information here. So much, in fact, that even if you spend every single night on this for the next month, it still won’t be sufficient to finish the learning.
How much? This much.
Press enter or click to view image in full size
![]()
Multiple screenshots from different sections of[https://docs.llamaindex.ai/](https://docs.llamaindex.ai/), stitched together.
And when you click on the arrows above.. you get even more stuff!
Press enter or click to view image in full size
![]()
Multiple screenshots from different sections of[https://docs.llamaindex.ai/](https://docs.llamaindex.ai/), stitched together.
I don’t know everything here, and I’m not afraid to say it out loud.
The idea of this section is to just give everyone the confidence to get started. As mentioned at the beginning, no more task paralysis.
### 4.1 The basics
We start by looking at the LlamaIndex equivalent of sections 2.1 and 3.1.
```
from llama\_index.llms.openai import OpenAI
from llama\_index.core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from\_messages(
[
(&quot;system&quot;, &quot;You are a helpful assistant.&quot;),
(&quot;user&quot;, &quot;{input}&quot;)
]
)
llm = OpenAI(model=&quot;gpt-3.5-turbo&quot;)
messages = prompt.format\_messages(input=&quot;&quot;Is the Earth flat?&quot;&quot;)
response = llm.chat(messages)
print(&quot;Output:&quot;, response.message.content)
```
Notice that although`llm = ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)`is used here, the`ChatOpenAI`is actually different. It was previously imported from`langchain\_openai`, but now from`llama\_index.llms.openai`. There is no need for any additional set up, because we are still using the OpenAI token in the`.env`file, as described in Section 1.2.
There are actually adapters, that allow us to wrap a LangChain object to make it compatible with a LlamaIndex workflow. We will talk about Workflows later.
In our example above, the`messages`returned by`prompt.format\_messages`is a list of`ChatMessage`objects from`llama\_index.core.base`. Therefore, it is also possible to pass in a list of`ChatMessage`objects directly if we choose to, for example:
```
response = llm.chat([
ChatMessage(role=MessageRole.SYSTEM, content=&quot;You are a helpful assistant.&quot;),
ChatMessage(role=MessageRole.USER, content=&quot;What century are we in?&quot;)
])
```
You may also do away with the system prompt, and just call`llm.chat(ChatMessage(&quot;What century are we in?&quot;))`.
In fact, if it is just a single message, we can also pass in the string directly using`complete`instead of`chat`, for example,`llm.complete(&quot;What century are we in?&quot;)`.
### 4.2 Multiple key-val input
Recall that in LangChain and LangGraph, we can build the chain/graph to take in multiple inputs, as follows.
Press enter or click to view image in full size
![]()
Snippets covered in Sections 2.2 and 3.2.
For LlamaIndex, it is possible to achieve something similar with`QueryPipeline`(deprecated, and removed in v0.13), but doing so for this use-case would be quite deliberate/unnatural. Instead, I would just define the individual components one step at a time.
```
llm = OpenAI(model=&quot;gpt-3.5-turbo&quot;)
prompt\_with\_character = ChatPromptTemplate.from\_messages([
(&quot;system&quot;, &quot;You are a {character}. If a trickster, deliberately give the wrong answer.&quot;),
(&quot;user&quot;, &quot;{input}&quot;)
])
messages = prompt\_with\_character.format\_messages(
character=&quot;trickster&quot;,
input=&quot;Is the Earth flat?&quot;
)
response = llm.chat(messages)
print(response.message.content)
```
It is trivial to wrap everything as a function, in which case it would be neater to remove`ChatPromptTemplate`while at it.
```
def wrapped(llm, character, input):
response = llm.chat([
ChatMessage(role=MessageRole.SYSTEM, content=f&quot;You are a {character}. If a trickster, deliberately give the wrong answer.&quot;),
ChatMessage(role=MessageRole.USER, content=f&quot;{input}&quot;)
])
return response.message.content
wrapped(llm, character=&quot;trickster&quot;, input=&quot;Is the Earth flat?&quot;)
```
### 4.3 Adding Memory
Recall that in Section 2.3, we enabled memory in LangChain using:
```
&quot;&quot;&quot;
from langchain\_core.runnables.history import RunnableWithMessageHistory
from langchain\_core.chat\_history import InMemoryChatMessageHistory
store = {}
def get\_session\_history(session\_id):
if session\_id not in store:
store[session\_id] = InMemoryChatMessageHistory()
return store[session\_id]
chain\_with\_history = RunnableWithMessageHistory(
chain,
get\_session\_history,
input\_messages\_key=&quot;&quot;input&quot;&quot;
)&quot;&quot;&quot;
```
Meanwhile, in Section 3.3, we enabled memory in LangGraph using:
```
&quot;&quot;&quot;
from langgraph.checkpoint.memory import MemorySaver
memory = MemorySaver()
graph\_with\_memory = builder.compile(checkpointer=memory)
messages = graph\_with\_memory.invoke(
{&quot;messages&quot;: [ ... ]},
{&quot;&quot;configurable&quot;&quot;: {&quot;&quot;thread\_id&quot;&quot;: &quot;&quot;1&quot;&quot;}}
)
&quot;&quot;&quot;
```
The same effect can be achieved using a few lines with LlamaIndex, using`FunctionAgent`and`Context`.
We could also import`Memory`from`llama\_index.core.memory`if more granular control of the chat history is required, such as limiting the number of tokens. For now, using`Context`alone works just fine.
```
from llama\_index.core.agent.workflow import FunctionAgent
from llama\_index.core.workflow import Context
from llama\_index.core.memory import Memory
llm = OpenAI(model=&quot;gpt-3.5-turbo&quot;)
agent = FunctionAgent(llm=llm, system\_prompt=&quot;&quot;Always answer in one sentence.&quot;&quot;)
ctx = Context(agent)
response1 = await agent.run(&quot;Is the Earth flat?&quot;, ctx=ctx)
print(&#x27;response1: &#x27;, response1)
response2 = await agent.run(&quot;Why are you lying?&quot;, ctx=ctx)
print(&#x27;response2: &#x27;, response2)
```
Press enter or click to view image in full size
![]()
We can see that response\_2 is a continuation to response\_1.### 4.4 Workflow
In sections 2.4 and 3.4, we learnt how to incorporate a function (our`modify\_earth`that changes`Earth`to`&lt;redacted&gt;`) within our pipeline. We will be using LlamaIndex Workflows to achieve this.
It in turn requires you to have some foundational knowledge of python’s[typing](https://docs.python.org/3/library/typing.html)and[pydantic](https://docs.pydantic.dev/latest/concepts/models/). If you don’t, refer to their documentations. Avoid going down the rabbit’s hole. For the former, you’ll be fine with just the basics, which should take no more than a few minutes. For the latter, the link comprises many sections, but just first (Models) would give sufficient information to proceed; in fact, just learning till around a quarter of that page, stopping before*Error handling*, would enable you to understand the examples that will be covered.
Here, I will go straight into incorporating`modify\_earth`into Workflows.
We define the stucture for`LLMEvent`(which is the output of`prompt\_node`and input of`llm\_node`) and for`ParseEvent`(which is the output of`llm\_node`and input of`parse\_node`). The`modify\_earth`function remains unchanged from earlier and hence we do not repeat it here.
```
from llama\_index.core.workflow import Workflow, step, Context, Event, StartEvent, StopEvent
class LLMEvent(Event):
messages: Sequence[ChatMessage]
config: dict
class ParseEvent(Event):
messages: ChatMessage
config: dict
```
The`&lt;&lt;something&gt;&gt;\_node`functions are each decorated with`@step`, which sets them up to be a part of the workflow. This enables you to link multiple steps together, each triggered by the specific event which it is looking for (hence the definitions in the code block above, and the type hinting in the functions, are important). Each function is asynchronous, which is just the way`Workflows`are designed, hence the`async`and`await`.
```
class Flow(Workflow):
llm = OpenAI(model=&quot;gpt-3.5-turbo&quot;)
@step
async def prompt\_node(self, ctx: Context, ev: StartEvent) -&gt;&gt; LLMEvent:
messages = ChatPromptTemplate.from\_messages([
(&quot;system&quot;, &quot;You are a helpful assistant.&quot;),
(&quot;user&quot;, f&quot;{ev.messages}&quot;)
])
return LLMEvent(messages=messages.message\_templates, config=ev.config)
@step
async def llm\_node(self, ctx: Context, ev: LLMEvent) -&gt;&gt; ParseEvent:
response = llm.chat(ev.messages)
return ParseEvent(messages=response.message, config=ev.config)
@step
async def parse\_node(self, ctx: Context, ev: ParseEvent) -&gt;&gt; StopEvent:
last\_msg = ev.messages.blocks[0].text
output = modify\_earth(last\_msg, ev.config)
return StopEvent(result=output)
workflow = Flow()
config = {&quot;&quot;configurable&quot;&quot;: {&quot;&quot;thread\_id&quot;&quot;: &quot;&quot;1&quot;&quot;, &quot;&quot;clearance&quot;&quot;: 4}}
result = await workflow.run(
messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Is the Earth flat?&quot;}], config=config
)
print(&quot;Final result:&quot;, result)
```
It is possible for multiple different steps to be looking for the same events. Also, it is possible to have a step that takes as input the output of multiple earlier steps, not just a single one. However, these are beyond the scope of discussion, and won’t be covered here.
### 4.5 Tools
Using Tavily from the LlamaIndex library is straightforward with just a couple of lines. It is so short that I will just squeeze it here (for completeness, since we have Tavily at Sections 2.5 and 3.5) rather than allocate as a standalone sub-section.
```
from llama\_index.tools.tavily\_research.base import TavilyToolSpec
tavily\_tool = TavilyToolSpec(api\_key=os.getenv(&#x27;&#x27;TAVILY\_API\_KEY&#x27;&#x27;))
result = tavily\_tool.search(&quot;&quot;What is the weather in Singapore now?&quot;&quot;)
print(type(result[0]), result[0].text)
```
### 4.5.1 RAG
LlamaIndex stands out in the area of retrieval-augmented generation (RAG) and knowledge integration. Our external data sources can be simple text documents, to large databases. It is not to say that this could*only*be done by LlamaIndex. Instead, it is fair to say that the developers made this process smooth and user-friendly.
The official[documentations by LlamaIndex](https://docs.llamaindex.ai/en/stable/understanding/rag/)explained the process of building a RAG pipeline succinctly, and here I will just go straight into the code. The concept and workings of RAG will be treated as pre-requisite knowledge here.
At its most basic form (accepting all the defaults), we can query a knowledge store (e.g. a folder`RL\_papers`comprising one or more pdf documents) and get a output which incorporates that knowledge, in well under 10 lines of code. For you to replicate this illustration quickly, I used just two pdf documents in my folder`RL\_papers`, namely the[PPO (2017)](https://arxiv.org/abs/1707.06347)and[GRPO (2024)](https://arxiv.org/abs/2402.03300)papers. You can check that`len(documents)`is 42 here; this is because`SimpleDirectoryReader.load\_data()`creates one node for each page.
```
from llama\_index.core import VectorStoreIndex
documents = SimpleDirectoryReader(&quot;&quot;./RL\_papers&quot;&quot;).load\_data()
index = VectorStoreIndex.from\_documents(documents)
query\_engine = index.as\_query\_engine()
response = query\_engine.query(&quot;&quot;How does GRPO work?&quot;&quot;)
print(response)
```
Press enter or click to view image in full size
![]()
Response from query\_engine.query()
Could the LLM have already came equipped with this knowledge pre-trained? We can easily verify it is not the case. Using just the GPT-4 api, we get the following from`llm.complete(“Do you know about the RL algorithm GRPO”)`.
Press enter or click to view image in full size
![]()
By default, LlamaIndex uses OpenAI’s gpt-3.5-turbo as the underlying LLM, with number of matches (k) retrieved set at 2. You can change both by defining the args in`.as\_query\_engine`. Since`index`was already defined above, there is no need for repetition; all we need is to redefine`query\_engine`. While at it, let’s also throw in the`knowledge\_base.txt`file from Section 2.5. You will get a logical output from below.
```
query\_engine = index.as\_query\_engine(
llm=Gemini(model=&quot;&quot;models/gemini-1.5-flash&quot;&quot;), similarity\_top\_k=5
)
new\_response = query\_engine.query(&quot;&quot;Why would cars be at puka puka?&quot;&quot;)
```
What happens if there exist some critical information that spans across two pages? Ideally, we will want the entire text to be in the same chunk, and this would not be the case using the simple defaults shown above. This is where`chunk\_overlap`comes in. In addition, it is also wasteful to do repeated work. Once we have indexed the documents and collected the embeddings, there is actually no need to repeat this (spending time and money in the process). Let’s address both at the same time.
```
import os
from collections import defaultdict
from llama\_index.core.node\_parser import SentenceSplitter
from llama\_index.core import StorageContext, load\_index\_from\_storage
if os.path.exists(&quot;&quot;./index\_store&quot;&quot;):
storage\_context = StorageContext.from\_defaults(persist\_dir=&quot;&quot;./index\_store&quot;&quot;)
index = load\_index\_from\_storage(storage\_context)
else:
os.mkdir(&quot;&quot;index\_store&quot;&quot;)
documents = SimpleDirectoryReader(&quot;&quot;./RL\_papers&quot;&quot;).load\_data()
file\_to\_pages = defaultdict(list)
for doc in documents:
file\_to\_pages[doc.metadata[&quot;&quot;file\_name&quot;&quot;]].append(doc.text)
merged\_documents = [
Document(text=&quot;&quot;\\n&quot;&quot;.join(pages), metadata={&quot;&quot;file\_name&quot;&quot;: file\_name})
for file\_name, pages in file\_to\_pages.items()
]
splitter = SentenceSplitter(chunk\_size=1024, chunk\_overlap=32)
all\_nodes = []
for doc in merged\_documents:
all\_nodes.extend(splitter([doc]))
index = VectorStoreIndex(all\_nodes)
index.storage\_context.persist(persist\_dir=&quot;&quot;./index\_store&quot;&quot;)
```
Here, we check for the presense of an index\_store folder (you can name it anyway you like). If it does not exist, we create the folder using`os`, combine all the default chunks into a single document for*each*distinct file, and then apply the`SentenceSplitter`. All the nodes go into a single list, which is then passed into`VectorStoreIndex`, and subsequently saved as the following files.
![]()
If the folder exists, we simply`load\_index\_from\_storage`.
Of course, this is just scratching the surface of what is possible. If you are interested to explore deeper, do check out the articles written by two of our MITB alumni,
[Tituslhy](https://medium.com/u/c50ac8f269da?source=post_page---user_mention--5d470fb51e29---------------------------------------)
and
[Jinkett A. Yee](https://medium.com/u/4298d8c06a21?source=post_page---user_mention--5d470fb51e29---------------------------------------)
.
### 4.5.2 Tool Selection and Calling with LlamaIndex
In the above sub-section 4.5.1, we had been quite deliberate in the steps. What if we want to give the agent more autonomy in deciding what to do?
We will use an off-the-shelf ReAct agent from LlamaIndex. This will be analogous to the`create\_react\_agent`which we imported from`langgraph.prebuilt`.
Like before, we define the functions for`tavily\_search`and`internal\_lookup`. Note that it contains`tavily\_tool\_spec`and`query\_engine`, respectively, both of which are imported from the`llama\_index`library.
```
from llama\_index.core.tools import FunctionTool
from llama\_index.core.agent.workflow import ReActAgent
tavily\_tool\_spec = TavilyToolSpec(api\_key=os.getenv(&#x27;&#x27;TAVILY\_API\_KEY&#x27;&#x27;))
def tavily\_search(query: str) -&gt;&gt; str:
&quot;&quot;&quot;Search the internet using Tavily for current information.&quot;&quot;&quot;
docs = tavily\_tool\_spec.search(query)
if docs:
return docs[0].text
else:
return &quot;No relevant online info found.&quot;
documents = SimpleDirectoryReader(&quot;&quot;./RL\_papers&quot;&quot;).load\_data()
index = VectorStoreIndex.from\_documents(documents)
query\_engine = index.as\_query\_engine(llm=llm, similarity\_top\_k=5)
def internal\_lookup(query: str) -&gt;&gt; str:
&quot;&quot;&quot;Look up terms from local knowledge base. Always try this first for unfamiliar terms.&quot;&quot;&quot;
response = query\_engine.query(query)
return str(response)
```
We define these as tools, and pass a list of tools into a`ReActAgent`object.
```
tavily\_tool = FunctionTool.from\_defaults(fn=tavily\_search)
internal\_tool = FunctionTool.from\_defaults(fn=internal\_lookup)
agent = ReActAgent(
tools=[tavily\_tool, internal\_tool],
llm=OpenAI(model=&quot;gpt-4o&quot;),
system\_prompt=&quot;&quot;You are an assistant that can use Tavily search and internal lookup tools.&quot;&quot;
)
response = await agent.run(&quot;How is GRPO different from PPO?&quot;)
print(response)
```
The underlying orchestrator (which is an LLM) in the ReActAgent will ‘re’ason (ie. think of what to tool should be used) and ‘act’ (ie. define tool calls). Outputs of the tool calls are observed by the LLM, which produces a final answer when it decides no further tool call is required.
### 4.6 Adapters
Remember the times when you travel overseas, and the power plugs are of a different configuration? What would have been a seemingly big problem is instantly solved when you have an adapter!
We’ll do the same here.
To keep things simple, we will just use the RAG component of LlamaIndex with the chaining + invoke style of LangChain.
```
### From 4.5
from llama\_index.llms.openai import OpenAI
llm = OpenAI(model=&quot;gpt-3.5-turbo&quot;)
documents = SimpleDirectoryReader(&quot;&quot;./RL\_papers&quot;&quot;).load\_data()
index = VectorStoreIndex.from\_documents(documents)
query\_engine = index.as\_query\_engine(llm=llm, similarity\_top\_k=5)
### From 2.4 (modified to replace &#x27;model&#x27; rather than &#x27;earth&#x27;)
def modify(string, config):
if config[&quot;configurable&quot;][&quot;clearance&quot;] &lt;= 4:
return string.replace(&quot;model&quot;, &quot;&lt;redacted&gt;&quot;)
else:
return string.replace(&quot;model&quot;, &quot;MODEL&quot;)
post\_processor = RunnableLambda(modify)
### Combine LlamaIndex portion with LangChain portion
rag\_component = RunnableLambda(lambda x: str(query\_engine.query(x[&quot;&quot;input&quot;&quot;])))
chain = rag\_component | post\_processor
resp = chain.invoke(
{&quot;input&quot;: &quot;How do GRPO work?&quot;},
config={&quot;configurable&quot;: {&quot;clearance&quot;: 4}}
)
print(resp)
```
We see that it works, and we managed to bring the best of both worlds together!
Press enter or click to view image in full size
![]()
Screenshot of output. The word ‘model’ is redacted.## Conclusion
It’s more than 30 minutes of read-time! Let’s sum up the things covered using a thousand words (ie. a picture).
Press enter or click to view image in full size
![]()
Diagram by author
*Disclaimer: All opinions and interpretations are that of the writer, and not of MITB. I declare that I have full rights to use the contents published here, and nothing is plagiarized. I declare that this article is written by me and not with any generative AI tool such as ChatGPT. I declare that no data privacy policy is breached, and that any data associated with the contents here are obtained legitimately to the best of my knowledge. I agree not to make any changes without first seeking the editors’ approval. Any violations may lead to this article being retracted from the publication.*
Editor’s note: Find out more about the Singapore Management University’s Master of IT in Business (MITB) programme at[https://smu.edu.sg/mitb](https://smu.edu.sg/mitb)
[
Langchain
](https://medium.com/tag/langchain?source=post_page-----5d470fb51e29---------------------------------------)
[
Langgraph
](https://medium.com/tag/langgraph?source=post_page-----5d470fb51e29---------------------------------------)
[
Llamaindex
](https://medium.com/tag/llamaindex?source=post_page-----5d470fb51e29---------------------------------------)
[
Agentic Ai
](https://medium.com/tag/agentic-ai?source=post_page-----5d470fb51e29---------------------------------------)
[
Data Science
](https://medium.com/tag/data-science?source=post_page-----5d470fb51e29---------------------------------------)
[
![MITB For All](https://miro.medium.com/v2/resize:fill:96:96/1*Yq5V5laVKBeLfIVDQi8Feg.png)
](https://medium.com/mitb-for-all?source=post_page---post_publication_info--5d470fb51e29---------------------------------------)
[
![MITB For All](https://miro.medium.com/v2/resize:fill:128:128/1*Yq5V5laVKBeLfIVDQi8Feg.png)
](https://medium.com/mitb-for-all?source=post_page---post_publication_info--5d470fb51e29---------------------------------------)
[## Published inMITB For All
](https://medium.com/mitb-for-all?source=post_page---post_publication_info--5d470fb51e29---------------------------------------)
[286 followers](https://medium.com/mitb-for-all/followers?source=post_page---post_publication_info--5d470fb51e29---------------------------------------)
·[Last publishedDec 13, 2025](https://medium.com/mitb-for-all/bridging-the-language-gap-technical-approaches-for-multilingual-ai-in-southeast-asia-f5e52d5dacae?source=post_page---post_publication_info--5d470fb51e29---------------------------------------)
Tech contents related to AI, Analytics, Fintech, and Digital Transformation. Written by MITB Alumni; open-access for everyone.
[
![James Koh, PhD](https://miro.medium.com/v2/resize:fill:96:96/1*rGYsYYA8SAiY3-6gN2xkQw.jpeg)
](https://medium.com/@byjameskoh?source=post_page---post_author_info--5d470fb51e29---------------------------------------)
[
![James Koh, PhD](https://miro.medium.com/v2/resize:fill:128:128/1*rGYsYYA8SAiY3-6gN2xkQw.jpeg)
](https://medium.com/@byjameskoh?source=post_page---post_author_info--5d470fb51e29---------------------------------------)
[## Written byJames Koh, PhD
](https://medium.com/@byjameskoh?source=post_page---post_author_info--5d470fb51e29---------------------------------------)
[1.1K followers](https://medium.com/@byjameskoh/followers?source=post_page---post_author_info--5d470fb51e29---------------------------------------)
·[54 following](https://medium.com/@byjameskoh/following?source=post_page---post_author_info--5d470fb51e29---------------------------------------)
Data Science Instructor - teaching Masters students
## No responses yet
[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--5d470fb51e29---------------------------------------)
[
Help
](https://help.medium.com/hc/en-us?source=post_page-----5d470fb51e29---------------------------------------)
[
Status
](https://status.medium.com/?source=post_page-----5d470fb51e29---------------------------------------)
[
About
](https://medium.com/about?autoplay=1&amp;source=post_page-----5d470fb51e29---------------------------------------)
[
Careers
](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----5d470fb51e29---------------------------------------)
[
Press
](mailto:pressinquiries@medium.com)
[
Blog
](https://blog.medium.com/?source=post_page-----5d470fb51e29---------------------------------------)
[
Privacy
](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5d470fb51e29---------------------------------------)
[
Rules
](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----5d470fb51e29---------------------------------------)
[
Terms
](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5d470fb51e29---------------------------------------)
[
Text to speech
](https://speechify.com/medium?source=post_page-----5d470fb51e29---------------------------------------)
