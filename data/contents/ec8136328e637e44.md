# Memory | LlamaIndex Python Documentation

**URL:** https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/
**Published:** 2018-06-09T00:00:00.000Z

---

## Summary

The provided web page is documentation for **Memory** within the **LlamaIndex Python Documentation**.

Here is a summary of the page relevant to your query:

*   **Agent Memory/Agentic Memory:** Memory is described as a core component of agentic systems, allowing for the storage and retrieval of past information.
*   **LlamaIndex Framework:** The documentation details how memory is implemented and customized within LlamaIndex agents.
*   **Memory Classes:** It discusses the current standard, the `Memory` class (which replaces the deprecated `ChatMemoryBuffer`), offering both short-term (FIFO queue of messages) and optional long-term memory.
*   **Short-Term Memory Configuration:** This is managed by setting `token_limit`, `chat_history_token_ratio`, and `token_flush_size`.
*   **Long-Term Memory (Memory Blocks):** Long-term memory is handled via `MemoryBlock` objects that process flushed short-term messages. Predefined blocks include:
    *   `StaticMemoryBlock`
    *   `FactExtractionMemoryBlock` (uses an LLM to extract facts)
    *   `VectorMemoryBlock` (stores/retrieves messages from a vector database)
*   **Customization:** Users can create custom memory blocks (e.g., `MentionCounter`).
*   **Remote Memory:** The default in-memory database can be replaced with remote databases (e.g., PostgreSQL) by changing the `async_database_uri`.
*   **Integration with Agents:** Memory is configured when running an agent (e.g., `FunctionAgent`) by passing the memory object into the `run()` method.
*   **Memory vs. Workflow Context:** It distinguishes the simpler `Memory` object (holding chat history/blocks) from the more complex, serializable `Context` object used in workflows.

**Regarding other terms in your query:**

The document focuses exclusively on **Memory** within the **LlamaIndex** framework. It does **not** mention:

*   MCP servers
*   Tool use (though agents using memory often use tools, this page doesn't detail tool use itself)
*   Agent frameworks (other than LlamaIndex itself)
*   OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK
*   Function calling
*   Structured outputs
*   Agent orchestration

---

## Full Content

Memory | LlamaIndex Python Documentation[Skip to content](#_top)
[![](https://docs.llamaindex.ai/python/_astro/llamaindex-light.m3GU97oV.svg)![](https://docs.llamaindex.ai/python/_astro/llamaindex-dark.B8ExLyh7.svg)LlamaIndex Python Documentation](https://docs.llamaindex.ai/)
SearchCtrlK
Cancel
[TypeScript](https://docs.llamaindex.ai/typescript/framework/)
[Twitter](https://x.com/llama_index)[LinkedIn](https://www.linkedin.com/company/llamaindex)[Bluesky](https://bsky.app/profile/llamaindex.bsky.social)[GitHub](https://github.com/run-llama/llama_index/)
Select themeDarkLightAuto
# Memory
## Concept
[Section titled “Concept”](#concept)
Memory is a core component of agentic systems. It allows you to store and retrieve information from the past.
In LlamaIndex, you can typically customize memory by using an existing`BaseMemory`class, or by creating a custom one.
As the agent runs, it will make calls to`memory.put()`to store information, and`memory.get()`to retrieve information.
**NOTE:**The`ChatMemoryBuffer`is deprecated. In a future release, the default will be replaced with the`Memory`class, which is more flexible and allows for more complex memory configurations. Examples in this section will use the`Memory`class. By default, the`ChatMemoryBuffer`is used across the framework, to create a basic buffer of chat history, which gives the agent the last X messages that fit into a token limit. The`Memory`class operates similarly, but is more flexible and allows for more complex memory configurations.
## Usage
[Section titled “Usage”](#usage)
Using the`Memory`class, you can create a memory that has both short-term memory (i.e. a FIFO queue of messages) and optionally long-term memory (i.e. extracting information over time).
### Configuring Memory for an Agent
[Section titled “Configuring Memory for an Agent”](#configuring-memory-for-an-agent)
You can set the memory for an agent by passing it into the`run()`method:
```
`
fromllama\_index.core.agent.workflowimportFunctionAgent
fromllama\_index.core.memoryimportMemory
memory=Memory.from\_defaults(session\_id="my\_session",token\_limit=40000)
agent=FunctionAgent(llm=llm,tools=tools)
response=awaitagent.run("&#x3C;&#x3C;question that invokes tool\>",memory=memory)
`
```
### Managing the Memory Manually
[Section titled “Managing the Memory Manually”](#managing-the-memory-manually)
You can also manage the memory manually by calling`memory.put\_messages()`and`memory.get()`directly, and passing in the chat history.
```
`
fromllama\_index.core.agent.workflowimportFunctionAgent
fromllama\_index.core.llmsimportChatMessage
fromllama\_index.core.memoryimportMemory
memory=Memory.from\_defaults(session\_id="my\_session",token\_limit=40000)
memory.put\_messages(
[
ChatMessage(role="user",content="Hello, world!"),
ChatMessage(role="assistant",content="Hello, world to you too!"),
]
)
chat\_history=memory.get()
agent=FunctionAgent(llm=llm,tools=tools)
# passing in the chat history overrides any existing memory
response=awaitagent.run(
"&#x3C;&#x3C;question that invokes tool\>",chat\_history=chat\_history
)
`
```
### Retrieving the Latest Memory from an Agent
[Section titled “Retrieving the Latest Memory from an Agent”](#retrieving-the-latest-memory-from-an-agent)
You can get the latest memory from an agent by grabbing it from the agent context:
```
`
fromllama\_index.core.workflowimportContext
ctx=Context(agent)
response=awaitctx.run("&#x3C;&#x3C;question that invokes tool\>",ctx=ctx)
# get the memory
memory=awaitctx.store.get("memory")
chat\_history=memory.get()
`
```
## Customizing Memory
[Section titled “Customizing Memory”](#customizing-memory)
### Short-Term Memory
[Section titled “Short-Term Memory”](#short-term-memory)
By default, the`Memory`class will store the last X messages that fit into a token limit. You can customize this by passing in`token\_limit`and`chat\_history\_token\_ratio`arguments to the`Memory`class.
* `token\_limit`(default: 30000): The maximum number of short-term and long-term tokens to store.
* `chat\_history\_token\_ratio`(default: 0.7): The ratio of tokens in the short-term chat history to the total token limit. If the chat history exceeds this ratio, the oldest messages will be flushed into long-term memory (if long-term memory is enabled).
* `token\_flush\_size`(default: 3000): The number of tokens to flush into long-term memory when the chat history exceeds the token limit.
```
`
memory=Memory.from\_defaults(
session\_id="my\_session",
token\_limit=40000,
chat\_history\_token\_ratio=0.7,
token\_flush\_size=3000,
)
`
```
### Long-Term Memory
[Section titled “Long-Term Memory”](#long-term-memory)
Long-term memory is represented as`Memory Block`objects. These objects receive the messages that are flushed from short-term memory, and optionally process them to extract information. Then when memory is retrieved, the short-term and long-term memories are merged together.
Currently, there are three predefined memory blocks:
* `StaticMemoryBlock`: A memory block that stores a static piece of information.
* `FactExtractionMemoryBlock`: A memory block that extracts facts from the chat history.
* `VectorMemoryBlock`: A memory block that stores and retrieves batches of chat messages from a vector database.
By default, depending on the`insert\_method`argument, the memory blocks will be inserted into the system message or the latest user message.
This sounds a bit complicated, but it’s actually quite simple. Let’s look at an example:
```
`
fromllama\_index.core.memoryimport(
StaticMemoryBlock,
FactExtractionMemoryBlock,
VectorMemoryBlock,
)
blocks=[
StaticMemoryBlock(
name="core\_info",
static\_content="My name is Logan, and I live in Saskatoon. I work at LlamaIndex.",
priority=0,
),
FactExtractionMemoryBlock(
name="extracted\_info",
llm=llm,
max\_facts=50,
priority=1,
),
VectorMemoryBlock(
name="vector\_memory",
# required: pass in a vector store like qdrant, chroma, weaviate, milvus, etc.
vector\_store=vector\_store,
priority=2,
embed\_model=embed\_model,
# The top-k message batches to retrieve
# similarity\_top\_k=2,
# optional: How many previous messages to include in the retrieval query
# retrieval\_context\_window=5
# optional: pass optional node-postprocessors for things like similarity threshold, etc.
# node\_postprocessors=[...],
),
]
`
```
Here, we’ve setup three memory blocks:
* `core\_info`: A static memory block that stores some core information about the user. The static content can either be a string or a list of`ContentBlock`objects like`TextBlock`,`ImageBlock`, etc. This information will always be inserted into the memory.
* `extracted\_info`: An extracted memory block that will extract information from the chat history. Here we’ve passed in the`llm`to use to extract facts from the flushed chat history, and set the`max\_facts`to 50. If the number of extracted facts exceeds this limit, the`max\_facts`will be automatically summarized and reduced to leave room for new information.
* `vector\_memory`: A vector memory block that will store and retrieve batches of chat messages from a vector database. Each batch is a list of the flushed chat messages. Here we’ve passed in the`vector\_store`and`embed\_model`to use to store and retrieve the chat messages.
You’ll also notice that we’ve set the`priority`for each block. This is used to determine the handling when the memory blocks content (i.e. long-term memory) + short-term memory exceeds the token limit on the`Memory`object.
When memory blocks get too long, they are automatically “truncated”. By default, this just means they are removed from memory until there is room again. This can be customized with subclasses of memory blocks that implement their own truncation logic.
* `priority=0`: This block will always be kept in memory.
* `priority=1, 2, 3, etc`: This determines the order in which memory blocks are truncated when the memory exceeds the token limit, to help the overall short-term memory + long-term memory content be less than or equal to the`token\_limit`.
Now, let’s pass these blocks into the`Memory`class:
```
`
memory=Memory.from\_defaults(
session\_id="my\_session",
token\_limit=40000,
memory\_blocks=blocks,
insert\_method="system",
)
`
```
As the memory is used, the short-term memory will fill up. Once the short-term memory exceeds the`chat\_history\_token\_ratio`, the oldest messages that fit into the`token\_flush\_size`will be flushed and sent to each memory block for processing.
When memory is retrieved, the short-term and long-term memories are merged together. The`Memory`object will ensure that the short-term memory + long-term memory content is less than or equal to the`token\_limit`. If it is longer, the`.truncate()`method will be called on the memory blocks, using the`priority`to determine the truncation order.
Tip
By default, tokens are counted using tiktoken. To customize this, you can set
the`tokenizer\_fn`argument to a custom callable that given a string, returns
a list. The length of the list is then used to determine the token count.
Once the memory has collected enough information, we might see something like this from the memory:
```
`
# optionally pass in a list of messages to get, which will be forwarded to the memory blocks
chat\_history=memory.get(messages=[...])
print(chat\_history[0].content)
`
```
Which will print something like this:
```
`
&#x3C;&#x3C;memory\>
&#x3C;&#x3C;static\_memory\>
My name is Logan, and I live in Saskatoon. I work at LlamaIndex.
&#x3C;&#x3C;/static\_memory\>
&#x3C;&#x3C;fact\_extraction\_memory\>
&#x3C;&#x3C;fact\>Fact 1&#x3C;&#x3C;/fact\>
&#x3C;&#x3C;fact\>Fact 2&#x3C;&#x3C;/fact\>
&#x3C;&#x3C;fact\>Fact 3&#x3C;&#x3C;/fact\>
&#x3C;&#x3C;/fact\_extraction\_memory\>
&#x3C;&#x3C;retrieval\_based\_memory\>
&#x3C;&#x3C;message role='user'\>Msg 1&#x3C;&#x3C;/message\>
&#x3C;&#x3C;message role='assistant'\>Msg 2&#x3C;&#x3C;/message\>
&#x3C;&#x3C;message role='user'\>Msg 3&#x3C;&#x3C;/message\>
&#x3C;&#x3C;/retrieval\_based\_memory\>
&#x3C;&#x3C;/memory\>
`
```
Here, the memory was inserted into the system message, with specific sections for each memory block.
## Customizing Memory Blocks
[Section titled “Customizing Memory Blocks”](#customizing-memory-blocks)
While predefined memory blocks are available, you can also create your own custom memory blocks.
```
`
fromtypingimportOptional, List, Any
fromllama\_index.core.llmsimportChatMessage
fromllama\_index.core.memory.memoryimportBaseMemoryBlock
# use generics to define the output type of the memory block
# can be str or List[ContentBlock]
classMentionCounter(BaseMemoryBlock[str]):
"""
A memory block that counts the number of times a user mentions a specific name.
"""
mention\_name:str="Logan"
mention\_count:int=0
asyncdef\_aget(
self,messages: Optional[List[ChatMessage]]=None,\*\*block\_kwargs: Any
)-\>str:
returnf"Logan was mentioned{self.mention\_count}times."
asyncdef\_aput(self,messages: List[ChatMessage])-\>None:
formessageinmessages:
ifself.mention\_nameinmessage.content:
self.mention\_count+=1
asyncdefatruncate(
self,content:str,tokens\_to\_truncate:int
)-\> Optional[str]:
return""
`
```
Here, we’ve defined a memory block that counts the number of times a user mentions a specific name.
Its truncate method is basic, just returning an empty string.
### Remote Memory
[Section titled “Remote Memory”](#remote-memory)
By default, the`Memory`class is using an in-memory SQLite database. You can plug in any remote database by changing the database URI.
You can customize the table name, and also optionally pass in an async engine directly. This is useful for managing your own connection pool.
```
`
fromllama\_index.core.memoryimportMemory
memory=Memory.from\_defaults(
session\_id="my\_session",
token\_limit=40000,
async\_database\_uri="postgresql+asyncpg://postgres:mark90@localhost:5432/postgres",
# Optional: specify a table name
# table\_name="memory\_table",
# Optional: pass in an async engine directly
# this is useful for managing your own connection pool
# async\_engine=engine,
)
`
```
## Memory vs. Workflow Context
[Section titled “Memory vs. Workflow Context”](#memory-vs-workflow-context)
At this point in the documentation, you may have encountered cases where you are using a Workflow and are serializing a`Context`object to save and resume a specific workflow state. The workflow`Context`is a complex object that holds runtime information about the workflow, as well as key/value pairs that are shared across workflow steps.
In comparison, the`Memory`object is a simpler object, holding only`ChatMessage`objects, and optionally a list of`MemoryBlock`objects for long-term memory.
In most practical cases, you will end up using both. If you aren’t customizing the memory, then serializing the`Context`object will be sufficient.
```
`
fromllama\_index.core.workflowimportContext
ctx=Context(workflow)
# serialize the context
ctx\_dict=ctx.to\_dict()
# deserialize the context
ctx=Context.from\_dict(workflow,ctx\_dict)
`
```
In other cases, like when using`FunctionAgent`,`AgentWorkflow`, or`ReActAgent`, if you customize the memory, then you will want to provide that as a separate runtime argument (especially since beyond the default, the`Memory`object is not serializable).
```
`
response=awaitagent.run("Hello!",memory=memory)
`
```
Lastly, there are cases ([like human-in-the-loop](https://docs.llamaindex.ai/python/framework/understanding/agent/human_in_the_loop)) where you will need to provide both the`Context`(to resume the workflow) and the`Memory`(to store the chat history).
```
`
response=awaitagent.run("Hello!",ctx=ctx,memory=memory)
`
```
## (Deprecated) Memory Types
[Section titled “(Deprecated) Memory Types”](#deprecated-memory-types)
In`llama\_index.core.memory`, we offer a few different memory types:
* `ChatMemoryBuffer`: A basic memory buffer that stores the last X messages that fit into a token limit.
* `ChatSummaryMemoryBuffer`: A memory buffer that stores the last X messages that fit into a token limit, and also summarizes the conversation periodically when it gets too long.
* `VectorMemory`: A memory that stores and retrieves chat messages from a vector database. It makes no guarantees about the order of the messages, and returns the most similar messages to the latest user message.
* `SimpleComposableMemory`: A memory that composes multiple memories together. Usually used to combine`VectorMemory`with`ChatMemoryBuffer`or`ChatSummaryMemoryBuffer`.
## Examples
[Section titled “Examples”](#examples)
You can find a few examples of memory in action below:
* [Memory](https://docs.llamaindex.ai/python/examples/memory/memory)
* [Manipulating Memory at Runtime](https://docs.llamaindex.ai/python/examples/memory/custom_memory)
* [Limiting Multi-Turn Confusion with Custom Memory](https://docs.llamaindex.ai/python/examples/memory/custom_multi_turn_memory)
**NOTE:**Deprecated examples:
* [Chat Memory Buffer](https://docs.llamaindex.ai/python/examples/agent/memory/chat_memory_buffer)
* [Chat Summary Memory Buffer](https://docs.llamaindex.ai/python/examples/agent/memory/summary_memory_buffer)
* [Composable Memory](https://docs.llamaindex.ai/python/examples/agent/memory/composable_memory)
* [Vector Memory](https://docs.llamaindex.ai/python/examples/agent/memory/vector_memory)
* [Mem0 Memory](https://docs.llamaindex.ai/python/examples/memory/mem0memory)
