# Reasoning about Uncertainty: 
 Do Reasoning Models Know When They Don‚Äôt Know?

**URL:** https://arxiv.org/html/2506.18183v1
**Published:** 2025-12-17T00:00:00.000Z

---

## Summary

The user query asks for a summary related to several topics concerning Reasoning LLMs, including chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction/detection, grounding, and factuality.

The provided webpage focuses heavily on **Reasoning LLMs**, **chain-of-thought (CoT)**, **inference-time compute** (referred to as "deeper reasoning" or "inference-time scaling"), **self-reflection** (introduced as "introspective uncertainty quantification"), **hallucination reduction and detection** (addressed via uncertainty quantification and calibration), and **factuality** (assessed using benchmarks like SimpleQA).

Here is a summary of the key findings from the page relevant to the query:

**Reasoning LLMs and Chain-of-Thought (CoT):**
*   Reasoning language models (like Claude 3.7 Sonnet, DeepSeek R1, Gemini 2 Flash Thinking, and o3-Mini) achieve SOTA performance by using multi-step reasoning, often induced via reinforcement learning or CoT prompting.
*   The study investigates the calibration (trustworthiness) of these models when they self-verbalize their confidence.

**Inference-Time Compute / Deeper Reasoning (Test-Time Scaling):**
*   Deeper reasoning (scaling reasoning effort/tokens) generally leads to **higher accuracy**.
*   However, as accuracy saturates, deeper reasoning causes models to become **even more overconfident** (calibration errors increase, especially ECE), particularly on wrongly-answered questions.
*   Increased reasoning depth can also lead to a **decrease in the completion rate** as models struggle with format instructions.

**Self-Reflection (Introspective Uncertainty Quantification - IUQ):**
*   The paper introduces **Introspective Uncertainty Quantification (IUQ)**, where a model reasons about its own chain-of-thought trace to update its confidence, mimicking human self-checking.
*   More critical introspection (IUQ-Medium and IUQ-High, which explicitly ask the model to find flaws) generally **improves calibration** (reduces miscalibration/overconfidence) for models like o3-Mini and DeepSeek R1, especially on challenging datasets.
*   However, this introspection **degrades the calibration of Claude 3.7 Sonnet**.

**Hall

The user query asks for a summary related to several topics in **reasoning and planning with LLMs**, including: Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction/detection, grounding, and factuality.

The provided webpage focuses on the **calibration and uncertainty quantification (UQ)** of **reasoning models**.

Here is a summary of the relevant points from the text concerning the user's query topics:

*   **Reasoning LLMs & Calibration:** The paper examines the calibration of reasoning models, finding that they are generally **overconfident** and become even more so with deeper reasoning, especially when accuracy doesn't improve.
*   **Chain-of-Thought (CoT) Prompting:** The study explored CoT prompting alongside basic, multi-step, and top-K prompts. It found that **CoT prompting and multi-step prompting do not significantly outperform the basic prompt strategy** for reasoning models. This is because reasoning models are already trained to generate reasoning traces by default, unlike non-reasoning models where CoT is effective for eliciting reasoning.
*   **Self-Reflection (Introspection):** Critical introspection (a form of self-reflection) **enables models to identify flaws and hallucinations** in their reasoning traces, leading to improved calibration by reducing overconfidence (though Claude became more overconfident).
*   **Hallucination Reduction/Detection:** Introspection is shown to help models identify flaws and **hallucinations** in their reasoning traces, which aids in confidence reassessment.
*   **Planning with LLMs / MCTS:** While the paper discusses reasoning models and their traces, it does not explicitly detail planning with LLMs or the use of MCTS (Monte Carlo Tree Search) for language models.
*   **Factuality/Grounding:** The paper focuses on *confidence* calibration rather than explicitly measuring grounding or factuality, although improving calibration is related to reducing incorrect outputs (hallucinations).
*   **Inference-time Compute / Test-time Scaling:** These topics are not directly addressed, although introspection and multi-step prompting relate to increased inference-time computation.

**In summary:** The page primarily discusses the calibration of reasoning models, finding that specialized prompt strategies like Chain-of-Thought do not significantly improve performance over basic prompting for these

---

## Full Content

Reasoning about Uncertainty: Do Reasoning Models Know When They Don‚Äôt Know?
1]Princeton University\\contribution[\*]Equal contribution.
# Reasoning about Uncertainty:
Do Reasoning Models Know When They Don‚Äôt Know?
Zhiting MeiChristina ZhangTenny YinJustin LidardOla ShorinwaAnirudha Majumdar[
###### Abstract
Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks,
enabled by multi-step reasoning induced using reinforcement learning. However, like previous language models, reasoning models are prone to generating confident, plausible responses that are incorrect (hallucinations). Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications. To this end, we explore uncertainty quantification of reasoning models in this work. Specifically, we ask three fundamental questions: First,*are reasoning models well-calibrated?*Second,*does deeper reasoning improve model calibration?*Finally, inspired by humans‚Äô innate ability to double-check their thought processes to verify the validity of their answers and their confidence, we ask:*can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces?*We introduce*introspective*uncertainty quantification (UQ) to explore this direction.
In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, we find that reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than85%percent8585\\%85 %particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated).
Lastly, we conclude with important research directions to design necessary UQ benchmarks and improve the calibration of reasoning models.
###### keywords:
Reasoning about Uncertainty, Reasoning Models, LLMs, Uncertainty Quantification.
![Refer to caption](x1.png)Figure 1:To examine if reasoning models know when they don‚Äôt know, we ask three fundamental questions: (i) are reasoning models well-calibrated? (ii) does deeper reasoning improve model calibration? and (iii) can introspection improve calibration? We find that reasoning models are typically overconfident and become more overconfident with deeper reasoning. However, through introspection, reasoning models can provide better calibrated confidence estimates.
## 1Introduction
Recent breakthroughs in large language models (LLMs) and vision-language models (VLMs) have largely been driven by*reasoning*language models‚Äîlanguage models that are trained to perform multi-step reasoning through reinforcement learning, e.g., Claude 3.7> [
[> 2
](https://arxiv.org/html/2506.18183v1#bib.bib2)> ]
, o3-Mini> [
[> 37
](https://arxiv.org/html/2506.18183v1#bib.bib37)> ]
, Gemini 2 Flash Thinking> [
[> 44
](https://arxiv.org/html/2506.18183v1#bib.bib44)> ]
, and DeepSeek R1> [
[> 18
](https://arxiv.org/html/2506.18183v1#bib.bib18)> ]
. By breaking down complex tasks into subtasks and analyzing and synthesizing responses from the resulting subtasks, reasoning models have achieved new state-of-the-art (SOTA) performance on a broad range of challenging real-world tasks, such as programming, e.g., Codeforces> [
[> 34
](https://arxiv.org/html/2506.18183v1#bib.bib34)> ]
; math, e.g., AIME> [
[> 3
](https://arxiv.org/html/2506.18183v1#bib.bib3)> ]
; engineering and science, e.g., GPQA> [
[> 38
](https://arxiv.org/html/2506.18183v1#bib.bib38)> ]
; and arts, e.g., MMLU> [
[> 21
](https://arxiv.org/html/2506.18183v1#bib.bib21)> ]
, outperforming domain experts on many benchmarks.
Despite their remarkable capabilities, SOTA reasoning language models often generate confident but factually-incorrect responses to questions, known as*hallucinations*> [
[> 5
](https://arxiv.org/html/2506.18183v1#bib.bib5)> , [> 31
](https://arxiv.org/html/2506.18183v1#bib.bib31)> ]
, similar to their non-reasoning counterparts.
Consequently, the safe integration of reasoning models into real-world tasks requires knowing when and how much to trust these models, i.e., uncertainty quantification (UQ) of these models.
Prior work has examined UQ of non-reasoning models, showing that non-reasoning models are poorly calibrated in general> [
[> 52
](https://arxiv.org/html/2506.18183v1#bib.bib52)> ]
.
However, UQ of reasoning models remains an unexplored frontier. To address this knowledge gap, we examine uncertainty quantification of reasoning models to better evaluate their trustworthiness.
Specifically, in this work, we ask three critical questions (illustrated in[Figure1](https://arxiv.org/html/2506.18183v1#S0.F1)).
First,*are reasoning models well-calibrated?*We evaluate the calibration of reasoning models across a broad range of benchmarks
to examine underconfidence/overconfidence in these models across different problem domains.
Our findings reveal that*reasoning models are typically overconfident,*often expressing confidence estimates greater than85%percent8585\\%85 %(in increments of5%percent55\\%5 %) even when they are incorrect.
Given that overconfidence is often masked by high accuracy, we find that reasoning models are well-calibrated in knowledge-retrieval tasks and poorly calibrated in tasks requiring domain expertise, e.g., graduate-level research domains. Moreover, our results highlight that prompt engineering is significantly less effective with reasoning models, unlike non-reasoning models.
Second,*does deeper reasoning improve model calibration?*Prior work> [
[> 37
](https://arxiv.org/html/2506.18183v1#bib.bib37)> ]
has shown that reasoning models can achieve higher accuracy through inference-time scaling of reasoning effort, measured by the number of available reasoning tokens. Here, we explore the effects of deeper reasoning on model calibration across low, medium, and high amounts of reasoning. Our evaluations reveal that*reasoning models become more overconfident with deeper reasoning.*Specifically on wrongly-answered questions, reasoning models become significantly more overconfident as we scale the reasoning effort.
Lastly, humans often double-check their problem-solving approach to verify their answers and estimate their confidence in the validity of the answers> [
[> 22
](https://arxiv.org/html/2506.18183v1#bib.bib22)> ]
. In line with this observation, we ask the question:*can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces?*We introduce*introspective*uncertainty quantification to explore this question. Introspective UQ utilizes a multi-stage reasoning paradigm to estimate the confidence in the accuracy of a given response: a model first reasons about a given question, and then a second model reasons about the chain of thought and final answer produced by the first model in order to assess its confidence in the answer. We find that*more critical introspection improves the calibration of reasoning models, e.g., o3-Mini and DeepSeek R1, although the calibration of Claude 3.7 Sonnet worsens.*
## 2Related Work
Reasoning in LLMs.Prior work> [
[> 48
](https://arxiv.org/html/2506.18183v1#bib.bib48)> , [> 28
](https://arxiv.org/html/2506.18183v1#bib.bib28)> , [> 47
](https://arxiv.org/html/2506.18183v1#bib.bib47)> ]
has demonstrated that language models can achieve significantly higher accuracy by generating intermediate reasoning steps, known as a chain of thought (CoT). Chain-of-thought prompting enables remarkable zero-shot or few-shot learning in LLMs, enabling generalist LLMs to outperform task-specific, finetuned models> [
[> 48
](https://arxiv.org/html/2506.18183v1#bib.bib48)> ]
. However, the complexity of the few-shot CoT examples significantly influences the induced accuracy of the models> [
[> 12
](https://arxiv.org/html/2506.18183v1#bib.bib12)> ]
, spurring research on prompt engineering for effective chain-of-thought reasoning> [
[> 55
](https://arxiv.org/html/2506.18183v1#bib.bib55)> , [> 23
](https://arxiv.org/html/2506.18183v1#bib.bib23)> , [> 20
](https://arxiv.org/html/2506.18183v1#bib.bib20)> ]
.
Subsequent work introduces multi-path reasoning strategies to enable backtracking on incorrect reasoning paths, e.g., Tree-of-Thought prompting> [
[> 53
](https://arxiv.org/html/2506.18183v1#bib.bib53)> ]
and Graph-of-Thoughts prompting> [
[> 7
](https://arxiv.org/html/2506.18183v1#bib.bib7)> ]
which constructs parallel reasoning traces; least-to-most prompting> [
[> 56
](https://arxiv.org/html/2506.18183v1#bib.bib56)> ]
, which solves a series of subproblems in sequence; and reasoning via planning> [
[> 19
](https://arxiv.org/html/2506.18183v1#bib.bib19)> ]
, which performs Monte Carlo Tree Search on a reasoning tree to find the optimal reasoning path.
However, existing approaches require domain expertise to construct effective chain-of-thought prompts/examples, posing a challenge.
By training language models to reason using reinforcement learning (RL), recent work> [
[> 25
](https://arxiv.org/html/2506.18183v1#bib.bib25)> , [> 18
](https://arxiv.org/html/2506.18183v1#bib.bib18)> , [> 2
](https://arxiv.org/html/2506.18183v1#bib.bib2)> , [> 37
](https://arxiv.org/html/2506.18183v1#bib.bib37)> , [> 44
](https://arxiv.org/html/2506.18183v1#bib.bib44)> , [> 45
](https://arxiv.org/html/2506.18183v1#bib.bib45)> ]
has shown that reasoning language models can solve complex tasks more accurately than non-reasoning LLMs. Through inference-time scaling, reasoning models can be made to think deeper to break down complex problems into multiple steps, achieving SOTA performance on many benchmarks> [
[> 2
](https://arxiv.org/html/2506.18183v1#bib.bib2)> , [> 37
](https://arxiv.org/html/2506.18183v1#bib.bib37)> , [> 44
](https://arxiv.org/html/2506.18183v1#bib.bib44)> , [> 18
](https://arxiv.org/html/2506.18183v1#bib.bib18)> ]
. However, the effects of reasoning on the calibration of confidence estimates of these models remains unknown, motivating uncertainty quantification of reasoning models.
Uncertainty Quantification of LLMs.Uncertainty quantification of LLMs has gained notable prominence given the tendency of LLMs to hallucinate. In general, existing UQ methods for LLMs eschew traditional techniques such as Bayesian networks> [
[> 26
](https://arxiv.org/html/2506.18183v1#bib.bib26)> ]
due to their computational cost, given that LLMs often have billions to trillions of parameters.
UQ methods for LLMs can be grouped into two broad classes:*white-box*and*black-box*methods> [
[> 39
](https://arxiv.org/html/2506.18183v1#bib.bib39)> ]
. White-box methods utilize the internal outputs of LLMs> [
[> 14
](https://arxiv.org/html/2506.18183v1#bib.bib14)> , [> 4
](https://arxiv.org/html/2506.18183v1#bib.bib4)> ]
, such as tokens for entropy/perplexity-based UQ> [
[> 32
](https://arxiv.org/html/2506.18183v1#bib.bib32)> , [> 10
](https://arxiv.org/html/2506.18183v1#bib.bib10)> , [> 30
](https://arxiv.org/html/2506.18183v1#bib.bib30)> , [> 51
](https://arxiv.org/html/2506.18183v1#bib.bib51)> , [> 29
](https://arxiv.org/html/2506.18183v1#bib.bib29)> ]
or neural activations for mechanistic interpretability (MI)> [
[> 6
](https://arxiv.org/html/2506.18183v1#bib.bib6)> , [> 9
](https://arxiv.org/html/2506.18183v1#bib.bib9)> , [> 54
](https://arxiv.org/html/2506.18183v1#bib.bib54)> , [> 13
](https://arxiv.org/html/2506.18183v1#bib.bib13)> , [> 42
](https://arxiv.org/html/2506.18183v1#bib.bib42)> , [> 11
](https://arxiv.org/html/2506.18183v1#bib.bib11)> ]
. While these UQ metrics can be well calibrated for pre-trained models> [
[> 27
](https://arxiv.org/html/2506.18183v1#bib.bib27)> ]
, reinforcement learning from human feedback (RLHF) leads to poorly calibrated token-level UQ> [
[> 36
](https://arxiv.org/html/2506.18183v1#bib.bib36)> ]
since RL-training objectives are not proper scoring rules> [
[> 16
](https://arxiv.org/html/2506.18183v1#bib.bib16)> ]
.
In contrast to white-box methods, black-box methods estimate uncertainty without access to the model‚Äôs internal outputs which might not be available, given the notable shift from fully open-source language models to open-weight or closed-source models, e.g.,> [
[> 24
](https://arxiv.org/html/2506.18183v1#bib.bib24)> , [> 2
](https://arxiv.org/html/2506.18183v1#bib.bib2)> ]
. In fact, state-of-the-art closed-source reasoning models generally do not provide access to the models‚Äô logits, which is required for token-based, semantic similarity-based, or MI-based UQ.
Consequently, self-verbalized UQ methods> [
[> 33
](https://arxiv.org/html/2506.18183v1#bib.bib33)> , [> 41
](https://arxiv.org/html/2506.18183v1#bib.bib41)> , [> 43
](https://arxiv.org/html/2506.18183v1#bib.bib43)> , [> 1
](https://arxiv.org/html/2506.18183v1#bib.bib1)> ]
, which directly prompt an LLM for its confidence in natural language, have become increasingly important.
In contrast to prior work which has focused exclusively on non-reasoning models, we assess the uncertainty of reasoning models without finetuning, using self-verbalized UQ methods given the lack of access to the models‚Äô internal outputs.
## 3Preliminaries
Consider a model‚Ñ≥‚Ñ≥\\mathcal{M}caligraphic\_Mthat takes an input random variableX‚ààùí≥ùëãùí≥X\\in\\mathcal{X}italic\_X ‚ààcaligraphic\_Xand predicts an outputY^‚ààùí¥^ùëåùí¥{\\hat{Y}\\in\\mathcal{Y}}over^ start\_ARG italic\_Y end\_ARG ‚ààcaligraphic\_Y, along with a confidence scoreP^‚àà[0,1]^ùëÉ01\\hat{P}\\in[0,1]over^ start\_ARG italic\_P end\_ARG ‚àà[ 0 , 1 ]. We focus on settings with a deterministic ground-truthYùëåYitalic\_Ygiven the inputXùëãXitalic\_X.111This applies to a wide range of question-and-answering problems, e.g., multiple-choice questions, true-or-false questions, or any question that has an explicit correct answer.
The definition ofY^=Y^ùëåùëå\\hat{Y}=Yover^ start\_ARG italic\_Y end\_ARG = italic\_Yis not constrained to exact correspondence in form, i.e.,Y^=Y^ùëåùëå\\hat{Y}=Yover^ start\_ARG italic\_Y end\_ARG = italic\_YifY^^ùëå\\hat{Y}over^ start\_ARG italic\_Y end\_ARGandYùëåYitalic\_Yhave the same meaning. For example, both ‚Äútrue‚Äù and ‚ÄúTrue‚Äù are correct for a true-or-false question, ifYùëåYitalic\_Y=True, while for freeform Q&amp;A,Y^^ùëå\\hat{Y}over^ start\_ARG italic\_Y end\_ARGis correct if it captures the ground truth> [
[> 49
](https://arxiv.org/html/2506.18183v1#bib.bib49)> ]
. If the ground truth answerYùëåYitalic\_Yis inherently stochastic, then it should be expressed as a stochastic distribution described in natural language, for example, a Gaussian distribution withŒº=0ùúá0\\mu=0italic\_Œº = 0andœÉ2=1superscriptùúé21\\sigma^{2}=1italic\_œÉ start\_POSTSUPERSCRIPT 2 end\_POSTSUPERSCRIPT = 1.In this section, we provide a brief overview of calibration of uncertainty and calibration metrics.
Calibration of Uncertainty.The model‚Ñ≥‚Ñ≥\\mathcal{M}caligraphic\_Mis defined to be perfectly calibrated if its confidence estimateP^^ùëÉ\\hat{P}over^ start\_ARG italic\_P end\_ARGmatches the true probabilitypùëùpitalic\_pthat the associated answerY^^ùëå\\hat{Y}over^ start\_ARG italic\_Y end\_ARGis correct. Formally, perfect calibration> [
[> 17
](https://arxiv.org/html/2506.18183v1#bib.bib17)> ]
can be expressed as:
|‚Ñôùíµ‚Å¢[Y^=Y‚à£P^=p]=p,subscript‚Ñôùíµdelimited-[]^ùëåconditionalùëå^ùëÉùëùùëù\\mathbb{P}\_{\\mathcal{Z}}[\\hat{Y}={Y}\\mid\\hat{P}=p]=p,blackboard\_P start\_POSTSUBSCRIPT caligraphic\_Z end\_POSTSUBSCRIPT [ over^ start\_ARG italic\_Y end\_ARG = italic\_Y ‚à£over^ start\_ARG italic\_P end\_ARG = italic\_p ] = italic\_p ,||(1)|
where the probability is over the joint distribution onùíµ=ùí≥√óùí¥ùíµùí≥ùí¥\\mathcal{Z}=\\mathcal{X}\\times\\mathcal{Y}caligraphic\_Z = caligraphic\_X √ócaligraphic\_Y. However, achieving perfect calibration is practically impossible, motivating metrics that capture deviation from¬†([1](https://arxiv.org/html/2506.18183v1#S3.E1)) empirically.
Calibration Metrics.Calibration metrics evaluate how closely a model‚Äôs predicted confidence aligns with its true accuracy. Widely-used metrics include the expected calibration error (ECE) and maximum calibration error (MCE).
The ECE measures the average absolute difference between predicted confidence (right-hand side of ([1](https://arxiv.org/html/2506.18183v1#S3.E1))) and empirical accuracy (left-hand side of ([1](https://arxiv.org/html/2506.18183v1#S3.E1))) with:
|ùîºP^[|‚Ñô[Y=Y^‚à£P^=p]‚àíp|].\\mathbb{E}\_{\\hat{P}}\\left[|\\mathbb{P}[Y=\\hat{Y}\\mid\\hat{P}=p]-p|\\right].blackboard\_E start\_POSTSUBSCRIPT over^ start\_ARG italic\_P end\_ARG end\_POSTSUBSCRIPT [ | blackboard\_P [ italic\_Y = over^ start\_ARG italic\_Y end\_ARG ‚à£over^ start\_ARG italic\_P end\_ARG = italic\_p ] - italic\_p | ] .||(2)|
Since this expectation is generally intractable, the ECE is typically approximated by grouping the empirical confidence estimates intoMùëÄMitalic\_Mequal-width bins. Given a set of samplesBmsubscriptùêµùëöB\_{m}italic\_B start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPTwhose confidence falls into binmùëömitalic\_mwith the bin average accuracyacc‚Å¢(Bm)accsubscriptùêµùëö\\text{acc}(B\_{m})acc ( italic\_B start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT )and average confidenceconf‚Å¢(Bm)confsubscriptùêµùëö\\text{conf}(B\_{m})conf ( italic\_B start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT ), the ECE is approximated by:
|ECE=‚àëm=1M|Bm|n‚Å¢|acc‚Å¢(Bm)‚àíconf‚Å¢(Bm)|,ECEsuperscriptsubscriptùëö1ùëÄsubscriptùêµùëöùëõaccsubscriptùêµùëöconfsubscriptùêµùëö\\text{ECE}=\\sum\_{m=1}^{M}\\frac{|B\_{m}|}{n}\\left|\\text{acc}(B\_{m})-\\text{conf}(%
B\_{m})\\right|,ECE = ‚àëstart\_POSTSUBSCRIPT italic\_m = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_M end\_POSTSUPERSCRIPT divide start\_ARG | italic\_B start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT | end\_ARG start\_ARG italic\_n end\_ARG | acc ( italic\_B start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT ) - conf ( italic\_B start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT ) | ,||(3)|
wherenùëõnitalic\_nrepresents the total number of samples across all bins.
On the other hand, the MCE captures the worst-case discrepancy between confidence and accuracy, i.e., the maximum deviation between the left- and right-hand sides of ([1](https://arxiv.org/html/2506.18183v1#S3.E1)):
|maxp‚àà[0,1]|‚Ñô[Y=Y^‚à£P^=p]‚àíp|.\\max\_{p\\in[0,1]}\\left|\\mathbb{P}[Y=\\hat{Y}\\mid\\hat{P}=p]-p\\right|.roman\_max start\_POSTSUBSCRIPT italic\_p ‚àà[ 0 , 1 ] end\_POSTSUBSCRIPT | blackboard\_P [ italic\_Y = over^ start\_ARG italic\_Y end\_ARG ‚à£over^ start\_ARG italic\_P end\_ARG = italic\_p ] - italic\_p | .||(4)|
Similar to the ECE, we approximate the MCE by taking the largest bin-wise error> [
[> 35
](https://arxiv.org/html/2506.18183v1#bib.bib35)> ]
:
|MCE=maxm‚àà{1,‚Ä¶,M}‚Å°|acc‚Å¢(Bm)‚àíconf‚Å¢(Bm)|.MCEsubscriptùëö1‚Ä¶ùëÄaccsubscriptùêµùëöconfsubscriptùêµùëö\\text{MCE}=\\max\_{m\\in\\{1,...,M\\}}\\left|\\text{acc}(B\_{m})-\\text{conf}(B\_{m})%
\\right|.MCE = roman\_max start\_POSTSUBSCRIPT italic\_m ‚àà{ 1 , ‚Ä¶, italic\_M } end\_POSTSUBSCRIPT | acc ( italic\_B start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT ) - conf ( italic\_B start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT ) | .||(5)|
## 4Method
In order to effectively explore uncertainty quantification in reasoning language models, we consider a broad range of prompting strategies to examine the ability of reasoning models to accurately express their confidence. We draw insights from existing work on prompt engineering to identify potentially effective prompt designs for reasoning models. Beyond the established prompt strategies, we examine the effects of*introspection*on a reasoning model‚Äôs ability to quantify its uncertainty. We call this approach*introspective uncertainty quantification*. We discuss these strategies in the subsequent subsections.
### 4.1Prompt Strategies
We design prompts for self-verbalized uncertainty quantification of reasoning models following prompt templates for LLMs introduced in prior work> [
[> 52
](https://arxiv.org/html/2506.18183v1#bib.bib52)> ]
. Specifically, we consider zero-shot basic prompting, chain-of-thought prompting> [
[> 48
](https://arxiv.org/html/2506.18183v1#bib.bib48)> , [> 28
](https://arxiv.org/html/2506.18183v1#bib.bib28)> ]
, multi-step prompting> [
[> 52
](https://arxiv.org/html/2506.18183v1#bib.bib52)> ]
, and Top-K prompting> [
[> 46
](https://arxiv.org/html/2506.18183v1#bib.bib46)> ]
. We emphasize that the LLM is prompted zero-shot, i.e., without any examples. In[Table1](https://arxiv.org/html/2506.18183v1#S4.T1), we provide a template for each of these prompts to illustrate their composition, with the full prompts available in Appendix[9](https://arxiv.org/html/2506.18183v1#S9).
Table 1:Prompt Strategies for LLMs.|Strategy|Prompt|
Basic|Provide an answer
and the confidence in your answer between 0 and 100.|
Chain-of-Thought|Analyze step by step
and provide an answer
and confidence.|
Multi-Step|Break down the problem into multiple steps,
each with your confidence.|
Top-K|GiveKùêæKitalic\_Kbest guesses and your confidence in each guess.|
Basic Prompt Strategy.In the basic prompt strategy, we provide the question directly to the LLM and ask for its answer to the question. For example, in[Table1](https://arxiv.org/html/2506.18183v1#S4.T1), we ask the LLM to provide its answer and confidence without any description of the problem-solving approach.
Chain-of-Thought Prompt Strategy.Prior work> [
[> 48
](https://arxiv.org/html/2506.18183v1#bib.bib48)> , [> 28
](https://arxiv.org/html/2506.18183v1#bib.bib28)> , [> 55
](https://arxiv.org/html/2506.18183v1#bib.bib55)> ]
has shown that LLMs are highly capable zero-shot reasoners when asked to*think step by step*. Although prior work has demonstrated the effectiveness of chain-of-thought prompting in improving accuracy, the effects of chain-of-thought prompting on calibration of self-verbalized UQ estimates remain relatively unexplored. We explore this research direction in this paper. Specifically, we ask the LLM to*analyze*the question*step by step*to induce intermediate reasoning and ask for its confidence, as shown in[Table1](https://arxiv.org/html/2506.18183v1#S4.T1).
Multi-Step Prompt Strategy.The multi-step prompt strategy takes chain-of-thought prompting a step further. In the multi-step prompt strategy, we ask the LLM not only to reason about the problem, but to also provide its confidence at each step. By doing so, we hope to induce the LLM to reason about its confidence to improve the calibration of its self-verbalized confidence estimates. As shown in[Table1](https://arxiv.org/html/2506.18183v1#S4.T1), we ask the LLM to*break down*the problem into*multiple steps*and provide its answer and confidence in each step.
Top-K Prompt Strategy.Identifying candidate answers to a given question (and in particular, how likely these answers are to be correct) can result in better estimation of the model‚Äôs confidence, as demonstrated in prior work> [
[> 46
](https://arxiv.org/html/2506.18183v1#bib.bib46)> ]
, where the LLM is asked to provide its top-K guesses for the answer to a question and the associated confidence. We explore this prompt strategy for reasoning models in our work.
![Refer to caption](x2.png)Figure 2:Introspective UQ.Although the reasoning model is initially highly overconfident and wrong (as shown in[Figure1](https://arxiv.org/html/2506.18183v1#S0.F1)), through introspection, the model can identify flaws in its chain-of-thought traces to provide better calibrated confidence estimates, mitigating overconfidence.
### 4.2Introspective Uncertainty Quantification
We explore the effect of reasoning on the calibration of uncertainty quantification of reasoning models (an open question in LLM research).
Drawing inspiration from how humans double-check their thought processes to ascertain their confidence in a given response, we introduce*introspective uncertainty quantification*: a self-verbalized UQ method which utilizes a multi-stage reasoning architecture to reason about and update the confidence of reasoning models.[Figure2](https://arxiv.org/html/2506.18183v1#S4.F2)illustrates the two-stage introspective UQ procedure using the question shown in[Figure1](https://arxiv.org/html/2506.18183v1#S0.F1), where a reasoning model is asked to identify flaws in the prior reasoning trace and to provide its confidence on the correctness of the first answer. The model identifies weaknesses in the chain-of-thought traces and correctly reduces the confidence in the original response, ultimately improving its calibration.
At the initial stage of introspective UQ, we ask a reasoning model to answer a question and provide its confidence in the answer. In subsequent stages, we provide the reasoning model with the reasoning traces at earlier stages and ask the model to analyze these traces to identify flaws and to ultimately provide a confidence estimate in the answer provided in the initial stage. Specifically in the second stage, we consider three kinds of prompts for introspection defined by varying conservativeness¬†‚Äî¬†(i)*IUQ-Low*: a neutral prompt asking the model to reason about its uncertainty given earlier reasoning traces and associated confidence estimates (without any hints on possible flawed reasoning), (ii)*IUQ-Medium*: a more*conservative*prompt explicitly asking the model to find flaws in the input reasoning traces including the prior confidence estimates, and (iii)*IUQ-High*: the most*conservative*prompt which utilizes the same prompt as*IUQ-Medium*without the prior confidence estimates. We provide a template of these prompts in Appendix[Section8](https://arxiv.org/html/2506.18183v1#S8). Although introspective UQ can utilize many stages, we limit the number of stages to two in our work and leave an exhaustive examination of the optimal number of stages to future work.
We restrict the second stage of introspective UQ to*reasoning about uncertainty*and do not ask the model to update the answer to the original question for two key reasons: (i) to encourage focused reflection on prior reasoning traces, and (ii) to guard against hallucinations, especially in problems where the model is not confident. Prior work> [
[> 31
](https://arxiv.org/html/2506.18183v1#bib.bib31)> ]
has shown that language models tend to hallucinate or vacillate between different choices when prompted with a question that they are not confident about, e.g., in knowledge-retrieval problems. Such vacillations present unnecessary distractions, undermining the effectiveness of introspective UQ.
With introspective UQ, we seek to elicit human-like internal reflections in reasoning models for calibrated uncertainty quantification, which could be important in downstream applications such as hallucination detection and mitigation. However, we note that humans sometimes become unjustifiably more confident in their responses after analyzing their reasoning process> [
[> 40
](https://arxiv.org/html/2506.18183v1#bib.bib40)> ]
, a challenge that could also arise with reasoning models. We explore these questions in the experiments discussed in[Section5](https://arxiv.org/html/2506.18183v1#S5).
## 5Experiments
We evaluate the calibration and accuracy of reasoning models on benchmark datasets and assess the confidence estimates of reasoning models using the self-verbalized UQ methods presented in[Section4](https://arxiv.org/html/2506.18183v1#S4).
Specifically, we seek to answer the following questions:
1. 1.
Are reasoning models calibrated, systemically underconfident, or overconfident?
2. 2.
Is accuracy correlated with calibration (positively or negatively, strongly or weakly)?
3. 3.
Does deeper reasoning (e.g., greater number of reasoning steps) improve calibration?
4. 4.
Can reasoning models reason about their uncertainty for better calibration?
Lastly, we explore the effects of different prompt strategies on the calibration and accuracy of reasoning models.
### 5.1Evaluation Setup
We summarize the experiment setup, including the reasoning models, datasets, prompt strategies, and evaluation metrics.
Models.We evaluate SOTA reasoning models that are publicly accessible via an API. Specifically, we consider the following LLMs: (i) Claude 3.7 Sonnet (*claude-3-7-sonnet-20250219*, released 02/19/2025), (ii) DeepSeek R1 (*deepseek-reasoner*, released 01/20/2025), (iii) Gemini (*gemini-2.0-flash-thinking-exp*, released 12/19/2024), and (iv) o3-mini (*o3-mini*, released 01/31/2025). We do not benchmark Grok 3 Beta, since we were not able to get access to the closed beta.
Datasets.We consider five standard question-answering benchmark datasets spanning a wide variety of challenging tasks, including arithmetic, humanities, science, and professional knowledge problems. We evaluate the models on the following datasets: (i)¬†AI2 Reasoning Challenge (*ARC-Challenge*) Set> [
[> 8
](https://arxiv.org/html/2506.18183v1#bib.bib8)> ]
, the challenging subset of the ARC dataset; (ii)¬†Graduate-Level Google-Proof Q&amp;A (*GPQA*) Benchmark> [
[> 38
](https://arxiv.org/html/2506.18183v1#bib.bib38)> ]
, consisting of questions determined to be challenging by PhD-level domain experts; (iii)¬†Measuring Massive Multitask Language Understanding (*MMLU*) dataset> [
[> 21
](https://arxiv.org/html/2506.18183v1#bib.bib21)> ]
, which encompasses a broad range of fields; (iv)*StrategyQA*> [
[> 15
](https://arxiv.org/html/2506.18183v1#bib.bib15)> ]
, which requires implicit reasoning steps, and (v)*SimpleQA*> [
[> 49
](https://arxiv.org/html/2506.18183v1#bib.bib49)> ]
, which examines the factuality of LLMs. While SimpleQA consists of open-ended questions, all other datasets use the multiple-choice format. For adequate coverage of a range of tasks in MMLU, we select the abstract algebra, professional accounting, professional medicine, international law, and sociology subsets, representing the more challenging subsets.
Prompt Strategies and Metrics.We utilize the prompt strategies discussed in[Section4.1](https://arxiv.org/html/2506.18183v1#S4.SS1), namely: (i) basic prompting, (ii) chain-of-thought prompting, (iii) multi-step prompting, and (iv) top-K prompting (withK=5ùêæ5{K=5}italic\_K = 5). For each strategy, we quantify the calibration of the self-verbalized confidence estimates from the model using the expected calibration error (ECE) and maximum calibration error (MCE), introduced in[Section3](https://arxiv.org/html/2506.18183v1#S3). We additionally present the accuracy and the successful query-completion rate (completion) to show how well the models follow instructions.
Lastly, we visualize the reliability diagram associated with the estimated confidence scores to visually examine the distribution of the confidence scores. The reliability diagram visualizes the deviation between the left-hand aside and right-hand side of ([1](https://arxiv.org/html/2506.18183v1#S3.E1)), showing the deviation between the bin-wise confidence and accuracy which is indicative of model underconfidence or overconfidence.
### 5.2Calibration of Reasoning Models
Here, we assess the uncertainty of reasoning models to characterize their calibration across the aforementioned benchmarks.[Figure3](https://arxiv.org/html/2506.18183v1#S5.F3)summarizes the ECE, MCE, accuracy, and completion rate of each model across these problem domains, showing the average performance of each model over the four prompt strategies. We provide each model‚Äôs individual performance for each prompt strategy in Appendix[10](https://arxiv.org/html/2506.18183v1#S10).
In general, we find that reasoning models are not well-calibrated, particularly in challenging problem domains (e.g., SimpleQA). For example, the original GPT-4 without RLHF was better calibrated with an ECE of0.70.70.70.7on a subset of the MMLU dataset> [
[> 36
](https://arxiv.org/html/2506.18183v1#bib.bib36)> ]
, which is significantly lower than the ECE of all the models in[Figure3](https://arxiv.org/html/2506.18183v1#S5.F3). Specifically, reasoning models tend to be overconfident, although this issue is less obvious on benchmarks where models achieve near-perfect accuracy (e.g., ARC-Challenge). Next, we provide a detailed discussion of these results.
![Refer to caption](x3.png)Figure 3:Are reasoning models calibrated?Reasoning models are well-calibrated on the*ARC-Challenge*and*MMLU*benchmarks and poorly calibrated on*StrategyQA*,*GPQA*, and*SimpleQA*, suggesting better model calibration on older benchmarks and worse calibration on more recent benchmarks.
Model Calibration on Benchmarks.From[Figure3](https://arxiv.org/html/2506.18183v1#S5.F3), reasoning models are better calibrated on the ARC-Challenge and MMLU benchmarks; however, calibration of these models degrades significantly on StrategyQA, GPQA, and SimpleQA. These results suggest that reasoning models are better calibrated on older benchmarks, e.g., ARC-Challenge (released in 2018) and MMLU (released in 2020), compared to more recent benchmarks where the models are poorly calibrated, e.g., StrategyQA (released in 2021), GPQA (released in 2023), and SimpleQA (released in 2024). We hypothesize that the difference in calibration of these models can be explained by the difference in accuracy across these benchmarks and explore these connections in[Section5.3](https://arxiv.org/html/2506.18183v1#S5.SS3).
The results highlight that calibration on a particular dataset is not always indicative of calibration on another dataset, especially in cases with near-perfect model accuracy where overconfidence is difficult to detect. This finding raises a common, important challenge in benchmarking LLMs:*existing benchmarks become relatively uninformative for UQ as new reasoning models are trained to beat these benchmarks, necessitating the introduction of newer benchmarks.*Next, we characterize the nature of the calibration of reasoning models, i.e., underconfidence vs. overconfidence of these models.
Are Reasoning Models Underconfident or Overconfident?We plot the reliability diagrams of the reasoning models in[Figure4](https://arxiv.org/html/2506.18183v1#S5.F4)for the three challenging datasets (StrategyQA, GPQA, and SimpleQA), aggregating the empirical confidence estimates across all prompt strategies.
In these more challenging datasets, we see that reasoning models are generally overconfident, with confidence estimates typically greater than85%percent8585\\%85 %. This is seen by examining the density of samples in each confidence bin (i.e., the number of times the model predicts a confidence level within the given bin divided by the total number of queries), denoted by the red crosses in[Figure4](https://arxiv.org/html/2506.18183v1#S5.F4).
Larger red gaps in the reliability diagram signify greater overconfidence (miscalibration), representing the deviation between the estimated bin-wise confidence and perfect calibration.[Figure4](https://arxiv.org/html/2506.18183v1#S5.F4)shows that Gemini and o3-Mini are higly overconfident, with significant concentration of their confidence estimates in the85%percent8585\\%85 %to100%percent100100\\%100 %confidence bins. In particular, the accuracy of the samples within these bins significantly deviates from the estimated bin-wise confidence.
DeepSeek also exhibits overconfidence, although to a smaller degree. We observe large miscalibration and high sample densities especially between the70%percent7070\\%70 %and80%percent8080\\%80 %confidence intervals.
Claude is the best calibrated model, with significantly smaller miscalibration gaps. However, Claude still appears to be overconfident, particularly in the60%percent6060\\%60 %and80%percent8080\\%80 %confidence range.
We provide the relibability diagram for all datasets in Appendix[10](https://arxiv.org/html/2506.18183v1#S10).
![Refer to caption](x4.png)Figure 4:Are reasoning models consistently underconfident or overconfident?Reasoning models are systematically overconfident, with Gemini and o3-Mini being the most overconfident followed by DeepSeek, while Claude is the best calibrated one, highlighted by the reliability diagrams for StrategyQA, GPQA, and SimpleQA aggregated over all prompt strategies.
Comparison among the Models.We compare the models‚Äô calibration, accuracy, and completion capabilities to identify the best-performing model.
From[Figures3](https://arxiv.org/html/2506.18183v1#S5.F3)and[4](https://arxiv.org/html/2506.18183v1#S5.F4), we find that Claude is the most calibrated model by a significant margin, followed by DeepSeek, based on the ECE. This finding appears to be consistent across all datasets, except StrategyQA and GPQA, where DeepSeek achieves the best calibration.
In contrast, Gemini is the least calibrated model, although the calibration of o3-Mini is almost the same as that of Gemini in many datasets.
In addition, Claude 3.7 Sonnet is about2%percent22\\%2 %more accurate than DeepSeek (the best-competing method) and over4%percent44\\%4 %more accurate than o3-Mini (the least accurate model), which is not entirely surprising given that the compact o3-Mini trades off performance for cost efficiency relative to o3. Further, DeepSeek and Gemini achieve almost the same accuracy, with only a0.6%percent0.60.6\\%0.6 %gap.
The superior accuracy of Claude 3.7 Sonnet may be explained by the recency of its release compared to the other models: Claude 3.7 Sonnet is the latest model to be released among the models.
Lastly, all models completed more than97%percent9797\\%97 %of the questions, with o3-Mini achieving the highest completion percentage, indicating its ability to interpret the questions and follow the instructions required to provide valid answers.
### 5.3Correlation Between Accuracy and Calibration of Reasoning Models
The preceding results showed that reasoning models are poorly calibrated in datasets where these models have lower accuracies and better calibrated otherwise. Here, we investigate the relationship between accuracy and calibration of reasoning models by examining the correlation between these two metrics. We compute the average accuracy and ECE over the four prompt strategies per model.
![Refer to caption](x5.png)Figure 5:Is accuracy correlated with calibration in reasoning models?The ECE is strongly correlated with accuracy in state-of-the-art reasoning models. In essence, the self-verbalized confidence of reasoning models is generally not trustworthy in problems where they achieve low accuracy. Remarkably, Claude remains well-calibrated even in these low-accuracy settings. We observe the same trends in the correlation between accuracy and the MCE.
In[Figure5](https://arxiv.org/html/2506.18183v1#S5.F5), we observe a strong negative correlation between accuracy and ECE across all models, which in the case of Gemini, is a perfect negative correlation, i.e.,r=‚àí1ùëü1{r=-1}italic\_r = - 1. Similarly, DeepSeek and o3-Mini exhibit near-perfect correlation.
This finding underscores that reasoning models are poorly calibrated.
As shown in[Figure4](https://arxiv.org/html/2506.18183v1#S5.F4), reasoning models are overconfident, which explains the strong correlation between accuracy and calibration.
In general, if an overconfident model consistently outputs high confidence values, then its ECE will decrease as its accuracy increases.
The confidence of a well-calibrated model should reflect its accuracy, represented by a near-zero correlation coefficient between accuracy and ECE.
However, unlike the other models, Claude has a much weaker correlation withr=‚àí0.69ùëü0.69{r=-0.69}italic\_r = - 0.69(28%percent2828\\%28 %smaller than that of the second-best-performing model DeepSeek), showing its relatively better calibration.
Further, these results suggest that self-verbalized confidence estimates from reasoning models should be taken with great care, especially with poorly calibrated models.
Lastly, as with the ECE, accuracy and MCE have a strong negative correlation, with DeepSeek showing the weakest correlation.
Training reasoning models to not only be accurate but also be well-calibrated remains a fundamental research challenge, as highlighted by these results.
We discuss the correlation between calibration error and accuracy for each prompt in[Section11](https://arxiv.org/html/2506.18183v1#S11).
We explore open research problems and potential future research directions in Section[7](https://arxiv.org/html/2506.18183v1#S7).
### 5.4Reasoning Depth vs. Calibration
One of the most appealing properties of reasoning models is*inference-time scaling*: the ability to boost the accuracy of responses via*deeper reasoning*(albeit at the expense of increased inference time and energy costs). However, the impact of deeper reasoning on calibration has not been explored in prior work. Here, we examine the coupled effects of reasoning depth on the accuracy and calibration of reasoning models, highlighting some notable findings.
We summarize our results in[Figure6](https://arxiv.org/html/2506.18183v1#S5.F6), evaluating Claude 3.7 Sonnet and o3-Mini since DeepSeek-R1 and Gemini do not sufficiently support/utilize the specification of a reasoning token budget. We consider three reasoning levels:*RE-Low*,*RE-Medium*, and*RE-High*, implemented by the OpenAI API. We estimate the number of thinking tokens corresponding to these levels, and specify these parameters as a thinking budget to Claude. We compute the average of each metric across the two reasoning models.
![Refer to caption](x6.png)Figure 6:Does increasing reasoning depth improve calibration?Deeper reasoning leads to overall higher accuracy; however, as the accuracy of these models saturate, reasoning models become even more overconfident.![Refer to caption](x7.png)Figure 7:Deeper Reasoning vs. Underconfidence/Overconfidence?Reasoning models become more overconfident with deeper reasoning, evidenced by the increase in the density of samples in higher-confidence bins (e.g., the90%percent9090\\%90 %-95%percent9595\\%95 %interval) without a corresponding increase in accuracy.
On average, we find that deeper reasoning results in higher accuracies and better calibration, highlighted in[Figure6](https://arxiv.org/html/2506.18183v1#S5.F6). However, we observe a more nuanced relationship between reasoning depth, accuracy, and calibration upon closer examination. Specifically, our analysis of the results in[Figure6](https://arxiv.org/html/2506.18183v1#S5.F6)reveals three key findings.
First, model calibration improves with increasing reasoning depth whenever deeper reasoning leads to an increase in accuracy. This observation can be seen in the GPQA dataset, where an increase in reasoning depth from low to high leads to a10.8%percent10.810.8\\%10.8 %increase in accuracy and an associated5.5%percent5.55.5\\%5.5 %decrease in the ECE.
Second, model calibration and accuracy remain relatively unchanged with greater reasoning depth in ‚Äúeasier‚Äù benchmarks, i.e., in datasets where the model achieves relatively high accuracy, e.g., in StrategyQA. In these settings, deeper reasoning does not result in an appreciable increase in accuracy. Moreover, model overconfidence becomes difficult to identify, masked by high accuracy.
Third, as the accuracy of the model saturates in more challenging datasets, deeper reasoning leads to an*increase*in the calibration errors, which is visible in the SimpleQA benchmark. As the reasoning depth increases from low to high, we observe a relatively small increase in accuracy and a much larger increase in the ECE.
To further explore this finding, we provide the reliability diagrams associated with the reasoning depths aggregated over all datasets in[Figure7](https://arxiv.org/html/2506.18183v1#S5.F7), showing that the proportion of highly confident responses increases with reasoning depth, oftentimes without a corresponding increase in accuracy, degrading the calibration of the model. For example, in[Figure7](https://arxiv.org/html/2506.18183v1#S5.F7), we see that reasoning models are generally overconfident with most of the model‚Äôs confidence estimates residing between85%percent8585\\%85 %and100%percent100100\\%100 %. As we increase the reasoning depth, the density of samples in this interval increases while the accuracy of these samples decreases (e.g., samples in the90%percent9090\\%90 %-95%percent9595\\%95 %interval), underscoring that*reasoning models become even more overconfident with deeper reasoning.*
Further, MCE decreases with greater reasoning depth ([Figure6](https://arxiv.org/html/2506.18183v1#S5.F6)). We note that the worst-case calibration error is generally associated with confidence bins containing a single or only a few samples. Deeper reasoning generally increases either the accuracy of these samples or the confidence associated with these samples, since the affected samples typically move to a bin with more samples, ultimately reducing their influence on the MCE.
Lastly, from[Figure7](https://arxiv.org/html/2506.18183v1#S5.F7), the completion rate of reasoning models decreases with greater reasoning depth. Reasoning models tend to struggle more with following format instructions specified in the system prompt when provided with a greater thinking budget. This observation is more prominent in scientific domains, e.g., GPQA, where reasoning involves significant scientific notation and equations.
![Refer to caption](x8.png)Figure 8:Does increasing reasoning depth make the models more confidently wrong on average?On wrongly-answered questions, reasoning models become more overconfident, even though the correctness of their answers does not change.![Refer to caption](x9.png)Figure 9:Does increasing reasoning depth make the models more confidently wrong?With more reasoning effort, o3-Mini‚Äôs overconfidence worsens, while Claude‚Äôs overconfidence remains relatively constant.
Next, for a more detailed analysis, we examine the calibration errors as a function of reasoning depth on*wrongly-answered questions*, (i.e., on questions with incorrect responses at all reasoning depth levels).[Figure8](https://arxiv.org/html/2506.18183v1#S5.F8)reveals that model calibration degrades with reasoning depth in this domain with an increase in the average ECE and MCE. This result suggests that deeper reasoning makes reasoning models more confident in their incorrect responses by reinforcing their misguided thought process in these problems.
This result bears some similarity to human behavior, since prior work has shown that humans tend to provide higher estimates of their confidence in their answers> [
[> 40
](https://arxiv.org/html/2506.18183v1#bib.bib40)> ]
when given more time to think, even when their answers do not change.
Moreover, we find that the relative effect of reasoning depth on calibration varies significantly with the model. From[Figure9](https://arxiv.org/html/2506.18183v1#S5.F9), o3-Mini is more susceptible to reasoning-induced overconfidence, with a7%percent77\\%7 %absolute increase in the ECE from the low-reasoning to the high-reasoning setting, compared to a2%percent22\\%2 %increase for Claude. In fact, not only is Claude more robust, it is also better calibrated across all datasets.
Moreover, the MCE increases for all models as all models become more confident without a corresponding increase in their accuracy.
### 5.5Reasoning about Confidence
Next, we examine if model calibration can be improved by*reasoning about uncertainty*. Specifically, we utilize a two-stage introspective uncertainty quantification procedure (see Section[4.2](https://arxiv.org/html/2506.18183v1#S4.SS2)). In the first stage, we prompt each model to provide an answer to the question and an associated confidence in the answer. In the second stage, we provide the reasoning trace from the first stage to a fresh instance of the model and prompt the model to identify potential flaws in the the reasoning trace to provide an updated confidence estimate in the original answer.
We explore three prompt strategies for introspection, varying the level of conservativeness:*IUQ-Low*,*IUQ-Medium*, and*IUQ-High*, described in[Section4.2](https://arxiv.org/html/2506.18183v1#S4.SS2). In IUQ-Medium and IUQ-High, we specifically ask the model to identify flaws in the reasoning traces. In all prompts, we do not ask the model for an updated answer to discourage the model from directly reasoning about the original question, e.g., the validity of all options in a multi-choice question.
The second stage uses a new session with a re-initialized context window that does not contain information from the first session.
We do not include Gemini in these results due to insufficient support for analysis of the reasoning traces.
![Refer to caption](x10.png)Figure 10:Introspective UQ.Calibration of reasoning models improves with introspective uncertainty quantification, particularly in challenging datasets, e.g., SimpleQA, with more conservative prompts, e.g., IUQ-Medium and IUQ-High.![Refer to caption](x11.png)Figure 11:Introspective UQ vs. Underconfidence/Overconfidence.More critical introspection, e.g., IUQ-Medium and IUQ-High, improves the calibration of reasoning models, mitigating model overconfidence, unlike IUQ-Low.
We compute the resulting calibration errors of each model across the challenging datasets (GPQA, StrategyQA, and SimpleQA).
In[Figure10](https://arxiv.org/html/2506.18183v1#S5.F10), when averaging across all models, we observe that introspective UQ leads to a marginal increase in the average ECE in IUQ-Low and a decrease in IUQ-Medium and IUQ-High. The change in the ECE varies significantly with the dataset and the introspective prompt. Unlike the MCE which increases across all datasets, in the least conservative introspection, the ECE decreases in the GPQA and StrategyQA datasets but increases in the SimpleQA dataset. However, with IUQ-Medium and IUQ-High, we observe a decrease in the ECE across all datasets, except in StrategyQA in the case of IUQ-High. Asking the reasoning model to identify flaws in the prior reasoning traces tends to induce more critical introspection, leading to a significant decrease in miscalibration, especially in the most challenging dataset, SimpleQA.
Moreover, when prior confidence estimates are not provided to reasoning models (i.e., in IUQ-High), reasoning models reason more conservatively during introspection, which leads to better calibration in challenging datasets.
Conversely, on easier datasets, greater conservativeness from more critical introspection could slightly increase the ECE, e.g., IUQ-High on StrategyQA.
To further examine this finding, we visualize the reliability diagrams associated with the first and second stages in[Figure11](https://arxiv.org/html/2506.18183v1#S5.F11), showing that less critical introspection, e.g., IUQ-Low, can increase model overconfidence, i.e., a rightward shift in the confidence estimates from low-confidence bins to high-confidence bins. For example, we observe an increase in the density of samples in the0.850.850.850.85-1.01.01.01.0confidence bins, in IUQ-Low. However, more critical introspection, e.g., IUQ-Medium and IUQ-High, leads to better calibration, reducing overconfidence.
![Refer to caption](x12.png)Figure 12:Introspective UQ per Model.Introspective UQ improves the calibration of DeepSeek and o3-Mini, especially in the challenging dataset SimpleQA, but degrades the calibration of Claude.![Refer to caption](x13.png)Figure 13:Introspective UQ per Model vs. Underconfidence/Overconfidence.Through introspection, DeepSeek and o3-Mini become less overconfident, especially with IUQ-Medium and IUQ-High, unlike Claude, which becomes more overconfident.
We further analyze the calibration of introspective UQ estimates for each model, summarized in[Figure12](https://arxiv.org/html/2506.18183v1#S5.F12).
We find that calibration of Claude significantly degrades when the model reasons about its uncertainty, particularly in StrategyQA and SimpleQA. Specifically, Claude becomes more overconfident with introspective UQ, as highlighted in[Figure13](https://arxiv.org/html/2506.18183v1#S5.F13), showing that the proportion of confidence estimates in the0.90.90.90.9-0.950.950.950.95confidence bin nearly doubles, with a corresponding decrease in the density of almost all the bins below0.80.80.80.8. This observation may be due to the implementation details of Claude, which is closed-source.
In contrast, introspection improves the calibration of DeepSeek and o3-Mini, with bigger improvements with IUQ-Medium and IUQ-High in SimpleQA. During the second stage, critical introspection enables these models to identify flaws and hallucinations in their reasoning traces, leading to effective reassessment of their confidence.[Figure13](https://arxiv.org/html/2506.18183v1#S5.F13)highlights these findings, showing an increase in the density of the lower-confidence bins with a corresponding decrease in the density of the higher-confidence bins, ultimately reducing overconfidence.
In summary, these results suggest that introspective UQ improves the calibration of reasoning models, particularly in challenging problems where introspection enables these models to identify flaws in their reasoning.
### 5.6Calibration and Accuracy of Reasoning Models vs. Prompt Strategies
Prior work has shown that prompt engineering can improve the performance of LLMs significantly. For example, existing work has demonstrated that chain-of-thought (CoT) prompting leads to higher accuracy in non-reasoning LLMs, e.g., in zero-shot tasks. Here, we explore the question: do prompt strategies such as chain-of-prompting improve the accuracy and calibration of reasoning models? To do so, we compute the accuracy and calibration over the four prompt strategies: basic, CoT, multi-step, and top-K prompts, averaging the models‚Äô performance within each dataset.
![Refer to caption](x14.png)Figure 14:Do specialized prompt strategies (e.g., chain-of-thought prompting) improve calibration and accuracy?Chain-of-thought prompting and multi-step prompting do not outperform the basic prompt strategy. On average, calibration and accuracy of reasoning models do not depend on the prompt strategy.
From[Figure14](https://arxiv.org/html/2506.18183v1#S5.F14), we observe that the performance of reasoning models is relatively independent of the selected prompt strategy. In particular, specialized prompt strategies such as CoT prompting and multi-step prompting which have been shown to be effective in non-reasoning models do not significantly outperform basic prompting techniques that just ask the model directly for its answer and confidence in terms of calibration, accuracy, and completion rates.
In non-reasoning models, CoT prompting and multi-step prompting are generally effective due to their ability to elicit reasoning in these models.
However, reasoning models are trained to generate reasoning traces by default. Consequently, specialized prompt strategies are not necessary to induce reasoning, explaining the independence between the performance of reasoning models and prompt strategies. We provide more details on each model in Appendix[12](https://arxiv.org/html/2506.18183v1#S12).
## 6Conclusion
In this paper, we examine uncertainty quantification of reasoning models. To this end, we ask three critical questions:
First,*are reasoning models well-calibrated?*Second,*does deeper reasoning improve model calibration?*Third,*can reasoning models improve their calibration by explicitly reasoning about their chain-of-thought traces?*Through extensive evaluations, we observe that reasoning models are generally overconfident and become even more overconfident with deeper reasoning, especially in tasks where deeper reasoning does not correspond to higher accuracy. Lastly, the calibration of these model improves with more critical introspection, although introspection increases the overconfidence of Claude.
## 7Limitations and Future Work
The results in our work highlight that in general, reasoning models are poorly calibrated despite their remarkable accuracy, posing a challenge to real-world deployment. We believe that improving the calibration of reasoning models is essential to drive widespread safe adoption of these models. Here, we identify some valuable research directions for future work.
Reducing the Correlation between Accuracy and Calibration.As discussed in[Section5](https://arxiv.org/html/2506.18183v1#S5), the accuracy of reasoning models is strongly positively calibrated with their calibration. Stable, well-calibrated models should have a near-zero correlation between accuracy and calibration, i.e., the model should remain well-calibrated in relatively easier problem domains and relatively more challenging problem domains. Training stable, well-calibrated reasoning models remains an open challenge, presenting a valuable direction for future research.
Future work will explore techniques that preserve model calibration even when the accuracy of these models is low. For example, reasoning models can be trained to recognize challenging problems and provide more conservative confidence estimates in these knowledge domains.
Training Reasoning Models to Reason about their Uncertainty.Reinforcement-learning-induced multi-step reasoning has enabled reasoning models to set new performance records on a broad range of benchmarks. By decomposing challenging problems into easier-to-solve subproblems, reasoning models have achieved superior accuracy compared to prior language models. We believe that reasoning models can be trained to perform multi-step reasoning about their*uncertainty*through reinforcement learning, constituting an exciting direction for future work. By training models to explicitly reason about the boundaries of their knowledge, reasoning models can be empowered to better estimate their confidence in the correctness of their responses.
Benchmarks for Uncertainty Quantification.As noted in the preceding discussion in[Section5](https://arxiv.org/html/2506.18183v1#S5), the performance of reasoning models on many existing benchmarks has saturated, making these benchmarks essentially uninformative in uncertainty quantification. Future work will seek to introduce new benchmarks that better evaluate the calibration of reasoning models, across a range of disciplines. For example, new benchmarks for evaluating the calibration of reasoning models at intermediate steps in the problem-solving process would shed more light on the calibration of the thought processes of reasoning models.
## Acknowledgments
The authors were partially supported by the Office of Naval Research (N00014-23-1-2148) and a Sloan Fellowship.
## References
* Aichberger et¬†al. [2024]Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielanskyi, and Sepp Hochreiter.Semantically diverse language generation for uncertainty estimation in language models.*arXiv preprint arXiv:2406.04306*, 2024.
* Anthropic [2024]Anthropic.Claude 3.7 sonnet system card.Technical report, Anthropic, 2024.URL[https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf).
* Art of Problem Solving [2025]Art of Problem Solving.American Invitational Mathematics Examination.[https://artofproblemsolving.com/wiki/index.php/American\_Invitational\_Mathematics\_Examination](https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination), 2025.
* Azaria and Mitchell [2023]Amos Azaria and Tom Mitchell.The internal state of an llm knows when it‚Äôs lying.*arXiv preprint arXiv:2304.13734*, 2023.
* Bai et¬†al. [2024]Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike¬†Zheng Shou.Hallucination of multimodal large language models: A survey.*arXiv preprint arXiv:2404.18930*, 2024.
* Belinkov [2022]Yonatan Belinkov.Probing classifiers: Promises, shortcomings, and advances.*Computational Linguistics*, 48(1):207‚Äì219, 2022.
* Besta et¬†al. [2024]Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et¬†al.Graph of thoughts: Solving elaborate problems with large language models.In*Proceedings of the AAAI Conference on Artificial Intelligence*, volume¬†38, pages 17682‚Äì17690, 2024.
* Clark et¬†al. [2018]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.Think you have solved question answering? try arc, the ai2 reasoning challenge.*arXiv preprint arXiv:1803.05457*, 2018.
* Dunefsky et¬†al. [2024]Jacob Dunefsky, Philippe Chlenski, and Neel Nanda.Transcoders find interpretable llm feature circuits.*arXiv preprint arXiv:2406.11944*, 2024.
* Fadeeva et¬†al. [2024]Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, et¬†al.Fact-checking the output of large language models via token-level uncertainty quantification.*arXiv preprint arXiv:2403.04696*, 2024.
* Ferrando et¬†al. [2024]Javier Ferrando, Oscar Obeso, Senthooran Rajamanoharan, and Neel Nanda.Do i know this entity? knowledge awareness and hallucinations in language models.*arXiv preprint arXiv:2411.14257*, 2024.
* Fu et¬†al. [2022]Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot.Complexity-based prompting for multi-step reasoning.*arXiv preprint arXiv:2210.00720*, 2022.
* Gao et¬†al. [2024]Leo Gao, Tom¬†Dupr√© la¬†Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu.Scaling and evaluating sparse autoencoders.*arXiv preprint arXiv:2406.04093*, 2024.
* Geva et¬†al. [2020]Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.Transformer feed-forward layers are key-value memories.*arXiv preprint arXiv:2012.14913*, 2020.
* Geva et¬†al. [2021]Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant.Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.*Transactions of the Association for Computational Linguistics*, 9:346‚Äì361, 2021.
* Gneiting and Raftery [2007]Tilmann Gneiting and Adrian¬†E Raftery.Strictly proper scoring rules, prediction, and estimation.*Journal of the American statistical Association*, 102(477):359‚Äì378, 2007.
* Guo et¬†al. [2017]Chuan Guo, Geoff Pleiss, Yu¬†Sun, and Kilian¬†Q. Weinberger.On calibration of modern neural networks.In Doina Precup and Yee¬†Whye Teh, editors,*Proceedings of the 34th International Conference on Machine Learning*, volume¬†70 of*Proceedings of Machine Learning Research*, pages 1321‚Äì1330. PMLR, 06‚Äì11 Aug 2017.URL[https://proceedings.mlr.press/v70/guo17a.html](https://proceedings.mlr.press/v70/guo17a.html).
* Guo et¬†al. [2025]Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et¬†al.Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.*arXiv preprint arXiv:2501.12948*, 2025.
* Hao et¬†al. [2023]Shibo Hao, Yi¬†Gu, Haodi Ma, Joshua¬†Jiahua Hong, Zhen Wang, Daisy¬†Zhe Wang, and Zhiting Hu.Reasoning with language model is planning with world model.*arXiv preprint arXiv:2305.14992*, 2023.
* Hao et¬†al. [2024]Shibo Hao, Yi¬†Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et¬†al.Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models.*arXiv preprint arXiv:2404.05221*, 2024.
* Hendrycks et¬†al. [2020]Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.Measuring massive multitask language understanding.*arXiv preprint arXiv:2009.03300*, 2020.
* Hewitt et¬†al. [2016]Tanya Hewitt, Samia Chreim, and Alan Forster.Double checking: a second look.*Journal of evaluation in clinical practice*, 22(2):267‚Äì274, 2016.
* Huang and Chang [2022]Jie Huang and Kevin Chen-Chuan Chang.Towards reasoning in large language models: A survey.*arXiv preprint arXiv:2212.10403*, 2022.
* Hurst et¬†al. [2024]Aaron Hurst, Adam Lerer, Adam¬†P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ¬†Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et¬†al.Gpt-4o system card.*arXiv preprint arXiv:2410.21276*, 2024.
* Jaech et¬†al. [2024]Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et¬†al.Openai o1 system card.*arXiv preprint arXiv:2412.16720*, 2024.
* Jospin et¬†al. [2022]Laurent¬†Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Bennamoun.Hands-on bayesian neural networks‚Äîa tutorial for deep learning users.*IEEE Computational Intelligence Magazine*, 17(2):29‚Äì48, 2022.
* Kadavath et¬†al. [2022]Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et¬†al.Language models (mostly) know what they know.*arXiv preprint arXiv:2207.05221*, 2022.
* Kojima et¬†al. [2022]Takeshi Kojima, Shixiang¬†Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.Large language models are zero-shot reasoners.*Advances in neural information processing systems*, 35:22199‚Äì22213, 2022.
* Kuhn et¬†al. [2023]Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar.Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.*arXiv preprint arXiv:2302.09664*, 2023.
* Ling et¬†al. [2024]Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, et¬†al.Uncertainty decomposition and quantification for in-context learning of large language models.*arXiv e-prints*, pages arXiv‚Äì2402, 2024.
* Liu et¬†al. [2024]Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke¬†Wang, Liping Hou, Rongjun Li, and Wei Peng.A survey on hallucination in large vision-language models.*arXiv preprint arXiv:2402.00253*, 2024.
* Manakul et¬†al. [2023]Potsawee Manakul, Adian Liusie, and Mark¬†JF Gales.Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.*arXiv preprint arXiv:2303.08896*, 2023.
* Mielke et¬†al. [2022]Sabrina¬†J Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau.Reducing conversational agents‚Äô overconfidence through linguistic calibration.*Transactions of the Association for Computational Linguistics*, 10:857‚Äì872, 2022.
* Mikhail Mirzayanov and Codeforces Team [2025]Mikhail Mirzayanov and Codeforces Team.Codeforces.[https://codeforces.com](https://codeforces.com), 2025.Competitive Programming Platform.
* Naeini et¬†al. [2015]Mahdi¬†P. Naeini, Gregory Cooper, and Milos Hauskrecht.Obtaining well calibrated probabilities using bayesian binning.*Proceedings of the AAAI Conference on Artificial Intelligence*, 29, 2015.
* OpenAI [2024a]OpenAI.Gpt-4 technical report, 2024a.URL[https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774).
* OpenAI [2024b]OpenAI.Openai o3-mini system card.Technical report, OpenAI, 2024b.URL[https://cdn.openai.com/o3-mini-system-card.pdf](https://cdn.openai.com/o3-mini-system-card.pdf).
* Rein et¬†al. [2023]David Rein, Betty¬†Li Hou, Asa¬†Cooper Stickland, Jackson Petty, Richard¬†Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel¬†R Bowman.Gpqa: A graduate-level google-proof q&amp;a benchmark.*arXiv preprint arXiv:2311.12022*, 2023.
* Shorinwa et¬†al. [2025]Ola Shorinwa, Zhiting Mei, Justin Lidard, Allen¬†Z. Ren, and Anirudha Majumdar.A survey on uncertainty quantification of large language models: Taxonomy, open research challenges, and future directions.*ACM Comput. Surv.*, June 2025.ISSN 0360-0300.[10.1145/3744238](https:/doi.org/10.1145/3744238).URL[https://doi.org/10.1145/3744238](https://doi.org/10.1145/3744238).
* Shynkaruk and Thompson [2006]Jody¬†M Shynkaruk and Valerie¬†A Thompson.Confidence and accuracy in deductive reasoning.*Memory &amp; cognition*, 34(3):619‚Äì632, 2006.
* Smith et¬†al. [2020]Eric¬†Michael Smith, Diana Gonzalez-Rico, Emily Dinan, and Y-Lan Boureau.Controlling style in generated dialogue.*arXiv preprint arXiv:2009.10855*, 2020.
* Tamkin et¬†al. [2023]Alex Tamkin, Mohammad Taufeeque, and Noah¬†D Goodman.Codebook features: Sparse and discrete interpretability for neural networks.*arXiv preprint arXiv:2310.17230*, 2023.
* Tanneru et¬†al. [2024]Sree¬†Harsha Tanneru, Chirag Agarwal, and Himabindu Lakkaraju.Quantifying uncertainty in natural language explanations of large language models.In*International Conference on Artificial Intelligence and Statistics*, pages 1072‚Äì1080. PMLR, 2024.
* Team et¬†al. [2024]Gemini Team, Petko Georgiev, Ving¬†Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et¬†al.Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.*arXiv preprint arXiv:2403.05530*, 2024.
* Team [2025]Qwen Team.Qwq-32b: Embracing the power of reinforcement learning, March 2025.URL[https://qwenlm.github.io/blog/qwq-32b/](https://qwenlm.github.io/blog/qwq-32b/).
* Tian et¬†al. [2023]Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher¬†D Manning.Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback.*arXiv preprint arXiv:2305.14975*, 2023.
* Wang et¬†al. [2022]Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed¬†Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.Self-consistency improves chain of thought reasoning in language models.*arXiv preprint arXiv:2203.11171*, 2022.
* Wei et¬†al. [2022]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed¬†Chi, Quoc¬†V Le, Denny Zhou, et¬†al.Chain-of-thought prompting elicits reasoning in large language models.*Advances in neural information processing systems*, 35:24824‚Äì24837, 2022.
* Wei et¬†al. [2024]Jason Wei, Nguyen Karina, Hyung¬†Won Chung, Yunxin¬†Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus.Measuring short-form factuality in large language models.*arXiv preprint arXiv:2411.04368*, 2024.
* Wu et¬†al. [2024]Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky√ºrek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim.Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.In*Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)*, pages 1819‚Äì1862, 2024.
* Xiao and Wang [2021]Yijun Xiao and William¬†Yang Wang.On hallucination and predictive uncertainty in conditional language generation.*arXiv preprint arXiv:2103.15025*, 2021.
* Xiong et¬†al. [2023]Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi.Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms.*arXiv preprint arXiv:2306.13063*, 2023.
* Yao et¬†al. [2023]Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.Tree of thoughts: Deliberate problem solving with large language models.*Advances in neural information processing systems*, 36:11809‚Äì11822, 2023.
* Yun et¬†al. [2021]Zeyu Yun, Yubei Chen, Bruno¬†A Olshausen, and Yann LeCun.Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors.*arXiv preprint arXiv:2103.15949*, 2021.
* Zhang et¬†al. [2022]Zhuosheng Zhang, Aston Zhang, Mu¬†Li, and Alex Smola.Automatic chain of thought prompting in large language models.*arXiv preprint arXiv:2210.03493*, 2022.
* Zhou et¬†al. [2022]Denny Zhou, Nathanael Sch√§rli, Le¬†Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et¬†al.Least-to-most prompting enables complex reasoning in large language models.*arXiv preprint arXiv:2205.10625*, 2022.
\\beginappendix
## 8Introspective Uncertainty Quantification
Here, we provide the full system prompts for the three introspective UQ prompt strategies: IUQ-Low, IUQ-Medium, and IUQ-High.
Introspective UQ-LowYou are provided with the reasoning trace of a model asked to answer a question
and provide the associated confidence between 0 and 100.
Your task is to think about the reasoning trace from the first
model and provide your confidence in the correctness of the
answer provided by the first model.
Introspective UQ-MediumYou are provided with the reasoning trace of a
model asked to answer a question
and provide the associated confidence between 0 and 100.
Your task is to identify the flaws in the reasoning trace
from the first model and provide your confidence in the correctness of the
answer provided by the first model.
Introspective UQ-HighYou are provided with the reasoning trace of a
model asked to answer a question.
Your task is to identify the flaws in the reasoning trace
from the first model and provide your confidence in the correctness of the
answer provided by the first model.
Lastly, we provide an example of the full prompt with the question and reasoning trace using IUQ-Medium. We append the text ‚ÄúYou must include your reasoning trace in your response,‚Äù particularly for models that do not natively support access to the internal reasoning trace.
Introspective UQ: Example Full PromptSystem:You are a smart entity. Do not use the internet.System:You are provided with the reasoning trace of a
model asked to answer a question
and provide the associated confidence between 0 and 100.
Your task is to identify the flaws in the reasoning trace
from the first model and provide your confidence in the correctness of the
answer provided by the first model.User:The question is: How many original artworks were shown in the Society of Illustrators‚Äô first ‚ÄùAnnual Exhibition‚Äù?The given reasoning process and answer is: ‚Ä¶System:Given the answer and confidence from the first model,
what is your confidence in the validity of the
first model‚Äôs answer
as a number in the form {rec, Y}?
The first element in the curly braces should always be rec.
For example, if you are 80 percent confident
that the first model is correct, your answer should be
{rec, 80}.
## 9LLM Prompt Strategies
For completeness, we provide the full prompts for the basic, chain-of-thought, multi-step, and top-K prompt strategies, introduced in prior work.
Standard PromptProvide an answer and the confidence in your answer between 0 and 100.
Chain-of-Thought PromptAnalyze step by step and provide an answer and the confidence in your answer between 0 and 100.
Multi-Step PromptBreak down the problem into multiple steps, and provide an answer to each step, along with a justification and a confidence in each step. Give your final answer with your confidence in your final answer and the confidence in your answer between 0 and 100.
Top-K PromptGiveK=5ùêæ5K=5italic\_K = 5best guesses and your confidence in each guess (i.e., the probability that each one is correct). Give your final answer with your confidence in your final answer and the confidence in your answer between 0 and 100.
The system prompts are followed with a description of the dataset including the desired answer format, as well as the actual question. We show an example full prompt for the SimpleQA dataset, with the basic prompt strategy.
Example Full PromptSystem:You are a smart entity. Do not use the internet.System:Analyze step by step and provide an answer and the confidence in your answer between 0 and 100.System:Each question requires a simple, short fact as the answer: only provide your final answer, which could be a specific name, date, or other fact, and a number for the confidence in the form X, Y. For example, if you think the answer is 1950 and you are 80 percent confident, then you should only say {1950, 80}.User:Who received the IEEE Frank Rosenblatt Award in 2010?
## 10Calibration of Reasoning Models.
We further discuss the calibration and accuracy of reasoning models, across the basic, chain-of-thought, multi-step, and top-K prompt strategies, in addition to the results in[Section5](https://arxiv.org/html/2506.18183v1#S5).
![Refer to caption](x15.png)Figure 15:Are reasoning models consistently underconfident or overconfident?The reliability diagrams are aggregated over all prompt strategies and all datasets. Reasoning models are overconfident, but it‚Äôs less obvious in datasets with very high model accuracies.
Are Reasoning Models Underconfident or Overconfident?In[Figure15](https://arxiv.org/html/2506.18183v1#S10.F15), we plot the reliability diagrams aggregated across all datasets, showing that reasoning models are typically highly confident, with most of their confidence estimates in the90%percent9090\\%90 %-100%percent100100\\%100 %confidence bins. However, the high accuracies of the models makes overconfidence detection difficult. For example, in[Figure16](https://arxiv.org/html/2506.18183v1#S10.F16), we visualize the reliability diagrams for the easier datasets (ARC-Challenge and MMLU), showing that overconfidence is masked by the models‚Äô high accuracy.
![Refer to caption](x16.png)Figure 16:Are reasoning models consistently underconfident or overconfident?We show the reliability diagrams on the easier datasets . Overconfidence is difficult to detect in these datasets due to the high accuracy of the models.
Model Calibration for Each Prompt Strategy.In[Figures17](https://arxiv.org/html/2506.18183v1#S10.F17),[18](https://arxiv.org/html/2506.18183v1#S10.F18),[19](https://arxiv.org/html/2506.18183v1#S10.F19)and[20](https://arxiv.org/html/2506.18183v1#S10.F20), we provide the calibration and accuracies of each model across all datasets for each prompt strategy. In line with the results in[Section5.2](https://arxiv.org/html/2506.18183v1#S5.SS2), across all the prompt strategies, Claude achieves the lowest ECE by relatively significant margins followed by DeepSeek (except in the top-K prompt strategy). In general, we find that Claude 3.7 Sonnet is the best calibrated and most accurate of the reasoning models considered in this work. Further, Gemini and o3-Mini achieve almost the same calibration and accuracy, with Gemini outperforming o3-Mini with basic and top-K prompts by narrow margins.
![Refer to caption](x17.png)Figure 17:Basic Prompt Strategy.Calibration and Accuracy of Reasoning Models.![Refer to caption](x18.png)Figure 18:Chain-of-Thought Prompt Strategy.Calibration and Accuracy of Reasoning Models.![Refer to caption](x19.png)Figure 19:Multi-Step Prompt Strategy.Calibration and Accuracy of Reasoning Models.![Refer to caption](x20.png)Figure 20:Top-K Prompt Strategy.Calibration and Accuracy of Reasoning Models.
## 11Coupling between Accuracy and Calibration of Reasoning Models
Here, we discuss the accuracy of each model on the benchmarks and examine the correlation between accuracy and calibration.
Model Accuracy on Benchmarks.We find that the accuracy of reasoning models varies significantly across the benchmarks. In[Figure3](https://arxiv.org/html/2506.18183v1#S5.F3), all reasoning models achieve over90%percent9090\\%90 %accuracy on the ARC-Challenge and MMLU datasets, with about a10‚àí20%10percent2010-20\\%10 - 20 %absolute drop on the StrategyQA and GPQA datasets. The drop in accuracy can be explained by the relative difficulty of the more recent benchmarks StrategyQA and GPQA, which emphasize analytical problem-solving over memorization, compared to ARC-Challenge and MMLU. Prior work has shown that LLMs struggle more with analytical tasks compared to information-retrieval tasks (which require recitation)> [
[> 50
](https://arxiv.org/html/2506.18183v1#bib.bib50)> ]
, which is in line with our results.
Likewise, the results indicate that the accuracy of the reasoning models decreases with the recency of the benchmark, which is not surprising, given that these models are trained to perform well on these benchmarks.
Notably, all reasoning models achieve less than40%percent4040\\%40 %accuracy on the SimpleQA dataset, which was was constructed adversarially from GPT-4‚Äôs responses (i.e., a question was added to SimpleQA, if at least one GPT-4 response was incorrect). We observe that SimpleQA remains notably challenging for reasoning models, making it useful for uncertainty quantification.
Correlation between Accuracy and Calibration.
![Refer to caption](x21.png)Figure 21:Basic Prompt Strategy.Correlation between Calibration and Accuracy.![Refer to caption](x22.png)Figure 22:Chain-of-Thought Prompt Strategy.Correlation between Calibration and Accuracy.![Refer to caption](x23.png)Figure 23:Multi-Step Prompt Strategy.Correlation between Calibration and Accuracy.![Refer to caption](x24.png)Figure 24:Top-K Prompt Strategy.Correlation between Calibration and Accuracy.
In[Section5.3](https://arxiv.org/html/2506.18183v1#S5.SS3), we show that calibration and accuracy are strongly correlated ([Figure5](https://arxiv.org/html/2506.18183v1#S5.F5)), with the results averaged over the four prompt strategies. Here, we show the correlation between calibration errors and accuracy for each prompt strategy in Figures[21](https://arxiv.org/html/2506.18183v1#S11.F21)through[24](https://arxiv.org/html/2506.18183v1#S11.F24). For all prompt strategies, we observe a strong negative correlation between ECE and accuracy, as well as between MCE and accuracy. Similar to results discussed in[Section5.3](https://arxiv.org/html/2506.18183v1#S5.SS3), among all models, Claude‚Äôs calibration is least strongly correlated accuracy, while Gemini almost always exhibits perfect negative correlation. For MCE and accuracy, the trend is weaker, affected by many outliers, but o3-Mini and Gemini generally show stronger correlations compared to Claude and DeepSeek.
## 12Effects of LLM Prompt Strategies on Calibration and Accuracy of Reasoning Models
As discussed in[Section5.6](https://arxiv.org/html/2506.18183v1#S5.SS6), on average, existing prompt strategies do not significantly affect the performance of reasoning models. Here, we examine the variability in calibration and accuracy for each model for each prompt strategy.
Unlike the other models, Claude suffers a notable dip in its accuracy and calibration in the Top-K strategy, where the model is asked to provide multiple guesses along with its confidence in the correctness of each guess. This finding suggests that Claude might be more likely to change its answers to potentially incorrect ones when asked to reason about false statements/claims. However, Claude still outperforms the other models in the Top-K prompt strategy, albeit by much smaller margins.
In fact, we observe that GPT, DeepSeek, and Gemini achieve lower calibration errors with Top-K prompts compared to the other prompts, suggesting that explicitly reasoning about alternative answers improves the calibration of these models even though the accuracy remains relatively constant.
Lastly,[Figure26](https://arxiv.org/html/2506.18183v1#S12.F26)shows that chain-of-thought and multi-step prompting provide a slight boost to the accuracy of o3-Mini.
Overall, we reiterate that the specialized prompt strategies seem to have minimal effect on the calibration and accuracy of reasoning models.
![Refer to caption](x25.png)Figure 25:Claude 3.7 Sonnet.Calibration and accuracy for each prompt strategy.![Refer to caption](x26.png)Figure 26:o3-Mini.Calibration and accuracy for each prompt strategy.![Refer to caption](x27.png)Figure 27:DeepSeek R1.Calibration and accuracy for each prompt strategy.![Refer to caption](x28.png)Figure 28:Gemini 2.0 Flash-Thinking-Exp.Calibration and accuracy for each prompt strategy.
## 13Introspective UQ Reasoning Trace Examples
Here, we provide some examples of the prompts, reasoning traces, and answers provided by Claude, DeepSeek, and o3-Mini for all introspective prompts in the challenging datasets (GPQA, StrategyQA, and SimpleQA). All models get the GPQA question wrong and are highly overconfident, demonstrating poor calibration. On the relatively easier dataset StrategyQA, all models provide highly confident correct responses (overconfidence is masked by high accuracy in these problems). In SimpleQA where all models answer the question incorrectly, introspection provide better calibrated confidence estimates, especially in the case of o3-Mini, although DeepSeek reamins overconfident.
### 13.1GPQA
Question and Ground-Truth AnswerQ:
The Cope rearrangement is a chemical reaction where a 1,5-diene molecule undergoes rearrangement, resulting in a change in the positions of its carbon-carbon double bonds. This rearrangement can be initiated by heat or light and is valuable for creating complex organic compounds with changed structures.Select the major products from the following rearrangements:1. (((3-methylbut-2-en-1-yl)oxy)methyl)benzene + (1. BuLi, 2. H+) ‚Äî-¬ø A2. 3,4,5,7,8,9-hexamethyl-1,11-dimethylene-2,6,10,11,11a,11b-hexahydro-1H-benzo[cd]indeno[7,1-gh]azulene + Heat ‚Äî¬øB.Options:(A)A=4ùê¥4A=4italic\_A = 4-methyl-1-phenylpent-3-en-1-olB=2,3,4,6,7,8ùêµ234678B=2,3,4,6,7,8italic\_B = 2 , 3 , 4 , 6 , 7 , 8-hexamethyl-5,9,9a,10,11,11a-hexahydro-1H-benzo[3,4]azuleno[1,8,7,6-cdef]fluorene(B)A=(Z)ùê¥ùëçA=(Z)italic\_A = ( italic\_Z )-2-methyl-5-phenylpent-2-en-1-olB=2,3,4,6,7,8ùêµ234678B=2,3,4,6,7,8italic\_B = 2 , 3 , 4 , 6 , 7 , 8-hexamethyl-5,9,10,11-tetrahydro-1H-benzo[3,4]azuleno[1,8,7,6-cdef]fluorine(C)A=(Z)ùê¥ùëçA=(Z)italic\_A = ( italic\_Z )-2-methyl-5-phenylpent-2-en-1-olB=2,3,4,6,7,8ùêµ234678B=2,3,4,6,7,8italic\_B = 2 , 3 , 4 , 6 , 7 , 8-hexamethyl-5,9,9a,10,11,11a-hexahydro-1H-benzo[3,4]azuleno[1,8,7,6-cdef]fluorene(D)A=4ùê¥4A=4italic\_A = 4-methyl-1-phenylpent-3-en-1-olB=2,3,4,6,7,8ùêµ234678B=2,3,4,6,7,8italic\_B = 2 , 3 , 4 , 6 , 7 , 8-hexamethyl-5,9,10,11-tetrahydro-1H-benzo[3,4]azuleno[1,8,7,6-cdef]fluorineA: D
o3-MiniFirst round:Reasoning Trace:1.For the first reaction, the starting benzyl allyl ether,((3‚Å¢-‚Å¢methylbut‚Å¢-‚Å¢2‚Å¢-‚Å¢en‚Å¢-‚Å¢1‚Å¢-‚Å¢yl)‚Å¢oxy)‚Å¢methylbenzene3-methylbut-2-en-1-yloxymethylbenzene((3\\text{-}\\mathrm{methylbut}\\text{-}2\\text{-}\\mathrm{en}\\text{-}1\\text{-}%
\\mathrm{yl})\\mathrm{oxy})\\mathrm{methyl}\\mathrm{benzene}( ( 3 - roman\_methylbut - 2 - roman\_en - 1 - roman\_yl ) roman\_oxy ) roman\_methylbenzene, under BuLi/H+superscriptH\\text{H}^{+}H start\_POSTSUPERSCRIPT + end\_POSTSUPERSCRIPTconditions undergoes a rearrangement that effectively ‚Äúumpoled‚Äù the benzyl fragment. Analysis of the connectivity shows that nucleophilic attack leads to a five-carbon chain where the benzyl group ends up at one end. Distinguishing between the possible products, the structure where the new double bond is positioned between C2‚ÄìC3 (and with a methyl substituent at C2) ‚Äìi.e.,(Z)Z(\\text{Z})( Z )-2-methyl-5-phenylpent-2-en-1-ol ‚Äìfits the mechanistic pathway and stereoelectronic demands.2.In the second reaction, the substrate is a highly substituted 1,11-dimethylene system undergoing a Cope rearrangement upon heating. Evaluating the potential rearranged structures, the product retaining a ‚Äúhexahydro‚Äù system ‚Äìspecifically, 2,3,4,6,7,8-hexamethyl-5,9,9a,10,11,11a-hexahydro-1H-benzo[3,4]azuleno[1,8,7,6-cdef]fluorene ‚Äìis the more consistent outcome given that the rearrangement does not result in extensive dehydrogenation.3.Comparing the options, option (C) presents the pair of products that match these conclusions:(Z)Z(\\text{Z})( Z )-2-methyl-5-phenylpent-2-en-1-ol for product A and the ‚Äúhexahydro‚Äù fluorene derivative for product B.4.Thus, the reasoning leads to the final choice of option C with high confidence.{C, 95}IUQ-Low:Here‚Äôs my reasoning trace:1.I carefully reviewed the provided explanation of the two reactions and the selection from the answer choices. The explanation shows that the first reaction produces (Z)-2-methyl-5-phenylpent-2-en-1-ol (product A) by the mechanism described and that the Cope rearrangement in the second reaction leads to a hexahydro fluorene derivative (product B).2.The explanation correctly distinguishes between the isomeric outcomes by considering stereoelectronic requirements and the substituent pattern, which aligns with option (C).3.The reasoning steps seem well justified, and the final answer from the first model appears internally consistent and chemically plausible.4.Based on this, I am highly confident that the first model‚Äôs answer is correct with 95 percent certainty.{rec, 95}
o3-Mini (Continued)IUQ-Medium:Below is my reasoning trace and final evaluation:1.In the first reaction the model argues that the benzyl allyl ether is ‚Äúumpoled‚Äù under BuLi/H+conditions, leading to the formation of (Z)-2-methyl-5-phenylpent-2-en-1-ol. Although the idea of a polarity inversion is mentioned, the trace does not detail why the (Z)-isomer is favored over a possible (E)-isomer. It also glosses over the mechanistic steps (e.g., the formation of reactive intermediates, stereoelectronic factors, or potential competing pathways).2.For the second reaction (the Cope rearrangement), the model simply argues that among potential rearranged structures the one retaining a ‚Äúhexahydro‚Äù framework is more consistent. Again, the reasoning lacks expanded discussion on how subtle changes in unsaturation levels arise from the rearrangement. It does not fully clarify why the product should not undergo further dehydrogenation.3.Although the final chosen option (C) agrees with the general expectations ‚Äî(Z)-2-methyl-5-phenylpent-2-en-1-ol (for A) and a hexahydro fluorene derivative (for B) ‚Äîthe chain-of-thought omits several detailed mechanistic arguments that would strengthen the case for selection versus the other close options.In summary, while the final answer of option C is plausible given what is expected from the reactions, the reasoning trace has weaknesses in its clarity and depth of mechanistic justification.Based on these observations I am moderately confident in the final answer and reasoning. My final confidence rating is: {rec, 75}.{rec, 75}IUQ-High:Reasoning Trace:1.I examined the given reasoning from the first model. It argues that for reaction A the product should be (Z)-2-methyl-5-phenylpent-2-en-1-ol‚Äîthis is attributed to a rearrangement pathway initiated by BuLi/H+that ‚Äúumpolars‚Äù the benzyl fragment. Although a detailed mechanism was not provided, this outcome (with the (Z) configuration) is in line with a typical allyl rearrangement producing an allylic alcohol.2.For reaction B the model argued that the Cope rearrangement of a highly substituted dienic system produces a product that preserves a hexahydro framework, leading to a structure matching 2,3,4,6,7,8-hexamethyl-5,9,9a,10,11,11a-hexahydro-1H-benzo[3,4]azuleno[1,8,7,6-cdef]fluorene.3.Comparing these two outcomes with the provided multiple-choice options, option (C) exactly matches: A is (Z)-2-methyl-5-phenylpent-2-en-1-ol and B is the ‚Äúhexahydro‚Äù fluorene derivative.4.While the explanation does not detail every mechanistic nuance and leaves some mechanistic assumptions implicit, the overall conclusion appears plausible and consistent with the reaction conditions and structural outcomes described.5.Thus, my assessment is that the first model‚Äôs answer is reasonably substantiated.{rec, 80}
ClaudeFirst round:
I need to determine the products of two different reactions.For the first reaction:(((3-methylbut-2-en-1-yl)oxy)methyl)benzene+(1. BuLi2. H+)‚ü∂A‚ü∂(((3-methylbut-2-en-1-yl)oxy)methyl)benzene1. BuLi2. H+ùê¥\\text{(((3-methylbut-2-en-1-yl)oxy)methyl)benzene}+\\left(\\begin{array}[]{l}%
\\text{1. BuLi}\\\\
\\text{2. H${}^{+}$}\\end{array}\\right)\\longrightarrow A(((3-methylbut-2-en-1-yl)oxy)methyl)benzene + ( start\_ARRAY start\_ROW start\_CELL 1. BuLi end\_CELL end\_ROW start\_ROW start\_CELL 2. H end\_CELL end\_ROW end\_ARRAY ) ‚ü∂italic\_AThis involves a [2,3]-Wittig rearrangement. Let‚Äôs analyze the starting material:‚Ä¢It‚Äôs a benzyl ether of 3-methylbut-2-en-1-ol‚Ä¢Structure: Ph-CH2-O-CH2-C(CH3)=CH-CH3The mechanism:1.BuLi deprotonates the benzylic position, creating a carbanion2.This carbanion undergoes a [2,3]-sigmatropic rearrangement, forming a new C-C bond with theŒ≥ùõæ\\gammaitalic\_Œ≥-carbon of the allylic system3.The hydroxyl group forms at the original benzylic position4.H+workup gives the final productThe product is: Ph-CH(OH)-CH2-C(CH3)=CH-CH3, which is4-methyl-1-phenylpent-3-en-1-ol.For the second reaction (Cope rearrangement):‚Ä¢Cope rearrangements are [3,3]-sigmatropic rearrangements of 1,5-dienes‚Ä¢They preserve the degree of unsaturation (number of double bonds)‚Ä¢The starting material has ‚Äúhexahydro‚Äù in its name‚Ä¢After rearrangement, the product should also have ‚Äúhexahydro‚Äù (not ‚Äútetrahydro‚Äù)Looking at the options, only (A) matches both criteria:‚Ä¢Product A is 4-methyl-1-phenylpent-3-en-1-ol‚Ä¢Product B maintains the ‚Äúhexahydro‚Äù descriptor{A, 90}IUQ-Low:I‚Äôll evaluate the first model‚Äôs reasoning and provide my confidence in their answer.For the first reaction involving (((3-methylbut-2-en-1-yl)oxy)methyl)benzene:‚Ä¢The model correctly identified this as a [2,3]-Wittig rearrangement, which is appropriate when a benzyl ether with an allylic system is treated with BuLi.‚Ä¢The mechanistic explanation is accurate: BuLi deprotonates at the benzylic position, followed by sigmatropic rearrangement where the carbanion attacks theŒ≥ùõæ\\gammaitalic\_Œ≥-carbon of the allylic system.‚Ä¢The product structure Ph-CH(OH)-CH2-C(CH3)=CH-CH3corresponds to 4-methyl-1-phenylpent-3-en-1-ol, which appears in options A and D.For the second reaction (Cope rearrangement):‚Ä¢The model correctly identified key principles of Cope rearrangements (they‚Äôre [3,3]-sigmatropic rearrangements that preserve the degree of unsaturation).‚Ä¢The crucial observation about preserving the ‚Äúhexahydro‚Äù descriptor is chemically sound.‚Ä¢Only options A and C maintain the ‚Äúhexahydro‚Äù descriptor in product B.When combining these analyses:‚Ä¢Product A (4-methyl-1-phenylpent-3-en-1-ol) appears in options A and D.‚Ä¢Product B with ‚Äúhexahydro‚Äù descriptor appears in options A and C.‚Ä¢Therefore, only option A satisfies both requirements.The model‚Äôs reasoning is thorough and chemically accurate for both reactions. The analysis properly applies organic chemistry principles to identify the correct products.{rec, 95}
Claude (Continued)IUQ-Medium:To evaluate the first model‚Äôs reasoning and confidence, I‚Äôll analyze each part of the solution.
First reaction:(((3-methylbut-2-en-1-yl)oxy)methyl)benzene + (1. BuLi, 2. H+)‚ü∂‚ü∂\\longrightarrow‚ü∂AThe model correctly:‚Ä¢Identified this as a [2,3]-Wittig rearrangement,‚Ä¢Recognized that BuLi deprotonates the benzylic position (most acidic),‚Ä¢Understood the sigmatropic rearrangement mechanism,‚Ä¢Determined the correct structure of the product (4-methyl-1-phenylpent-3-en-1-ol).The mechanism and product determination are chemically sound. The benzyl ether starting material undergoes the expected rearrangement to form the alcohol with the correct connectivity.
Second reaction:Complex polycyclic compound + Heat‚ü∂‚ü∂\\longrightarrow‚ü∂BThe model correctly:‚Ä¢Identified this as a Cope rearrangement ([3,3]-sigmatropic),‚Ä¢Applied the key principle that Cope rearrangements preserve the degree of unsaturation,‚Ä¢Used the ‚Äúhexahydro‚Äù descriptor to distinguish between possible products,‚Ä¢Selected the product that maintained the same hydrogenation state.This reasoning is correct ‚Äîa thermal Cope rearrangement would preserve the number of hydrogen atoms, so the ‚Äúhexahydro‚Äù descriptor should be maintained in the product.The model also correctly noted that the product should be a ‚Äúfluorene‚Äù derivative (not ‚Äúfluorine‚Äù as appears in options B and D, which is likely a typo).
Overall, the model‚Äôs reasoning is thorough and chemically accurate. I don‚Äôt detect any significant flaws in the analysis, and the selection of option A is well-justified based on both reactions.{rec, 95}
Claude (Continued)IUQ-High:I need to evaluate the first model‚Äôs reasoning for both reactions to determine my confidence in their answer.First Reaction Analysis:The model correctly identifies this as a [2,3]-Wittig rearrangement with:‚Ä¢Starting material: (((3-methylbut-2-en-1-yl)oxy)methyl)benzene (Ph-CH2-O-CH2-C(CH3)=CH-CH3)‚Ä¢BuLi deprotonates the benzylic position2,3-sigmatropic rearrangement occurs‚Ä¢Final product: Ph-CH(OH)-CH2-C(CH3)=CH-CH3(4-methyl-1-phenylpent-3-en-1-ol)This is mechanistically sound. The carbanion formed at the benzylic position rearranges to form a new C-C bond at theŒ≥ùõæ\\gammaitalic\_Œ≥-carbon of the allylic system, with the oxygen ending up as a hydroxyl group. This produces 4-methyl-1-phenylpent-3-en-1-ol, not the (Z)-2-methyl-5-phenylpent-2-en-1-ol structure.Second Reaction Analysis:The model correctly identifies:‚Ä¢Cope rearrangement is a [3,3]-sigmatropic rearrangement‚Ä¢It preserves the degree of unsaturation‚Ä¢The starting material has ‚Äúhexahydro‚Äù in its name, meaning it has 6 hydrogen atoms adding to the basic structureThe model rightly concludes that the product should maintain the ‚Äúhexahydro‚Äù descriptor rather than changing to ‚Äútetrahydro,‚Äù which would indicate a change in unsaturation.
Looking at the options, only (A) and (C) have ‚Äúhexahydro‚Äù in product B. Since product A in option (A) matches our expected 4-methyl-1-phenylpent-3-en-1-ol, option (A) is correct.
The reasoning is chemically sound and follows established principles of organic chemistry reactions. I see no significant flaws in the analysis.{rec, 95}
DeepSeekFirst round:
The first reaction involves (((3-methylbut-2-en-1-yl)oxy)methyl)benzene treated with BuLi followed by H+. BuLi, a strong base, deprotonates the allylic positionŒ±ùõº\\alphaitalic\_Œ±to the oxygen in the 3-methylbut-2-en-1-yl group, forming an anion. This anion undergoes a [2,3]-Wittig rearrangement, resulting in a sigmatropic shift that forms a new carbon-carbon bond. After protonation with H+, the product is 4-methyl-1-phenylpent-3-en-1-ol (Ph-CH(OH)-CH2-CH=C(CH3)2). The double bond in this product is trisubstituted with no stereoisomers due to the gem-dimethyl group, so no (Z) or (E) designation is needed.For the second reaction, 3,4,5,7,8,9-hexamethyl-1,11-dimethylene-2,6,10,11,11a,11b-hexahydro-1H-benzo[cd]indeno[7,1-gh]azulene undergoes a thermal Cope rearrangement upon heating. The Cope rearrangement is a [3,3]-sigmatropic shift that rearranges the 1,5-diene system, shifting the double bonds and changing the ring connectivity without altering the saturation level. The product is 2,3,4,6,7,8-hexamethyl-5,9,9a,10,11,11a-hexahydro-1H-benzo[3,4]azuleno[1,8,7,6-cdef]fluorene. The ‚Äúhexahydro‚Äù designation is consistent with the starting material, as the Cope rearrangement does not change the degree of unsaturation.Comparing the options:‚Ä¢Option (A) matches both products: A is 4-methyl-1-phenylpent-3-en-1-ol and B is the hexahydro compound.‚Ä¢Option (B) has A as the (Z)-2-methyl-5-phenylpent-2-en-1-ol, which is incorrect for the first reaction, and B as a tetrahydro compound, which is inconsistent with the Cope rearrangement.‚Ä¢Option (C) has A as the (Z)-2-methyl-5-phenylpent-2-en-1-ol, which is incorrect, though B is the correct hexahydro compound.‚Ä¢Option (D) has A as 4-methyl-1-phenylpent-3-en-1-ol, which is correct, but B as a tetrahydro compound, which is incorrect.Thus, only option (A) correctly identifies both products. Confidence is high due to the mechanistic clarity of the [2,3]-Wittig and Cope rearrangements.
{A, 95}
DeepSeek (Continued)IUQ-Low:After reviewing the reasoning trace provided by the first model, I have evaluated its validity based on the mechanisms described for both reactions. Here is my reasoning:1. First Reaction ([2,3]-Wittig Rearrangement):‚Ä¢The starting material, (((3-methylbut-2-en-1-yl)oxy)methyl)benzene, is Ph-CH2-O-CH2-CH=C(CH3)2.‚Ä¢BuLi deprotonates the benzylic position (Ph-CH2-O-), forming an anion at the benzylic carbon. This is chemically sound, as benzylic protons are acidic.‚Ä¢The [2,3]-Wittig rearrangement involves a sigmatropic shift where the benzylic carbon bonds to theŒ≥ùõæ\\gammaitalic\_Œ≥-carbon of the allyl system (the =C(CH3)2group), yielding Ph-CH(OH)-CH2-CH=C(CH3)2after protonation.‚Ä¢The product, 4-methyl
