# Rerankers and Two-Stage Retrieval

**URL:** https://www.pinecone.io/learn/series/rag/rerankers/
**Published:** None

---

## Summary

The webpage discusses **Rerankers and Two-Stage Retrieval** as a method to improve **Retrieval Augmented Generation (RAG)** pipelines when out-of-the-box RAG performance is suboptimal.

Key points related to your query:

*   **RAG Architectures:** It focuses on a two-stage retrieval system where the first stage uses a fast **vector search** (relying on **embeddings**/bi-encoders) to retrieve a larger set of documents, and the second stage uses a slower but more accurate **reranker** to reorder and select the most relevant documents before passing them to the LLM.
*   **Embeddings:** It mentions that the first stage uses embedding models (like `multilingual-e5-large`) to transform text into vectors for similarity search in a **vector database** (Pinecone is used in the example).
*   **Rerankers:** Rerankers (cross-encoders) are significantly more accurate than embedding models because they analyze the query and document pair directly, avoiding the information loss inherent in compressing text into a single vector. They are used to maximize LLM recall by minimizing noise in the context window.
*   **Chunking Strategies:** The example uses pre-chunked data from the `jamescalam/ai-arxiv-chunked` dataset, where each record is 1-2 paragraphs long.

The page does not explicitly detail **new efficient models** for embeddings, **alternatives** to RAG, or **hybrid search** strategies, although it implies that the first stage of retrieval could use sparse embedding models alongside vector search.

---

## Full Content

Rerankers and Two-Stage Retrieval | Pinecone
[Pinecone Dedicated Read Nodes are in Public Preview: Predictable speed and cost for billion-vector and high-QPS workloads-Learn more](https://www.pinecone.io/blog/dedicated-read-nodes/)Dismiss
[](https://www.pinecone.io/)
# Rerankers and Two-Stage Retrieval
Jump to section
* [Recall vs. Context Windows](#Recall-vs.-Context-Windows)
* [Power of Rerankers](#Power-of-Rerankers)
* [Implementing Two-Stage Retrieval with Reranking](#Implementing-Two-Stage-Retrieval-with-Reranking)
* [References](#References)
[Retrieval Augmented Generation (RAG)](https://www.pinecone.io/learn/retrieval-augmented-generation/)is an overloaded term. It promises the world, but after developing a RAG pipeline, there are many of us left wondering why it doesn&#x27;t work as well as we had expected.
As with most tools, RAG is easy to use but hard to master. The truth is that there is more to RAG than putting documents into a vector DB and adding an LLM on top. That*can work*, but it won&#x27;t always.
This ebook aims to tell you what to do when out-of-the-box RAG*doesn&#x27;t*work. In this first chapter, we&#x27;ll look at what is often the easiest and fastest to implement solution for suboptimal RAG pipelines —we&#x27;ll be learning about*rerankers*.
Video companion for this chapter.
## Recall vs. Context Windows
Before jumping into the solution, let&#x27;s talk about the problem. With RAG, we are performing a*semantic search*across many text documents —these could be tens of thousands up to tens of billions of documents.
To ensure fast search times at scale, we typically use vector search —that is, we transform our text into vectors, place them all into a vector space, and compare their proximity to a query vector using a similarity metric like cosine similarity.
For vector search to work, we need vectors. These vectors are essentially compressions of the &quot;meaning&quot; behind some text into (typically) 768 or 1536-dimensional vectors. There is some information loss because we&#x27;re compressing this information into a single vector.
Because of this information loss, we often see that the top three (for example) vector search documents will miss relevant information. Unfortunately, the retrieval may return relevant information below our`top\_k`cutoff.
What do we do if relevant information at a lower position would help our LLM formulate a better response? The easiest approach is to increase the number of documents we&#x27;re returning (increase`top\_k`) and pass them all to the LLM.
The metric we would measure here is*recall*— meaning &quot;how many of the relevant documents are we retrieving&quot;. Recall does not consider the total number of retrieved documents —so we can hack the metric and get*perfect*recall by returning*everything*.
Unfortunately, we cannot return everything. LLMs have limits on how much text we can pass to them —we call this limit the*context window*. Some LLMs have huge context windows, like Anthropic&#x27;s Claude, with a context window of 100K tokens [1]. With that, we could fit many tens of pages of text —so could we return many documents (not quite all) and &quot;stuff&quot; the context window to improve recall?
Again, no. We cannot use context stuffing because this reduces the LLM&#x27;s*recall*performance —note that this is the LLM recall, which is different from the retrieval recall we have been discussing so far.
![When storing information in the middle of a context window, an LLM&#x27;s ability to recall that information becomes worse than had it not been provided in the first place](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fca206b6ada9163bffad313e0e18feee0b460c768-1212x688.png&amp;w=3840&amp;q=75)
When storing information in the middle of a context window, an LLM&#x27;s ability to recall that information becomes worse than had it not been provided in the first place [2].
LLM recall refers to the ability of an LLM to find information from the text placed within its context window. Research shows that LLM recall degrades as we put more tokens in the context window [2]. LLMs are also less likely to follow instructions as we stuff the context window —so context stuffing is a bad idea.
We can increase the number of documents returned by our vector DB to increase retrieval recall, but we cannot pass these to our LLM without damaging LLM recall.
The solution to this issue is to maximize retrieval recall by retrieving plenty of documents and then maximize LLM recall by*minimizing*the number of documents that make it to the LLM. To do that, we reorder retrieved documents and keep just the most relevant for our LLM —to do that, we use*reranking*.
## Power of Rerankers
A[reranking model](https://docs.pinecone.io/models/bge-reranker-v2-m3)— also known as a**cross-encoder**— is a type of model that, given a query and document pair, will output a similarity score. We use this score to reorder the documents by relevance to our query.
![A two-stage retrieval system. The vector DB step will typically include a bi-encoder or sparse embedding model.](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F906c3c0f8fe637840f134dbf966839ef89ac7242-3443x1641.png&amp;w=3840&amp;q=75)
A two-stage retrieval system. The vector DB step will typically include a bi-encoder or sparse embedding model.
Search engineers have used rerankers in two-stage retrieval systems for*a long time*. In these two-stage systems, a first-stage model (an embedding model/retriever) retrieves a set of relevant documents from a larger dataset. Then, a second-stage model (the reranker) is used to rerank those documents retrieved by the first-stage model.
We use two stages because retrieving a small set of documents from a large dataset is much faster than reranking a large set of documents —we&#x27;ll discuss why this is the case soon —but TL;DR, rerankers are slow, and retrievers are*fast*.
### Why Rerankers?
If a reranker is so much slower, why bother using them? The answer is that rerankers are much more accurate than[embedding models](https://www.pinecone.io/learn/series/rag/embedding-models-rundown/).
The intuition behind a bi-encoder&#x27;s inferior accuracy is that bi-encoders must compress all of the possible meanings of a document into a single vector —meaning we lose information. Additionally, bi-encoders have no context on the query because we don&#x27;t know the query until we receive it (we create embeddings before user query time).
On the other hand, a reranker can receive the raw information directly into the large transformer computation, meaning less information loss. Because we are running the reranker at user query time, we have the added benefit of analyzing our document&#x27;s meaning specific to the user query —rather than trying to produce a generic, averaged meaning.
Rerankers avoid the information loss of bi-encoders —but they come with a different penalty —*time*.
![A bi-encoder model compresses the document or query meaning into a single vector. Note that the bi-encoder processes our query in the same way as it does documents, but at user query time.](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F4509817116ab72e27bae809c38cb48fbf1578b5d-2760x1420.png&amp;w=3840&amp;q=75)
A bi-encoder model compresses the document or query meaning into a single vector. Note that the bi-encoder processes our query in the same way as it does documents, but at user query time.
When using bi-encoder models with vector search, we frontload all of the heavy transformer computation to when we are creating the initial vectors —that means that when a user queries our system, we have already created the vectors, so all we need to do is:
1. Run a single transformer computation to create the query vector.
2. Compare the query vector to document vectors with*cosine similarity*(or another lightweight metric).
With rerankers, we are not pre-computing anything. Instead, we&#x27;re feeding our query and a single other document into the transformer, running a whole transformer inference step, and outputting a single similarity score.
![A reranker considers query and document to produce a single similarity score over a full transformer inference step. Note that document A here is equivalent to our query.](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100.png&amp;w=3840&amp;q=75)
A reranker considers query and document to produce a single similarity score over a full transformer inference step. Note that document A here is equivalent to our query.
Given 40M records, if we use a**small**reranking model like BERT on a V100 GPU —we&#x27;d be waiting more than 50 hours to return a single query result [3]. We can do the same in &lt;100ms with encoder models and vector search.
## Implementing Two-Stage Retrieval with Reranking
Now that we understand the idea and reason behind two-stage retrieval with rerankers, let&#x27;s see how to implement it (you can follow along with[this notebook](https://github.com/pinecone-io/examples/blob/master/learn/generation/better-rag/00-rerankers.ipynb). To begin we will set up our prerequisite libraries:
```
`!pip install -qU \\
datasets==2.14.5 \\
&quot;&quot;pinecone[grpc]&quot;&quot;==5.1.0`
```
### Data Preparation
Before setting up the retrieval pipeline, we need data to retrieve! We will use the[`jamescalam/ai-arxiv-chunked`](https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked)dataset from Hugging Face Datasets. This dataset contains more than 400 ArXiv papers on ML, NLP, and LLMs —including the Llama 2, GPTQ, and GPT-4 papers.
The dataset contains 41.5K pre-chunked records. Each record is 1-2 paragraphs long and includes additional metadata about the paper from which it comes. Here is an example:
We&#x27;ll be feeding this data into Pinecone, so let&#x27;s reformat the dataset to be more Pinecone-friendly when it does come to the later embed and index process. The format will contain`id`,`text`(which we will embed), and`metadata`. For this example, we won&#x27;t use metadata, but it can be helpful to include if we want to do[metadata filtering](https://www.pinecone.io/learn/vector-search-filtering/)in the future.
### Embed and Index
To store everything in the vector DB, we need to encode everything with an embedding / bi-encoder model. We will use the open source`multilingial-e5-large`via Pinecone Inference. We need a [free Pinecone API key](https://app.pinecone.io) to authenticate ourselves via the client:
```
`from pinecone.grpc import PineconeGRPC
# get API key from app.pinecone.io
api\_key = &quot;&quot;PINECONE\_API\_KEY&quot;&quot;
embed\_model = &quot;&quot;multilingual-e5-large&quot;&quot;
# configure client
pc = PineconeGRPC(api\_key=api\_key)`
```
Now, we create our vector DB to store our vectors. We set`dimension`equal to the dimensionality of E5 large (`1024`) and use a`metric`compatible with E5 —ie`cosine`.
```
`import time
index\_name = &quot;&quot;rerankers&quot;&quot;
existing\_indexes = [
index\_info[&quot;&quot;name&quot;&quot;] for index\_info in pc.list\_indexes()
]
# check if index already exists (it shouldn&#x27;&#x27;t if this is first time)
if index\_name not in existing\_indexes:
# if does not exist, create index
pc.create\_index(
index\_name,
dimension=1024, # dimensionality of e5-large
metric=&#x27;&#x27;cosine&#x27;&#x27;,
spec=spec
)
# wait for index to be initialized
while not pc.describe\_index(index\_name).status[&#x27;&#x27;ready&#x27;&#x27;]:
time.sleep(1)
# connect to index
index = pc.Index(index\_name)
time.sleep(1)
# view index stats
index.describe\_index\_stats()`
```
We create a new function, `embed`, to handle embedding with our model. Within the function, we also include the handling of rate limit errors.
```
`from pinecone\_plugins.inference.core.client.exceptions import PineconeApiException
def embed(batch: list[str]) -&gt;&gt; list[float]:
# create embeddings (exponential backoff to avoid RateLimitError)
for j in range(5): # max 5 retries
try:
res = pc.inference.embed(
model=embed\_model,
inputs=batch,
parameters={
&quot;&quot;input\_type&quot;&quot;: &quot;&quot;passage&quot;&quot;, # for docs/context/chunks
&quot;&quot;truncate&quot;&quot;: &quot;&quot;END&quot;&quot;, # truncate to max length
}
)
passed = True
except PineconeApiException:
time.sleep(2\*\*j) # wait 2^j seconds before retrying
print(&quot;&quot;Retrying...&quot;&quot;)
if not passed:
raise RuntimeError(&quot;&quot;Failed to create embeddings.&quot;&quot;)
# get embeddings
embeds = [x[&quot;&quot;values&quot;&quot;] for x in res.data]
return embeds`
```
We&#x27;re now ready to begin populating the index using the E5 embedding model like so:
```
`from tqdm.auto import tqdm
batch\_size = 96 # how many embeddings we create and insert at once
for i in tqdm(range(0, len(data), batch\_size)):
passed = False
# find end of batch
i\_end = min(len(data), i+batch\_size)
# create batch
batch = data[i:i\_end]
embeds = embed(batch[&quot;&quot;text&quot;&quot;])
to\_upsert = list(zip(batch[&quot;&quot;id&quot;&quot;], embeds, batch[&quot;&quot;metadata&quot;&quot;]))
# upsert to Pinecone
index.upsert(vectors=to\_upsert)`
```
Our index is now populated and ready for us to query!
### Retrieval Without Reranking
Before reranking, let&#x27;s see how our results look*without*it. We will define a function called`get\_docs`to return documents using the first stage of retrieval only:
```
`def get\_docs(query: str, top\_k: int) -&gt;&gt; list[str]:
# encode query
res = pc.inference.embed(
model=embed\_model,
inputs=[query],
parameters={
&quot;&quot;input\_type&quot;&quot;: &quot;&quot;query&quot;&quot;, # for queries
&quot;&quot;truncate&quot;&quot;: &quot;&quot;END&quot;&quot;, # truncate to max length
}
)
xq = res.data[0][&quot;&quot;values&quot;&quot;]
# search pinecone index
res = index.query(vector=xq, top\_k=top\_k, include\_metadata=True)
# get doc text
docs = [{
&quot;&quot;id&quot;&quot;: str(i),
&quot;&quot;text&quot;&quot;: x[&quot;&quot;metadata&quot;&quot;][&#x27;&#x27;text&#x27;&#x27;]
} for i, x in enumerate(res[&quot;&quot;matches&quot;&quot;])]
return docs`
```
Let&#x27;s ask about**R**einforcement**L**earning with**H**uman**F**eedback —a popular fine-tuning method behind the sudden performance gains demonstrated by ChatGPT when it was released.
We get reasonable performance here —notably relevant chunks of text:
|Document|Chunk|
0|&quot;enabling significant improvements in their performance&quot;|
0|&quot;iteratively aligning the models&#x27; responses more closely with human expectations and preferences&quot;|
0|&quot;instruction fine-tuning and RLHF can help fix issues with factuality, toxicity, and helpfulness&quot;|
1|&quot;increasingly popular technique for reducing harmful behaviors in large language models&quot;|
The remaining documents and text cover RLHF but don&#x27;t answer our specific question of*&quot;why we would want to do rlhf?&quot;*.
### Reranking Responses
We will use Pinecone&#x27;s rerank endpoint for this. We use the same Pinecone client but now hit`inference.rerank`like so:
```
`rerank\_name = &quot;&quot;bge-reranker-v2-m3&quot;&quot;
rerank\_docs = pc.inference.rerank(
model=rerank\_name,
query=query,
documents=docs,
top\_n=25,
return\_documents=True
)`
```
This returns a`RerankResult`object:
```
`RerankResult(
model=&#x27;&#x27;bge-reranker-v2-m3&#x27;&#x27;,
data=[
{ index=1, score=0.9071478,
document={id=&quot;&quot;1&quot;&quot;, text=&quot;&quot;RLHF Response ! I...&quot;&quot;} },
{ index=9, score=0.6954414,
document={id=&quot;&quot;9&quot;&quot;, text=&quot;&quot;team, instead of ...&quot;&quot;} },
... (21 more documents) ...,
{ index=17, score=0.13420755,
document={id=&quot;&quot;17&quot;&quot;, text=&quot;&quot;helpfulness and h...&quot;&quot;} },
{ index=23, score=0.11417085,
document={id=&quot;&quot;23&quot;&quot;, text=&quot;&quot;responses respons...&quot;&quot;} }
],
usage={&#x27;&#x27;rerank\_units&#x27;&#x27;: 1}
)`
```
We access the text content of the docs via`rerank\_docs.data[0][&quot;&quot;document&quot;&quot;][&quot;&quot;text&quot;&quot;]`.
Let&#x27;s create a function that will allow us to quickly compare original vs. reranked results.
```
`def compare(query: str, top\_k: int, top\_n: int):
# first get vec search results
top\_k\_docs = get\_docs(query, top\_k=top\_k)
# rerank
top\_n\_docs = pc.inference.rerank(
model=rerank\_name,
query=query,
documents=docs,
top\_n=top\_n,
return\_documents=True
)
original\_docs = []
reranked\_docs = []
# compare order change
print(&quot;&quot;[ORIGINAL] -&gt;&gt; [NEW]&quot;&quot;)
for i, doc in enumerate(top\_n\_docs.data):
print(str(doc.index)+&quot;&quot;\\t-&gt;&gt;\\t&quot;&quot;+str(i))
if i != doc.index:
reranked\_docs.append(f&quot;&quot;[{doc.index}]\\n&quot;&quot;+doc[&quot;&quot;document&quot;&quot;][&quot;&quot;text&quot;&quot;])
original\_docs.append(f&quot;&quot;[{i}]\\n&quot;&quot;+top\_k\_docs[i][&#x27;&#x27;text&#x27;&#x27;])
else:
reranked\_docs.append(doc[&quot;&quot;document&quot;&quot;][&quot;&quot;text&quot;&quot;])
original\_docs.append(None)
# print results
for orig, rerank in zip(original\_docs, reranked\_docs):
if not orig:
print(f&quot;&quot;SAME:\\n{rerank}\\n\\n---\\n&quot;&quot;)
else:
print(f&quot;&quot;ORIGINAL:\\n{orig}\\n\\nRERANKED:\\n{rerank}\\n\\n---\\n&quot;&quot;)`
```
We start with our RLHF query. This time, we do a more standard retrieval-rerank process of retrieving 25 documents (`top\_k=25`) and reranking to the top three documents (`top\_n=3`).
Looking at these, we have dropped the one relevant chunk of text from document`1`and*no relevant*chunks of text from document`2`— the following relevant pieces of information now replace these:
|Original Position|Rerank Position|Chunk|
23|1|&quot;train language models that act as helpful and harmless assistants&quot;|
23|1|&quot;RLHF training also improves honesty&quot;|
23|1|&quot;RLHF improves helpfulness and harmlessness by a huge margin&quot;|
23|1|&quot;enhance the capabilities of large models&quot;|
14|2|&quot;the model outputs safe responses&quot;|
14|2|&quot;often more detailed than what the average annotator writes&quot;|
14|2|&quot;RLHF to reach the model how to write more nuanced responses&quot;|
14|2|&quot;make the model more robust to jailbreak attempts&quot;|
After reranking, we have*far more*relevant information. Naturally, this can result in significantly better performance for RAG. It means we maximize relevant information while minimizing noise input into our LLM.
Reranking is one of the simplest methods for dramatically improving recall performance in**R**etrieval**A**ugmented**G**eneration (RAG) or any other retrieval-based pipeline.
We&#x27;ve explored why[rerankers can provide so much better performance](https://www.pinecone.io/learn/refine-with-rerank/)than their embedding model counterparts —and how a two-stage retrieval system allows us to get the best of both, enabling search at scale while maintaining quality performance.
## References
[1][Introducing 100K Context Windows](https://www.anthropic.com/index/100k-context-windows)(2023), Anthropic
[2] N. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, P. Liang,[Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)(2023),
[3] N. Reimers, I. Gurevych,[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf)(2019), UKP-TUDA
Share:
[](https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/series/rag/rerankers)[](https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/series/rag/rerankers)[](https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/series/rag/rerankers)
Was this article helpful?
YesNo
![Learn how to build advanced retrieval augmented generation (RAG) pipelines. (series cover image)](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F41f6dc86b6d799a261f385a7b9e9b1b3c3ec3721-1432x1847.png&amp;w=3840&amp;q=100)[Retrieval Augmented Generation](https://www.pinecone.io/learn/series/rag/)
Chapters
1. [Rerankers for RAG](https://www.pinecone.io/learn/series/rag/rerankers/)
* [Recall vs. Context Windows](#Recall-vs.-Context-Windows)
* [Power of Rerankers](#Power-of-Rerankers)
* [Implementing Two-Stage Retrieval with Reranking](#Implementing-Two-Stage-Retrieval-with-Reranking)
* [References](#References)
* [Embedding Models](https://www.pinecone.io/learn/series/rag/embedding-models-rundown/)
* [Agent Evaluation](https://www.pinecone.io/learn/series/rag/ragas/)
