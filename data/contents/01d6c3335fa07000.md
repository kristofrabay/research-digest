# Top 7 Rerankers for RAG

**URL:** https://www.analyticsvidhya.com/blog/2025/06/top-rerankers-for-rag/
**Published:** 2025-06-27T01:01:00.000Z

---

## Summary

The webpage focuses on **Rerankers for Retrieval-Augmented Generation (RAG)** systems.

It explains that RAG improves LLMs by retrieving external information, but initial retrieval (like vector search) can be noisy. **Rerankers** are introduced as a crucial second step to refine these initial results by reordering documents based on their specific relevance to the user's query, ensuring only the best context is passed to the LLM.

The page details the **enhanced RAG pipeline**: Retrieve $\rightarrow$ **Rerank** $\rightarrow$ Generate.

It then reviews the **Top 7 Reranking Models in 2025**, including:
*   **Proprietary/API-based:** Cohere Rerank, Voyage Rerank.
*   **Open-Source:** bge-reranker, ColBERT, FlashRank, MixedBread (mxbai-rerank-v2).
*   **Mixed/Specialized:** Jina Reranker (which also offers Jina-ColBERT for long documents).

The summary also covers how to **evaluate** rerankers (using metrics like NDCG and MRR) and factors to consider when **choosing** one (accuracy, latency, cost, scalability).

**Regarding your specific query components:**

*   **Vector databases:** Mentioned as the source for initial retrieval in the RAG architecture diagram.
*   **Embeddings (new efficient models):** Mentioned in the context of initial retrieval, and specific embedding models (like VoyageAIEmbeddings) are used in the examples.
*   **Rerankers:** This is the primary topic of the entire page.
*   **RAG architectures:** The standard and enhanced RAG pipelines are described.
*   **RAG alternatives:** Not explicitly discussed.
*   **Hybrid search:** Not explicitly discussed.
*   **Chunking strategies:** Mentioned in the context of splitting documents before retrieval in the code examples (using `RecursiveCharacterTextSplitter`).

---

## Full Content

Top 7 Rerankers for RAG
[
Master Generative AI with 10+ Real-world Projects in 2025!
* ###### d:
* ###### h:
* ###### m:
* ###### s
Download Projects](https://www.analyticsvidhya.com/pinnacleplus/pinnacleplus-projects?utm_source=blog_india&utm_medium=desktop_flashstrip&utm_campaign=15-Feb-2025||&utm_content=projects)
[Interview Prep](https://www.analyticsvidhya.com/blog/category/interview-questions/?ref=category)
[Career](https://www.analyticsvidhya.com/blog/category/career/?ref=category)
[GenAI](https://www.analyticsvidhya.com/blog/category/generative-ai/?ref=category)
[Prompt Engg](https://www.analyticsvidhya.com/blog/category/prompt-engineering/?ref=category)
[ChatGPT](https://www.analyticsvidhya.com/blog/category/chatgpt/?ref=category)
[LLM](https://www.analyticsvidhya.com/blog/category/llms/?ref=category)
[Langchain](https://www.analyticsvidhya.com/blog/category/langchain/?ref=category)
[RAG](https://www.analyticsvidhya.com/blog/category/rag/?ref=category)
[AI Agents](https://www.analyticsvidhya.com/blog/category/ai-agent/?ref=category)
[Machine Learning](https://www.analyticsvidhya.com/blog/category/machine-learning/?ref=category)
[Deep Learning](https://www.analyticsvidhya.com/blog/category/deep-learning/?ref=category)
[GenAI Tools](https://www.analyticsvidhya.com/blog/category/ai-tools/?ref=category)
[LLMOps](https://www.analyticsvidhya.com/blog/category/llmops/?ref=category)
[Python](https://www.analyticsvidhya.com/blog/category/python/?ref=category)
[NLP](https://www.analyticsvidhya.com/blog/category/nlp/?ref=category)
[SQL](https://www.analyticsvidhya.com/blog/category/sql/?ref=category)
[AIML Projects](https://www.analyticsvidhya.com/blog/category/project/?ref=category)
#### Reading list
##### Introduction to Generative AI
[What is Generative AI?](https://www.analyticsvidhya.com/blog/2023/04/what-is-generative-ai/)
##### Introduction to Generative AI applications
[Overview of generative AI applications and their impact](https://www.analyticsvidhya.com/blog/2023/11/beyond-the-buzz-exploring-the-practical-applications-of-generative-ai-in-industries/)
##### No-code Generative AI app development
[Introduction to No-code AI Development](https://www.analyticsvidhya.com/blog/2024/03/step-by-step-guide-to-training-ml-model-with-no-code/)
##### Code-focused Generative AI App Development
[Introduction to LangChain, ChatGPT and Gemini Pro](https://www.analyticsvidhya.com/blog/2023/12/google-gemini-api/)
##### Introduction to Responsible AI
[Introduction to Responsible AI](https://www.analyticsvidhya.com/blog/2023/08/responsible-ai/)
##### LLMS
[What are Large Language Models?](https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/)[GPT models](https://www.analyticsvidhya.com/blog/2022/12/chatgpt-unlocking-the-potential-of-artificial-intelligence-for-human-like-conversation/)[Mistral](https://www.analyticsvidhya.com/blog/2024/07/mistral-large-2-powerful-enough-to-challenge-llama-3-1-405b/)[Llama](https://www.analyticsvidhya.com/blog/2023/08/getting-started-with-llama-2/)[Gemini](https://www.analyticsvidhya.com/blog/2023/12/what-is-google-gemini-features-usage-and-limitations/)[How to build diffferent LLM AppIications?](https://www.analyticsvidhya.com/blog/2023/07/building-llm-powered-applications-with-langchain/)
##### Prompt Engineering
[Introduction to Prompt Engineering](https://www.analyticsvidhya.com/blog/2023/05/what-is-prompt-engineering-guide/)[Best Practices and Guidelines for Prompt Engineering](https://www.analyticsvidhya.com/blog/2023/06/what-is-prompt-engineering/)[N shot prompting](https://www.analyticsvidhya.com/blog/2023/09/power-of-llms-zero-shot-and-few-shot-prompting/)[Chain of Thought](https://www.analyticsvidhya.com/blog/2023/12/what-is-chain-of-thought-prompting-and-its-benefits/)[Tree of Thoughts](https://www.analyticsvidhya.com/blog/2024/07/tree-of-thoughts/)[Skeleton of Thoughts](https://www.analyticsvidhya.com/blog/2024/07/skeleton-of-thoughts/)[Chain of Emotion](https://www.analyticsvidhya.com/blog/2024/07/chain-of-emotion-in-prompt-engineering/)
##### Finetuning LLMs
[Introduction to Finetuning LLMs](https://www.analyticsvidhya.com/blog/2023/08/finetuning-large-language-models-llms/)[Parameter-Efficient Finetuning (PEFT)](https://www.analyticsvidhya.com/blog/2023/10/llm-fine-tuning-with-peft-techniques/)[LORA](https://www.analyticsvidhya.com/blog/2024/05/ludwig-a-comprehensive-guide-to-llm-fine-tuning-using-lora/)[QLORA](https://www.analyticsvidhya.com/blog/2023/08/lora-and-qlora/)[using Unsloth](https://www.analyticsvidhya.com/blog/2024/04/fine-tuning-google-gemma-with-unsloth/)[using Huggingface](https://www.analyticsvidhya.com/blog/2023/10/hugging-face-fine-tuning-tutorial/)
##### Training LLMs from Scratch
[What do you mean by Training LLMs from Scratch?](https://www.analyticsvidhya.com/blog/2023/07/beginners-guide-to-build-large-language-models-from-scratch/)
##### Langchain
[Intro to the LangChain Ecosystem](https://www.analyticsvidhya.com/blog/2024/06/langchain-guide/)[Core Components of LangChain](https://www.analyticsvidhya.com/blog/2023/10/a-comprehensive-guide-to-using-chains-in-langchain/)[Applications of LCEL Chains](https://www.analyticsvidhya.com/blog/2024/06/langchain-expression-language/)[RAG using LangChain](https://www.analyticsvidhya.com/blog/2023/12/multi-modal-rag-pipeline-with-langchain/)[LangGraph](https://www.analyticsvidhya.com/blog/2024/07/langgraph-revolutionizing-ai-agent/)[LangSmith](https://www.analyticsvidhya.com/blog/2024/07/ultimate-langsmith-guide/)
##### RAG
[Introduction to RAG systems](https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/)[Evaluation of RAG systems](https://www.analyticsvidhya.com/blog/2024/05/a-beginners-guide-to-evaluating-rag-pipelines-using-ragas/)
##### LlamaIndex
[Getting Started with LlamaIndex](https://www.analyticsvidhya.com/blog/2023/07/llamaindex-qa-system/)[Components of LlamaIndex](https://www.analyticsvidhya.com/blog/2023/10/rag-pipeline-with-the-llama-index/)[Advanced approaches for powerful RAG system](https://www.analyticsvidhya.com/blog/2024/08/improving-real-world-rag-systems/)
##### Stable Diffusion
[Introduction to Stable Diffusion](https://www.analyticsvidhya.com/blog/2023/12/what-is-stable-diffusion/)[Generating image using Stable diffusion](https://www.analyticsvidhya.com/blog/2023/09/image-generation-using-stable-diffusion/)[Diffusion models](https://www.analyticsvidhya.com/blog/2024/09/what-are-diffusion-models/)[Prompt Engineering Concepts for Stable Diffusion](https://www.analyticsvidhya.com/blog/2023/05/how-to-generate-images-using-stable-diffusion/)[MidJourney](https://www.analyticsvidhya.com/blog/2023/10/how-to-use-midjourney-ai/)[Understanding Dalle 3](https://www.analyticsvidhya.com/blog/2024/07/dall-e3/)
# Top 7 Rerankers for RAG
[![Harsh Mishra](https://av-eks-lekhak.s3.amazonaws.com/media/lekhak-profile-images/converted_image_0fBqNLi.webp)](https://www.analyticsvidhya.com/blog/author/harsh9480979/)
[Harsh Mishra](https://www.analyticsvidhya.com/blog/author/harsh9480979/)Last Updated : 03 Dec, 2025
16min read
[Retrieval-Augmented Generation](https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/), or RAG, marks an important step forward for natural language processing. It helps[large language models](https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/)(LLMs) perform better by letting them check information outside their training data before creating a response. This means LLMs can work well with specific company knowledge or new information without costly retraining. Rerankers for RAG play a crucial role in refining retrieved information, ensuring the most relevant context is provided. RAG blends information retrieval with text generation, resulting in accurate, relevant answers that sound natural.
## Table of contents
* [Why Initial Retrieval Isn&#8217;t Enough](#h-why-initial-retrieval-isn-t-enough)
* [Enter Rerankers: Refining the Search](#h-enter-rerankers-refining-the-search)
* [How Reranking Improves RAG](#h-how-reranking-improves-rag)
* [Top Reranking Models in 2025](#h-top-reranking-models-in-2025)
* [Cohere Rerank](#h-cohere-rerank)
* [bge-reranker (Base/Large)](#h-bge-reranker-base-large)
* [Voyage Rerank](#h-voyage-rerank)
* [Jina Reranker](#h-jina-reranker)
* [ColBERT](#h-colbert)
* [FlashRank](#h-flashrank)
* [MixedBread](#h-mixedbread)
* [How to Tell if Your Reranker is Working](#h-how-to-tell-if-your-reranker-is-working)
* [Choosing the Right Reranker for Your Needs](#h-choosing-the-right-reranker-for-your-needs)
* [Conclusion](#h-conclusion)
* [Frequently Asked Questions](#h-frequently-asked-questions)
## Why Initial Retrieval Isn&#8217;t Enough
The first step in RAG involves finding documents related to a user&#8217;s query. Systems often use methods like keyword search or vector similarity. These methods are good starting points, but they can return many documents that aren&#8217;t all equally useful. The embedding models used might not grasp the fine details needed to pick the most relevant information.
Vector search, which looks for similar meanings, can struggle with short queries or specialized terms. Also, LLMs have limits on how much context they can handle well. Feeding them too many documents, even slightly relevant ones, can confuse the model and lower the quality of the final answer. This initial &#8220;noisy&#8221; retrieval can dilute the LLM&#8217;s focus. We need a way to refine this first batch of information.
![rag system architecture](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/04/rag-system-architecture.webp)
This image depicts the retrieval and generation steps of RAG, a question is asked by the user and then our system extracts the results based on the question by searching the Vector store. Then the retrieved content is passed to the LLM along with the question and LLM provides a structured output.
## Enter Rerankers: Refining the Search
This is where rerankers become essential. Reranking improves the precision of search results. Rerankers use smart algorithms to look at the initially retrieved documents and reorder them based on how well they match the user&#8217;s specific question and intent.
In RAG, rerankers act as a quality filter. They examine the first set of results and prioritize the documents that offer the best information for the query. The goal is to lift the most relevant pieces to the top. Think of a reranker as a specialist that double-checks the initial search, using a deeper understanding of language to find the best fit between the documents and the question.
![Rerankers](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/04/Rerankers.webp)Source:[Cohere](https://txt.cohere.com/rerank/)
This image illustrates a two-stage search process. Reranking is the second stage, where an initial set of search results, based on semantic or keyword matching, is refined to significantly improve the relevance and ordering of the final results, delivering a more accurate and useful outcome for the user&#8217;s query.
## How Reranking Improves RAG
Rerankers boost the accuracy of the context given to the LLM. They analyze the meaning and relationship between the user&#8217;s question and each retrieved document, going beyond simple keyword matching. This deeper understanding helps identify the most useful information.
By focusing the LLM on a smaller, better set of documents, rerankers lead to more precise answers. The LLM gets high-quality context, allowing it to form more informed and direct responses. Rerankers calculate a score showing how semantically close a document is to a query, allowing for a better final ordering. They can find relevant information even without exact keyword matches.
This focus on quality context helps reduce LLM &#8220;hallucinations&#8221;‚Äîinstances where the model generates incorrect but plausible information. Grounding the LLM in documents verified by a reranker makes the final output more trustworthy.
The standard RAG process involves retrieval then generation. An enhanced RAG pipeline adds a reranking step in the middle.
* **Retrieve:**Fetch an initial set of candidate documents.
* **Rerank:**Use a reranking model to reorder these documents based on relevance to the query.
* **Generate:**Provide only the top-ranked, most relevant documents to the LLM to create the answer.
This two-stage method lets the initial retrieval cast a wide net (recall), while the reranker focuses on picking the best items from that net (precision). This division improves the overall process and gives the LLM the best possible input.
![Reranking Improves RAG](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/04/Reranking-Improves-RAG.webp)Source:[PineCone](https://www.pinecone.io/learn/series/rag/rerankers/)
A query is used to search a vector database, retrieving the top 25 most relevant documents. These documents are then passed to a &#8220;Reranker&#8221; module. The reranker refines the results, selecting the top 3 most relevant documents for the final output.
## Top Reranking Models in 2025
Let us look into the top reranking models in 2025.
![Reranking models](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/04/unnamed-56.webp)Source:[MixedBread](https://www.mixedbread.com/docs/reranking/models)
Several reranking models are popular choices for RAG pipelines:
Reranker|Model Type|Source|Strength|Weakness|Best For|
[Cohere](https://cohere.com/rerank)|Cross-encoder( API)|Private|High Accuracy, Multilingual, Ease of Use, Speed (Nimble)|Cost (API fees), Closed-source|General RAG, Enterprise, Multilingual, Ease of Use|
[bge-reranker](https://huggingface.co/BAAI/bge-reranker-large)|Cross-encoder|Open-Source|High Accuracy, Open-source, Runs on moderate hardware|Requires self-hosting|General RAG, Open-source preference, Budget-conscious|
[Voyage](https://docs.voyageai.com/docs/reranker)|Cross-encoder( API)|Private|Top-tier Relevance/Accuracy|Cost (API fees), Potentially higher latency (top model)|Max Accuracy Needs (Finance, Legal), Relevance-critical apps|
[Jina](https://huggingface.co/jinaai/jina-embeddings-v2-base-en)|Cross-encoder / ColBERT variant|Mixed|Balanced Performance, Cost-effective, Long Docs (Jina-ColBERT)|May not reach peak accuracy|General RAG, Long documents, Balanced cost/performance|
[FlashRank](https://github.com/PrithivirajDamodaran/FlashRank)|Lightweight Cross-encoder|Open-Source|Very Fast, Low Resource Use, Easy Integration|Accuracy lower than large models|Speed-critical apps, Resource-constrained environments|
[ColBERT](https://huggingface.co/colbert-ir/colbertv2.0)|Multi-vector (Late Interaction)|Open-Source|Efficient at Scale (Large Collections), Fast Retrieval|Indexing compute/storage intensive|Very large document sets, Efficiency at scale|
[MixedBread (mxbai-rerank-v2)](https://www.mixedbread.com/blog/mxbai-rerank-v2)|Cross-encoder|Open-Source|SOTA Perf (claimed), Fast Inference, Multilingual, Long Context, Versatile|Requires self-hosting, Relatively new|High-Performance RAG, Multilingual, Long Docs/Code/JSON, Open-Source Pref|
### Cohere Rerank
Cohere Rerank uses a sophisticated[neural network](https://www.analyticsvidhya.com/blog/2022/01/introduction-to-neural-networks/), likely based on the transformer architecture, acting as a cross-encoder. It processes the query and document together to precisely judge relevance. It is a proprietary model accessed via an API.
* **Key Features:**A major feature is its support for over 100 languages, making it versatile for global applications. It integrates easily as a hosted service. Cohere also offers &#8220;Rerank 3 Nimble,&#8221; a version designed for significantly faster performance in production environments while retaining high accuracy.
* **Performance:**Cohere Rerank consistently delivers high accuracy across various embedding models used in the initial retrieval step. The Nimble variant reduces response time considerably. Costs depend on API usage.
* **Strengths:**Easy integration via API, strong and reliable performance, excellent multilingual capabilities, and a speed-optimized option (Nimble).
* **Weaknesses:**It is a closed-source, commercial service, so you pay per use and cannot modify the model.
* **Ideal Use Cases:**Good for general RAG applications, enterprise search platforms, customer support chatbots, and situations needing broad language support without managing model infrastructure.#### Example Code
First install the Cohere library.
```
`%pip install --upgrade --quiet cohere`
```
Set up the Cohere and ContextualCompressionRetriever.
```
`from langchain.retrievers.contextual\_compression import ContextualCompressionRetriever
from langchain\_cohere import CohereRerank
from langchain\_community.llms import Cohere
from langchain.chains import RetrievalQA
llm = Cohere(temperature=0)
compressor = CohereRerank(model="rerank-english-v3.0")
compression\_retriever = ContextualCompressionRetriever(
base\_compressor=compressor, base\_retriever=retriever
)
chain = RetrievalQA.from\_chain\_type(
llm=Cohere(temperature=0), retriever=compression\_retriever
)`
```
**Output:**
```
{'query': 'What did the president say about Ketanji Brown Jackson',
'result': " The president speaks highly of Ketanji Brown Jackson, stating that she
is one of the nation's top legal minds, and will continue the legacy of excellence
of Justice Breyer. The president also mentions that he worked with her family and
that she comes from a family of public school educators and police officers. Since
her nomination, she has received support from various groups, including the
Fraternal Order of Police and judges from both major political parties. \\n\\nWould
you like me to extract another sentence from the provided text? "}
```
### bge-reranker (Base/Large)
These models come from the Beijing Academy of Artificial Intelligence (BAAI) and are open-source (Apache 2.0 license). They are transformer-based, likely cross-encoders, designed specifically for reranking tasks. They are available in different sizes, like Base and Large.
* **Key Features:**Being open-source gives users freedom to deploy and modify them. The bge-reranker-v2-m3 model, for example, has under 600 million parameters, allowing it to run efficiently on common hardware, including consumer GPUs.
* **Performance:**These models perform very well, especially the large versions, often achieving results close to top commercial models. They demonstrate strong Mean Reciprocal Rank (MRR) scores. The cost is primarily the compute resources needed for self-hosting.
* **Strengths:**No licensing fees (open-source), strong accuracy, flexibility for self-hosting, and good performance even on moderate hardware.
* **Weaknesses:**Requires users to manage deployment, infrastructure, and updates. Performance depends on the hosting hardware.
* **Ideal Use Cases:**Suitable for general RAG tasks, research projects, teams preferring open-source tools, budget-aware applications, and users comfortable with self-hosting.#### Example Code
```
`from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document\_compressors import CrossEncoderReranker
from langchain\_community.cross\_encoders import HuggingFaceCrossEncoder
model = HuggingFaceCrossEncoder(model\_name="BAAI/bge-reranker-base")
compressor = CrossEncoderReranker(model=model, top\_n=3)
compression\_retriever = ContextualCompressionRetriever(
base\_compressor=compressor, base\_retriever=retriever
)
compressed\_docs = compression\_retriever.invoke("What is the plan for the economy?")
pretty\_print\_docs(compressed\_docs)`
```
**Output:**
```
Document 1:
More infrastructure and innovation in America.
More goods moving faster and cheaper in America.
More jobs where you can earn a good living in America.
And instead of relying on foreign supply chains, let‚Äôs make it in America.
Economists call it ‚Äúincreasing the productive capacity of our economy.‚Äù
I call it building a better America.
My plan to fight inflation will lower your costs and lower the deficit.
----------------------------------------------------------------------------------------------------
Document 2:
Second ‚Äìcut energy costs for families an average of $500 a year by combatting
climate change.
Let‚Äôs provide investments and tax credits to weatherize your homes and businesses to
be energy efficient and you get a tax credit; double America‚Äôs clean energy
production in solar, wind, and so much more; lower the price of electric vehicles,
saving you another $80 a month because you‚Äôll never have to pay at the gas pump
again.
----------------------------------------------------------------------------------------------------
Document 3:
Look at cars.
Last year, there weren‚Äôt enough semiconductors to make all the cars that people
wanted to buy.
And guess what, prices of automobiles went up.
So‚Äîwe have a choice.
One way to fight inflation is to drive down wages and make Americans poorer.
I have a better plan to fight inflation.
Lower your costs, not your wages.
Make more cars and semiconductors in America.
More infrastructure and innovation in America.
More goods moving faster and cheaper in America.
```
### Voyage Rerank
Voyage AI provides proprietary neural network models (voyage-rerank-2, voyage-rerank-2-lite) accessed via API. These are likely advanced cross-encoders finely tuned for maximum relevance scoring.
* **Key Features:**Their main distinction is achieving top-tier relevance scores in benchmark tests. Voyage provides a simple Python client library for easy integration. The lite version offers a balance between performance and speed/cost.
* **Performance:**voyage-rerank-2 often leads benchmarks in terms of pure relevance accuracy. The lite model performs comparably to other strong contenders. The high-accuracy rerank-2 model might have slightly higher latency than some competitors. Costs are tied to API usage.
* **Strengths:**State-of-the-art relevance, potentially the most accurate option available. Easy to use via their Python client.
* Weaknesses: Proprietary API-based service with associated costs. The highest accuracy model might be marginally slower than others.
* **Ideal Use Cases:**Best suited for applications where maximizing relevance is critical, such as financial analysis, legal document review, or high-stakes question answering where accuracy outweighs slight speed differences.#### Example Code
First install the voyage library
```
`%pip install --upgrade --quiet voyageai
%pip install --upgrade --quiet langchain-voyageai`
```
Set up the Cohere and ContextualCompressionRetriever
```
`from langchain\_community.document\_loaders import TextLoader
from langchain\_community.vectorstores import FAISS
from langchain.retrievers import ContextualCompressionRetriever
from langchain\_openai import OpenAI
from langchain\_voyageai import VoyageAIRerank
from langchain\_text\_splitters import RecursiveCharacterTextSplitter
from langchain\_voyageai import VoyageAIEmbeddings
documents = TextLoader("../../how\_to/state\_of\_the\_union.txt").load()
text\_splitter = RecursiveCharacterTextSplitter(chunk\_size=500, chunk\_overlap=100)
texts = text\_splitter.split\_documents(documents)
retriever = FAISS.from\_documents(
texts, VoyageAIEmbeddings(model="voyage-law-2")
).as\_retriever(search\_kwargs={"k": 20})
llm = OpenAI(temperature=0)
compressor = VoyageAIRerank(
model="rerank-lite-1", voyageai\_api\_key=os.environ&#91;&#91;"VOYAGE\_API\_KEY"], top\_k=3
)
compression\_retriever = ContextualCompressionRetriever(
base\_compressor=compressor, base\_retriever=retriever
)
compressed\_docs = compression\_retriever.invoke(
"What did the president say about Ketanji Jackson Brown"
)
pretty\_print\_docs(compressed\_docs)`
```
**Output:**
```
Document 1:
One of the most serious constitutional responsibilities a President has is
nominating someone to serve on the United States Supreme Court.
And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji
Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice
Breyer‚Äôs legacy of excellence.
----------------------------------------------------------------------------------------------------
Document 2:
So let‚Äôs not abandon our streets. Or choose between safety and equal justice.
Let‚Äôs come together to protect our communities, restore trust, and hold law
enforcement accountable.
That‚Äôs why the Justice Department required body cameras, banned chokeholds, and
restricted no-knock warrants for its officers.
----------------------------------------------------------------------------------------------------
Document 3:
I spoke with their families and told them that we are forever in debt for their
sacrifice, and we will carry on their mission to restore the trust and safety every
community deserves.
I‚Äôve worked on these issues a long time.
I know what works: Investing in crime prevention and community police officers
who‚Äôll walk the beat, who‚Äôll know the neighborhood, and who can restore trust and
safety.
So let‚Äôs not abandon our streets. Or choose between safety and equal justice.
```
### Jina Reranker
This offers reranking solutions including neural models like Jina Reranker v2 and Jina-ColBERT. Jina Reranker v2 is likely a cross-encoder style model. Jina-ColBERT implements the ColBERT architecture (explained next) using Jina&#8217;s base models.
* **Key Features:**Jina provides cost-effective options with good performance. A standout feature is Jina-ColBERT&#8217;s ability to handle very long documents, supporting context lengths up to 8,000 tokens. This reduces the need to aggressively chunk long texts. Open-source components are also part of Jina&#8217;s ecosystem.
* **Performance:**Jina Reranker v2 offers a good mix of speed, cost, and relevance. Jina-ColBERT excels when dealing with long source documents. Costs are generally competitive.
* **Strengths:**Balanced performance, cost-effective, excellent handling of long documents via Jina-ColBERT, flexibility with available open-source parts.
* **Weaknesses:**Standard Jina rerankers might not hit the absolute peak accuracy of specialized models like Voyage&#8217;s top tier.
* **Ideal Use Cases:**General RAG systems, applications processing long documents (technical manuals, research papers, books), projects needing a good balance between cost and performance.#### Example Code
```
`from langchain\_community.document\_loaders import TextLoader
from langchain\_community.embeddings import JinaEmbeddings
from langchain\_community.vectorstores import FAISS
from langchain\_text\_splitters import RecursiveCharacterTextSplitter
documents = TextLoader(
"../../how\_to/state\_of\_the\_union.txt",
).load()
text\_splitter = RecursiveCharacterTextSplitter(chunk\_size=500, chunk\_overlap=100)
texts = text\_splitter.split\_documents(documents)
embedding = JinaEmbeddings(model\_name="jina-embeddings-v2-base-en")
retriever = FAISS.from\_documents(texts, embedding).as\_retriever(search\_kwargs={"k": 20})
query = "What did the president say about Ketanji Brown Jackson"
docs = retriever.get\_relevant\_documents(query)`
```
Doing Reranking with Jina
```
`from langchain.retrievers import ContextualCompressionRetriever
from langchain\_community.document\_compressors import JinaRerank
compressor = JinaRerank()
compression\_retriever = ContextualCompressionRetriever(
base\_compressor=compressor, base\_retriever=retriever
)
compressed\_docs = compression\_retriever.get\_relevant\_documents(
"What did the president say about Ketanji Jackson Brown"
)
pretty\_print\_docs(compressed\_docs)`
```
**Output:**
```
Document 1:
So let‚Äôs not abandon our streets. Or choose between safety and equal justice.
Let‚Äôs come together to protect our communities, restore trust, and hold law
enforcement accountable.
That‚Äôs why the Justice Department required body cameras, banned chokeholds, and
restricted no-knock warrants for its officers.
----------------------------------------------------------------------------------------------------
Document 2:
I spoke with their families and told them that we are forever in debt for their
sacrifice, and we will carry on their mission to restore the trust and safety every
community deserves.
I‚Äôve worked on these issues a long time.
I know what works: Investing in crime prevention and community police officers
who‚Äôll walk the beat, who‚Äôll know the neighborhood, and who can restore trust and
safety.
So let‚Äôs not abandon our streets. Or choose between safety and equal justice.
```
### ColBERT
ColBERT (Contextualized Late Interaction over BERT) is a multi-vector model. Instead of representing a document with one vector, it creates multiple contextualized vectors (often one per token). It uses a &#8220;late interaction&#8221; mechanism where query vectors are compared against the many document vectors after encoding. This allows document vectors to be pre-calculated and indexed.
* **Key Features:**Its architecture allows for very efficient retrieval from large collections once documents are indexed. The multi-vector approach enables fine-grained comparisons between query terms and document content. It is an open-source approach.
* **Performance:**ColBERT offers a strong balance between retrieval effectiveness and efficiency, especially at scale. Retrieval latency is low after the initial indexing step. The main cost is compute for indexing and self-hosting.
* **Strengths:**Highly efficient for large document sets, scalable retrieval, open-source flexibility.
* Weaknesses: The initial indexing process can be computationally intensive and require significant storage.
* **Ideal Use Cases:**Large-scale RAG applications, systems needing fast retrieval over millions or billions of documents, scenarios where pre-computation time is acceptable.#### Example Code
Install the Ragtouille library for using ColBERT reranker.
```
`pip install -U ragatouille`
```
Now setting the up the ColBERT reranker
```
`from ragatouille import RAGPretrainedModel
from langchain.retrievers import ContextualCompressionRetriever
RAG = RAGPretrainedModel.from\_pretrained("colbert-ir/colbertv2.0")
compression\_retriever = ContextualCompressionRetriever(
base\_compressor=RAG.as\_langchain\_document\_compressor(), base\_retriever=retriever
)
compressed\_docs = compression\_retriever.invoke(
"What animation studio did Miyazaki found"
)
print(compressed\_docs&#91;&#91;0])`
```
**Output:**
```
Document(page\_content='In June 1985, Miyazaki, Takahata, Tokuma and Suzuki founded
the animation production company Studio Ghibli, with funding from Tokuma Shoten.
Studio Ghibli\\'s first film, Laputa: Castle in the Sky (1986), employed the same
production crew of Nausica√§. Miyazaki\\'s designs for the film\\'s setting were
inspired by Greek architecture and "European urbanistic templates". Some of the
architecture in the film was also inspired by a Welsh mining town; Miyazaki
witnessed the mining strike upon his first', metadata={'relevance\_score':
26.5194149017334})
```
### FlashRank
FlashRank is designed as a very lightweight and fast reranking library, typically leveraging smaller, optimized transformer models (often distilled or pruned versions of larger models). It aims to provide significant relevance improvements over simple similarity search with minimal computational overhead. It functions like a cross-encoder but uses techniques to accelerate the process. It&#8217;s usually available as an open-source Python library.
* **Key Features:**Its primary feature is speed and efficiency. It&#8217;s designed for easy integration and low resource consumption (CPU or moderate GPU usage). It often requires minimal code to implement.
* **Performance:**While not reaching the peak accuracy of the largest cross-encoders like Cohere or Voyage, FlashRank aims to deliver substantial gains over no reranking or basic bi-encoder reranking. Its speed makes it suitable for real-time or high-throughput scenarios. Cost is minimal (compute for self-hosting).
* **Strengths:**Very fast inference speed, low computational requirements, easy to integrate, open-source.
* **Weaknesses:**Accuracy might be lower than larger, more complex reranking models. Model choices might be more limited compared to broader frameworks.
* **Ideal Use Cases:**Applications needing quick reranking on resource-constrained hardware (like CPUs or edge devices), high-volume search systems where latency is critical, projects looking for a simple &#8220;better-than-nothing&#8221; reranking step with minimal complexity.#### Example Code
```
`from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document\_compressors import FlashrankRerank
from langchain\_openai import ChatOpenAI
llm = ChatOpenAI(temperature=0)
compressor = FlashrankRerank()
compression\_retriever = ContextualCompressionRetriever(
base\_compressor=compressor, base\_retriever=retriever
)
compressed\_docs = compression\_retriever.invoke(
"What did the president say about Ketanji Jackson Brown"
)
print(&#91;&#91;doc.metadata&#91;&#91;"id"] for doc in compressed\_docs])
pretty\_print\_docs(compressed\_docs)`
```
This code snippet utilizes FlashrankRerank within a ContextualCompressionRetriever to improve the relevance of retrieved documents. It specifically reranks documents obtained by a base retriever (represented by a retriever) based on their relevance to the query &#8220;What did the president say about Ketanji Jackson Brown&#8221;. Finally, it prints the document IDs and the compressed, reranked documents.
**Output:**
```
[0, 5, 3]
Document 1:
One of the most serious constitutional responsibilities a President has is
nominating someone to serve on the United States Supreme Court.
And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji
Brown Jackson. One of our nation‚Äôs top legal minds, who will continue Justice
Breyer‚Äôs legacy of excellence.
----------------------------------------------------------------------------------------------------
Document 2:
He met the Ukrainian people.
From President Zelenskyy to every Ukrainian, their fearlessness, their courage,
their determination, inspires the world.
Groups of citizens blocking tanks with their bodies. Everyone from students to
retirees teachers turned soldiers defending their homeland.
In this struggle as President Zelenskyy said in his speech to the European
Parliament ‚ÄúLight will win over darkness.‚Äù The Ukrainian Ambassador to the United
States is here tonight.
----------------------------------------------------------------------------------------------------
Document 3:
And tonight, I‚Äôm announcing that the Justice Department will name a chief prosecutor
for pandemic fraud.
By the end of this year, the deficit will be down to less than half what it was
before I took office.
The only president ever to cut the deficit by more than one trillion dollars in a
single year.
Lowering your costs also means demanding more competition.
I‚Äôm a capitalist, but capitalism without competition isn‚Äôt capitalism
It‚Äôs exploitation‚Äîand it drives up prices.
The output shoes it reranks the retrieved chunks based on the relevancy.
```
### MixedBread
Provided by Mixedbread AI, this family includes mxbai-rerank-base-v2 (0.5B parameters) and mxbai-rerank-large-v2 (1.5B parameters). They are open-source (Apache 2.0 license) cross-encoders based on the Qwen-2.5 architecture. A key differentiator is their training process, which incorporates a three-stage reinforcement learning (RL) approach (GRPO, Contrastive Learning, Preference Learning) on top of initial training.
* **Key Features:**Claims state-of-the-art performance across benchmarks (like BEIR). Supports over 100 languages. Handles long contexts up to 8k tokens (and is compatible with 32k). Designed to work well with diverse data types including text, code, JSON, and for LLM tool selection. Available via Hugging Face and a Python library.
* **Performance:**Benchmarks published by Mixedbread show these models outperforming other top open-source and closed-source competitors like Cohere and Voyage on BEIR (Large achieving 57.49, Base 55.57). They also demonstrate significant speed advantages, with the 1.5B parameter model being notably faster than other large open-source rerankers in latency tests. Cost is compute resources for self-hosting.
* **Strengths:**High benchmark performance (claimed SOTA), open-source license, fast inference speed relative to accuracy, broad language support, very long context window, versatile across data types (code, JSON).
* **Weaknesses:**Requires self-hosting and infrastructure management. As relatively new models, long-term performance and community vetting are ongoing.
* **Ideal Use Cases:**General RAG needing top-tier performance, multilingual applications, systems dealing with code, JSON, or long documents, LLM tool/function calling selection, teams preferring high-performing open-source models.#### Example Code
```
`!pip install mxbai\_rerank
from mxbai\_rerank import MxbaiRerankV2
# Load the model, here we use our base sized model
model = MxbaiRerankV2("mixedbread-ai/mxbai-rerank-base-v2")
# Example query and documents
query = "Who wrote To Kill a Mockingbird?"
documents = &#91;&#91;"To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.",
"The novel Moby-Dick was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.",
"Harper Lee, an American novelist widely known for her novel To Kill a Mockingbird, was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.",
"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.",
"The Harry Potter series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.",
"The Great Gatsby, a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan."
]
# Calculate the scores
results = model.rank(query, documents)
print(results)`
```
**Output:**
```
[RankResult(index=0, score=9.847987174987793, document='To Kill a Mockingbird is a
novel by Harper Lee published in 1960. It was immediately successful, winning the
Pulitzer Prize, and has become a classic of modern American literature.'),
RankResult(index=2, score=8.258672714233398, document='Harper Lee, an American
novelist widely known for her novel To Kill a Mockingbird, was born in 1926 in
Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.'),
RankResult(index=3, score=3.579845428466797, document='Jane Austen was an English
novelist known primarily for her six major novels, which interpret, critique and
comment upon the British landed gentry at the end of the 18th century.'),
RankResult(index=4, score=2.716982841491699, document='The Harry Potter series,
which consists of seven fantasy novels written by British author J.K. Rowling, is
among the most popular and critically acclaimed books of the modern era.'),
RankResult(index=1, score=2.233165740966797, document='The novel Moby-Dick was
written by Herman Melville and first published in 1851. It is considered a
masterpiece of American literature and deals with complex themes of obsession,
revenge, and the conflict between good and evil.'),
RankResult(index=5, score=1.8150043487548828, document='The Great Gatsby, a novel
written by American author F. Scott Fitzgerald, was published in 1925. The story is
set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit
of Daisy Buchanan.')]
```
## How to Tell if Your Reranker is Working
Evaluating rerankers is important. Common metrics help measure their effectiveness:
* **Accuracy@k:**How often a relevant document appears in the top k results.
* **Precision@k:**The proportion of relevant documents within the top k results.
* **Recall@k:**The fraction of*all*relevant documents found within the top k results.
* **Normalized Discounted Cumulative Gain (NDCG):**Measures ranking quality by considering both relevance and position. Higher-ranked relevant documents contribute more to the score. It&#8217;s normalized (0 to 1), allowing comparisons.
* **Mean Reciprocal Rank (MRR):**Focuses on the rank of the first relevant document found. It&#8217;s the average of 1/rank across multiple queries. Useful when finding one good result quickly is important.
* **F1-score:**The harmonic mean of precision and recall, offering a balanced view.## Choosing the Right Reranker for Your Needs
Selecting the best reranker involves balancing several factors:
* **Relevance Needs:**How accurate do the results need to be for your application?
* **Latency:**How quickly must the reranker return results? Speed is crucial for real-time applications.
* **Scalability:**Can the model handle your current and future data volume and user load?
* **Integration:**How easily does the reranker fit into your existing RAG pipeline (embedding models, vector database, LLM framework)?
* **Domain Specificity:**Do you need a model trained on data specific to your field?
* **Cost:**Consider API fees for private models or computing costs for self-hosted ones.
**There are trade-offs:**
* Cross-encoders offer high precision but are slower.
* Bi-encoders are faster and scalable but might be slightly less precise.
* LLM-based rerankers can be highly accurate but expensive and slow.
* Multi-vector models aim for a balance.
* Score-based methods are fastest but may lack semantic depth.
**To choose wisely:**
* Define your goals for accuracy and speed.
* Analyze your data characteristics (size, domain).
* Evaluate different models on your data using metrics like NDCG and MRR.
* Consider integration ease and budget.
The best reranker fits your specific performance, efficiency, and cost requirements.
## Conclusion
Rerankers for RAG are vital for getting the most out of RAG systems. They refine the information given to LLMs, leading to better, more trustworthy answers. With various models available, from highly precise cross-encoders to efficient bi-encoders and specialized options like ColBERT, developers have choices. Selecting the right one requires understanding the trade-offs between accuracy, speed, scalability, and cost. As RAG evolves, especially towards handling diverse data types, rerankers for RAG will continue to play a crucial role in building smarter, more reliable AI applications. Careful evaluation and selection remain key to success.
## Frequently Asked Questions
**Q1. What is Retrieval-Augmented Generation (RAG)?**
A. RAG is a technique that improves large language models (LLMs) by allowing them to retrieve external information before generating responses. This makes them more accurate, adaptable, and able to incorporate new knowledge without retraining.
**Q2.**Why is initial retrieval not enough in RAG systems?****
A. Initial retrieval methods like keyword search or vector similarity can return many documents, but not all are highly relevant. This can lead to noisy inputs that reduce LLM performance. Refining these results is necessary to improve answer quality.
**Q3. What is the role of rerankers in RAG?**
A. Rerankers reorder retrieved documents based on their relevance to the query. They act as a quality filter, ensuring the most relevant information is prioritized before being passed to the LLM for answer generation.
**Q4. What makes Cohere Rerank a strong choice?**
A. Cohere Rerank provides high accuracy, multilingual support, and API-based integration. Its &#8220;Nimble&#8221; variant is optimized for faster responses, making it ideal for real-time applications.
**Q5. Why is bge-reranker popular among open-source users?**
A. bge-reranker is open-source and can be self-hosted, reducing costs while maintaining high accuracy. It is suitable for teams that prefer full control over their models.
[![Harsh Mishra](https://av-eks-lekhak.s3.amazonaws.com/media/lekhak-profile-images/converted_image_0fBqNLi.webp)](https://www.analyticsvidhya.com/blog/author/harsh9480979/)
[Harsh Mishra](https://www.analyticsvidhya.com/blog/author/harsh9480979/)
Harsh Mishra is an AI/ML Engineer who spends more time talking to Large Language Models than actual humans. Passionate about GenAI, NLP, and making machines smarter (so they don‚Äôt replace him just yet). When not optimizing models, he‚Äôs probably optimizing his coffee intake. üöÄ‚òï[Advanced](https://www.analyticsvidhya.com/blog/category/advanced/)[LLMs](https://www.analyticsvidhya.com/blog/category/llms/)[Python](https://www.analyticsvidhya.com/blog/category/python-2/)[RAG](https://www.analyticsvidhya.com/blog/category/rag/)
#### Login to continue reading and enjoy expert-curated content.
Keep Reading for Free
## Free Courses
[
![Generative AI](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/04/Banner-image.webp)5
#### Build a Document Retriever Search Engine with LangChain
‚ÄãLearn to create a document retrieval search engine using LangChain. ‚Äã](https://www.analyticsvidhya.com/courses/build-a-document-retriever-search-engine-with-langchain/?utm_source=blog&utm_medium=free_course_recommendation)
#### Recommended Articles
* [
GPT-4 vs. Llama 3.1 ‚ÄìWhich Model is Better?
](https://www.analyticsvidhya.com/blog/2024/08/gpt-4-vs-llama-3-1/)
* [
Llama-3.1-Storm-8B: The 8B LLM Powerhouse Surpa...
](https://www.analyticsvidhya.com/blog/2024/08/llama-3-1-storm-8b/)
* [
A Comprehensive Guide to Building Agentic RAG S...
](https://www.analyticsvidhya.com/blog/2024/07/building-agentic-rag-systems-with-langgraph/)
* [
Top 10 Machine Learning Algorithms in 2025
](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/)
* [
45 Questions to Test a Data Scientist on Basics...
](https://www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/)
* [
90+ Python Interview Questions and Answers (202...
](https://www.analyticsvidhya.com/blog/2022/07/python-coding-interview-questions-for-freshers/)
* [
8 Easy Ways to Access ChatGPT for Free
](https://www.analyticsvidhya.com/blog/2023/12/chatgpt-4-for-free/)
* [
Prompt Engineering: Definition, Examples, Tips ...
](https://www.analyticsvidhya.com/blog/2023/06/what-is-prompt-engineering/)
* [
What is LangChain?
](https://www.analyticsvidhya.com/blog/2024/06/langchain-guide/)
* [
What is Retrieval-Augmented Generation (RAG)?
](https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/)
### Responses From Readers
[Cancel reply](https://www.analyticsvidhya.com/blog/2025/06/top-rerankers-for-rag/#respond) 
ClearSubmit reply
&#916;
[## Become an Author
Share insights, grow your voice, and inspire the data community.
](https://www.analyticsvidhya.com/become-an-author)[
* Reach a Global Audience
* Share Your Expertise with the World
* Build Your Brand & Audience
* Join a Thriving AI Community
* Level Up Your AI Game
* Expand Your Influence in Genrative AI](https://www.analyticsvidhya.com/become-an-author)
[![imag](https://www.analyticsvidhya.com/wp-content/themes/analytics-vidhya/images/Write-for-us.webp)](https://www.analyticsvidhya.com/become-an-author)
## Flagship Programs
[GenAI Pinnacle Program](https://www.analyticsvidhya.com/genaipinnacle/?ref=footer)|[GenAI Pinnacle Plus Program](https://www.analyticsvidhya.com/pinnacleplus/?ref=blogflashstripfooter)|[AI/ML BlackBelt Program](https://www.analyticsvidhya.com/bbplus?ref=footer)|[Agentic AI Pioneer Program](https://www.analyticsvidhya.com/agenticaipioneer?ref=footer)
## Free Courses
[Generative AI](https://www.analyticsvidhya.com/courses/genai-a-way-of-life/?ref=footer)|[DeepSeek](https://www.analyticsvidhya.com/courses/getting-started-with-deepseek/?ref=footer)|[OpenAI Agent SDK](https://www.analyticsvidhya.com/courses/demystifying-openai-agents-sdk/?ref=footer)|[LLM Applications using Prompt Engineering](https://www.analyticsvidhya.com/courses/building-llm-applications-using-prompt-engineering-free/?ref=footer)|[DeepSeek from Scratch](https://www.analyticsvidhya.com/courses/deepseek-from-scratch/?ref=footer)|[Stability.AI](https://www.analyticsvidhya.com/courses/exploring-stability-ai/?ref=footer)|[SSM & MAMBA](https://www.analyticsvidhya.com/courses/building-smarter-llms-with-mamba-and-state-space-model/?ref=footer)|[RAG Systems using LlamaIndex](https://www.analyticsvidhya.com/courses/building-first-rag-systems-using-llamaindex/?ref=footer)|[Building LLMs for Code](https://www.analyticsvidhya.com/courses/building-large-language-models-for-code/?ref=footer)|[Python](https://www.analyticsvidhya.com/courses/introduction-to-data-science/?ref=footer)|[Microsoft Excel](https://www.analyticsvidhya.com/courses/microsoft-excel-formulas-functions/?ref=footer)|[Machine Learning](https://www.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners/?ref=footer)|[Deep Learning](https://www.analyticsvidhya.com/courses/getting-started-with-deep-learning/?ref=footer)|[Mastering Multimodal RAG](https://www.analyticsvidhya.com/courses/mastering-multimodal-rag-and-embeddings-with-amazon-nova-and-bedrock/?ref=footer)|[Introduction to Transformer Model](https://www.analyticsvidhya.com/courses/introduction-to-transformers-and-attention-mechanisms/?ref=footer)|[Bagging & Boosting](https://www.analyticsvidhya.com/courses/bagging-boosting-ML-Algorithms/?ref=footer)|[Loan Prediction](https://www.analyticsvidhya.com/courses/loan-prediction-practice-problem-using-python/?ref=footer)|[Time Series Forecasting](https://www.analyticsvidhya.com/courses/creating-time-series-forecast-using-python/?ref=footer)|[Tableau](https://www.analyticsvidhya.com/courses/tableau-for-beginners/?ref=footer)|[Business Analytics](https://www.analyticsvidhya.com/courses/introduction-to-analytics/?ref=footer)|[Vibe Coding in Windsurf](https://www.analyticsvidhya.com/courses/guide-to-vibe-coding-in-windsurf/?ref=footer)|[Model Deployment using FastAPI](https://www.analyticsvidhya.com/courses/model-deployment-using-fastapi/?ref=footer)|[Building Data Analyst AI Agent](https://www.analyticsvidhya.com/courses/building-data-analyst-AI-agent/?ref=footer)|[Getting started with OpenAI o3-mini](https://www.analyticsvidhya.com/courses/getting-started-with-openai-o3-mini/?ref=footer)|[Introduction to Transformers and Attention Mechanisms](https://www.analyticsvidhya.com/courses/introduction-to-transformers-and-attention-mechanisms/?ref=footer)
## Popular Categories
[AI Agents](https://www.analyticsvidhya.com/blog/category/ai-agent/?ref=footer)|[Generative AI](https://www.analyticsvidhya.com/blog/category/generative-ai/?ref=footer)|[Prompt Engineering](https://www.analyticsvidhya.com/blog/category/prompt-engineering/?ref=footer)|[Generative AI Application](https://www.analyticsvidhya.com/blog/category/generative-ai-application/?ref=footer)|[News](https://news.google.com/publications/CAAqBwgKMJiWzAswyLHjAw?hl=en-IN&gl=IN&ceid=IN:en)|[Technical Guides](https://www.analyticsvidhya.com/blog/category/guide/?ref=footer)|[AI Tools](https://www.analyticsvidhya.com/blog/category/ai-tools/?ref=footer)|[Interview Preparation](https://www.analyticsvidhya.com/blog/category/interview-questions/?ref=footer)|[Research Papers](https://www.analyticsvidhya.com/blog/category/research-paper/?ref=footer)|[Success Stories](https://www.analyticsvidhya.com/blog/category/success-story/?ref=footer)|[Quiz](https://www.analyticsvidhya.com/blog/category/quiz/?ref=footer)|[Use Cases](https://www.analyticsvidhya.com/blog/category/use-cases/?ref=footer)|[Listicles](https://www.analyticsvidhya.com/blog/category/listicle/?ref=footer)
## Generative AI Tools and Techniques
[GANs](https://www.analyticsvidhya.com/blog/2021/10/an-end-to-end-introduction-to-generative-adversarial-networksgans/?ref=footer)|[VAEs](https://www.analyticsvidhya.com/blog/2023/07/an-overview-of-variational-autoencoders/?ref=footer)|[Transformers](https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models?ref=footer)|[StyleGAN](https://www.analyticsvidhya.com/blog/2021/05/stylegan-explained-in-less-than-five-minutes/?ref=footer)|[Pix2Pix](https://www.analyticsvidhya.com/blog/2023/10/pix2pix-unleashed-transforming-images-with-creative-superpower?ref=footer)|[Autoencoders](https://www.analyticsvidhya.com/blog/2021/06/autoencoders-a-gentle-introduction?ref=footer)|[GPT](https://www.analyticsvidhya.com/blog/2022/10/generative-pre-training-gpt-for-natural-language-understanding/?ref=footer)|[BERT](https://www.analyticsvidhya.com/blog/2022/11/comprehensive-guide-to-bert/?ref=footer)|[Word2Vec](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/?ref=footer)|[LSTM](https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm?ref=footer)|[Attention Mechanisms](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/?ref=footer)|[Diffusion Models](https://www.analyticsvidhya.com/blog/2024/09/what-are-diffusion-models/?ref=footer)|[LLMs](https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/?ref=footer)|[SLMs](https://www.analyticsvidhya.com/blog/2024/05/what-are-small-language-models-slms/?ref=footer)|[Encoder Decoder Models](https://www.analyticsvidhya.com/blog/2023/10/advanced-encoders-and-decoders-in-generative-ai/?ref=footer)|[Prompt Engineering](https://www.analyticsvidhya.com/blog/2023/06/what-is-prompt-engineering/?ref=footer)|[LangChain](https://www.analyticsvidhya.com/blog/2024/06/langchain-guide/?ref=footer)|[LlamaIndex](https://www.analyticsvidhya.com/blog/2023/10/rag-pipeline-with-the-llama-index/?ref=footer)|[RAG](https://www.analyticsvidhya.com/blog/2023/09/retrieval-augmented-generation-rag-in-ai/?ref=footer)|[Fine-tuning](https://www.analyticsvidhya.com/blog/2023/08/fine-tuning-large-language-models/?ref=footer)|[LangChain AI Agent](https://www.analyticsvidhya.com/blog/2024/07/langchains-agent-framework/?ref=footer)|[Multimodal Models](https://www.analyticsvidhya.com/blog/2023/12/what-are-multimodal-models/?ref=footer)|[RNNs](https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/?ref=footer)|[DCGAN](https://www.analyticsvidhya.com/blog/2021/07/deep-convolutional-generative-adversarial-network-dcgan-for-beginners/?ref=footer)|[ProGAN](https://www.analyticsvidhya.com/blog/2021/05/progressive-growing-gan-progan/?ref=footer)|[Text-to-Image Models](https://www.analyticsvidhya.com/blog/2024/02/llm-driven-text-to-image-with-diffusiongpt/?ref=footer)|[DDPM](https://www.analyticsvidhya.com/blog/2024/08/different-components-of-diffusion-models/?ref=footer)|[Document Question Answering](https://www.analyticsvidhya.com/blog/2024/04/a-hands-on-guide-to-creating-a-pdf-based-qa-assistant-with-llama-and-llamaindex/?ref=footer)|[Imagen](https://www.analyticsvidhya.com/blog/2024/09/google-imagen-3/?ref=footer)|[T5 (Text-to-Text Transfer Transformer)](https://www.analyticsvidhya.com/blog/2024/05/text-summarization-using-googles-t5-base/?ref=footer)|[Seq2seq Models](https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/?ref=footer)|[WaveNet](https://www.analyticsvidhya.com/blog/2020/01/how-to-perform-automatic-music-generation/?ref=footer)|[Attention Is All You Need (Transformer Architecture)](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/?ref=footer)|[WindSurf](https://www.analyticsvidhya.com/blog/2024/11/windsurf-editor/?ref=footer)|[Cursor](https://www.analyticsvidhya.com/blog/2025/03/vibe-coding-with-cursor-ai/?ref=footer)
## Popular GenAI Models
[Llama 4](https://www.analyticsvidhya.com/blog/2025/04/meta-llama-4/?ref=footer)|[Llama 3.1](https://www.analyticsvidhya.com/blog/2024/07/meta-llama-3-1/?ref=footer)|[GPT 4.5](https://www.analyticsvidhya.com/blog/2025/02/openai-gpt-4-5/?ref=footer)|[GPT 4.1](https://www.analyticsvidhya.com/blog/2025/04/open-ai-gpt-4-1/?ref=footer)|[GPT 4o](https://www.analyticsvidhya.com/blog/2025/03/updated-gpt-4o/?ref=footer)|[o3-mini](https://www.analyticsvidhya.com/blog/2025/02/openai-o3-mini/?ref=footer)|[Sora](https://www.analyticsvidhya.com/blog/2024/12/openai-sora/?ref=footer)|[DeepSeek R1](https://www.analyticsvidhya.com/blog/2025/01/deepseek-r1/?ref=footer)|[DeepSeek V3](https://www.analyticsvidhya.com/blog/2025/01/ai-application-with-deepseek-v3/?ref=footer)|[Janus Pro](https://www.analyticsvidhya.com/blog/2025/01/deepseek-janus-pro-7b/?ref=footer)|[Veo 2](https://www.analyticsvidhya.com/blog/2024/12/googles-veo-2/?ref=footer)|[Gemini 2.5 Pro](https://www.analyticsvidhya.com/blog/2025/03/gemini-2-5-pro-experimental/?ref=footer)|[Gemini 2.0](https://www.analyticsvidhya.com/blog/2025/02/gemini-2-0-everything-you-need-to-know-about-googles-latest-llms/?ref=footer)|[Gemma 3](https://www.analyticsvidhya.com/blog/2025/03/gemma-3/?ref=footer)|[Claude Sonnet 3.7](https://www.analyticsvidhya.com/blog/2025/02/claude-sonnet-3-7/?ref=footer)|[Claude 3.5 Sonnet](https://www.analyticsvidhya.com/blog/2024/06/claude-3-5-sonnet/?ref=footer)|[Phi 4](https://www.analyticsvidhya.com/blog/2025/02/microsoft-phi-4-multimodal/?ref=footer)|[Phi 3.5](https://www.analyticsvidhya.com/blog/2024/09/phi-3-5-slms/?ref=footer)|[Mistral Small 3.1](https://www.analyticsvidhya.com/blog/2025/03/mistral-small-3-1/?ref=footer)|[Mistral NeMo](https://www.analyticsvidhya.com/blog/2024/08/mistral-nemo/?ref=footer)|[Mistral-7b](https://www.analyticsvidhya.com/blog/2024/01/making-the-most-of-mistral-7b-with-finetuning/?ref=footer)|[Bedrock](https://www.analyticsvidhya.com/blog/2024/02/building-end-to-end-generative-ai-models-with-aws-bedrock/?ref=footer)|[Vertex AI](https://www.analyticsvidhya.com/blog/2024/02/build-deploy-and-manage-ml-models-with-google-vertex-ai/?ref=footer)|[Qwen QwQ 32B](https://www.analyticsvidhya.com/blog/2025/03/qwens-qwq-32b/?ref=footer)|[Qwen 2](https://www.analyticsvidhya.com/blog/2024/06/qwen2/?ref=footer)|[Qwen 2.5 VL](https://www.analyticsvidhya.com/blog/2025/01/qwen2-5-vl-vision-model/?ref=footer)|[Qwen Chat](https://www.analyticsvidhya.com/blog/2025/03/qwen-chat/?ref=footer)|[Grok 3](https://www.analyticsvidhya.com/blog/2025/02/grok-3/?ref=footer)
## AI Development Frameworks
[n8n](https://www.analyticsvidhya.com/blog/2025/03/content-creator-agent-with-n8n/?ref=footer)|[LangChain](https://www.analyticsvidhya.com/blog/2024/06/langchain-guide/?ref=footer)|[Agent SDK](https://www.analyticsvidhya.com/blog/2025/03/open-ai-responses-api/?ref=footer)|[A2A by Google](https://www.analyticsvidhya.com/blog/2025/04/agent-to-agent-protocol/?ref=footer)|[SmolAgents](https://www.analyticsvidhya.com/blog/2025/01/smolagents/?ref=footer)|[LangGraph](https://www.analyticsvidhya.com/blog/2024/07/langgraph-revolutionizing-ai-agent/?ref=footer)|[CrewAI](https://www.analyticsvidhya.com/blog/2024/01/building-collaborative-ai-agents-with-crewai/?ref=footer)|[Agno](https://www.analyticsvidhya.com/blog/2025/03/agno-framework/?ref=footer)|[LangFlow](https://www.analyticsvidhya.com/blog/2023/06/langflow-ui-for-langchain-to-develop-applications-with-llms/?ref=footer)|[AutoGen](https://www.analyticsvidhya.com/blog/2023/11/launching-into-autogen-exploring-the-basics-of-a-multi-agent-framework/?ref=footer)|[LlamaIndex](https://www.analyticsvidhya.com/blog/2024/08/implementing-ai-agents-using-llamaindex/?ref=footer)|[Swarm](https://www.analyticsvidhya.com/blog/2024/12/managing-multi-agent-systems-with-openai-swarm/?ref=footer)|[AutoGPT](https://www.analyticsvidhya.com/blog/2023/05/learn-everything-about-autogpt/?ref=footer)
## Data Science Tools and Techniques
[Python](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/?ref=footer)|[R](https://www.analyticsvidhya.com/blog/2016/02/complete-tutorial-learn-data-science-scratch/?ref=footer)|[SQL](https://www.analyticsvidhya.com/blog/2022/01/learning-sql-from-basics-to-advance/?ref=footer)|[Jupyter Notebooks](https://www.analyticsvidhya.com/blog/2018/05/starters-guide-jupyter-notebook/?ref=footer)|[TensorFlow](https://www.analyticsvidhya.com/blog/2021/11/tensorflow-for-beginners-with-examples-and-python-implementation/?ref=footer)|[Scikit-learn](https://www.analyticsvidhya.com/blog/2021/08/complete-guide-on-how-to-learn-scikit-learn-for-data-science/?ref=footer)|[PyTorch](https://www.analyticsvidhya.com/blog/2018/02/pytorch-tutorial/?ref=footer)|[Tableau](https://www.analyticsvidhya.com/blog/2021/09/a-complete-guide-to-tableau-for-beginners-in-data-visualization/?ref=footer)|[Apache Spark](https://www.analyticsvidhya.com/blog/2022/08/introduction-to-on-apache-spark-and-its-datasets/?ref=footer)|[Matplotlib](https://www.analyticsvidhya.com/blog/2021/10/introduction-to-matplotlib-using-python-for-beginners/?ref=footer)|[Seaborn](https://www.analyticsvidhya.com/blog/2021/02/a-beginners-guide-to-seaborn-the-simplest-way-to-learn/?ref=footer)|[Pandas](https://www.analyticsvidhya.com/blog/2021/03/pandas-functions-for-data-analysis-and-manipulation/?ref=footer)|[Hadoop](https://www.analyticsvidhya.com/blog/2022/05/an-introduction-to-hadoop-ecosystem-for-big-data/?ref=footer)|[Docker](https://www.analyticsvidhya.com/blog/2021/10/end-to-end-guide-to-docker-for-aspiring-data-engineers/?ref=footer)|[Git](https://www.analyticsvidhya.com/blog/2021/09/git-and-github-tutorial-for-beginners/?ref=footer)|[Keras](https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/?ref=footer)|[Apache Kafka](https://www.analyticsvidhya.com/blog/2022/12/introduction-to-apache-kafka-fundamentals-and-working/?ref=footer)|[AWS](https://www.analyticsvidhya.com/blog/2020/09/what-is-aws-amazon-web-services-data-science/?ref=footer)|[NLP](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/?ref=footer)|[Random Forest](https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/?ref=footer)|[Computer Vision](https://www.analyticsvidhya.com/blog/2020/01/computer-vision-learning-path/?ref=footer)|[Data Visualization](https://www.analyticsvidhya.com/blog/2021/04/a-complete-beginners-guide-to-data-visualization/?ref=footer)|[Data Exploration](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/?ref=footer)|[Big Data](https://www.analyticsvidhya.com/blog/2021/05/what-is-big-data-introduction-uses-and-applications/?ref=footer)|[Common Machine Learning Algorithms](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?ref=footer)|[Machine Learning](https://www.analyticsvidhya.com/blog/category/Machine-Learning/?ref=footer)|[Google Data Science Agent](https://www.analyticsvidhya.com/blog/2025/03/gemini-data-science-agent/?ref=footer)
SKIP
## Continue your learning for FREE
Login with GoogleLogin with Email[Forgot your password?](https://id.analyticsvidhya.com/auth/password/reset/?utm_source=newhomepage)
I accept the[Terms and Conditions](https://www.analyticsvidhya.com/terms)
Receive updates on WhatsApp
## Enter email address to continue
Email address
Get OTP
## Enter OTP sent to
Edit
Wrong OTP.
### Enter the OTP
Resend OTP
Resend OTP in45s
Verify OTP
