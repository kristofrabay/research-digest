# Agents | LlamaIndex Python Documentation

**URL:** https://docs.llamaindex.ai/en/latest/module_guides/deploying/agents/
**Published:** 2013-05-09T00:00:00.000Z

---

## Summary

The LlamaIndex documentation defines an **agent** as a system that uses an LLM, memory, and tools to handle user inputs. It contrasts this with **agentic**, which refers to any system involving LLM decision-making.

Key components and concepts discussed in the context of LlamaIndex agents include:

*   **Agent Types:** Examples include `FunctionAgent` (which uses function/tool calling capabilities), `ReActAgent`, and `CodeActAgent`.
*   **Tools:** These can be defined as Python functions or customized using classes like `FunctionTool` and `QueryEngineTool`. LlamaIndex also offers pre-defined tools via **Tool Specs**.
*   **Memory:** A core component; by default, LlamaIndex agents use a `ChatMemoryBuffer`.
*   **Agent Workflow:** The process involves the agent receiving input, sending tool schemas and history to the LLM, responding with either a direct answer or tool calls, executing the tools, updating the history, and invoking the agent again.
*   **Multi-Modal Agents:** Agents can handle inputs involving multiple modalities, such as images and text, using content blocks.
*   **Multi-Agent Systems:** Agents can be combined into systems (e.g., using `AgentWorkflow`) to coordinate task completion.
*   **Manual Agents:** Provides a lower-level approach for building custom agent loops using `LLM` objects directly for full control over tool calling and error handling.

The document focuses specifically on the **LlamaIndex framework** for building agents and does not explicitly detail general concepts like **MCP servers**, **agent orchestration** (beyond the mention of `AgentWorkflow`), **agent frameworks** other than LlamaIndex, or specific SDKs like **OpenAI Agents SDK**, **Anthropic Agents SDK**, or **Google SDK**. It mentions **function calling** and **structured outputs** implicitly through the `FunctionAgent`. **Agent memory** is covered under the "Memory" section.

---

## Full Content

Agents | LlamaIndex Python Documentation[Skip to content](#_top)
[![](https://docs.llamaindex.ai/python/_astro/llamaindex-light.m3GU97oV.svg)![](https://docs.llamaindex.ai/python/_astro/llamaindex-dark.B8ExLyh7.svg)LlamaIndex Python Documentation](https://docs.llamaindex.ai/)
SearchCtrlK
Cancel
[TypeScript](https://docs.llamaindex.ai/typescript/framework/)
[Twitter](https://x.com/llama_index)[LinkedIn](https://www.linkedin.com/company/llamaindex)[Bluesky](https://bsky.app/profile/llamaindex.bsky.social)[GitHub](https://github.com/run-llama/llama_index/)
Select themeDarkLightAuto
# Agents
In LlamaIndex, we define an “agent” as a specific system that uses an LLM, memory, and tools, to handle inputs from outside users. Contrast this with the term “agentic”, which generally refers to a superclass of agents, which is any system with LLM decision making in the process.
To create an agent in LlamaIndex, it takes only a few lines of code:
```
`
importasyncio
fromllama\_index.core.agent.workflowimportFunctionAgent
fromllama\_index.llms.openaiimportOpenAI
# Define a simple calculator tool
defmultiply(a:float,b:float)-\>float:
"""Useful for multiplying two numbers."""
returna\*b
# Create an agent workflow with our calculator tool
agent=FunctionAgent(
tools=[multiply],
llm=OpenAI(model="gpt-4o-mini"),
system\_prompt="You are a helpful assistant that can multiply two numbers.",
)
asyncdefmain():
# Run the agent
response=awaitagent.run("What is 1234 \* 4567?")
print(str(response))
# Run the agent
if\_\_name\_\_=="\_\_main\_\_":
asyncio.run(main())
`
```
Calling this agent kicks off a specific loop of actions:
* Agent gets the latest message + chat history
* The tool schemas and chat history get sent over the API
* The Agent responds either with a direct response, or a list of tool calls
* Every tool call is executed
* The tool call results are added to the chat history
* The Agent is invoked again with updated history, and either responds directly or selects more calls
The`FunctionAgent`is a type of agent that uses an LLM provider’s function/tool calling capabilities to execute tools. Other types of agents, such as[`ReActAgent`](https://docs.llamaindex.ai/python/examples/agent/react_agent)and[`CodeActAgent`](https://docs.llamaindex.ai/python/examples/agent/code_act_agent), use different prompting strategies to execute tools.
You can visit[the agents guide](https://docs.llamaindex.ai/python/framework/understanding/agent)to learn more about agents and their capabilities.
Tip
Some models might not support streaming LLM output. While streaming is enabled
by default, if you encounter an error, you can always set`FunctionAgent(..., streaming=False)`to disable streaming.
## Tools
[Section titled “Tools”](#tools)
Tools can be defined simply as python functions, or further customized using classes like`FunctionTool`and`QueryEngineTool`. LlamaIndex also provides sets of pre-defined tools for common APIs using something called`Tool Specs`.
You can read more about configuring tools in the[tools guide](https://docs.llamaindex.ai/python/framework/module_guides/deploying/agents/tools)
## Memory
[Section titled “Memory”](#memory)
Memory is a core-component when building agents. By default, all LlamaIndex agents are using a ChatMemoryBuffer for memory.
To customize it, you can declare it outside the agent and pass it in:
```
`
fromllama\_index.core.memoryimportChatMemoryBuffer
memory=ChatMemoryBuffer.from\_defaults(token\_limit=40000)
response=awaitagent.run(...,memory=memory)
`
```
You can read more about configuring memory in the[memory guide](https://docs.llamaindex.ai/python/framework/module_guides/deploying/agents/memory)
## Multi-Modal Agents
[Section titled “Multi-Modal Agents”](#multi-modal-agents)
Some LLMs will support multiple modalities, such as images and text. Using chat messages with content blocks, we can pass in images to an agent for reasoning.
For example, imagine you had a screenshot of the[slide from this presentation](https://docs.google.com/presentation/d/1wy3nuO9ezGS4R99mzP3Q3yvrjAkZ26OGI2NjfqtwAaE/edit?usp=sharing).
You can pass this image to an agent for reasoning, and see that it reads the image and acts accordingly.
```
`
fromllama\_index.core.agent.workflowimportFunctionAgent
fromllama\_index.core.llmsimportChatMessage, ImageBlock, TextBlock
fromllama\_index.llms.openaiimportOpenAI
llm=OpenAI(model="gpt-4o-mini",api\_key="sk-...")
defadd(a:int,b:int)-\>int:
"""Useful for adding two numbers together."""
returna+b
workflow=FunctionAgent(
tools=[add],
llm=llm,
)
msg=ChatMessage(
role="user",
blocks=[
TextBlock(text="Follow what the image says."),
ImageBlock(path="./screenshot.png"),
],
)
response=awaitworkflow.run(msg)
print(str(response))
`
```
## Multi-Agent Systems
[Section titled “Multi-Agent Systems”](#multi-agent-systems)
You can combine agents into a multi-agent system, where each agent is able to hand off control to another agent to coordinate while completing tasks.
```
`
fromllama\_index.core.agent.workflowimportAgentWorkflow
multi\_agent=AgentWorkflow(agents=[FunctionAgent(...),FunctionAgent(...)])
resp=awaitagent.run("query")
`
```
This is only one way to build multi-agent systems. Read on to learn more about[multi-agent systems](https://docs.llamaindex.ai/python/framework/understanding/agent/multi_agent).
## Manual Agents
[Section titled “Manual Agents”](#manual-agents)
While the agent classes like`FunctionAgent`,`ReActAgent`,`CodeActAgent`, and`AgentWorkflow`abstract away a lot of details, sometimes its desirable to build your own lower-level agents.
Using the`LLM`objects directly, you can quickly implement a basic agent loop, while having full control over how the tool calling and error handling works.
```
`
fromllama\_index.core.llmsimportChatMessage
fromllama\_index.core.toolsimportFunctionTool
fromllama\_index.llms.openaiimportOpenAI
defselect\_song(song\_name:str)-\>str:
"""Useful for selecting a song."""
returnf"Song selected:{song\_name}"
tools=[FunctionTool.from\_defaults(select\_song)]
tools\_by\_name={t.metadata.name: tfortin[tool]}
# call llm with initial tools + chat history
chat\_history=[ChatMessage(role="user",content="Pick a random song for me")]
resp=llm.chat\_with\_tools([tool],chat\_history=chat\_history)
# parse tool calls from response
tool\_calls=llm.get\_tool\_calls\_from\_response(
resp,error\_on\_no\_tool\_call=False
)
# loop while there are still more tools to call
whiletool\_calls:
# add the LLM's response to the chat history
chat\_history.append(resp.message)
# call every tool and add its result to chat\_history
fortool\_callintool\_calls:
tool\_name=tool\_call.tool\_name
tool\_kwargs=tool\_call.tool\_kwargs
print(f"Calling{tool\_name}with{tool\_kwargs}")
tool\_output=tool(\*\*tool\_kwargs)
chat\_history.append(
ChatMessage(
role="tool",
content=str(tool\_output),
# most LLMs like OpenAI need to know the tool call id
additional\_kwargs={"tool\_call\_id": tool\_call.tool\_id},
)
)
# check if the LLM can write a final response or calls more tools
resp=llm.chat\_with\_tools([tool],chat\_history=chat\_history)
tool\_calls=llm.get\_tool\_calls\_from\_response(
resp,error\_on\_no\_tool\_call=False
)
# print the final response
print(resp.message.content)
`
```
## Examples / Module Guides
[Section titled “Examples / Module Guides”](#examples--module-guides)
You can find a more complete list of examples and module guides in the[module guides page](https://docs.llamaindex.ai/python/framework/module_guides/deploying/agents/modules).
