# Reasoning in LLMs as MCTS over Tokens (Motivation and three papers)

**URL:** https://www.linkedin.com/pulse/reasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc
**Published:** 2024-12-09T00:00:00.000Z

---

## Summary

The webpage discusses the integration of **Monte Carlo Tree Search (MCTS)** with **Large Language Models (LLMs)** to enhance their **reasoning and planning** capabilities.

**Key points related to the query:**

*   **Reasoning and Planning with LLMs:** LLMs often struggle with long-term planning and goal-conditioned reasoning because they are primarily trained for next-token prediction, leading to locally plausible but globally suboptimal outputs.
*   **MCTS for Language Models:** MCTS is proposed as a method to navigate the vast search space of language generation by exploring multiple potential continuations, simulating outcomes, and using rewards to guide the process.
*   **Benefits of MCTS Integration:**
    *   **Improve Planning:** Helps LLMs evaluate steps ahead to align with long-term objectives.
    *   **Improve Coherence:** Reduces contradictions by assessing simulated outcomes.
    *   **Optimise Reasoning:** Allows exploration of diverse problem-solving strategies.
    *   **Handle Large Token Spaces:** Systematically explores vocabulary options.
*   **Reformulation (State, Action, Reward):** Reasoning is reframed as an MCTS search where:
    *   **State:** The current status of the reasoning process (e.g., partially constructed solution).
    *   **Actions:** Meaningful reasoning moves (e.g., applying a rule or making an inference).
    *   **Reward:** Assigned based on the correctness and relevance of the path's outcome, guiding the search toward desirable solutions.
*   **Inference-Time Compute/Test-Time Scaling:** The MCTS process inherently involves iterative exploration and simulation at inference time, which addresses the need for strategic computation beyond simple next-token prediction.

The page focuses heavily on MCTS as a mechanism for improving planning and reasoning in LLMs, but it **does not explicitly detail** concepts like "self-reflection," "grounding," "factuality," or specific techniques for "hallucination reduction and detection" beyond the general improvement in coherence and logical consistency provided by MCTS.

---

## Full Content

Agree & Join LinkedIn

By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).

LinkedIn

LinkedIn is better on the app

Don‚Äôt have the app? Get it in the Microsoft Store.

[Open the app](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_desktop_upsell)

 [Skip to main content](https://www.linkedin.com/www.linkedin.com#main-content)

Monte Carlo Tree Search (MCTS) is a heuristic method for making optimal decisions. It benefits areas with many choices, such as games, planning and model-based optimisation problems. While the concepts behind MCTS have been around for decades, the method gained significant attention in the mid-2000s due to its success in game-playing AI, e.g., AlphaGo, which used MCTS to beat professional human players in the game of Go. In addition, MCTS has found applications in robotics, automated planning \\cite{MCTSPlanning}, and even drug discovery. Those successes and the effectiveness of MCTS in handling large search spaces without brute force search made it a valuable tool for modern AI.

In the 2010s and early 2020s, AI graduated from computer games and mastered human-like text generation with the advent of large-scale neural networks, large computing clusters, and internet-scale data. This period saw the rise of large language models (LLMs), which leveraged massive datasets and billions of parameters to achieve unprecedented performance in natural language processing tasks. These models demonstrated a remarkable ability to generate coherent, contextually appropriate text, making them transformative tools that reshaped academia, industry and humanity, I dare to say.

Why MCTS for LLMs: While LLMs excel at producing fluent and contextually relevant responses, they face challenges in tasks requiring long-term planning, reasoning, or exploration of complex decision spaces. This behaviour is unsurprising, given that those models are trained primarily on next-token prediction. Their objective is to generate the next most likely token based on prior context rather than to think ahead strategically or work towards a specific goal. As a result, they could refrain from engaging in goal-conditioned reasoning and instead focus on producing locally plausible outputs that may lack logical consistency. For example, when solving complex mathematical problems or making strategic decisions in a game-like setting, LLMs may generate locally plausible outputs but lack global optimality.

This is one place where Monte Carlo tree search can play a crucial role for LLMs. By integrating MCTS into LLMs' reasoning and decision-making process, it becomes possible to explore multiple potential continuations, simulate their outcomes, and use these simulations to guide the generation process. MCTS provides a potentially practical approach to navigating the vast search space of language generation, balancing the exploration of creative possibilities while exploiting promising paths that meet the goal in question. Incorporating MCTS with LLMs opens up opportunities to 1) Improve Planning: MCTS can help LLMs evaluate multiple steps ahead, ensuring that decisions align with long-term objectives or constraints; 2) Improve Coherence: By simulating and assessing the outcomes of different branches, MCTS reduces the risk of contradictions or inconsistencies in the generated text; 3) Optimise Reasoning: MCTS allows LLMs to explore diverse problem-solving strategies and identify the most effective approach; 4) Handle Large Action/Token Spaces: MCTS can systematically explore the vast vocabulary and possible sequences, focusing on the most promising options.

This synergy between MCTS and LLMs holds the potential to address critical limitations in current language models, enabling them to tackle more complex, multi-step reasoning tasks with greater precision and reliability.

How to reformulate: Reformulating the reasoning capabilities of large language models (LLMs) through Monte Carlo Tree Search (MCTS) offers a structured and efficient framework for navigating complex problem-solving tasks. Traditionally, LLMs generate responses by predicting one token at a time, leading to sprawling and unmanageable search spaces, especially for multi-step reasoning. By integrating MCTS, we can transform the reasoning process into a strategic exploration of possible solutions, enhancing the efficiency and accuracy of the model‚Äôs outputs.

At the core of this approach is the concept of the state. In the context of MCTS-driven reasoning, a state represents the current status of the reasoning process. This includes all the information accumulated up to that point, such as the partially constructed solution, relevant context from the input prompt, and any intermediate conclusions drawn. Unlike raw text, which can be vast and unstructured, each state serves as a concise snapshot that captures the essential elements needed to proceed further in the reasoning chain. This structured representation allows MCTS to evaluate and navigate through the different stages of problem-solving effectively.

Actions within this framework are defined as the potential next steps the model can take from any given state. These actions correspond to meaningful reasoning moves, such as applying a specific mathematical operation, invoking a particular rule or theorem, or making an inference based on the current information. By discretizing the possible actions, MCTS can systematically explore various paths, assessing which actions are most likely to lead to a successful and coherent solution. This contrasts with the token-by-token generation approach, where the model lacks a strategic overview of the reasoning process.

The reward mechanism is integral to guiding the MCTS process. Rewards are assigned based on the outcomes of reasoning paths, reflecting the correctness, efficiency, and relevance of the solutions generated. For instance, successfully solving a problem or arriving at a valid conclusion would yield a high reward, while incorrect or irrelevant paths receive lower or no rewards. This reward structure enables MCTS to prioritise actions more likely to lead to desirable outcomes, effectively learning which reasoning strategies are most effective over time.

Implementing MCTS with LLMs involves constructing a search tree where each node represents a state, and each edge represents an action leading to a new state. The MCTS algorithm iteratively builds this tree by selecting promising nodes to explore, expanding them by applying possible actions, simulating potential outcomes, and backpropagating the rewards to update the value estimates of the traversed states. This process balances the exploration of new reasoning paths with the exploitation of known successful strategies, ensuring a comprehensive yet efficient search for optimal solutions.

## Recommended by LinkedIn

[AGI - ASI Auditable Gen/Super Intelligence\
\
Ali Zareiee\
3 months ago](https://www.linkedin.com/pulse/agi-asi-auditable-gensuper-intelligence-ali-zareiee-qqtse)

[Kimi K2, An Open-weight Agentic Model From Moonshot AI\
\
DigitalOcean\
1 month ago](https://www.linkedin.com/pulse/kimi-k2-open-weight-agentic-model-from-moonshot-ai-digitalocean-rnukc)

[Man Vs. Machine: The 6 Greatest AI Challenges To‚Ä¶\
\
Bernard Marr\
6 years ago](https://www.linkedin.com/pulse/man-vs-machine-6-greatest-ai-challenges-showcase-power-bernard-marr)

One of the key advantages of framing reasoning as an MCTS search is the enhanced decision-making capability it provides to LLMs. By systematically evaluating multiple reasoning paths and leveraging the reward signals, MCTS helps the model avoid common pitfalls such as getting stuck in loops or following incorrect logic. Additionally, this structured approach improves the interpretability of the reasoning process, as each step can be traced and analyzed based on the actions taken and the resulting states. This transparency is particularly valuable for complex tasks where understanding the reasoning pathway is as important as the final answer.

Here are three paper summaries that attempted to do that, which I found to be interesting:

I will continue to add those as I go through the literature. If you find it interesting, make sure to like, follow and research.

See you soon!

[Less-Hyped AI](https://www.linkedin.com/newsletters/less-hyped-ai-6980067977496797184)

### Less-Hyped AI

#### 3,737 followers

[\+ Subscribe](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc)

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_like-toggle_like-cta)

[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_comment-cta)

- Copy
- LinkedIn
- Facebook
- Twitter

Share

[80](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_likes-count_social-actions-reactions) [8 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_likes-count_social-actions-comments)

[Jamal El Kuweiss](https://de.linkedin.com/in/jamal-el-kuweiss?trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment_actor-name)

Consultant

9mo

- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)

Interesting read, thank you :)
Couple of questions:
1) What do you foresee as a major obstacle in the implementation of MCTS into commercial LLMS?
2) How would one set the rewards for the MCTS process in this case? How would it look like? What kind of data?

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment_reply) [2¬†Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment_reactions) 3¬†Reactions

[Warren Powell](https://www.linkedin.com/in/warrenbpowell?trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment_actor-name)

Professor Emeritus, Princeton University/
Co-Founder, Optimal Dynamics/
Executive-in-Residence Rutgers Business School

9mo

- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)

MCTS is a model-based direct lookahead approximation‚Ä¶ this is fundamentally different than training a LLM which is completely dependent on a training dataset. MCTS requires a model of the problem (more precisely, an approximate model for the lookahead policy) but does not require a training dataset.

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment_reply) [11¬†Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment_reactions) 12¬†Reactions

[See more comments](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_comments_comment-see-more)

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_x-social-details_feed-cta-banner-cta)

## More articles by Haitham Bou-Ammar

- [From VR to Grippers: How We Built a Bimanual Robot That Learns](https://www.linkedin.com/pulse/from-vr-grippers-how-we-built-bimanual-robot-learns-haitham-bou-ammar-0a5fe)



Apr 6, 2025



### From VR to Grippers: How We Built a Bimanual Robot That Learns



In recent years, we‚Äôve seen enormous progress in AI ‚Äî from large language models to powerful vision systems. However‚Ä¶





33



3 Comments

- [Short Circuit ‚Äî Let AI Design Your Chips](https://www.linkedin.com/pulse/short-circuit-let-ai-design-your-chips-haitham-bou-ammar-6wkxe)



Aug 28, 2024



### Short Circuit ‚Äî Let AI Design Your Chips



Introduction The rapid expansion of artificial intelligence (AI) applications has significantly increased computational‚Ä¶



29

- [New Grounds in Theorem Proving with DeepSeek-Prover-V1.5](https://www.linkedin.com/pulse/new-grounds-theorem-proving-deepseek-prover-v15-%D9%87%D9%8A%D8%AB%D9%85-%D8%A8%D9%88-%D8%B9%D9%85%D8%A7%D8%B1-x7sle)



Aug 18, 2024



### New Grounds in Theorem Proving with DeepSeek-Prover-V1.5



DeepSeek-Prover-V1.5 represents a significant leap forward from its predecessor, DeepSeek-Prover-V1.



17

- [Deriving DPO's Loss](https://www.linkedin.com/pulse/deriving-dpos-loss-haitham-bou-ammar-%D9%87%D9%8A%D8%AB%D9%85-%D8%A8%D9%88-%D8%B9%D9%85%D8%A7%D8%B1-ul4bf)



Aug 15, 2024



### Deriving DPO's Loss



Direct preference optimisation has become critical for aligning LLMs with human preferences. I have been talking to‚Ä¶



21

- [Model Predictive Control from Mere Natural Language Commands](https://www.linkedin.com/pulse/model-predictive-control-from-mere-natural-language-%D9%87%D9%8A%D8%AB%D9%85-%D8%A8%D9%88-%D8%B9%D9%85%D8%A7%D8%B1-egqee)



Aug 14, 2024



### Model Predictive Control from Mere Natural Language Commands



Generating model predictive control without domain expertise via large language models! Significant progress is being‚Ä¶



16

- [DATA INTERPRETER: AN LLM AGENT FOR DATA SCIENCE](https://www.linkedin.com/pulse/data-interpreter-llm-agent-science-haitham-bou-ammar-%D9%87%D9%8A%D8%AB%D9%85-%D8%A8%D9%88-%D8%B9%D9%85%D8%A7%D8%B1-whape)



Aug 13, 2024



### DATA INTERPRETER: AN LLM AGENT FOR DATA SCIENCE



Introduction In the rapidly evolving field of artificial intelligence, large language models (LLMs) have emerged as‚Ä¶



17

- [A Leap Towards Human-Like AI: Recreating Human Memory in LLMs](https://www.linkedin.com/pulse/leap-towards-human-like-ai-recreating-human-memory-%D9%87%D9%8A%D8%AB%D9%85-%D8%A8%D9%88-%D8%B9%D9%85%D8%A7%D8%B1-gz8ce)



Aug 10, 2024



### A Leap Towards Human-Like AI: Recreating Human Memory in LLMs



In artificial intelligence, large language models (LLMs) have demonstrated remarkable capabilities in understanding and‚Ä¶





42



5 Comments

- [Pluralistic Alignment of LLMs: Fix your Algorithm not just your data](https://www.linkedin.com/pulse/pluralistic-alignment-llms-fix-your-algorithm-just-%D9%87%D9%8A%D8%AB%D9%85-%D8%A8%D9%88-%D8%B9%D9%85%D8%A7%D8%B1-1cose)



Aug 6, 2024



### Pluralistic Alignment of LLMs: Fix your Algorithm not just your data



Interjection: Recent studies have found that large language models (LLMs) are biased, with many articles demonstrating‚Ä¶



10

- [Robotic Parkour with Deep RL](https://www.linkedin.com/pulse/robotic-parkour-deep-rl-haitham-bou-ammar)



Jul 8, 2023



### Robotic Parkour with Deep RL



This post summarises the fantastic work from Hoeller et. 2023.





61



3 Comments

- [Empowering Efficient BO Transfer with Neural Acquisition Process (NAP)](https://www.linkedin.com/pulse/empowering-efficient-bo-transfer-neural-acquisition-nap-bou-ammar)



Jun 5, 2023



### Empowering Efficient BO Transfer with Neural Acquisition Process (NAP)



General Objectives & Results: Our primary objective is to enhance the effectiveness of Bayesian Optimisation (BO) by‚Ä¶



57


Show more

[See all articles](https://uk.linkedin.com/in/haitham-bou-ammar-a723a932/recent-activity/articles/)

## Sign in

Stay updated on your professional world

[Sign in](https://www.linkedin.com/uas/login?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_xandr-ad-fallback_signin)

By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=article-ssr-frontend-pulse_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=article-ssr-frontend-pulse_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=article-ssr-frontend-pulse_auth-button_cookie-policy).

New to LinkedIn? [Join now](https://www.linkedin.com/signup/cold-join?session_redirect=%2Fpulse%2Freasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc&trk=article-ssr-frontend-pulse_xandr-ad-fallback_join-link)

## Others also viewed

- [**üîã Fixing AI's Energy Consumption** \
\
Pascal Biese\
11mo](https://www.linkedin.com/pulse/fixing-ais-energy-consumption-pascal-biese-sndjf)
- [**Introduction to Prompt Engineering: The Alchemy of AI and the Future of Human-Machine Creativity** \
\
Reuven Cohen\
2y](https://www.linkedin.com/pulse/introduction-prompt-engineering-alchemy-ai-future-creativity-cohen)
- [**Unveiling the Current Depths of AI's Mathematical Reasoning** \
\
Jen Zhu Scott\
11mo](https://www.linkedin.com/pulse/unveiling-current-depths-ais-mathematical-reasoning-jen-zhu-scott-5kwnc)
- [**DeepSeek‚Äôs ‚ÄúAha Moment‚Äù: The Next AI Revolution or Just an Incremental Step?** \
\
Walter Adamson\
7mo](https://www.linkedin.com/pulse/deepseeks-aha-moment-next-ai-revolution-just-step-walter-adamson-dtj4c)
- [**Artificial Intelligence: Your work, your future, and your power** \
\
Dr Geanie Asante\
1y](https://www.linkedin.com/pulse/artificial-intelligence-your-work-future-power-dr-geanie-asante-thrvc)
- [**Exploring Goose: An RNN with the Advantages of a Transformer** \
\
Rudina Seseri\
2mo](https://www.linkedin.com/pulse/exploring-goose-rnn-advantages-transformer-rudina-seseri-hdthc)
- [**Defying the Algorithm: A Rebel's Call for True AI Intelligence** \
\
Mustafa Ramzan\
4mo](https://www.linkedin.com/pulse/defying-algorithm-rebels-call-true-ai-intelligence-mustafa-ramzan-12lcf)
- [**Attention is All You Need /\ Until You Need to Remember.** \
\
David de Santiago\
8mo](https://www.linkedin.com/pulse/attention-all-you-need-until-remember-david-de-santiago-b4r6e)
- [**Artificial Intelligence - Part One: Three things we think machines can never do, but they very well might** \
\
Amane Dannouni\
2y](https://www.linkedin.com/pulse/limits-can-machines-think-part-i-amane-dannouni)
- [**Unlocking the Connection: How High School Vectors Shape Today's AI** \
\
Deepak Bhandari\
1y](https://www.linkedin.com/pulse/unlocking-connection-how-high-school-vectors-shape-todays-bhandari-9t7gc)

Show more

Show less

## Explore content categories

- [Career](https://www.linkedin.com/top-content/career/)
- [Productivity](https://www.linkedin.com/top-content/productivity/)
- [Finance](https://www.linkedin.com/top-content/finance/)
- [Soft Skills & Emotional Intelligence](https://www.linkedin.com/top-content/soft-skills-emotional-intelligence/)
- [Project Management](https://www.linkedin.com/top-content/project-management/)
- [Education](https://www.linkedin.com/top-content/education/)
- [Technology](https://www.linkedin.com/top-content/technology/)
- [Leadership](https://www.linkedin.com/top-content/leadership/)
- [Ecommerce](https://www.linkedin.com/top-content/ecommerce/)
- [User Experience](https://www.linkedin.com/top-content/user-experience/)
- [Recruitment & HR](https://www.linkedin.com/top-content/recruitment-hr/)
- [Customer Experience](https://www.linkedin.com/top-content/customer-experience/)
- [Real Estate](https://www.linkedin.com/top-content/real-estate/)
- [Marketing](https://www.linkedin.com/top-content/marketing/)
- [Sales](https://www.linkedin.com/top-content/sales/)
- [Retail & Merchandising](https://www.linkedin.com/top-content/retail-merchandising/)
- [Science](https://www.linkedin.com/top-content/science/)
- [Supply Chain Management](https://www.linkedin.com/top-content/supply-chain-management/)
- [Future Of Work](https://www.linkedin.com/top-content/future-of-work/)
- [Consulting](https://www.linkedin.com/top-content/consulting/)
- [Writing](https://www.linkedin.com/top-content/writing/)
- [Economics](https://www.linkedin.com/top-content/economics/)
- [Artificial Intelligence](https://www.linkedin.com/top-content/artificial-intelligence/)
- [Employee Experience](https://www.linkedin.com/top-content/employee-experience/)
- [Workplace Trends](https://www.linkedin.com/top-content/workplace-trends/)
- [Fundraising](https://www.linkedin.com/top-content/fundraising/)
- [Networking](https://www.linkedin.com/top-content/networking/)
- [Corporate Social Responsibility](https://www.linkedin.com/top-content/corporate-social-responsibility/)
- [Negotiation](https://www.linkedin.com/top-content/negotiation/)
- [Communication](https://www.linkedin.com/top-content/communication/)
- [Engineering](https://www.linkedin.com/top-content/engineering/)
- [Hospitality & Tourism](https://www.linkedin.com/top-content/hospitality-tourism/)
- [Business Strategy](https://www.linkedin.com/top-content/business-strategy/)
- [Change Management](https://www.linkedin.com/top-content/change-management/)
- [Organizational Culture](https://www.linkedin.com/top-content/organizational-culture/)
- [Design](https://www.linkedin.com/top-content/design/)
- [Innovation](https://www.linkedin.com/top-content/innovation/)
- [Event Planning](https://www.linkedin.com/top-content/event-planning/)
- [Training & Development](https://www.linkedin.com/top-content/training-development/)

Show more

Show less
