# UniDoc-Bench: A Unified Benchmark for Document-Centric Multimodal RAG

**URL:** https://arxiv.org/html/2510.03663v2
**Published:** 2023-02-01T00:00:00.000Z

---

## Summary

The webpage introduces **UniDoc-Bench**, a large-scale, realistic benchmark specifically designed for **Document-Centric Multimodal Retrieval-Augmented Generation (MM-RAG)**.

**Key aspects related to your query:**

*   **Multimodal RAG and Document Understanding:** The benchmark addresses the limitations of current evaluations by focusing on real-world PDF documents (70k pages across 8 domains) that contain interleaved text, tables, and figures. It aims to evaluate systems that need to retrieve and reason over information across these modalities.
*   **PDF Parsing and Chart/Table Extraction:** The data curation process involves parsing PDFs to extract text chunks, tables, and figures. Tables and figures are stored separately, and placeholders are inserted into the text to represent their location, facilitating multimodal grounding.
*   **Report Generation with LLMs (MM-RAG):** The core purpose is to evaluate MM-RAG pipelines. The benchmark includes 1,600 human-verified Question-Answer (QA) pairs covering factual retrieval, comparison, summarization, and logical reasoning, requiring evidence from text, tables, and images.
*   **Vision-Language Models (VLMs) and Multimodal Models:** The experiments compare various RAG paradigms, including text-only, image-only, multimodal joint retrieval (using models like GME), and multimodal text-image fusion. The results indicate that **text-image fusion RAG** (combining separate text and image retrievals) performs best, outperforming joint multimodal embedding approaches.
*   **GPT-4V, Claude vision, Gemini:** While the text mentions using **GPT-4** and **Gemini-Pro** in the data synthesis pipeline (for QA generation and verification), it does not specifically benchmark or detail the performance of dedicated vision models like GPT-4V, Claude vision, or Gemini as the final RAG generator/reasoner, though they are implicitly involved in the broader VLM landscape discussed in related works.
*   **Structured Document Output:** The evaluation metrics focus on answer **completeness** and **faithfulness**, which are crucial for ensuring the generated reports/answers are accurate and grounded in the retrieved multimodal evidence.

**In summary, UniDoc-Bench provides a unified platform to benchmark MM-RAG systems that handle complex document understanding tasks involving text, tables, and charts, finding that fusing separate text and image retrieval is currently superior to joint

The user query asks for a summary related to **multimodal and generation** topics, specifically mentioning: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.

The provided webpage text is an appendix section from a paper, likely titled "UniDoc-Bench: A Unified Benchmark for Document-Centric Multimodal RAG," which details dataset creation, QA synthesizing prompts, human annotation guidelines, and experimental results comparing different RAG systems (Text-only, Image-only, Multimodal (MM), and Text+Image fusion (T++I)).

Here is a summary of how the text relates to the query:

*   **Multimodal RAG & Vision-Language Models:** The entire document focuses on **Multimodal RAG** (Retrieval-Augmented Generation) over documents. It references several RAG systems (MM, T++I) and compares retrieval performance based on different modalities required for the answer (Text-only, Img-only, Text+Img). It also compares multimodal embeddings (Voyage vs. GME).
*   **Document Understanding, PDF Parsing, Chart/Table Extraction:** Appendix B.3 explicitly mentions parsing PDFs into "text chunks, images of figures, and images of tables" using `unstructured`. Appendix B.4 provides detailed question templates for **Factual Retrieval, Comparison, Summarization, and Causal/Reasoning** questions specifically tailored for a finance domain, implying structured data extraction is a key component. Appendix F.1 discusses classifying images as "content-rich" (providing information not present in the text) or "illustrative," which relates directly to extracting information from visual elements like charts/tables.
*   **Report Generation with LLMs:** Appendix A mentions using **LLMs** for "polishing grammar and improving readability" and "assisting in the evaluation of RAG outputs" and "synthesizing the QA pairs." While it doesn't detail *report generation* itself, it confirms the use of LLMs in the RAG pipeline for generation/assistance tasks.
*   **GPT-4V, Claude vision, Gemini:** The text mentions using **Gemini-2.5-pro** in Appendix F.1 to classify images. It does not explicitly mention GPT-4V or Claude vision, though the context implies

---

## Full Content

UniDoc-Bench: A Unified Benchmark for Document-Centric Multimodal RAG
# UniDoc-Bench: A Unified Benchmark for Document-Centric Multimodal RAG
Xiangyu Peng â€ƒCan Qin11footnotemark:1Zeyuan Chen â€ƒRan Xu â€ƒCaiming Xiong â€ƒChien-Sheng Wu
Salesforce AI Research
{becky.peng, cqin, wu.jason}@salesforce.comEqual contribution.
###### Abstract
Multimodal retrieval-augmented Generation (MM-RAG) is a key approach for applying large language models (LLMs) and agents to real-world knowledge bases, yet current evaluations are fragmentedâ€”focusing on either text or images in isolation, or simplified multimodal setup, failing to capture document-centric multimodal use cases.
In this paper, we introduceUniDoc-Bench111The code and data will be available at: https://github.com/SalesforceAIResearch/UniDOC-Bench, the first large-scale, realistic benchmark for MM-RAG built from7070k real-world PDF pages across88domains.
Our pipeline extracts and links evidence from text, tables, and
figures, then generates1,6001,600multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries.
To ensure reliability,20%20\\%of QA pairs
are validated by multiple annotators and expert adjudication.UniDoc-Benchsupports apples-to-apples comparison across four paradigms â€”1) text-only, 2) image-only, 3)*multimodal*textâ€“image fusion and 4)*multimodal*joint retrieval â€”under a unified protocol with standardized candidate pools, prompts, and evaluation metrics.
Our experiments show that multimodal textâ€“image fusion RAG systems consistently outperform both unimodal and jointly multimodal embeddingâ€“based retrieval, indicating that neither text nor images alone are sufficient and that current multimodal embeddings remain inadequate. Beyond benchmarking, our analysis reveals when and how visual context complements textual evidence, uncovers systematic failure modes, and offers actionable guidance for developing more robust MM-RAG pipelines.
## 1Introduction
Retrieval-augmented generation (RAG) has become a widely used approach for applying large language models (LLMs) and agents to real-world knowledge bases> (Gao etÂ al., [> 2023
](https://arxiv.org/html/2510.03663v2#bib.bib6)> ; Fan etÂ al., [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib3)> )
. The dominant text-only pipeline applies Optical Character Recognition (OCR)> (Li etÂ al., [> 2022
](https://arxiv.org/html/2510.03663v2#bib.bib10)> ; Xue etÂ al., [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib30)> ; Poznanski etÂ al., [> 2025
](https://arxiv.org/html/2510.03663v2#bib.bib21)> )
to flatten document pages into text, indexes them as chunks, retrieves top-k text passages, and feeds them to a generator.
However, many answers depend on information embedded in figures, charts, tables, and complex layouts, where OCR often discards crucial spatial and visual semantics (e.g., map, axes, bar lengths, color encodings)> (Ma etÂ al., [> 2024a
](https://arxiv.org/html/2510.03663v2#bib.bib13)> ; Faysse etÂ al., [> 2024a
](https://arxiv.org/html/2510.03663v2#bib.bib4)> )
.
These limitations have driven the rapid development of multimodal RAG (MM-RAG), which embeds documents across modalities (text, tables, and images) and retrieves and reasons over them jointly, emerging as a key paradigm for document intelligence.
Current MM-RAG evaluation benchmarks exhibit substantial limitations, as summarized in Table[1](https://arxiv.org/html/2510.03663v2#S1.T1).
Many are restricted to a single image or a single document page as reference> (Mathew etÂ al., [> 2021
](https://arxiv.org/html/2510.03663v2#bib.bib16)> ; [> 2022
](https://arxiv.org/html/2510.03663v2#bib.bib17)> ; Zhu etÂ al., [> 2022
](https://arxiv.org/html/2510.03663v2#bib.bib36)> ; Li etÂ al., [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib11)> ; Ma etÂ al., [> 2024b
](https://arxiv.org/html/2510.03663v2#bib.bib14)> )
, cover narrow domains> Mathew etÂ al. (
[> 2021
](https://arxiv.org/html/2510.03663v2#bib.bib16)> ; [> 2022
](https://arxiv.org/html/2510.03663v2#bib.bib17)> ); Zhu etÂ al. (
[> 2022
](https://arxiv.org/html/2510.03663v2#bib.bib36)> ); Li etÂ al. (
[> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib11)> )
, under-represent modalities> (Li etÂ al., [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib11)> ; Mathew etÂ al., [> 2022
](https://arxiv.org/html/2510.03663v2#bib.bib17)> )
, operate at limited scale (few queries/pages)> (Ma etÂ al., [> 2024b
](https://arxiv.org/html/2510.03663v2#bib.bib14)> ; Wang etÂ al., [> 2025b
](https://arxiv.org/html/2510.03663v2#bib.bib26)> )
or lack a highly relevant database for RAG evaluation> (Ma etÂ al., [> 2024b
](https://arxiv.org/html/2510.03663v2#bib.bib14)> )
.
These gaps hinder fair and comprehensive comparison across methods.
Moreover, debatable claims have emerged â€”such as that â€œimage retrieval is all you needâ€> (Faysse etÂ al., [> 2024a
](https://arxiv.org/html/2510.03663v2#bib.bib4)> ; Su etÂ al., [> 2025
](https://arxiv.org/html/2510.03663v2#bib.bib24)> )
or that multimodal retrieval is inherently superior> (Zhang etÂ al., [> 2024b
](https://arxiv.org/html/2510.03663v2#bib.bib35)> ; Yu etÂ al., [> 2024b
](https://arxiv.org/html/2510.03663v2#bib.bib33)> )
â€”without enough fair and unified evaluation.
In response, we introduceUniDoc-Bench, a manually verified benchmark spanning88domains and covering text, chart, and table content, explicitly designed for cross-modality grounding with examples shown in Figure[1](https://arxiv.org/html/2510.03663v2#S1.F1).
Crucially,UniDoc-Benchenables apples-to-apples evaluation of text-retrieval, image-retrieval, multimodal text-image-fusion retrieval, and multimodal joint retrieval pipelines using highly relevant large document database and multi-type, cross-modality-grounding queries under a unified protocol.
This setup provides an unbiased view of when multimodal retrieval offers advantages beyond single modalities.
In practice,UniDoc-Benchquantifies multimodal gains, guides system design choices, and accelerates the development of effective MM-RAG systems for real-world document intelligence.
![Refer to caption](x1.png)Figure 1:UniDoc-Benchoverview.
We curate a high-quality multimodal RAG evaluation benchmark by designing and applying a classification-based filtering scheme to unlabeled, real-world PDF documents (PDFA> (Montalvo &amp; Wightman, [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib19)> )
), yielding7070k highly relevant pages across eight widely used domains â€”Finance, Legal, Healthcare, Commerce and Manufacturing, CRM, Energy, Education, and Constructionâ€”containing rich cross-modality content, including text, tables, and images.
We construct a knowledge graph that links cross-modality contents across documents via overlapping entities, and leverage these connections to synthesize 1,600 QA pairs spanning four question types:factual retrieval,comparison,summarization, andlogical reasoning, enabling multi-modality grounding and reflecting realistic retrieval scenarios.
To ensure quality,20%20\\%of the QA pairs are evaluated by three independent annotators for faithfulness, completeness, self-containment, human intent, and evidence usability, with disagreements resolved through expert adjudication.
Figure[2](https://arxiv.org/html/2510.03663v2#S3.F2)illustrates the full pipeline from PDF segmentation to dataset creation and evaluation.
In this paper, we compare text-only, image-only, multimodal joint, and text-image-fusion retrieval
augmented generation pipelines under a unified setup, using identical candidate pools, fixed top-kk, consistent prompts, and standardized evaluation criteria.
We report retrieval metrics (Recall@10,Precision@10), answercompletenessandfaithfulnessdefined at Section[4.2](https://arxiv.org/html/2510.03663v2#S4.SS2).
We observe consistent gains for textâ€“image-fusion RAG systems (completeness=68.4%68.4\\%) over multimodal joint retrieval systems (64.1%64.1\\%), text-retrieval systems (65.3%65.3\\%), and image-retrieval systems (54.5%54.5\\%).
This indicates that retrieving text and images separately using dedicated embeddings, then combining them in the final LLM query, outperforms unified embeddings or single-modality retrieval.
Moreover, visual evidence improves answer completeness and enhances faithfulness when paired with textual context, though image-only retrieval cannot fully capture the textual information contained in images.
Questions requiring images to answer remain challenging for all systems, suggesting that future RAG improvements should prioritize image-dependent queries. In contrast, performance differences across question types, such as comparison or factual retrieval, are minimal.
Table 1:Comparison of existing dataset withUniDoc-Bench.
Benchmarks|Domain|Evidence|# Queries|# Pages|RAG|Unified|Multiple|
of Doc|Suitable|Evaluation|Reference|
ArxivQA> (Li etÂ al., [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib11)> )
|single|![[Uncaptioned image]](figures/symbols/image-1.png)|0.5k|-|âœ—|âœ—|âœ—|
TAT-DQA> (Zhu etÂ al., [> 2022
](https://arxiv.org/html/2510.03663v2#bib.bib36)> )
|single|![[Uncaptioned image]](figures/symbols/text-format.png)![[Uncaptioned image]](figures/symbols/table.png)|1.6k|-|âœ—|âœ—|âœ—|
InfoVQA> (Mathew etÂ al., [> 2022
](https://arxiv.org/html/2510.03663v2#bib.bib17)> )
|single|![[Uncaptioned image]](figures/symbols/image-1.png)|0.5k|-|âœ—|âœ—|âœ—|
DocVQA> (Mathew etÂ al., [> 2021
](https://arxiv.org/html/2510.03663v2#bib.bib16)> )
|single|![[Uncaptioned image]](figures/symbols/image-1.png)![[Uncaptioned image]](figures/symbols/table.png)|0.5k|-|âœ—|âœ—|âœ—|
MMLONG> (Ma etÂ al., [> 2024b
](https://arxiv.org/html/2510.03663v2#bib.bib14)> )
|multiple|![[Uncaptioned image]](figures/symbols/text-format.png)![[Uncaptioned image]](figures/symbols/image-1.png)![[Uncaptioned image]](figures/symbols/table.png)|1k|6k|âœ—|âœ—|âœ“|
REALMM> (Wasserman etÂ al., [> 2025
](https://arxiv.org/html/2510.03663v2#bib.bib29)> )
|multiple|![[Uncaptioned image]](figures/symbols/text-format.png)![[Uncaptioned image]](figures/symbols/image-1.png)![[Uncaptioned image]](figures/symbols/table.png)|5k|8k|âœ“|âœ—|âœ—|
ViDoSeek> (Wang etÂ al., [> 2025b
](https://arxiv.org/html/2510.03663v2#bib.bib26)> )
|multiple|![[Uncaptioned image]](figures/symbols/text-format.png)![[Uncaptioned image]](figures/symbols/image-1.png)![[Uncaptioned image]](figures/symbols/table.png)|1.2k|10k|âœ“|âœ—|âœ—|
UniDoc-Bench(ours)|multiple|![[Uncaptioned image]](figures/symbols/text-format.png)![[Uncaptioned image]](figures/symbols/image-1.png)![[Uncaptioned image]](figures/symbols/table.png)|1.6k|70k|âœ“|âœ“|âœ“|
* â€¢RAG Suitable: The dataset provides RAG-style data: queries are self-contained and reflect realistic human questions, with each paired to a grounding corpus (text, images, tables) for retrieval-conditioned answering, supported by a large, highly relevant knowledge base to evaluate retrieval.
* â€¢Unified Evaluation: Apples-to-apples comparison across different baseline RAG systems.Multiple Reference: Supports multi-hop, multi-modality, multi-source grounding.
In this paper, we make the following contributions:
* â€¢We introduce a new multimodal RAG benchmark
built from real-world PDF documents, comprising 70k pages across 8 domains, with 1,600 human-verified QA pairs referencing text, figures, and tables, spanning44question types.
* â€¢We present an associated data synthesizing pipeline for creating multimodal RAG evaluation datasets, designed to be compatible with any document database.
* â€¢We propose a fair and reproducible evaluation framework by fixing candidate pools across modalities
, and measuring retrieval effectiveness, answer faithfulness, and completeness end-to-end across different RAG systems. Specifically, to ensure fairness when comparing against text-only RAG, we caption images and tables and match them back to the retrieved text chunks before final generation, thereby maintaining a consistent candidate pool.
* â€¢We conduct a systematic comparison of text-retrieval, image-retrieval, textâ€“image fusion, and multimodal joint retrieval pipelines, analyzing which retrieval strategy performs best under different question types, evidence modalities, and document characteristics, providing practical guidance for choosing MM-RAG systems in real-world data settings.
## 2Related Works
### 2.1Multimodal Retrieval-augmented generation (MM-RAG)
Recent advances in multimodal understanding highlight the importance of MM-RAG in reducing hallucinations. VLM2Vec> (Jiang etÂ al., [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib8)> ; Meng etÂ al., [> 2025
](https://arxiv.org/html/2510.03663v2#bib.bib18)> )
demonstrated that instruction-tuning vision-language models significantly enhances their ability to produce robust embeddings, leading to strong performance across diverse textâ€“image alignment tasks. Similarly, SeBe> (Chen etÂ al., [> 2025
](https://arxiv.org/html/2510.03663v2#bib.bib1)> )
adapts LLaVA-1.5> (Liu etÂ al., [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib12)> )
by finetuning it into a retrieval-oriented embedding model, aligning user queries with external knowledge sources. GME> (Zhang etÂ al., [> 2024a
](https://arxiv.org/html/2510.03663v2#bib.bib34)> )
proposed a unified multimodal embedding model that is able to perform both text-to-image, image-to-text, and text-to-text retrieval. Uni-Retrieval> (Jia etÂ al., [> 2025
](https://arxiv.org/html/2510.03663v2#bib.bib7)> )
extends the paradigm by integrating VLMs with prompt-tuning strategies, enabling flexible handling of heterogeneous queries and modalities. Routing-based methods such as UniversalRAG> (Yeo etÂ al., [> 2025
](https://arxiv.org/html/2510.03663v2#bib.bib31)> )
and UniRAG> (Sharifymoghaddam etÂ al., [> 2025
](https://arxiv.org/html/2510.03663v2#bib.bib23)> )
introduce adaptive query routing mechanisms that dynamically select the most appropriate modality and level of granularity.
### 2.2Visual Document Understanding and Evaluation
The challenge of document understanding with interleaved textual and visual components has recently prompted the development of specialized vision-based RAG pipelines> (Yu etÂ al., [> 2024a
](https://arxiv.org/html/2510.03663v2#bib.bib32)> ; Wang etÂ al., [> 2025a
](https://arxiv.org/html/2510.03663v2#bib.bib25)> ; [> c
](https://arxiv.org/html/2510.03663v2#bib.bib27)> )
that directly take screenshots of documents as input. A notable example is ColPali> (Faysse etÂ al., [> 2024a
](https://arxiv.org/html/2510.03663v2#bib.bib4)> )
, which leverages VLMs to jointly encode textual queries and visual documents with the MaxSim operations> (Khattab &amp; Zaharia, [> 2020
](https://arxiv.org/html/2510.03663v2#bib.bib9)> )
.
ViDoRAG> (Wang etÂ al., [> 2025a
](https://arxiv.org/html/2510.03663v2#bib.bib25)> )
introduces a multi-agent reasoning architecture designed for complex queries that require iterative cross-modal reasoning.
In parallel, optimization-focused approaches such as VRAG> (Wang etÂ al., [> 2025c
](https://arxiv.org/html/2510.03663v2#bib.bib27)> )
apply reinforcement learning strategies, including GRPO-based> Shao etÂ al. (
[> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib22)> )
training, to adapt VLMs for end-to-end document understanding.
However, the comparisons with text-only baselines are not entirely fair, as most of these baselines exclude non-text modalities in response generation.
Moreover, existing evaluations are conducted on datasets not designed for RAG.
MMLongBench-Doc> (Ma etÂ al., [> 2024c
](https://arxiv.org/html/2510.03663v2#bib.bib15)> )
targets long-context multimodal document understanding, but its database is not highly relevant and thus unsuitable for retrieval tasks.
REAL-MM> (Wasserman etÂ al., [> 2025
](https://arxiv.org/html/2510.03663v2#bib.bib29)> )
and VidoSeek> (Wang etÂ al., [> 2025b
](https://arxiv.org/html/2510.03663v2#bib.bib26)> )
are designed for MM-RAG, yet they lack cross-modality and multi-page evidence, limiting their ability to provide comprehensive and unified evaluation across RAG systems.
The other benchmarks> (Mathew etÂ al., [> 2021
](https://arxiv.org/html/2510.03663v2#bib.bib16)> ; [> 2022
](https://arxiv.org/html/2510.03663v2#bib.bib17)> ; Zhu etÂ al., [> 2022
](https://arxiv.org/html/2510.03663v2#bib.bib36)> ; Li etÂ al., [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib11)> )
are typically limited to a single image or a single document page, covering narrow domains, under-representing modalities, or operating at limited scale with only a few queries or pages, as summarized in Table[1](https://arxiv.org/html/2510.03663v2#S1.T1). To address these gaps, we introduceUniDoc-Bench, a benchmark tailored to practical RAG use cases.
## 3Dataset Curation
First, a large-scale, high-quality multi-modal database is needed for evaluating RAG systems, where each document contains content-rich figures, tables and corresponding textual information. Documents should be domain-specific and exhibit high inter-document similarity to evaluate effective retrieval.
The construction of this database is detailed in Section[3.1](https://arxiv.org/html/2510.03663v2#S3.SS1).
Then, we require high-quality queryâ€“answer pairs to evaluate the RAG system.
Each query is designed to reflect realistic human intent and is written as a self-contained question. The corresponding ground-truth answer must be retrievable solely from the curated database and supported by evidence across multiple modalities.
In Section[3.2](https://arxiv.org/html/2510.03663v2#S3.SS2), we describe our synthetic QA pipeline, and in Section[3.3](https://arxiv.org/html/2510.03663v2#S3.SS3), we validate dataset quality through human annotation.
### 3.1Source Document Collection
We use PDFA> (Montalvo &amp; Wightman, [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib19)> )
as our data source, containing diverse formats (e.g., reports, slides, posters) and covering broad domains, but it lacks tags or labels.
Therefore, our first step is data filtering to collect a high-quality database.
We design a field scheme (see Appendix[B.1](https://arxiv.org/html/2510.03663v2#A2.SS1)) that captures key metadata, including domain, subdomain, language, modality (e.g., text, tables, figures), image quality (whether the resolution is clear), and text proportion. This allows us to standardize the data and build a high-quality cross-modality database.
As shown in Figure[1](https://arxiv.org/html/2510.03663v2#S1.F1)(c), we select88domains based on differences across industries and define many subdomains within each, grouping similar documents.
To ensure high inter-document similarity, we retain only documents from3â€‹â€“â€‹53â€“5related subdomains containing multiple modalities, yielding on average8,0008,000pages per domain.
The final dataset spansLegal, Commerce and Manufacturing, Education, Energy, Construction, Finance, Healthcare, and CRM, with detailed subdomain descriptions in Appendix[B.2](https://arxiv.org/html/2510.03663v2#A2.SS2).
![Refer to caption](x2.png)Figure 2:Data Construction pipeline. (a) We filter and tag PDFA documents to curate a high-quality database of7070k pages spanning88domains.
(b) We parse documents into text, figures, and tables, then synthesize initial QA pairs covering four question types and three modalities using adapted templates.
(c) We ground answers in supporting evidence, refine questions for human-intent and self-containment, and verify responses for factuality and completeness, yielding1,6001,600QA pairs. To ensure quality,20%20\\%of the dataset is validated by three independent human annotators.
### 3.2Question and Answer Synthesis Pipeline
As shown in Figure[2](https://arxiv.org/html/2510.03663v2#S3.F2), we introduce a data-synthesis pipeline for building multimodal RAG evaluation datasets with high-quality QA pairs, compatible with various document databases.
#### 3.2.1Evidence Collection
PDF Parsing.We first parse our curated PDF document database222https://unstructured.io/by extracting text chunks, tables, and figures, with the latter two stored separately as image files.
Within the parsed text chunk, each image and table is replaced with a unique placeholder tag (e.g.,&lt;&lt;fig-XXX&gt;&gt;or&lt;&lt;tab-XXX&gt;&gt;), along with its corresponding caption and parsed content to fully represent interleaved multimodal content.
An example of this parsing process is provided in Appendix[B.3](https://arxiv.org/html/2510.03663v2#A2.SS3).
Chunks Grouping.To support multimodal evidence QA, we construct a knowledge graph (ð’¢i\\mathcal{G}\_{i})> (ExplodingGradients, [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib2)> ; Peng etÂ al., [> 2024
](https://arxiv.org/html/2510.03663v2#bib.bib20)> )
over the parsed chunks for domainii, where nodes (Ni={niâ€‹1,niâ€‹2,â€¦}N\_{i}=\\{n\_{i1},n\_{i2},...\\}) represent chunks and edges (EiE\_{i}) denote overlapping entities (e.g., â€œAI Agent Platformâ€).
Chunks across three modalities (text, tables, figures), from within or across documents, are linked to form ground-truth evidence, which are then used for QA synthesis in the next step.
#### 3.2.2Question and Answer Generation
Template Choice.First, we ensure the synthesized questions arediverseand span multiple categories, since focusing on a single category or using only the same few-shot example questions can introduce bias and limit the comprehensiveness of RAG evaluation.
We designed four RAG question types: 1)factual retrieval, 2)comparison, 3)summarization, and 4)logical reasoning.
For each question type and document domain, we design1010â€“1515general templates (see Appendix[B.4](https://arxiv.org/html/2510.03663v2#A2.SS4)).
We then sample linked chunks (niâ€‹j,eiâ€‹j,niâ€‹kn\_{ij},e\_{ij},n\_{ik}) and prompt the LLM to select11â€“33templates (Tiâ€‹jT\_{ij}) that best match the provided chunks and are most likely to produce QA pairs that humans would naturally ask, thereby improving both the diversity and coverage of the questions.
Evidence Grounding.To ensure comprehensive evaluation of MM-RAG, we design fouranswer typeswith distinct evidence requirements, each supported by specialized prompts:
* â€¢Text-only: The question can be fully answered using natural language text from the documents.
* â€¢Image-only: The question requires information exclusively from an image, such as numerical values shown only in a figure, thereby testing the systemâ€™s ability to interpret visual content.
* â€¢Image-plus-text: Answering the question requires integrating information from both text and images, testing the modelâ€™s ability to reason across modalities.
* â€¢Table-required: The question required tabular information to answer, requiring the system to understand table structure and content.
To construct QA pairs, we promptGPT-4.1with parsed text chunks and extracted figures/tables (PNG format), guided by promptsPnP\_{n}corresponding to the above answer types (see details in Appendix[B.5](https://arxiv.org/html/2510.03663v2#A2.SS5)) and templatesTiâ€‹jT\_{ij}.
We then employGemini-Pro-2.5â€” to mitigate single-LLM bias â€”to verify that the ground-truth answers are correctly grounded in the referenced text, tables, or images, ensuring factual correctness and re-classifying question types when necessary.
Rewriting.To ensure that questions areself-containedand reflect realistichuman intent, we refine the initially synthesized QA pairs.
In the first stage, many synthesized questions follow a long-context QA style and may include vague references such as â€œin this reportâ€ or â€œin FigureÂ 8.â€
To make them suitable for RAG evaluation, we rewrite these questions to ensure they are self-contained and understandable without external context (Appendix[B.6](https://arxiv.org/html/2510.03663v2#A2.SS6)).
Additionally, many QA pairs are grounded in images, leading to VQA-style questions (e.g., â€œHow many logos are in Apple Inc.â€™s 2023 report?â€). Such questions do not reflect natural human queries in a RAG context, so we filter and rewrite them to better align with realistic human intent.
To ensure comprehensive evaluation, ground-truth answers must becompleteanddiverse. In the final step, we revise answers to cover all relevant aspects of their corresponding questions (see Appendix[B.7](https://arxiv.org/html/2510.03663v2#A2.SS7)).
Deduplication and Balance.Additionally, we remove duplicated questionâ€“answer pairs that are highly similar in the question or the answer (similarity&gt;0.75&gt;0.75333https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
to maintain dataset quality and diversity.
We also rebalance the dataset by question type and answer type to provide a fair evaluation.
Dataset Statistics.Based on the above stages, we construct an evaluation benchmark consisting of200200QA pairs for each category, in total16001600QAs as described in Section[3.1](https://arxiv.org/html/2510.03663v2#S3.SS1).
Within each set of200200QA pairs, we maintain an equal distribution of5050text-only, image-only, text-plus-image, and table-only questions.
In total, the dataset contains800800single-modality and800800multi-modality questions. On average, each question requires2.152.15evidence items (text chunks, images, or tables) for a complete answer, highlighting the need for RAG systems to retrieve multiple pieces of evidence. We further ensure a balanced distribution across the four main question types: factual retrieval, summarization, comparison, and logical reasoning.
More details can be found in Figure[1](https://arxiv.org/html/2510.03663v2#S1.F1)(b).
### 3.3Dataset Quality
We evaluate whether our constructed dataset is of sufficient quality to support reliable evaluation of different RAG systems
by sampling20%20\\%of our datasetâ€”4040QA pairs from each domain, resulting in a total of320320QA pairsâ€”for human evaluation.
We recruited33human annotators to evaluate the questionâ€“response pairs against the provided source documents.
In cases where33annotators disagreed, a44th senior reviewer mediated the discussion and guided the annotators toward a consensus decision.
For each item, annotators were directed to a folder containing all relevant source materials, including text extracted from PDF documents and associated images.
The annotation process involved assessing each question-response pair across five dimensions (More details about this human annotation task can be found in Appendix[C](https://arxiv.org/html/2510.03663v2#A3)):
* â€¢Factuality: evaluates whether the claims made in the question (Factuality-Question) and the response (Factuality-Response) were factually supported by the source documents.
* â€¢Completeness: assesses whether the response incorporates all necessary information from the retrieved sources to fully answer the question.
* â€¢Grounding: assesses whether each source chunk (text, image, or table) used to generate the ground-truth response is necessary to answer the question, by labeling it as eitherrequiredornot required.
* â€¢Self-Contained: assesses whether the question was understandable and answerable on its own, without needing external context beyond the provided documents.
* â€¢Human-like Intent: evaluates whether the question reflected a natural, meaningful query that a human would plausibly ask to retrieve information.
Table 2:Human evaluation quality on a20%20\\%sample (n=320n{=}320). Each cell shows % and (count/320)
||Factualityâ€“Q|Factualityâ€“R|Completeness|Self-Contained|Human-like Intent|Grounding|
% &amp; Count|99.70% (319/320)|91.90% (294/320)|91.90% (294/320)|99.70% (319/320)|97.50% (312/320)|84.38% (270/320)|
As shown in Table[2](https://arxiv.org/html/2510.03663v2#S3.T2), the sample shows near-perfect question factuality and self-containment, with strong response factuality and completeness (eachâ‰ˆ\\approx294/320). Human-like intent remains high (312/320). Grounding label accuracy is solid (270/320) as well.
## 4Experiments
To fairly evaluate different RAG systems, we focus on two aspects: retrieval and end-to-end performance.
In this section, we first evaluate the retrieval performance of four embedding and retrieval models, including text-only, image-only, and two multimodal approaches (Section[4.1](https://arxiv.org/html/2510.03663v2#S4.SS1)).
We then assess the end-to-end response performance of six RAG systems that vary in their use of embeddings, retrieval strategies, and LLMs (Section[4.2](https://arxiv.org/html/2510.03663v2#S4.SS2)).
Together, these experiments highlight the utility of our dataset and provide practical guidance for selecting RAG components.
### 4.1Retrieval Performance
Baselines.We use the curated PDF documents as the knowledge base and the synthesized QA pairs to evaluate44embeddingâ€“retrieval models. For all methods, we retrieve the top-k=10k=10candidates.
* â€¢Text:PDF pages are parsed into text chunks, each embedded with OpenAIâ€™stext-embedding-3-small, and retrieved via vector search.
* â€¢Image:Each PDF page is converted to a JPEG image, which is embedded usingColQwen2.5-v0.2> (Faysse etÂ al., [> 2024b
](https://arxiv.org/html/2510.03663v2#bib.bib5)> )
for image retrieval.
* â€¢MM:Both text chunks and page-level images are embedded.
* â€“MM (GME): Text and images are jointly embedded usingGME-Qwen2-VL-7B-Instruct> (Zhang etÂ al., [> 2024a
](https://arxiv.org/html/2510.03663v2#bib.bib34)> )
, enabling multimodal retrieval.
* â€“MM (T+I): A fusion baseline that selects the top-5 candidates from Text and the top-5 from Image retrieval.
Metrics.We reportPrecision@10andRecall@10as the retrieval metrics. Since no re-ranker is applied, recall is more informative thannDCGfor evaluation.
Since we need to evaluate both image and text retrieval, each retrieved text chunk or PDF image-page is mapped back to its original PDF page, and the ground-truth contexts are mapped in the same way.
Consequently, a retrieved chunk may span multiple consecutive pages of the source document (e.g., pages 2â€“3 of document A).
A retrieval is considered a true positive if the retrieved text chunk or image-page matches the ground-truth context in both page number and file.
This criterion may slightly inflateRecall@10, since partial overlaps (e.g., retrieved pages 1â€“3 vs. ground-truth pages 3â€“5, with the answer on page 5) are still treated as correct.
However, this approach offers the most practical and fair basis for comparing text and image retrieval.
Thus, absolute scores should not be overinterpreted; the key is the relative performance differences across methods, which remain reliable.
Table 3:Retrieval performance (Precision@10/Recall@10) of44RAG systems on16001600QA pairs across eight domains, with average recall reported across all domains.|Domain|Text (OpenAI)|Image (colqwen)|Multimodal|
GME|Text + Image|
Precision|Recall|Precision|Recall|Precision|Recall|Precision|Recall|
Com.|0.430|0.813|0.294|0.831|0.354|0.895|0.523|0.886|
Constr.|0.377|0.750|0.263|0.794|0.336|0.881|0.451|0.833|
CRM|0.400|0.808|0.283|0.829|0.343|0.884|0.486|0.876|
Edu|0.414|0.843|0.268|0.843|0.366|0.912|0.460|0.880|
Energy|0.382|0.772|0.257|0.822|0.257|0.822|0.459|0.863|
Fin.|0.384|0.778|0.291|0.812|0.376|0.857|0.484|0.867|
HC|0.420|0.741|0.252|0.849|0.370|0.835|0.460|0.837|
Legal|0.440|0.864|0.291|0.855|0.327|0.876|0.510|0.891|
Avg.|0.406|0.796|0.275|0.829|0.341|0.870|0.479|0.867|
Table[3](https://arxiv.org/html/2510.03663v2#S4.T3)reports the retrieval performance of the four RAG embedding-retrieval models.
We observe thatimage-based retrieval achieves consistently higher recall but lower precision than text-based retrieval, as page-image chunks cover more information than individual text chunks.
Combining text and image retrieval (T++I) further improves both recall and precision, effectively leveraging the strengths of both modalities.
In contrast, multimodal embeddings (gme-Qwen2-VL-7B-Instruct), which encode text and images jointly rather than separately, achieve comparable recall but substantially lower precision, suggesting that current multimodal embeddings still lag behind fusion of unimodal embeddings.
We also break down retrieval performance by question and answer types in Appendix[E.1](https://arxiv.org/html/2510.03663v2#A5.SS1).
### 4.2End-to-End Performance
Baselines.
* â€¢Image-only RAG:Each PDF page is converted to a JPEG and retrieved via image embeddings.
* â€“Image-only RAG(IMG): Uses LlamaIndex withcolqwen2.5-v0.2> (Faysse etÂ al., [> 2024b
](https://arxiv.org/html/2510.03663v2#bib.bib5)> )
for image retrieval andGPT-4.1as the final MM-LLM. Each PDF page is converted to a JPEG image and embedded. After retrieval, the question and retrieved images are provided toGPT-4.1to obtain the final response.
* â€“VRAG> (Wang etÂ al., [> 2025d
](https://arxiv.org/html/2510.03663v2#bib.bib28)> )
: a multimodal RAG agent that leverages a vision-specific action spaceâ€”including operations such as cropping and scalingâ€”to iteratively extract information from image-formatted PDF pages in a coarse-to-fine manner. The embedding model iscolqwen2.5-v0.2, and the final LLM isGPT-4.1.
* â€¢Text-only RAG:Most multimodal RAG studies> (Wang etÂ al., [> 2025b
](https://arxiv.org/html/2510.03663v2#bib.bib26)> ; Faysse etÂ al., [> 2024a
](https://arxiv.org/html/2510.03663v2#bib.bib4)> )
compare only against text-only baselines. For a fairer comparison, PDF pages are parsed into text chunks, embedded for retrieval, with associated images/tables linked back for final responses.
* â€“TEXT: Each text chunk is embedded usingtext-embedding-3-smalland retrieved. The retrieved text chunks, along with their associated images, are then fed intoGPT-4.1to generate the final response.
* â€“Vertex AI: following the official tutorial444https://www.cloudskillsboost.google/focuses/85643?parent=catalog,
PDFs are parsed into text and images, with images auto-captioned by Gemini. Only the text (document text and image captions) is indexed bytext-embedding-004and retrieved, and the retrieved chunks along with the corresponding images are passed togemini-2.5-flashfor final response.
* â€¢MM-RAG:Both text chunks and image-format page images are embedded and retrieved for responses.
* â€“Multimodal Text-Image-Fusion RAG (T++I):Retrieves text and images separately usingtext-embedding-3-smallandcolqwen2.5-v0.2, then combines them for generation withGPT-4.1.
* â€“Multimodal-joint-Retrieval RAG(MM): Usesgme-Qwen2-VL-7B-Instruct> (Zhang etÂ al., [> 2024a
](https://arxiv.org/html/2510.03663v2#bib.bib34)> )
as a multimodal embedding model for both text and image content. Unlike T++I, where text and images are embedded and retrieved separately, the text chunks and image-formatted PDF pages are embedded together, retrieved jointly, and then fed intoGPT-4.1for the final response.
Table 4:Completeness of six RAG systems on 1,600 QA pairs across eight domains. Average recall is reported across all domains, with similarity top-kkset to 10 and 20, computed against the ground-truth responses.|Domain|Image-only RAG|Text-only RAG (+img matched)|Multimodal RAG|
IMG|VRAG|TEXT|Vertex AI|MM (GME)|T+I|
top-1010|top-2020|top-1010|top-2020|top-1010|top-2020|top-1010|top-2020|top-1010|top-2020|top-1010|top-2020|
Com.|0.545|0.552|0.547|0.550|0.633|0.673|0.613|0.630|0.617|0.611|0.693|0.733|
Constr.|0.502|0.601|0.536|0.542|0.561|0.587|0.558|0.621|0.616|0.609|0.607|0.647|
CRM|0.524|0.524|0.523|0.544|0.643|0.663|0.628|0.625|0.623|0.637|0.647|0.703|
Edu|0.569|0.560|0.517|0.524|0.692|0.702|0.613|0.633|0.640|0.668|0.688|0.691|
Energy|0.535|0.566|0.558|0.589|0.607|0.637|0.627|0.677|0.669|0.666|0.649|0.680|
Fin.|0.500|0.499|0.529|0.535|0.584|0.626|0.557|0.605|0.627|0.636|0.638|0.636|
HC|0.481|0.492|0.481|0.492|0.602|0.639|0.638|0.643|0.642|0.664|0.621|0.666|
Legal|0.558|0.568|0.599|0.595|0.629|0.696|0.642|0.675|0.609|0.629|0.689|0.716|
Avg.|0.527|0.545|0.536|0.546|0.619|0.653|0.610|0.639|0.630|0.641|0.654|0.684|
Metrics.Forend-to-endperformance, we use an LLM-based judge to measure faithfulness and completeness.
Specifically, we first ask the LLM to extract the facts required to answer each question and then verify whether these facts are grounded in the ground-truth chunks; this is measured asfaithfulness.
Next, we ask the LLM to extract the facts required to answer the question from the ground-truth answer and then check whether each fact appears in the systemâ€™s response; this is measured ascompleteness. Higher faithfulness and completeness scores are better.
Table[4](https://arxiv.org/html/2510.03663v2#S4.T4)reports the completeness of responses generated by the six RAG systems under varying similarity top-kkretrieval settings.Text-only RAG(0.6530.653)substantially outperforms Image-only RAG systems(IMG:0.5450.545, VRAG:0.5460.546), highlighting the significant performance gap between text-based and image-based retrieval in current RAG architectures.
Although image retrieval achieves higher completeness at the retrieval stage, this advantage does not translate into better end-to-end performance, since multimodal LLMs (GPT-4.1) are more effective when processing text and image chunks together rather than page-level image PDFs alone.
The text-image-fusion RAG
achieves the best overall performance (0.6840.684) across eight domains, demonstrating that image-based PDF representations can effectively complement text retrieval.
Although VRAG leverages cropping and scaling to enhance image-based retrieval (0.5360.536for VRAG vs.0.5270.527for IMG (topk=10k=10)), it still lags behind the combined Text&amp;Image-Retrieval approach, underscoring the advantage of explicitly integrating both modalities.
Multimodal joint-retrieval RAG systems (MM;0.6410.641) also fall short of the simple combination of the best text and image embeddings.
This indicates that current multimodal embedding approaches still have substantial room for improvement, and that explicitlycombining separate text and image embeddings remains the most effective strategyfor leveraging multimodal documents.
More notably, multimodal-joint RAG (MM;0.6410.641) performs worse than text-only RAG (0.6530.653), demonstrating that current multimodal models still fall short of strong unimodal baselines.
These results also highlight the importance of establishing fair baselines and the value of our dataset: multimodal RAG systems should be benchmarked against strong, balanced baselines on diverse and high-quality datasets rather than against overly weak text-only settings.
Table[5](https://arxiv.org/html/2510.03663v2#S4.T5)shows that questions requiring only text are most effectively handled by RAG systems with text-embedding.Questions requiring tables are also relatively easy for RAG systems, as tables can be accurately parsed as text, which is a straightforward step before embedding documents for text-based retrieval.
In contrast, questions requiring images remain challenging across all embedding types â€”text, image, or multimodal â€”highlighting that futureRAG improvements should prioritize image-required questions.
We also observe thatquestion type has minimal impact on overall RAG performance.
We provide detailed case studies in Appendix[D](https://arxiv.org/html/2510.03663v2#A4).
Table 5:Faithfulness and Completeness of six RAG systems across different question and answer types on 1,600 QA pairs spanning eight domains, with average recall reported across all domains.|Type|Image-only RAG|Text-only RAG (+img matched)|Multimodal RAG|
IMG|VRAG|TEXT|Vertex AI|MM (GME)|T+I|
faith.|complet.|faith.|complet.|faith.|complet.|faith.|complet.|faith.|complet.|faith.|complet.|
F.R.|0.640|0.536|0.581|0.536|0.698|0.629|0.563|0.557|0.668|0.599|0.763|0.704|
Comp.|0.669|0.510|0.611|0.513|0.739|0.619|0.634|0.644|0.744|0.656|0.755|0.641|
Summary|0.727|0.536|0.706|0.602|0.736|0.613|0.694|0.670|0.752|0.670|0.781|0.651|
Logical|0.738|0.526|0.650|0.584|0.769|0.607|0.690|0.660|0.744|0.678|0.780|0.621|
Text-only|0.812|0.580|0.767|0.624|0.877|0.656|0.817|0.758|0.849|0.771|0.880|0.700|
Img-only|0.512|0.448|0.453|0.483|0.580|0.606|0.359|0.447|0.463|0.436|0.620|0.615|
Text + Img|0.678|0.498|0.576|0.523|0.716|0.601|0.581|0.556|0.707|0.583|0.749|0.630|
Table-req.|0.693|0.587|0.662|0.554|0.714|0.601|0.716|0.670|0.819|0.747|0.811|0.716|
### 4.3Additional findings
MM-RAG systems can offer both improved end-to-end performance and lower cost compared to
text-only RAG.As reported in Appendix[E.3](https://arxiv.org/html/2510.03663v2#A5.SS3), text-only RAG is the most expensive, image-only RAG has the lowest cost and latency, and multimodal RAG is cheaper than text-only RAG while maintaining comparable latency.
Open-source and commercial multimodal embeddings perform comparably.We compare RAG systems using different multimodal embeddings (Table[8](https://arxiv.org/html/2510.03663v2#A5.T8), Table[9](https://arxiv.org/html/2510.03663v2#A5.T9)) and find that the commercialvoyage-multimodal-3achieves similar performance to the open-sourceGME, though both still lag behind multimodal textâ€“image fusion RAG systems.
Content-rich images increase difficulty.We classify images usinggemini-2.5-proas content-rich (containing information not in the text) or illustrative.
Content-rich images are more prevalent in finance (62.8%) and construction (69.3%) than in commerce manufacturing (40.0%) and legal (49.5%), indicating that domains with more content-rich images pose greater challenges for RAG, consistent with the results in Table[4](https://arxiv.org/html/2510.03663v2#S4.T4).
Details are in Appendix[F.1](https://arxiv.org/html/2510.03663v2#A6.SS1).
Question type affects difficulty.We further analyzed fined-grained evidence types and found that RAG performance depends on answer modality: text retrieval excels at entity recognition (53.9% better than image retrieval), comparative analysis (37.6%), contextual numerical reasoning (34.8%), and quantity estimation (29.1%), while image retrieval is stronger on chart/table interpretation (64.2% better than text retrieval), temporal trends (40.0%), and spatial/geographic reasoning (13.3%).
Detailed examples and analysis are in Appendix[D.1](https://arxiv.org/html/2510.03663v2#A4.SS1), Appendix[D.2](https://arxiv.org/html/2510.03663v2#A4.SS2)and Appendix[F.2](https://arxiv.org/html/2510.03663v2#A6.SS2).
We also summarize in Appendix[F](https://arxiv.org/html/2510.03663v2#A6)that single document page numbers and formats do not significantly affect MM-RAG performance.
## 5Conclusion
In this paper, we introducedUniDoc-Bench, a large-scale benchmark for document-centric multimodal RAG, built from7070k real-world PDF pages across88domains with1,6001,600human-verified QA pairs. Our experiments establish a clear performance hierarchy, showing thattext-image fusion RAG performs the best, consistently outperforming both joint multimodal (MM) RAG and single-modality RAG systems. This key finding demonstrates that fusing separate, strong retrievers for text and images is currently a more effective strategy than relying on a single joint multimodal embedding or a single modality alone. Our analysis further pinpoints image-dependent queries as the primary challenge for all systems. By providing a standardized platform for fair comparison,UniDoc-Benchserves as a crucial resource to guide the development of more robust and faithful document intelligence systems.
#### Reproducibility Statement
We ensure the reproducibility of our work. All source datasets we employed are publicly available (PDFA). We are making our code available in the supplementary materials to enable replication of our findings.
## Acknowledgements
We would like to sincerely thank Nesrine Yakoubi, Caitlyn Cline, Wyatt Miller, Michael Thuo, John Soledad, and Fabriana Louisa Pita for their invaluable efforts in annotating the datasets. Their careful work and dedication were essential to the success of this research.
## References
* Chen etÂ al. (2025)Boqi Chen, Anuj Khare, Gaurav Kumar, Arjun Akula, and Pradyumna Narayana.Seeing beyond: Enhancing visual question answering with multi-modal retrieval.In*Proceedings of the 31st International Conference on Computational Linguistics: Industry Track*, pp. 410â€“421, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics.URL[https://aclanthology.org/2025.coling-industry.35/](https://aclanthology.org/2025.coling-industry.35/).
* ExplodingGradients (2024)ExplodingGradients.Ragas: Supercharge your llm application evaluations.[https://github.com/explodinggradients/ragas](https://github.com/explodinggradients/ragas), 2024.
* Fan etÂ al. (2024)Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li.A survey on rag meeting llms: Towards retrieval-augmented large language models.In*Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining*, pp. 6491â€“6501, 2024.
* Faysse etÂ al. (2024a)Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, CÃ©line Hudelot, and Pierre Colombo.Colpali: Efficient document retrieval with vision language models.*arXiv preprint arXiv:2407.01449*, 2024a.
* Faysse etÂ al. (2024b)Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, CÃ©line Hudelot, and Pierre Colombo.Colpali: Efficient document retrieval with vision language models, 2024b.URL[https://arxiv.org/abs/2407.01449](https://arxiv.org/abs/2407.01449).
* Gao etÂ al. (2023)Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang.Retrieval-augmented generation for large language models: A survey.*arXiv preprint arXiv:2312.10997*, 2(1), 2023.
* Jia etÂ al. (2025)Yanhao Jia, Xinyi Wu, Hao Li, Qinglin Zhang, Yuxiao Hu, Shuai Zhao, and Wenqi Fan.Uni-retrieval: A multi-style retrieval framework for stemâ€™s education.*arXiv preprint arXiv:2502.05863*, 2025.
* Jiang etÂ al. (2024)Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen.Vlm2vec: Training vision-language models for massive multimodal embedding tasks.*arXiv preprint arXiv:2410.05160*, 2024.
* Khattab &amp; Zaharia (2020)Omar Khattab and Matei Zaharia.Colbert: Efficient and effective passage search via contextualized late interaction over bert.In*Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval*, pp. 39â€“48, 2020.
* Li etÂ al. (2022)Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du, Yuning Du, Lingfeng Zhu, Baohua Lai, Xiaoguang Hu, etÂ al.Pp-ocrv3: More attempts for the improvement of ultra lightweight ocr system.*arXiv preprint arXiv:2206.03001*, 2022.
* Li etÂ al. (2024)Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and QiÂ Liu.Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models.*arXiv preprint arXiv:2403.00231*, 2024.
* Liu etÂ al. (2024)Haotian Liu, Chunyuan Li, Yuheng Li, and YongÂ Jae Lee.Improved baselines with visual instruction tuning.In*Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 26296â€“26306, 2024.
* Ma etÂ al. (2024a)Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin.Unifying multimodal retrieval via document screenshot embedding.*arXiv preprint arXiv:2406.11251*, 2024a.
* Ma etÂ al. (2024b)Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, etÂ al.Mmlongbench-doc: Benchmarking long-context document understanding with visualizations.*Advances in Neural Information Processing Systems*, 37:95963â€“96010, 2024b.
* Ma etÂ al. (2024c)Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, etÂ al.Mmlongbench-doc: Benchmarking long-context document understanding with visualizations.*arXiv preprint arXiv:2407.01523*, 2024c.
* Mathew etÂ al. (2021)Minesh Mathew, Dimosthenis Karatzas, and CVÂ Jawahar.Docvqa: A dataset for vqa on document images.In*Proceedings of the IEEE/CVF winter conference on applications of computer vision*, pp. 2200â€“2209, 2021.
* Mathew etÂ al. (2022)Minesh Mathew, Viraj Bagal, RubÃ¨n Tito, Dimosthenis Karatzas, Ernest Valveny, and CVÂ Jawahar.Infographicvqa.In*Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pp. 1697â€“1706, 2022.
* Meng etÂ al. (2025)Rui Meng, Ziyan Jiang, YeÂ Liu, Mingyi Su, Xinyi Yang, Yuepeng Fu, Can Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, etÂ al.Vlm2vec-v2: Advancing multimodal embedding for videos, images, and visual documents.*arXiv preprint arXiv:2507.04590*, 2025.
* Montalvo &amp; Wightman (2024)Pablo Montalvo and Ross Wightman.pixparse/pdfa-eng-wds [dataset].Hugging Face Datasets, 2024.URL[https://huggingface.co/datasets/pixparse/pdfa-eng-wds](https://huggingface.co/datasets/pixparse/pdfa-eng-wds).Accessed August 2025.
* Peng etÂ al. (2024)Xiangyu Peng, PrafullaÂ Kumar Choubey, Caiming Xiong, and Chien-Sheng Wu.Unanswerability evaluation for retrieval augmented generation.*arXiv preprint arXiv:2412.12300*, 2024.
* Poznanski etÂ al. (2025)Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Christopher Wilhelm, Kyle Lo, and Luca Soldaini.olmocr: Unlocking trillions of tokens in pdfs with vision language models.*arXiv preprint arXiv:2502.18443*, 2025.
* Shao etÂ al. (2024)Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YKÂ Li, Yang Wu, etÂ al.Deepseekmath: Pushing the limits of mathematical reasoning in open language models.*arXiv preprint arXiv:2402.03300*, 2024.
* Sharifymoghaddam etÂ al. (2025)Sahel Sharifymoghaddam, Shivani Upadhyay, Wenhu Chen, and Jimmy Lin.Unirag: Universal retrieval augmentation for large vision language models.In*Findings of the Association for Computational Linguistics: NAACL 2025*, pp. 2026â€“2039, 2025.
* Su etÂ al. (2025)Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, etÂ al.Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers.*arXiv preprint arXiv:2506.23918*, 2025.
* Wang etÂ al. (2025a)Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, and Feng Zhao.Vidorag: Visual document retrieval-augmented generation via dynamic iterative reasoning agents.*arXiv preprint arXiv:2502.18017*, 2025a.
* Wang etÂ al. (2025b)Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, and Feng Zhao.Vidorag: Visual document retrieval-augmented generation via dynamic iterative reasoning agents.*arXiv preprint arXiv:2502.18017*, 2025b.
* Wang etÂ al. (2025c)Qiuchen Wang, Ruixue Ding, YuÂ Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, and Feng Zhao.Vrag-rl: Empower vision-perception-based rag for visually rich information understanding via iterative reasoning with reinforcement learning.*arXiv preprint arXiv:2505.22019*, 2025c.
* Wang etÂ al. (2025d)Qiuchen Wang, Ruixue Ding, YuÂ Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, and Feng Zhao.Vrag-rl: Empower vision-perception-based rag for visually rich information understanding via iterative reasoning with reinforcement learning, 2025d.URL[https://arxiv.org/abs/2505.22019](https://arxiv.org/abs/2505.22019).
* Wasserman etÂ al. (2025)Navve Wasserman, Roi Pony, Oshri Naparstek, AdiÂ Raz Goldfarb, Eli Schwartz, Udi Barzelay, and Leonid Karlinsky.Real-mm-rag: A real-world multi-modal retrieval benchmark.*arXiv preprint arXiv:2502.12342*, 2025.
* Xue etÂ al. (2024)LeÂ Xue, Manli Shu, Anas Awadalla, Jun Wang, AnÂ Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, MichaelÂ S Ryoo, etÂ al.xgen-mm (blip-3): A family of open large multimodal models.*arXiv preprint arXiv:2408.08872*, 2024.
* Yeo etÂ al. (2025)Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, and SungÂ Ju Hwang.Universalrag: Retrieval-augmented generation over multiple corpora with diverse modalities and granularities.*arXiv preprint arXiv:2504.20734*, 2025.
* Yu etÂ al. (2024a)Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, XuÂ Han, Zhiyuan Liu, etÂ al.Visrag: Vision-based retrieval-augmented generation on multi-modality documents.*arXiv preprint arXiv:2410.10594*, 2024a.
* Yu etÂ al. (2024b)Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, XuÂ Han, Zhiyuan Liu, etÂ al.Visrag: Vision-based retrieval-augmented generation on multi-modality documents.*arXiv preprint arXiv:2410.10594*, 2024b.
* Zhang etÂ al. (2024a)Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang.Gme: Improving universal multimodal retrieval by multimodal llms.*arXiv preprint arXiv:2412.16855*, 2024a.
* Zhang etÂ al. (2024b)Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang.Gme: Improving universal multimodal retrieval by multimodal llms.*arXiv preprint arXiv:2412.16855*, 2024b.
* Zhu etÂ al. (2022)Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua.Towards complex document understanding by discrete reasoning.In*Proceedings of the 30th ACM International Conference on Multimedia*, pp. 4857â€“4866, 2022.
## Appendix AThe Use of Large Language Models (LLMs)
We used LLMs for three purposes: (i) polishing grammar and improving readability, and (ii) assisting in the evaluation of RAG outputs (iii) synthesizing the QA pairs. All research ideas and analyses were conducted by the authors, who take full responsibility for the content.
## Appendix BDataset Creation Details
### B.1Document fields
We classify each PDF document into the following fields:
* â€¢domain: one or more from {Healthcare, Finance, Technology and Software, Commerce and Manufacturing, Marketing, Arts and Entertainment, Government, Legal, Education, Scientific Research and Development, Customer Relationship Management (CRM). others}
* â€¢subdomain: optional finer-grained categories
* â€¢date: year or estimated year (e.g., 2005)
* â€¢language: language of the document (e.g.,en)
* â€¢modality: possible values include {text, table, figure, formula, image, drawing}
* â€¢quality: parsing confidence, values {easy-parse, hard-parse}
* â€¢format: one or more from {form, report, notice, paper, slide, poster, book, newspaper, article, textbook, note, webpage, document, record}
* â€¢text\_proportion: percentage of textual content (e.g., 25%)
As described in Section[3.1](https://arxiv.org/html/2510.03663v2#S3.SS1), we do not include every domain or subdomain in our benchmark. Instead, we filter the source data and retain eight highly representative domains.
### B.2Domain Definitions
We classify documents into domains and subdomains, each with a brief description for clarity.
These labels are used for tagging.
As detailed in Section[3.1](https://arxiv.org/html/2510.03663v2#S3.SS1), we filter the source data and retain eight highly representative domains rather than including all possible ones.
|Domain|Subdomain|Description|
Healthcare|Clinical &amp; Patient Care|Direct provider-patient interaction: diagnosis, treatment, and care management.|
Healthcare|Pharmaceuticals &amp; Biotechnology|Development and regulation of drugs, vaccines, and biotechnological products (no patient records).|
Healthcare|Medical Devices &amp; Diagnostics|Design, production, and regulation of medical equipment and diagnostic tools (no patient records).|
Healthcare|Clinical Research &amp; Trials|Controlled studies testing treatments, drugs, or therapies.|
Healthcare|Public Health &amp; Policy|Population-level promotion, disease prevention, accessibility (not individual records).|
Healthcare|Other Healthcare Topics|Healthcare economics, law, and alternative medicine.|
Finance|Investments &amp; Wealth Management|Stock portfolios, retirement planning, mutual funds, hedge funds.|
Finance|Insurance &amp; Risk Management|Health, life, auto, property insurance; actuarial analysis.|
Finance|Corporate Finance &amp; Treasury|Budgeting, fundraising, M&amp;A, investor relations, corporate structure.|
Finance|Personal Finance &amp; FinTech|Budgeting apps, personal loans, P2P lending, digital wallets.|
Finance|Real Estate Finance|Mortgages, REITs, valuations, market dynamics.|
Finance|Macroeconomics &amp; Financial Markets|Markets, currency, fiscal/monetary policy, global economics.|
Finance|Other Finance Topics|Microfinance, Islamic banking, niche financial products.|
Technology &amp; Software|Software Engineering &amp; DevOps|Coding, testing, deployment, CI/CD, APIs.|
Technology &amp; Software|Cybersecurity &amp; Information Security|Risk management, encryption, compliance, network defense.|
Technology &amp; Software|Data Science, AI &amp; Analytics|ML, pipelines, visualization, BI tools.|
Technology &amp; Software|HCI &amp; UX|Design, prototyping, accessibility, usability studies.|
Technology &amp; Software|Emerging Technologies|AR/VR, quantum computing, IoT, blockchain.|
Technology &amp; Software|Other Tech Topics|Legacy systems, databases, systems architecture.|
Commerce &amp; Manufacturing|Supply Chain &amp; Logistics|Procurement, warehousing, transportation, inventory.|
Commerce &amp; Manufacturing|Industrial Engineering &amp; Production|Process optimization, quality control, Lean/Six Sigma.|
Commerce &amp; Manufacturing|Retail &amp; E-Commerce|Marketplaces, POS systems, consumer engagement.|
Commerce &amp; Manufacturing|Trade Policy &amp; Global Commerce|Tariffs, export-import regulation, global trade.|
Commerce &amp; Manufacturing|Other Commerce Topics|Business operations, sales, distribution.|
Marketing|Digital Marketing &amp; Advertising|Social media, SEO/SEM, online campaigns.|
Marketing|Consumer Behavior &amp; Market Research|Surveys, focus groups, data-driven insights.|
Marketing|Branding &amp; Corporate Identity|Logo, image, brand value, messaging.|
Marketing|Marketing Analytics &amp; Metrics|ROI, attribution models, dashboards.|
Marketing|Other Marketing Topics|Public relations, sponsorships, offline campaigns.|
Arts &amp; Entertainment|Performing Arts|Music, theater, dance, performance reviews.|
Arts &amp; Entertainment|Visual Arts &amp; Design|Painting, sculpture, illustration, graphic design.|
Arts &amp; Entertainment|Film, TV &amp; Media Studies|Criticism, production, audience reception.|
Arts &amp; Entertainment|Literature &amp; Writing|Fiction, non-fiction, literary analysis.|
Arts &amp; Entertainment|Games &amp; Interactive Media|Video games, role-playing, esports.|
Arts &amp; Entertainment|Other Arts Topics|Fashion, photography, cultural heritage.|
Government|Public Administration &amp; Policy|Bureaucracy, policymaking, implementation.|
Government|Law Enforcement &amp; Security|Policing, intelligence, defense, military studies.|
Government|International Relations &amp; Diplomacy|Foreign policy, treaties, global governance.|
Government|Elections &amp; Governance|Voting, political systems, representation.|
Government|Other Government Topics|Civil rights, immigration, taxation.|
Legal|Corporate &amp; Business Law|Contracts, mergers, compliance.|
Legal|Criminal &amp; Civil Law|Courts, trials, disputes, legal rights.|
Legal|Intellectual Property Law|Copyrights, patents, trademarks.|
Legal|International &amp; Comparative Law|Cross-border legal systems, treaties.|
Legal|Legal Theory &amp; Jurisprudence|Philosophy of law, frameworks.|
Legal|Other Legal Topics|Niche legal issues, regulatory law.|
Education|K-12 Education|Curriculum, pedagogy, assessments.|
Education|Higher Education &amp; Academia|Universities, research, accreditation.|
Education|Online &amp; Distance Learning|MOOCs, e-learning, virtual platforms.|
Education|Education Policy &amp; Reform|Accessibility, standards, funding.|
Education|Other Education Topics|Lifelong learning, teacher training.|
Scientific R&amp;D|Natural Sciences|Physics, chemistry, biology, earth science.|
Scientific R&amp;D|Engineering &amp; Applied Sciences|Electrical, mechanical, civil, aerospace.|
Scientific R&amp;D|Medical &amp; Life Sciences|Biomedical, genetics, ecology.|
Scientific R&amp;D|Computer Science &amp; Computational Fields|Algorithms, theory, AI, networks.|
Scientific R&amp;D|Other Science Topics|Interdisciplinary, niche fields.|
CRM|Customer Support &amp; Helpdesk|Call centers, chatbots, support tickets.|
CRM|Sales &amp; Lead Management|CRM tools, customer tracking, pipelines.|
CRM|Customer Analytics &amp; Insights|Segmentation, lifetime value, churn analysis.|
CRM|Customer Experience (CX) &amp; Engagement|Feedback, personalization, loyalty programs.|
CRM|Other CRM Topics|Partnerships, integrations, omni-channel strategies.|
### B.3Parsing Examples
We useunstructuredto parse each PDF into three components: text chunks, images of figures, and images of tables.
Since many figures (e.g., signatures or logos) are not informative, we only retain figures that include captions.
Figure[3](https://arxiv.org/html/2510.03663v2#A2.F3)shows an example of the parsing output, where figures are represented by placeholders such as&lt;&lt;fig-XXX&gt;&gt;and the parsed text from the figures.
![Refer to caption](figures/exp-parse.png)Figure 3:Example of PDF parsing with figure placeholders (&lt;&lt;fig-XXX&gt;&gt;).
### B.4Dataset Templates
This is the templates for the domain:finance. We create different templates for different domains, which can be found in our code files in the supplementary materials.
#### Factual Retrieval
|Template|Example|
What indicators, policies, or tools are described in the discussion of [Economic Topic/Financial Strategy]?|What inflation indicators are cited in the ECBâ€™s policy blog from June?|
Which markets, sectors, or instruments are emphasized in relation to [Trend/Event/Goal]?|Which sectors are favored in the 2025 sustainable investing outlook?|
What key positions or exposures are taken by [Investor/Desk/Division] in response to [Condition/Event]?|What position changes did the multi-asset team make in response to rising real yields?|
What assumptions, constraints, or parameters are specified in [Scenario/Strategy/Model]?|What assumptions are used in the stress testing scenario for oil price shocks?|
When was [Policy/Event/Adjustment] implemented, and what immediate actions followed?|When did the Bank of Japan change its yield curve control stance?|
Who oversees or initiates [Financial Decision/Policy/Investment Move] in the described context?|Who approves short-term borrowing requests in the global treasury function?|
How is [Strategy/Instrument/Term] defined or operationalized in this context?|How is â€œduration-neutral tiltâ€ defined in the Q3 fixed income note?|
How do you carry out or execute [Action/Transaction/Plan] in [Financial Context]?|How do you implement a covered call overlay in an income-focused portfolio?|
What are the procedural steps or controls listed for [Financial Task/Compliance/Change]?|What steps are required to evaluate bond ladder rollovers in rising rates?|
#### Comparison
|Template|Example|
How do [Strategies/Regions/Instruments] compare in terms of [Risk/Performance/Conditions]?|How do TIPS and gold compare for inflation protection in the current macro setup?|
Which asset class, sector, or product is better suited for [Objective/Environment]?|Which is better for income stability in retirement: dividend ETFs or bond ladders?|
What are the structural or tactical differences between [Financial Approaches]?|What are the key differences between liability-driven investment and balanced allocation strategies?|
How did [Metric/Position/Exposure] change between [Period A] and [Period B]?|How did corporate cash allocation to floating-rate debt shift over 2023?|
How do regulatory or monetary responses differ between [Jurisdictions]?|How does Fed liquidity provision compare to ECB emergency facilities post-crisis?|
#### Summarization
|Template|Example|
What are the key findings or takeaways from [Brief/Update/Policy/Strategy]?|What are the key points in the tactical asset allocation update from July?|
Summarize the main market movements, themes, or risks discussed in [Note/Newsletter/Memo].|Summarize the interest rate risk themes highlighted in the October bond outlook.|
What portfolio, liquidity, or policy adjustments are recommended or implemented?|What rebalancing steps were taken in the client model portfolios in Q1?|
List the major economic risks or opportunities discussed in [Period/Event/Note].|What macro risks are cited ahead of the U.S. election cycle?|
What are the key operational or structural features of [Product/Plan/Tool]?|What are the structural features of the new drawdown facility described in the treasury toolkit?|
#### Causal / Reasoning / Why Questions
|Template|Example|
Why did [Entity/Desk/Advisor] make [Move/Shift/Decision] in response to [Condition/Event]?|Why did the balanced portfolio reduce international equity in Q2?|
How did [Macro Event/Regulatory Shift] influence [Positioning/Allocation/Operations]?|How did the Basel III revisions alter corporate liquidity buffers?|
What drove the shift from [Approach A] to [Approach B] in [Context]?|What drove the shift from risk-parity to volatility-targeting in multi-asset allocation?|
Why was [Instrument/Policy/Vehicle] introduced or phased out?|Why was the internal netting structure retired in the 2024 treasury overhaul?|
What sequence of factors or events led to [Market Reaction/Portfolio Impact/Policy Result]?|What sequence of events led to capital outflows from EM debt in late 2023?|
### B.5QA Synthesizing Prompts
#### B.5.1Text-only
Prompt P.1: Text-only RAG Question GenerationPrompt: You are an assistant specialized in creating Multimodal RAG tasks. The task is the following: Given some natural language contexts and images inside these contexts, you will generate questions that can be asked by a user to retrieve information from a large documentary corpus.Requirements:â€¢The 2-hop synthesized question must be a single, self-contained question and must not use â€andâ€ to connect multiple questions.â€¢The answer of the synthesized question will only be found in the contexts.â€¢The answer of the synthesized question cannot be found in the images.â€¢The synthesized question must require all the chunks in the contexts to be answered.â€¢The synthesized question must be specific enough to locate the contexts in a large documentary corpus.â€¢You must also provide an explanation why the answer can only be found in the provided contexts.Question Template:â€¢Use the following template to generate the QA:
```
{{TEMPLATES}}
```
Output Format:
```
{
"questions": [
{
"question": "&lt;&lt;synthesized-question&gt;&gt;",
"answer": "&lt;&lt;answer-of-the-question&gt;&gt;",
"question\_type":
&lt;&lt;choose from "factual\_retrieval", "comparison",
"summarization", "causal\_reasoning"&gt;&gt;,
"explanation-chunks": "&lt;&lt;explanation-chunks&gt;&gt;",
"sentences-chunks-used": {"Chunk1": "sentences-chunk1",
"Chunk2": "sentences-chunk2", ...}
}
]
}
```
Input Data:â€¢Contexts: â€œ{{contexts}}â€â€¢Images: The image is as follows:Notes:â€¢If the image can only be used for visualization or illustration, return an empty list for â€˜sentences-chunks-usedâ€™.â€¢If you cannot use all the chunks in the answer, return an empty list for â€˜sentences-chunks-usedâ€™.
#### B.5.2Image-only
Prompt P.2: Image-only RAG Question GenerationPrompt: You are an assistant specialized in creating Multimodal RAG tasks. The task is the following: Given some natural language contexts and images inside these contexts, you will generate questions that can be asked by a user to retrieve information from a large documentary corpus.Requirements:1.The synthesized question must be a single, self-contained question and must not use â€œandâ€ to connect multiple questions.2.The answer of the synthesized question will only be found in the image and cannot be found in any sentences in the chunks of the provided contexts.3.The synthesized question must require chunks/contexts to locate the image and cannot mention the image directly.4.The synthesized question must be specific enough to locate the contexts in a large documentary corpus.5.Do not ask â€œwhat XYZ in the graph/image/figureâ€; the question must be general enough to be asked in a large corpus.6.If you cannot synthesize a question which can only be answered in the image based on the above requirements, do not synthesize anything.7.Provide an explanation why the answer can only be found in the image and cannot be found in the provided chunks/contexts.8.Avoid phrasing like â€œwhat is shown in the image,â€ e.g., â€what color/logo/name in the image.â€9.Emphasize reasoning, aggregation, temporal comparison, or retrieval from source data. Imagine the question being asked without the image still making partial sense.Question Template:â€¢Use the following template to generate the QA:
```
{{TEMPLATES}}
```
Output Format:
```
{
"questions": [
{
"question": "&lt;&lt;synthesized-question&gt;&gt;",
"answer": "&lt;&lt;answer-of-the-question&gt;&gt;",
"question\_type":
&lt;&lt;choose from "factual\_retrieval", "comparison",
"summarization", "causal\_reasoning"&gt;&gt;,
"image": "&lt;&lt;&lt;&lt;fig-aaaaa&gt;&gt;&gt;&gt;",
"explanation-image": "&lt;&lt;explanation-image&gt;&gt;",
"explanation-chunks": "&lt;&lt;explanation-chunks&gt;&gt;",
"sentences-chunks-used":
{"Chunk1": "sentences-chunk1",
"Chunk2": "sentences-chunk2", ...}
}
]
}
```
Input Data:â€¢Contexts: â€œ{{contexts}}â€â€¢Images: The image is as follows:Notes:â€¢If the image can only be used for visualization or illustration, return an empty list for â€˜sentences-chunks-usedâ€™.â€¢If you cannot use all the chunks in the answer, return an empty list for â€˜sentences-chunks-usedâ€™.
#### B.5.3Text-plus-Image
Prompt P.3: Text-plus-image RAG Question GenerationPrompt: You are an assistant specialized in creating Multimodal RAG tasks. The task is the following: Given some natural language contexts and images inside these contexts, you will generate questions that can be asked by a user to retrieve information from a large documentary corpus.Requirements:1.The 2-hop synthesized question must require both the provided contexts and images to answer.2.The concise answer of the synthesized question will directly require information in the image to answer.3.The concise answer of the synthesized question will also require information in the natural language contexts to answer.4.The synthesized question must require contexts to locate the image and cannot mention the image directly.5.The synthesized question must be specific enough to locate the contexts in a large documentary corpus.6.Provide an explanation indicating which part of the image is used to answer and which sentence in the contexts is used to answer the question.7.Do not ask â€œwhat XYZ in the graphâ€; the question must be general enough to be asked in a large corpus.8.If you cannot synthesize a question based on these requirements or directly use the information in the images, do not synthesize anything.9.If the image can only be used for visualization or illustration, do not synthesize anything. If you cannot use all the chunks in the answer, do not synthesize the question.10.The synthesized question must be a single, self-contained question and must not use â€œandâ€ to connect multiple questions.Question Template:â€¢Use the following template to generate the QA:
```
{{TEMPLATES}}
```
Output Format:
```
{
"questions": [
{
"question": "&lt;&lt;synthesized-question&gt;&gt;",
"answer": "&lt;&lt;answer-of-the-question&gt;&gt;",
"question\_type": &lt;&lt;choose from "factual\_retrieval",
"comparison", "summarization", "causal\_reasoning"&gt;&gt;,
"image": "&lt;&lt;&lt;&lt;fig-aaaaa&gt;&gt;&gt;&gt;",
"explanation-image": "&lt;&lt;explanation-image&gt;&gt;",
"explanation-chunks": "&lt;&lt;explanation-chunks&gt;&gt;",
"sentences-chunks-used":
{"Chunk1": "sentences-chunk1",
"Chunk2": "sentences-chunk2", ...}
},...
]
}
```
Input Data:â€¢Contexts: â€œ{{contexts}}â€â€¢Images: The image is as follows:Notes:â€¢If the image can only be used for visualization or illustration, return an empty list for â€˜sentences-chunks-usedâ€™.â€¢If you cannot use all the chunks in the answer, return an empty list for â€˜sentences-chunks-usedâ€™.
#### B.5.4Table-required
Prompt P.4: Table-required RAG Question GenerationPrompt: You are an assistant specialized in creating Multimodal RAG tasks. The task is the following: Given some natural language contexts containing tables, you will generate questions that can be asked by a user to retrieve information from a large documentary corpus.Requirements:1.The synthesized question must be a single, self-contained question and must not use â€œandâ€ to connect multiple questions.2.The answer of the synthesized question will only be found in the table (withinâŸ¨\\langletableâŸ©\\rangleandâŸ¨\\langle/tableâŸ©\\rangle) and cannot be found in any sentences outside theâŸ¨\\langletableâŸ©\\rangleandâŸ¨\\langle/tableâŸ©\\ranglein the chunks of the provided contexts.3.The synthesized question must require chunks/contexts to locate the table and cannot mention the â€˜tableâ€™ directly.4.The synthesized question must be specific enough to locate the contexts in a large documentary corpus.5.Do not ask â€œwhat XYZ in the tableâ€; the question must be general enough to be asked in a large corpus.6.If you cannot synthesize a question which can only be answered in the table based on the above requirements, do not synthesize anything.7.Provide an explanation why the answer can only be found in the table and cannot be found in other parts of the chunks/contexts.8.Emphasize reasoning, aggregation, temporal comparison, or retrieval from source data. Imagine the question being asked without the table still making partial sense.Question Template:â€¢Use the following template to generate the QA:
```
{{TEMPLATES}}
```
Output Format:
```
{
"questions": [
{
"question": "&lt;&lt;synthesized-question&gt;&gt;",
"answer": "&lt;&lt;answer-of-the-question&gt;&gt;",
"question\_type": &lt;&lt;choose from "factual\_retrieval", "comparison",
"summarization", "causal\_reasoning"&gt;&gt;,
"image": "&lt;&lt;&lt;&lt;tab-aaaaa&gt;&gt;&gt;&gt;",
"explanation-table": "&lt;&lt;explanation-table&gt;&gt;",
"explanation-chunks": "&lt;&lt;explanation-chunks&gt;&gt;",
"sentences-chunks-used":
{"Chunk1": "sentences-chunk1",
"Chunk2": "sentences-chunk2", ...}
},...
]
}
```
Input Data:â€¢Contexts: â€œ{{contexts}}â€â€¢Table: The table is included as â€˜âŸ¨\\langletableâŸ©\\rangleâ€¦âŸ¨\\langle/tableâŸ©\\rangleâ€™ in the context.Notes:â€¢If the table can be used only for visualization or illustration, return an empty list for â€˜sentences-chunks-usedâ€™.â€¢If you cannot use all the chunks in the answer, return an empty list for â€˜sentences-chunks-usedâ€™.
### B.6Rewriting prompts
Prompt P.5: Question RewritingPrompt: You are tasked with rewriting the following question in two different ways, using only the provided Contexts and without hallucinating any information.Date{{current\_date}}Tasks:1.Specific Rewrite: Add or substitute minimal keywords to tie the question to the Contexts, making retrieval unique while preserving meaning.2.Obscured Rewrite: Paraphrase the specific version to reduce keyword overlap while keeping all needed details intact.Requirements:â€¢No hallucinated facts.â€¢Do not remove critical content.â€¢Avoid source-referencing phrases (â€œin figureâ€, â€œin tableâ€, etc.).â€¢Rewrites must be standalone, fluent, faithful to Contexts.â€¢Only add essential keywords (avoid over-specification).Check if the original answer remains fully correct for both rewrites.
If not, set"answer\_wrong"="True", else"False".Output Format:
```
{
"specific\_question":
"More specific version with essential keywords.",
"obscured\_question":
"Paraphrased version with reduced keyword overlap.",
"answer\_wrong": "True/False"
}
```
Example 1:Original: â€œWhat is the revenue growth shown in Figure 3 in 2024â€™s report?â€
```
{
"specific\_question":
"What is the revenue growth for Company XYZ in 2024?",
"obscured\_question":
"How did XYZâ€™s financial outcomes change in 2024?",
"answer\_wrong": "False"
}
```
Example 2:Original: â€œWhat is the median differential rate between hurdle rates and costs of capital for cyclical and non-cyclical firms?â€
```
{
"specific\_question":
"What is the median differential between hurdle
rates and costs of capital for cyclical vs. non-cyclical firms in
the S&amp;&amp;P 500 according to the Corporate Finance Advisory?",
"obscured\_question":
"Within the Corporate Finance Advisory, what is the
median gap between
required returns and capital costs for S&amp;&amp;P 500 firms
sensitive to the economy vs. stable sectors?",
"answer\_wrong": "False"
}
```
### B.7Answer Rewriting Prompts
Prompt P.6: Answer RewritingPrompt: You are tasked with rewriting the following answer so that it
contains all the facts for answering the question, given the contexts
and the image.Instruction:â€¢Do not hallucinate any additional information. Use only the
provided contexts and images.â€¢The rewritten answer must include theold correct answer,
if it is correct.â€¢If the answer is already complete, you may leave it unchanged.â€¢Make the answer as concise as possible.â€¢If theold correct answeris incomplete, expand it so that
the"complete\_answer"fully addresses the question.Output Format:
```
{
"complete\_answer": "Final rewritten answer that is concise,
faithful to contexts and images, and fully answers the question."
}
```
Input Data:â€¢Question: â€œ{{rewritten\_question\_obscured}}â€â€¢Contexts: â€œ{{contexts}}â€â€¢Old Correct Answer: â€œ{{answer}}â€â€¢Images: The image is as follows:
## Appendix CHuman Annotation
Annotators were provided with the following instructions to evaluate the quality of synthesized questions and responses against source documents.
### C.1Task Overview
The primary task is to read a synthesized question and response, then evaluate their quality based on the provided PDF pages and images. The core evaluation criterion is factuality.
### C.2Factuality Evaluation
Annotators must determine whether the question and response are factually supported by the source material.
#### C.2.1Procedure
Annotators were instructed to follow these steps:
1. 1.
Open the folder corresponding to the given ID.
2. 2.
Read the text from the PDF pages located in thechunk\_Xsubfolder. Annotators were told to read all text, including tables and image captions, but to ignore the content of the images themselves.
3. 3.
Review the images in theimg\_Xsubfolder to understand which image is being referenced, then locate that image within the source PDF to read its context and caption.
4. 4.
Read the provided Question and Response pair.
5. 5.
Assign a factuality label to both the question and the response.
#### C.2.2Label Definitions
Factuality-Question: Factual
All facts and claims in the question are directly supported by the source material. There are no hallucinations or unsupported statements.
Factuality-Question: Not Factual
One or more facts or claims in the question are not supported by the source (i.e., contain hallucinated or fabricated content).
Factuality-Response: Factual
All facts and claims in the response are directly supported by the source material. There are no hallucinations or unsupported statements.
Factuality-Response: Not Factual
One or more facts or claims in the response are not supported by the source (i.e., contain hallucinated or fabricated content).
Note:The original instructions included a rule stating, â€If a question or response is not factual, it should be labeled as â€˜Incompleteâ€™.â€ However, the provided examples use the â€Not Factualâ€ label, which was the standard followed during annotation.
#### C.2.3Examples
The following examples were provided to the annotators for guidance.
[â¬‡](data:text/plain;base64,ewoJImlkIjogMCwKCSJxdWVzdGlvbiI6ICJXaGF0IGlzIHRoZSBsb2dvIG9mIGEgbWFqb3IgdGVsZWNvbW11bmljYXRpb25zIGNvbXBhbnkgbWVudGlvbmVkIGluIHRoZSBjb250ZXh0IHJlbGF0ZWQgdG8gcGVyc29uYWxpemF0aW9uIHN0cmF0ZWdpZXM/IiwKCSJyZXNwb25zZSI6ICJBVCZUIiwKfQoKIyBTdGVwczoKIyAxLiBJIG9wZW4gZm9sZGVyICIwIiwgcmVhZCBhbGwgdGhlIGNodW5rcyBhbmQgaW1hZ2VzLgojIDIuIFRoZSBxdWVzdGlvbiBzZWVtcyBmYWN0dWFsIGZyb20gb25lIG9mIHRoZSBjaHVuay4KIyAzLiBUaGUgcmVzcG9uc2Ugc2VlbXMgdG8gTk9UIGJlIHRoZSBjb3JyZWN0IGFuc3dlci4KCiMgVGhlbiwgSSBsYWJlbCBGYWN0dWFsLVF1ZXN0aW9uIGFzIGBGYWN0dWFsYAojIFRoZW4sIEkgbGFiZWwgRmFjdHVhbC1SZXNwb25zZSBhcyBgTm90IEZhY3R1YWxg)
{
"id":0,
"question":"Whatisthelogoofamajortelecommunicationscompanymentionedinthecontextrelatedtopersonalizationstrategies?",
"response":"AT&amp;T",
}
#Steps:
#1.Iopenfolder"0",readallthechunksandimages.
#2.Thequestionseemsfactualfromoneofthechunk.
#3.TheresponseseemstoNOTbethecorrectanswer.
#Then,IlabelFactual-Questionasâ€˜Factualâ€˜
#Then,IlabelFactual-Responseasâ€˜NotFactualâ€˜
ListingÂ 1:Example of a factual question with a non-factual response.
[â¬‡](data:text/plain;base64,ewoJImlkIjogNCwKCSJxdWVzdGlvbiI6ICJXaGF0IGJ1c2luZXNzZXMgYXJlIGxvY2F0ZWQgbmVhciB0aGUgcHJvcG9zZWQgZGV2ZWxvcG1lbnQgYXJlYSBpbiB0aGUgUHJvamVjdCBDYXRhbHlzdD8iLAoJInJlc3BvbnNlIjogIkFUJlQiLAp9CgojIFN0ZXBzOgojIDEuIEkgb3BlbiBmb2xkZXIgIjQiLCByZWFkIGFsbCB0aGUgY2h1bmtzIGFuZCBpbWFnZXMuCiMgMi4gVGhlIHF1ZXN0aW9uIHNlZW1zIHRvIGJlIE5PVCBmYWN0dWFsIGJlY2F1c2UgSSBkaWQgbm90IHNlZSBQcm9qZWN0IENhdGFseXN0IGluIHRoZSBwZGYgb3IgaW1hZ2VzLgojIDMuIFRoZSByZXNwb25zZSBzZWVtcyB0byBiZSBpbmNvcnJlY3QgYmVjYXVzZSB0aGUgcXVlc3Rpb24gaXMgbm90IGZhY3R1YWwuCgojIFRoZW4sIEkgbGFiZWwgRmFjdHVhbC1RdWVzdGlvbiBhcyBgTm90IEZhY3R1YWxgCiMgVGhlbiwgSSBsYWJlbCBGYWN0dWFsLVJlc3BvbnNlIGFzIGBOb3QgRmFjdHVhbGA=)
{
"id":4,
"question":"WhatbusinessesarelocatedneartheproposeddevelopmentareaintheProjectCatalyst?",
"response":"AT&amp;T",
}
#Steps:
#1.Iopenfolder"4",readallthechunksandimages.
#2.ThequestionseemstobeNOTfactualbecauseIdidnotseeProjectCatalystinthepdforimages.
#3.Theresponseseemstobeincorrectbecausethequestionisnotfactual.
#Then,IlabelFactual-Questionasâ€˜NotFactualâ€˜
#Then,IlabelFactual-Responseasâ€˜NotFactualâ€˜
ListingÂ 2:Example of a non-factual question and response.
### C.3Completeness Evaluation
This task assesses whether the response provides all the necessary information to fully answer the question, based on the provided source material.
#### C.3.1Procedure
The procedure for evaluating completeness is identical to the factuality task: annotators must review all provided PDF chunks and images before making a judgment.
#### C.3.2Label Definitions
Complete:
The response includes all the required facts and details present in the source material needed to comprehensively answer the question.
Incomplete:
The response omits one or more facts or claims that are present in the source and are necessary to fully answer the question.
#### Example 1: Incomplete Response
[â¬‡](data:text/plain;base64,ewoJImlkIjogMiwKCSJxdWVzdGlvbiI6ICJXaGF0IGJ1c2luZXNzZXMgYXJlIGxvY2F0ZWQgbmVhciB0aGUgcHJvcG9zZWQgZGV2ZWxvcG1lbnQgYXJlYSBpbiB0aGUgUHJvamVjdCBDYXRhbHlzdD8iLAoJInJlc3BvbnNlIjogIkF1dG9ab25lIEF1dG8gUGFydHMsIFBpenphIEh1dCwgU29uaWMgRHJpdmUgSW4sIEpvZSdzIFBpenphIEl0YWxpYW4iLAp9CgojIFN0ZXBzOgojIDEuIEkgb3BlbiBmb2xkZXIgIjIiLCByZWFkIGFsbCB0aGUgY2h1bmtzIGFuZCBpbWFnZXMuCiMgMi4gVGhlIHJlc3BvbnNlIHNlZW1zIHRvIG1pc3M6ICJNciBKaW0ncyBQaXp6YSwgSnVzdGluIFNwaXJpdHMsIEFsbHN1cCdzIENvbnZlbmllbmNlIFN0b3JlLiIKCiMgVGhlbiwgSSBsYWJlbCBDb21wbGV0ZW5lc3MgYXMgYEluY29tcGxldGVg)
{
"id":2,
"question":"WhatbusinessesarelocatedneartheproposeddevelopmentareaintheProjectCatalyst?",
"response":"AutoZoneAutoParts,PizzaHut,SonicDriveIn,Joeâ€™sPizzaItalian",
}
#Steps:
#1.Iopenfolder"2",readallthechunksandimages.
#2.Theresponseseemstomiss:"MrJimâ€™sPizza,JustinSpirits,Allsupâ€™sConvenienceStore."
#Then,IlabelCompletenessasâ€˜Incompleteâ€˜
ListingÂ 3:Example of a response that is missing information available in the source document.
#### Example 2: Complete Response
[â¬‡](data:text/plain;base64,ewoJImlkIjogMCwKCSJxdWVzdGlvbiI6ICJXaGF0IGlzIHRoZSBsb2dvIG9mIGEgbWFqb3IgdGVsZWNvbW11bmljYXRpb25zIGNvbXBhbnkgbWVudGlvbmVkIGluIHRoZSBjb250ZXh0IHJlbGF0ZWQgdG8gcGVyc29uYWxpemF0aW9uIHN0cmF0ZWdpZXM/IiwKCSJyZXNwb25zZSI6ICJBVCZUIiwKfQoKIyBTdGVwczoKIyAxLiBJIG9wZW4gZm9sZGVyICIwIiwgcmVhZCBhbGwgdGhlIGNodW5rcyBhbmQgaW1hZ2VzLgojIDIuIFRoZSByZXNwb25zZSBzZWVtcyB0byBiZSBjb21wbGV0ZS4gQVQmVCBpcyB0aGUgb25seSBhbnN3ZXIuCgojIFRoZW4sIEkgbGFiZWwgQ29tcGxldGVuZXNzIGFzIGBDb21wbGV0ZWA=)
{
"id":0,
"question":"Whatisthelogoofamajortelecommunicationscompanymentionedinthecontextrelatedtopersonalizationstrategies?",
"response":"AT&amp;T",
}
#Steps:
#1.Iopenfolder"0",readallthechunksandimages.
#2.Theresponseseemstobecomplete.AT&amp;Tistheonlyanswer.
#Then,IlabelCompletenessasâ€˜Completeâ€˜
ListingÂ 4:Example of a response that contains all necessary information.
### C.4Grounding Verification
For each question, annotators were required to verify which specific source materials (PDF text chunks or images) were necessary to formulate the answer.
#### C.4.1Procedure and Label Definitions
Grounding Verification-chunk-X:
After reading the question, the annotator must determine if the text content ofchunk\_X.pdfcontains any information used in, or required for, the answer.
* â€¢Required:The chunkâ€™s text contains information needed to answer the question.
* â€¢Not Required:The chunkâ€™s text does not contain any relevant information.
Grounding Verification-img-X:
The annotator must determine ifimg\_X(including its caption and context within the PDF) contains any information used in, or required for, the answer.
* â€¢Required:The image or its caption contains information needed to answer the question.
* â€¢Not Required:The image and its caption do not contain any relevant information.
#### Example: Grounding Verification
[â¬‡](data:text/plain;base64,ewoJImlkIjogMCwKCSJxdWVzdGlvbiI6ICJXaGF0IGJ1c2luZXNzZXMgYXJlIGxvY2F0ZWQgbmVhciB0aGUgcHJvcG9zZWQgZGV2ZWxvcG1lbnQgYXJlYSBpbiB0aGUgUHJvamVjdCBDYXRhbHlzdD8iLAoJInJlc3BvbnNlIjogIkF1dG9ab25lIEF1dG8gUGFydHMsIFBpenphIEh1dCwgU29uaWMgRHJpdmUgSW4sIEpvZSdzIFBpenphIEl0YWxpYW4iLAp9CgojIFN0ZXBzIGZvciBjaHVuay0wOgojIDEuIEkgb3BlbiBmb2xkZXIgIjAiIGFuZCB0aGVuIHRoZSBzdWItZm9sZGVyIGNodW5rXzAuCiMgMi4gSSByZWFkIHRoZSB0ZXh0IHdpdGhpbiBwYWdlcy5wZGYuCiMgMy4gSSBmaW5kIHBhcnQgb2YgdGhlIGFuc3dlciB0byB0aGUgcXVlc3Rpb24gaW4gdGhlIHRleHQuCiMgNC4gSSBsYWJlbCBgR3JvdW5kaW5nIFZlcmlmaWNhdGlvbi1jaHVuay0wYCBhcyBgUmVxdWlyZWRgLgoKIyBTdGVwcyBmb3IgY2h1bmstMToKIyAxLiBJIGNoZWNrIGZvciBhIHN1Yi1mb2xkZXIgbmFtZWQgY2h1bmtfMSBpbiBmb2xkZXIgIjAiLgojIDIuIE5vIGNodW5rXzEgc3ViLWZvbGRlciBleGlzdHMsIHNvIEkgc2tpcCB0aGlzIGxhYmVsLgoKIyBTdGVwcyBmb3IgaW1nLTA6CiMgMS4gSSBvcGVuIGZvbGRlciAiMCIgYW5kIHRoZW4gdGhlIHN1Yi1mb2xkZXIgaW1nXzAuCiMgMi4gSSB2aWV3IGltZ18wLmpwZyBhbmQgbG9jYXRlIGl0IGluIHRoZSBvcmlnaW5hbCBQREYgdG8gY2hlY2sgaXRzIGNvbnRleHQuCiMgMy4gSSBmaW5kIHBhcnQgb2YgdGhlIGFuc3dlciB0byB0aGUgcXVlc3Rpb24gaW4gdGhlIGltYWdlLgojIDQuIEkgbGFiZWwgYEdyb3VuZGluZyBWZXJpZmljYXRpb24taW1nLTBgIGFzIGBSZXF1aXJlZGAuCgojIFN0ZXBzIGZvciBpbWctMToKIyAxLiBJIG9wZW4gZm9sZGVyICIwIiBhbmQgdGhlbiB0aGUgc3ViLWZvbGRlciBpbWdfMS4KIyAyLiBJIHZpZXcgaW1nXzEuanBnIGFuZCBpdHMgY29udGV4dC4KIyAzLiBJIGRvIE5PVCBmaW5kIGFueSBwYXJ0IG9mIHRoZSBhbnN3ZXIgaW4gdGhpcyBpbWFnZS4KIyA0LiBJIGxhYmVsIGBHcm91bmRpbmcgVmVyaWZpY2F0aW9uLWltZy0xYCBhcyBgTm90IFJlcXVpcmVkYC4=)
{
"id":0,
"question":"WhatbusinessesarelocatedneartheproposeddevelopmentareaintheProjectCatalyst?",
"response":"AutoZoneAutoParts,PizzaHut,SonicDriveIn,Joeâ€™sPizzaItalian",
}
#Stepsforchunk-0:
#1.Iopenfolder"0"andthenthesub-folderchunk\_0.
#2.Ireadthetextwithinpages.pdf.
#3.Ifindpartoftheanswertothequestioninthetext.
#4.Ilabelâ€˜GroundingVerification-chunk-0â€˜asâ€˜Requiredâ€˜.
#Stepsforchunk-1:
#1.Icheckforasub-foldernamedchunk\_1infolder"0".
#2.Nochunk\_1sub-folderexists,soIskipthislabel.
#Stepsforimg-0:
#1.Iopenfolder"0"andthenthesub-folderimg\_0.
#2.Iviewimg\_0.jpgandlocateitintheoriginalPDFtocheckitscontext.
#3.Ifindpartoftheanswertothequestionintheimage.
#4.Ilabelâ€˜GroundingVerification-img-0â€˜asâ€˜Requiredâ€˜.
#Stepsforimg-1:
#1.Iopenfolder"0"andthenthesub-folderimg\_1.
#2.Iviewimg\_1.jpganditscontext.
#3.IdoNOTfindanypartoftheanswerinthisimage.
#4.Ilabelâ€˜GroundingVerification-img-1â€˜asâ€˜NotRequiredâ€˜.
ListingÂ 5:Example demonstrating how to label individual source chunks and images as required or not required.
### C.5Self-Contained Evaluation
This task assesses whether a question is understandable and complete on its own, without needing external context or references to specific, unnamed documents.
#### C.5.1Procedure
Annotators were instructed to read only the question and determine if it could be understood and answered without ambiguity, assuming one had access to a large database of documents.
#### C.5.2Label Definitions
True:
The question is self-contained. It is clearly phrased, makes sense on its own, and provides enough specific detail (e.g., names, topics, concepts) to be answerable. It does not rely on vague document references. For example, â€What are the key benefits of solar energy mentioned in the 2022 Department of Energy report?â€ is self-contained.
False:
The question depends on external or implicit context to be meaningful. It may contain vague deictic references (e.g., â€in the image above,â€ â€according to this chart,â€ â€what does this mean?â€) without clarifying what the reference points to. For example, â€What is the logo in the image?â€ is not self-contained as it requires seeing a specific, un-referenced image.
#### Example 1: Not Self-Contained
[â¬‡](data:text/plain;base64,ewoJImlkIjogMSwKCSJxdWVzdGlvbiI6ICJXaGF0IGlzIHRoZSBsb2dvIGluIHRoZSBpbWFnZT8iLAoJInJlc3BvbnNlIjogIkFUJlQiLAp9CgojIFN0ZXBzOgojIDEuIEkgcmVhZCB0aGUgcXVlc3Rpb24uCiMgMi4gSSBmaW5kIGl0IGlzIE5PVCBjbGVhcjsgIndoYXQgaW1hZ2U/IiBpcyBhbiB1bmFuc3dlcmVkIHByZXJlcXVpc2l0ZS4KIyAzLiBJIGxhYmVsIGBTZWxmLUNvbnRhaW5lZGAgYXMgYEZhbHNlYC4=)
{
"id":1,
"question":"Whatisthelogointheimage?",
"response":"AT&amp;T",
}
#Steps:
#1.Ireadthequestion.
#2.IfinditisNOTclear;"whatimage?"isanunansweredprerequisite.
#3.Ilabelâ€˜Self-Containedâ€˜asâ€˜Falseâ€˜.
ListingÂ 6:Example of a question that is not self-contained due to a vague reference (â€the imageâ€).
#### Example 2: Self-Contained
[â¬‡](data:text/plain;base64,ewoJImlkIjogMCwKCSJxdWVzdGlvbiI6ICJXaGF0IGlzIHRoZSBsb2dvIG9mIGEgbWFqb3IgdGVsZWNvbW11bmljYXRpb25zIGNvbXBhbnkgbWVudGlvbmVkIGluIHRoZSBjb250ZXh0IHJlbGF0ZWQgdG8gcGVyc29uYWxpemF0aW9uIHN0cmF0ZWdpZXM/IiwKCSJyZXNwb25zZSI6ICJBVCZUIiwKfQoKIyBTdGVwczoKIyAxLiBJIHJlYWQgdGhlIHF1ZXN0aW9uLgojIDIuIEkgZmluZCBpdCBpcyBjbGVhci4gSSBjYW4gdXNlIHRoZSBpbmZvcm1hdGlvbiB3aXRoaW4gdGhlIHF1ZXN0aW9uIHRvIHNlYXJjaCBmb3IgYSByZWxldmFudCBkb2N1bWVudC4KIyAzLiBJIGxhYmVsIGBTZWxmLUNvbnRhaW5lZGAgYXMgYFRydWVgLg==)
{
"id":0,
"question":"Whatisthelogoofamajortelecommunicationscompanymentionedinthecontextrelatedtopersonalizationstrategies?",
"response":"AT&amp;T",
}
#Steps:
#1.Ireadthequestion.
#2.Ifinditisclear.Icanusetheinformationwithinthequestiontosearchforarelevantdocument.
#3.Ilabelâ€˜Self-Containedâ€˜asâ€˜Trueâ€˜.
ListingÂ 7:Example of a question that is self-contained because it provides sufficient context (â€personalization strategies,â€ â€telecommunications companyâ€).
### C.6Human-like Intent Evaluation
This task assesses whether a question reflects a natural and meaningful information-seeking intent, typical of a human user interacting with a document or database.
#### C.6.1Procedure
Annotators were instructed to read the question and judge its authenticity as a genuine human query. The focus was on the nature of the questionâ€™s intent rather than its grammatical perfection.
#### C.6.2Label Definitions
True:
The question represents a reasonable and natural query a human would make. It seeks meaningful information such as facts, summaries, comparisons, or explanations, and is phrased in a way that reflects a real information need. For example: â€What were the companyâ€™s main revenue streams in the last fiscal year?â€
False:
The question is unnatural, trivial, or does not reflect a plausible human intent. This includes questions that are overly literal (e.g., counting word occurrences), focus on formatting (e.g., font sizes), are phrased robotically, or seek bizarrely specific details that a human would be unlikely to ask.
#### Example 1: Not Human-like
[â¬‡](data:text/plain;base64,ewoJImlkIjogMSwKCSJxdWVzdGlvbiI6ICJIb3cgbWFueSBsb2dvcyBpbiB0aGUgRmlndXJlIG9uZSBvZiB0aGUgbWFqb3IgdGVsZWNvbW11bmljYXRpb25zIGNvbXBhbnk/IiwKCSJyZXNwb25zZSI6ICIxMyIsCn0KCiMgU3RlcHM6CiMgMS4gSSByZWFkIHRoZSBxdWVzdGlvbi4KIyAyLiBJIGRvIG5vdCB0aGluayBhIHBlcnNvbiB1c2luZyBhbiBpbmZvcm1hdGlvbiByZXRyaWV2YWwgc3lzdGVtIHdvdWxkIGFzayB0aGlzIHN0eWxlIG9mIHF1ZXN0aW9uLgojIDMuIEkgbGFiZWwgYEh1bWFuLWxpa2VgIGFzIGBGYWxzZWAu)
{
"id":1,
"question":"HowmanylogosintheFigureoneofthemajortelecommunicationscompany?",
"response":"13",
}
#Steps:
#1.Ireadthequestion.
#2.Idonotthinkapersonusinganinformationretrievalsystemwouldaskthisstyleofquestion.
#3.Ilabelâ€˜Human-likeâ€˜asâ€˜Falseâ€˜.
ListingÂ 8:Example of a question that is not human-like due to its trivial, count-based nature.
#### Example 2: Human-like
[â¬‡](data:text/plain;base64,ewoJImlkIjogMywKCSJxdWVzdGlvbiI6ICJXaGF0IHdlcmUgdGhlIHRvcCB0d28gcmV2ZW51ZXMgZm9yIHRoZSBFTVMgZGl2aXNpb24gaW4gMjAxMj8iLAoJInJlc3BvbnNlIjogIkluIDIwMTIsIHRoZSByZXZlbnVlcyB3ZXJlIGFwcHJveGltYXRlbHkgSEskNDkzLDIwOCwwMDAgYW5kIEhLJDM5MSw2NzcsMDAwLiIsCn0KCiMgU3RlcHM6CiMgMS4gSSByZWFkIHRoZSBxdWVzdGlvbi4KIyAyLiBJIGZpbmQgaXQgaXMgY2xlYXIgYW5kIHJlZmxlY3RzIGEgc3BlY2lmaWMsIG1lYW5pbmdmdWwgZmluYW5jaWFsIGlucXVpcnkuCiMgMy4gSSBsYWJlbCBgSHVtYW4tbGlrZWAgYXMgYFRydWVgLg==)
{
"id":3,
"question":"WhatwerethetoptworevenuesfortheEMSdivisionin2012?",
"response":"In2012,therevenueswereapproximatelyHK$493,208,000andHK$391,677,000.",
}
#Steps:
#1.Ireadthequestion.
#2.Ifinditisclearandreflectsaspecific,meaningfulfinancialinquiry.
#3.Ilabelâ€˜Human-likeâ€˜asâ€˜Trueâ€˜.
ListingÂ 9:Example of a question that reflects a clear, natural, and meaningful information need.
## Appendix DExamples
### D.1Examples for text-retrieval better than image-retrieval
![Refer to caption](x3.png)Figure 4:Image-retrieval system fails to extract factual facts and details.
![Refer to caption](x4.png)Figure 5:Image-retrieval system fails to extract factual facts and details in the image.
### D.2Examples for image-retrieval better than image-retrieval
![Refer to caption](x5.png)Figure 6:Text-retrieval system fails to extract factual facts and details in the table.
![Refer to caption](x6.png)Figure 7:Text-retrieval system fails to extract factual facts and details in the table.
### D.3Examples for multimodal-retrieval better than single-modality-retrieval
![Refer to caption](x7.png)Figure 8:MM RAG system handles multi-modality-evidence questions better.
## Appendix EAdditional Experiments
### E.1Retrieval Performance
We break down retrieval performance by question and answer types, as reported in Table[7](https://arxiv.org/html/2510.03663v2#A5.T7).
We find that question type has minimal impact on retrieval recall, whereas answer type plays a significant role.
For text-only retrieval, performance is substantially higher on questions requiring text to answer, but markedly lower on image-required questions.
Conversely, for image-only retrieval, questions requiring image-based answers are retrieved more effectively than those requiring text, highlighting the modality-specific strengths of each embedding approach.
Combining both embeddings (T++I) effectively leverages the advantages of each modality, resulting in higher overall recall.
For multimodal embeddings, image-required questions tend to be retrieved more easily than text-required questions, suggesting that current multimodal embeddings function more like image retrieval in practice.
Table 7:Retrieval performance (Precision@10/Recall@10) of four RAG systems on 1600 QA pairs, averaged across eight domains and broken down by question and answer types.|Type|Text (OpenAI)|IMG (colqwen)|MM (GME)|T++I|
Prec.|Recall|Prec.|Recall|Prec.|Recall|Prec.|Recall|
Factual Retrieval|0.319|0.759|0.237|0.839|0.304|0.876|0.416|0.862|
Comparison|0.440|0.839|0.276|0.854|0.368|0.901|0.503|0.891|
Summary|0.497|0.856|0.329|0.830|0.400|0.907|0.563|0.883|
Logical|0.496|0.801|0.306|0.789|0.381|0.832|0.537|0.829|
Text-only|0.511|0.821|0.324|0.774|0.390|0.836|0.558|0.820|
Img-only|0.152|0.751|0.174|0.922|0.258|0.900|0.273|0.916|
Text + Img|0.489|0.850|0.306|0.833|0.392|0.907|0.555|0.880|
Table-required|0.431|0.773|0.270|0.798|0.339|0.872|0.493|0.851|
### E.2MM-embedding RAG Comparison
We compare RAG performance using two multimodal embeddings:voyage-multimodal-3andgme-Qwen2-VL-7B-Instruct, with results reported in Table[8](https://arxiv.org/html/2510.03663v2#A5.T8)and Table[9](https://arxiv.org/html/2510.03663v2#A5.T9). Whilevoyage-multimodal-3achieves slightly lower recall but higher precision in retrieval compared togme-Qwen2-VL-7B-Instruct, it delivers better overall performance when integrated into MM-RAG.
Table 8:Retrieval performance (Precision@10 / Recall@10) and end-to-end performance (Recallusing retrieved-top-10 and retrieved-top-20 candidates) of two MM-RAG systems on 200 QA pairs across eight domains, with average recall reported across all domains.|Domain|MM (Voyage)|MM (GME)|
Retrieval|End-to-end|Retrieval|End-to-end|
Prec.|Recall|top-1010|top-2020|Prec.|Recall|top-1010|top-2020|
Commerce|0.518|0.892|0.629|0.653|0.354|0.895|0.617|0.611|
Construction|0.406|0.733|0.603|0.609|0.336|0.881|0.601|0.616|
CRM|0.418|0.748|0.634|0.653|0.343|0.884|0.623|0.637|
Education|0.419|0.784|0.652|0.658|0.366|0.912|0.640|0.668|
Energy|0.418|0.783|0.659|0.680|0.331|0.847|0.669|0.666|
Finance|0.426|0.726|0.622|0.644|0.370|0.898|0.627|0.636|
Healthcare|0.388|0.766|0.638|0.668|0.376|0.857|0.642|0.664|
Legal|0.431|0.764|0.631|0.669|0.327|0.876|0.609|0.629|
Avg.|0.416|0.777|0.633|0.654|0.350|0.881|0.628|0.641|
Table 9:Precision and recall of two MM-RAG systems using the top1010retrieved chunks retrieved by their retrievers, evaluated across different question and answer types on1,6001,600QA pairs spanning eight domains, with average recall reported across all domains.|Type|MM (Voyage)|MM (GME)|
Prec.|Recall|Prec.|Recall|
Factual Retrieval|0.606|0.595|0.691|0.580|
Comparison|0.656|0.604|0.730|0.608|
Summary|0.694|0.738|0.802|0.655|
Logical Reasoning|0.699|0.727|0.837|0.679|
Text-only|0.871|0.824|0.868|0.759|
Img-only|0.414|0.348|0.436|0.312|
Text+Img|0.786|0.656|0.810|0.636|
Table-required|0.832|0.736|0.867|0.750|
### E.3Cost Comparison
We also calculate the average inference cost and latency of different RAG systems.
The image-only system (IMG) is the most efficient, while multimodal systems (MM) are the slowest, reflecting the trade-off between complexity and capability.
The text-only system consumes the most tokens and is therefore the most expensive. The T+I fusion RAG retrieves from text chunks first, then images, which increases latency. These results suggest that modern MM-RAG systems can offer both improved performance and lower cost compared to text-only RAG.
Table 10:Average cost of different RAG systems.||IMG|TEXT|MM (GME)|MM (T+I)|
Avg. Cost ($\\mathdollar)|0.012|0.036|0.022|0.029|
Avg. Latency (s)|5.606|7.290|7.897|9.383|
## Appendix FAdditional Analysis
### F.1Content-rich images increase difficulty
We analyzed all images in the documents of the easiest domain (commerce manufacturingandlegal) and the most difficult domains (financeandconstruction).
Usinggemini-2.5-pro, we classified images as either content-rich (providing information not present in the text) or illustrative.
In finance and construction, 62.8% and 69.3% of images, respectively, were content-rich, compared to 40.0% in commerce manufacturing and 49.5% in legal.
This suggests that domains with a higher proportion of content-rich images present a greater challenge for RAG, as these images require effective multimodal understanding beyond text.
### F.2Question type affects difficulty
As shown in Section[4.2](https://arxiv.org/html/2510.03663v2#S4.SS2), the type of context required to answer a question is the most significant factor influencing RAG performance.
Different categories of questions contribute unevenly to the advantage of either text- or image-retrieval RAG systems.
By carefully analyzing questions that can only be answered correctly by one of the two systems, we summarize the key distinguishing features:
Text-Retrieval Advantages:
* â€¢Entity Recognition(e.g., brands, organizations; 53.9% of text advantage): Strong at identifying specific people, companies, or organizations.
* â€¢Comparative Analysis(37.6%): Ranking, evaluating differences, or determining which option is preferable.
* 
