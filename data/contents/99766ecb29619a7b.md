# Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

**URL:** https://huggingface.co/papers/2201.11903
**Published:** 2024-04-19T00:00:00.000Z

---

## Summary

The webpage discusses **Chain-of-Thought Prompting**, a method that enhances the **reasoning capabilities** of large language models (LLMs) by having them generate a series of intermediate reasoning steps. This technique shows significant improvement on **arithmetic, commonsense, and symbolic reasoning tasks**. The text specifically mentions that providing a few chain-of-thought demonstrations as exemplars in the prompt elicits these reasoning abilities in sufficiently large LLMs. It also notes that this method, when applied to a 540B-parameter model, achieved state-of-the-art accuracy on the GSM8K benchmark for math word problems.

While the user query covers a broad range of topics related to reasoning and planning in LLMs (including MCTS, self-reflection, hallucination reduction, etc.), this specific page focuses primarily on **Chain-of-Thought Prompting** as a method for eliciting **reasoning** in LLMs.

---

## Full Content

Paper page - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
[![Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)Hugging Face](https://huggingface.co/)
[Papers](https://huggingface.co/papers)
arxiv:2201.11903
# Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
Published on Jan 28, 2022
[
Upvote
15
](https://huggingface.co/login?next=/papers/2201.11903)
* [![](https://huggingface.co/avatars/243c3c212374b0af42a4b67bd802d6c8.svg)](https://huggingface.co/lckr)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/63d10d4e8eaa4831005e92b5/7p7-OmWM6PqqCs7ZStPGD.jpeg)](https://huggingface.co/m-ric)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/64b0186de92886769eb46863/SjVa4w8SmzuxVyg4_GQ6x.jpeg)](https://huggingface.co/flauflauf)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/64bd994ef8f28a19b0d0acbe/igR6xy19rpgl2uqeh3iPa.jpeg)](https://huggingface.co/sbarman25)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/655ac762cb17ec19ef82719b/1kDncYrGLYS_2SR8cNdAL.png)](https://huggingface.co/matlok)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/6538119803519fddb4a17e10/ffJMkdx-rM7VvLTCM6ri_.jpeg)](https://huggingface.co/samusenps)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uaib754QhqmR6c0YPtI6h.jpeg)](https://huggingface.co/eshiraishi)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5fb6a9c7e6ae537272bdeb27/A5mzfpBETNPoWmcQwrAA6.png)](https://huggingface.co/SauravMaheshkar)
* +7
Authors:
Jason Wei,
Xuezhi Wang,
Dale Schuurmans,
Maarten Bosma,
Brian Ichter,
![](https://huggingface.co/avatars/8bfe3bf63f0609b3b6b7d7c5d7f6b3e9.svg)[Fei Xia](https://huggingface.co/fxia22),
Ed Chi,
Quoc Le,
![](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vXMxaoWwFziul8TqIhQw8.jpeg)[Denny Zhou](https://huggingface.co/dennyzhou)
## Abstract
Chain of thought prompting enhances the reasoning capabilities of large language models, achieving superior performance on arithmetic, commonsense, and symbolic reasoning tasks.
AI-generated summary
We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of[large language models](https://huggingface.co/papers?q=large%20language%20models)to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently[large language models](https://huggingface.co/papers?q=large%20language%20models)via a simple
method called[chain of thought prompting](https://huggingface.co/papers?q=chain%20of%20thought%20prompting), where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three[large language models](https://huggingface.co/papers?q=large%20language%20models)show that[chain of thought prompting](https://huggingface.co/papers?q=chain%20of%20thought%20prompting)improves performance
on a range of[arithmetic](https://huggingface.co/papers?q=arithmetic), commonsense, and[symbolic reasoning](https://huggingface.co/papers?q=symbolic%20reasoning)tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the[GSM8K benchmark](https://huggingface.co/papers?q=GSM8K%20benchmark)of math word problems, surpassing even
finetuned[GPT-3](https://huggingface.co/papers?q=GPT-3)with a verifier.
[View arXiv page](https://arxiv.org/abs/2201.11903)[View PDF](https://arxiv.org/pdf/2201.11903)[Add to collection](https://huggingface.co/login?next=/papers/2201.11903)
### Community
![](https://huggingface.co/avatars/cbfe89a35f626cb91cc65bc6b821c05c.svg)
[Chaudhary-zaryab](https://huggingface.co/Chaudhary-zaryab)
[Apr 19, 2024](#66220b765525ae69a48c8800)
attention all you need
Reply
EditPreview
Upload images, audio, and videos by dragging in the text input, pasting, orclicking here.
Tap or paste here to upload images
Comment
Â·[Sign up](https://huggingface.co/join?next=/papers/2201.11903)or[log in](https://huggingface.co/login?next=/papers/2201.11903)to comment
[
Upvote
15
](https://huggingface.co/login?next=/papers/2201.11903)
* [![](https://huggingface.co/avatars/243c3c212374b0af42a4b67bd802d6c8.svg)](https://huggingface.co/lckr)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/63d10d4e8eaa4831005e92b5/7p7-OmWM6PqqCs7ZStPGD.jpeg)](https://huggingface.co/m-ric)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/64b0186de92886769eb46863/SjVa4w8SmzuxVyg4_GQ6x.jpeg)](https://huggingface.co/flauflauf)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/64bd994ef8f28a19b0d0acbe/igR6xy19rpgl2uqeh3iPa.jpeg)](https://huggingface.co/sbarman25)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/655ac762cb17ec19ef82719b/1kDncYrGLYS_2SR8cNdAL.png)](https://huggingface.co/matlok)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/6538119803519fddb4a17e10/ffJMkdx-rM7VvLTCM6ri_.jpeg)](https://huggingface.co/samusenps)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uaib754QhqmR6c0YPtI6h.jpeg)](https://huggingface.co/eshiraishi)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/5fb6a9c7e6ae537272bdeb27/A5mzfpBETNPoWmcQwrAA6.png)](https://huggingface.co/SauravMaheshkar)
* [![](https://huggingface.co/avatars/713af47d263cded4f194ac8594aa1b92.svg)](https://huggingface.co/washingtonrUS)
* [![](https://huggingface.co/avatars/1213139fc135bd2c2fe695496a780996.svg)](https://huggingface.co/bubble-gum)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/64838b28c235ef76b63e4999/ZhQCYoU3vps71Ag7Jezj6.jpeg)](https://huggingface.co/Kseniase)
* [![](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zIols62RlaazFhkAvP0IE.jpeg)](https://huggingface.co/onslm)
* +3
## Models citing this paper2
## Datasets citing this paper5
[Browse 5 datasets citing this paper](https://huggingface.co/datasets?other=arxiv:2201.11903)### Spaces citing this paper12
## Collections including this paper31
