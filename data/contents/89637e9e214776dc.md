# Faithful Chain-of-Thought Reasoning | NSF Public Access Repository

**URL:** https://par.nsf.gov/biblio/10463284-faithful-chain-thought-reasoning
**Published:** 2023-11-01T00:00:00.000Z

---

## Summary

The webpage describes a research paper titled "Faithful Chain-of-Thought Reasoning." This work proposes a framework called **Faithful CoT** to address the issue that standard Chain-of-Thought (CoT) reasoning chains generated by Language Models (LMs) may not faithfully reflect how the model arrived at the answer.

Faithful CoT involves two stages:
1.  **Translation:** Converting the Natural Language query into a **symbolic reasoning chain** using an LM.
2.  **Problem Solving:** Using a **deterministic solver** on the reasoning chain to derive the final answer.

This approach guarantees that the reasoning chain provides a faithful explanation. Empirically, Faithful CoT outperforms standard CoT on 9 out of 10 benchmarks across diverse domains, showing significant accuracy gains in areas like **Math Word Problems (MWP)**, **Planning**, **Multi-hop Question Answering (QA)**, and **Relational Inference**. It also achieved new state-of-the-art few-shot performance on several datasets when tested with GPT-4 and Codex.

While the user query covers a broad range of topics related to Reasoning LLMs (including MCTS, self-reflection, grounding, and hallucination reduction), the provided text specifically focuses on **Chain-of-Thought (CoT) reasoning** and improving its **faithfulness** and **empirical performance**.

---

## Full Content

Faithful Chain-of-Thought Reasoning | NSF Public Access Repository
[skip to main content](#main_content)
![US Flag](https://par.nsf.gov//img/ui/us_flag.png)An official website of the United States governmentHere's how you know
![dot gov icon](https://par.nsf.gov//img/ui/icon-dot-gov.png)
**Official websites use .gov**
A**.gov**website belongs to an official government organization in the United States.
![https lock icon](https://par.nsf.gov//img/ui/icon-https.png)
**Secure .gov websites use HTTPS**
A**lock (lock) or https://**means you've safely connected to the .gov website. Share sensitive information only on official, secure websites.
[PAR Home](https://par.nsf.gov/)[Contact](https://par.nsf.gov/contact)[FAQ](https://par.nsf.gov/faq)
NSF PAGES NavigationToggle Navigation
* [PAR Home](https://par.nsf.gov/)
* [Contact](https://par.nsf.gov/contact)
* [FAQ](https://par.nsf.gov/faq)
[![NSF Public Access Repository, A partnership with the Department of Energy, Office of Scientific and Technical Information](https://par.nsf.gov/img/nsf/nsf-par-logo-2.png)](https://par.nsf.gov/)
[![National Science Foundation Logo](https://par.nsf.gov/img/nsf/nsf_logo.png)](https://par.nsf.gov/)
**Attention:**
The NSF Public Access Repository (PAR) system and access will be unavailable from 11:00 PM ET on Thursday, December 11 until 2:00 AM ET on Friday, December 12 due to maintenance. We apologize for the inconvenience.
 
Explore Research Products in the PAR
It may take a few hours for recently added research products to appear in PAR search results.
&times;
Search
[&#43; Advanced Search](#advfilters)
&times;
Advanced Search Options
**Search Across All Fields**
Subject(s) / Keyword(s):
&times;
Identifier Number:What does this include?
&times;
Resource Type:
AudiovisualBookBook ChapterConference PaperConference ProceedingData PaperDatasetDissertationEducational aid or CurriculumJournal ArticleModelPosted ContentServiceSoftwareSoundWorkshop Report
**Search a Specific Field**
Journal Name:
&times;
Description / Abstract:
&times;
Title:
&times;
Date Published:
&times;
to
&times;
Publisher or Repository Name:
&times;
Award ID:
&times;
Author / Creator:
&times;
Date Updated:
&times;
to
&times;
SearchCancel
* [NSF Public Access](https://par.nsf.gov/)
* Search Results
* *Faithful Chain-of-Thought Reasoning*
* [Citation Details](#tab-citation-details)
Title:Faithful Chain-of-Thought Reasoning
While Chain-of-Thought (CoT) prompting boosts Language Models’ (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query →symbolic reasoning chain) and Problem Solving (reasoning chain →answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.[more&raquo;&raquo;](#)[&laquo;&laquo;less](#)
Award ID(s):[1928474](https://par.nsf.gov/search/award_ids:1928474)PAR ID:10463284Author(s) / Creator(s):[Lyu, Qing]();[Havaldar, Shreya]();[Stein, Adam]();[Zhang, Li]();[Rao, Delip]();[Wong, Eric]();[Apidianaki, Marianna]();[Callison-Burch, Chris]()Date Published:2023-11-01Journal Name:The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2023)Format(s):Medium: XSponsoring Org:National Science Foundation
##### More Like this
1. [Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting](https://par.nsf.gov/biblio/10542779-language-models-dont-always-say-what-think-unfaithful-explanations-chain-thought-prompting)
Turpin, M;Michael, J;Perez, E;Bowman, S R(February2023, Advances in neural information processing systems)
Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM’s process for solving a task. This level of transparency into LLMs’ predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model’s prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs—e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always “(A)”—which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.
[more&raquo;&raquo;](#)[&laquo;&laquo;less](#)
2. [Making Natural Language Reasoning Explainable and Faithful](https://par.nsf.gov/biblio/10585439-making-natural-language-reasoning-explainable-faithful)
**[https://doi.org/10.1609/aaai.v38i20.30280](https://doi.org/10.1609/aaai.v38i20.30280)**
Du, Xinya(March2024, Proceedings of the AAAI Conference on Artificial Intelligence)
Neural models, including large language models (LLMs), achieve superior performance on logical reasoning tasks such as question answering. To elicit reasoning capabilities from LLMs, recent works propose using the chain-of-thought (CoT) mechanism to generate both the reasoning chain and the answer, which enhances the model’s capabilities in conducting reasoning. However, due to LLM’s uninterpretable nature and the extreme flexibility of free-form explanations, several challenges remain: such as struggling with inaccurate reasoning, hallucinations, and not aligning with human preferences. In this talk, we will focus on (1) our design of leveraging structured information (that is grounded to the context), for the explainable complex question answering and reasoning; (2) our multi-module interpretable framework for inductive reasoning, which conducts step-wise faithful reasoning with iterative feedback.
[more&raquo;&raquo;](#)[&laquo;&laquo;less](#)
3. [To Trust or Not to Trust? Enhancing Large Language Models' Situated Faithfulness to External Contexts](https://par.nsf.gov/biblio/10623913-trust-trust-enhancing-large-language-models-situated-faithfulness-external-contexts)
Huang, Yukun;Chen, Sanxing;Cai, Hongyi;Dhingra, Bhuwan(June2025, International Conference on Learning Representations (ICLR))
Large Language Models (LLMs) are often augmented with external contexts, such as those used in retrieval-augmented generation (RAG). However, these contexts can be inaccurate or intentionally misleading, leading to conflicts with the model’s internal knowledge. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context to resolve knowledge conflicts. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. Our results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs.
[more&raquo;&raquo;](#)[&laquo;&laquo;less](#)
4. [On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning](https://par.nsf.gov/biblio/10411934-second-thought-let-think-step-step-bias-toxicity-zero-shot-reasoning)
Shaikh, O.(July2023, Proceedings of the conference Association for Computational Linguistics Meeting)
Generating a chain of thought (CoT) can increase large language model (LLM) performance on a wide range of tasks. Zero-shot CoT evaluations, however, have been conducted primarily on logical tasks (e.g. arithmetic, commonsense QA). In this paper, we perform a controlled evaluation of zero-shot CoT across two sensitive domains: harmful questions and stereotype benchmarks. We find that using zero-shot CoT reasoning in a prompt can significantly increase a model's likelihood to produce undesirable output. Without future advances in alignment or explicit mitigation instructions, zero-shot CoT should be avoided on tasks where models can make inferences about marginalized groups or harmful topics.
[more&raquo;&raquo;](#)[&laquo;&laquo;less](#)
5. [Empowering Language Models with Knowledge Graph Reasoning for Question Answering](https://par.nsf.gov/biblio/10464911-empowering-language-models-knowledge-graph-reasoning-question-answering)
Hu, Ziniu;Xu, Yichong;Yu, Wenhao;Wang, Shuohang;Yang, Ziyi;Zhu, Chenguang;Chang, Kai-Wei;Sun, Yizhou(December2022, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing)
Answering open-domain questions requires world knowledge about in-context entities. As pre-trained Language Models (LMs) lack the power to store all required knowledge, external knowledge sources, such as knowledge graphs, are often used to augment LMs. In this work, we propose knOwledge REasOning empowered Language Model (OREOLM), which consists of a novel Knowledge Interaction Layer that can be flexibly plugged into existing Transformer-based LMs to interact with a differentiable Knowledge Graph Reasoning module collaboratively. In this way, LM guides KG to walk towards the desired answer, while the retrieved knowledge improves LM. By adopting OREOLM to RoBERTa and T5, we show significant performance gain, achieving state-of-art results in the Closed-Book setting. The performance enhancement is mainly from the KG reasoning’s capacity to infer missing relational facts. In addition, OREOLM provides reasoning paths as rationales to interpret the model’s decision.
[more&raquo;&raquo;](#)[&laquo;&laquo;less](#)
* Free Publicly Accessible Full Text
* [Accepted Manuscript1.0![Accepted Manuscript](https://par.nsf.gov/img/ui/page_white_acrobat.png)](https://par.nsf.gov/servlets/purl/10463284)
* * Conference Paper:
* The DOI is not currently available.
* Have feedback or suggestions for a way to improve these results?
!
* Citation Formats
* [MLA](#cite-mla)
&times;**Cite: MLA Format**
Lyu, Qing, Havaldar, Shreya, Stein, Adam, Zhang, Li, Rao, Delip, Wong, Eric, Apidianaki, Marianna, and Callison-Burch, Chris.*Faithful Chain-of-Thought Reasoning*. Retrieved from https://par.nsf.gov/biblio/10463284.*The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2023)*.
Close
* [APA](#cite-apa)
&times;**Cite: APA Format**
Lyu, Qing, Havaldar, Shreya, Stein, Adam, Zhang, Li, Rao, Delip, Wong, Eric, Apidianaki, Marianna, &amp; Callison-Burch, Chris.*Faithful Chain-of-Thought Reasoning*.*The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2023)*,**(). Retrieved from https://par.nsf.gov/biblio/10463284.
Close
* [Chicago](#cite-chi)
&times;**Cite: Chicago Format**
Lyu, Qing, Havaldar, Shreya, Stein, Adam, Zhang, Li, Rao, Delip, Wong, Eric, Apidianaki, Marianna, and Callison-Burch, Chris. "Faithful Chain-of-Thought Reasoning".*The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2023)*(). Country unknown/Code not available.[https://par.nsf.gov/biblio/10463284](https://par.nsf.gov/biblio/10463284).
Close
* [BibTeX](#cite-bib)
&times;**Cite: BibTeX Format**
@article{osti\_10463284,
place = {Country unknown/Code not available}, title = {Faithful Chain-of-Thought Reasoning}, url = {https://par.nsf.gov/biblio/10463284}, abstractNote = {While Chain-of-Thought (CoT) prompting boosts Language Models’ (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query →symbolic reasoning chain) and Problem Solving (reasoning chain →answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.}, journal = {The 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2023)}, author = {Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris}, }
Close
* * Export Metadata
* [EndNote](https://par.nsf.gov/endnote?osti_id=10463284)
* [Excel](https://par.nsf.gov/export/format:excel/osti-id:10463284)
* [CSV](https://par.nsf.gov/export/format:csv/osti-id:10463284)
* [XML](https://par.nsf.gov/export/format:xml/osti-id:10463284)
* * Save / Share this Record
* [Send to Email](#shareemail)
&times;**Send to Email**
Email address:
Content:
CloseSend
&times;
Author Select
Last Name:
First Name:
[Search](#)
* [Search Results](#authorselect-tab-res)
* [Selected Authors](#authorselect-tab-sel)
Type in a name, or the first few letters of a name, in one or both of appropriate search boxes above and select the search button. An attempt will be made to match authors that most closely relate to the text you typed.
No authors are currently selected. Choosing "Select These Authors" will enter a blank value for author search in the parent form.
CloseReview SelectionsAdd Selections
&times;
Editor Select
Last Name:
First Name:
[Search](#)
* [Search Results](#editorselect-tab-res)
* [Selected Editors](#editorselect-tab-sel)
Type in a name, or the first few letters of a name, in one or both of appropriate search boxes above and select the search button. An attempt will be made to match editors that most closely relate to the text you typed.
No editors are currently selected. Choosing "Select These Editors" will enter a blank value for editor search in the parent form.
CloseReview SelectionsAdd Selections
&times;#### **Warning: Leaving National Science Foundation Website**
![National Science Foundation Logo](https://par.nsf.gov/img/nsf/nsf_logo.png)
You are now leaving the National Science Foundation website to go to a non-government website.
Website:
NSF takes no responsibility for and exercises no control over the views expressed or the accuracy of the information contained on this site. Also be aware that NSF's privacy policy does not apply to this site.
Continue to SiteCancel
