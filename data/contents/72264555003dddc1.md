# Model context protocol (MCP)

**URL:** https://openai.github.io/openai-agents-python/mcp/
**Published:** None

---

## Summary

The webpage describes the **Model Context Protocol (MCP)**, an open standard that dictates how applications expose tools and context to language models, likened to a "USB-C port for AI applications."

The **OpenAI Agents SDK** understands multiple MCP transports, allowing agents to reuse existing MCP servers or build new ones to expose tools (like filesystem, HTTP, or connector-backed tools).

Key integration options discussed include:

1.  **Hosted MCP server tools (`HostedMCPTool`):** Tool execution is pushed into OpenAI's infrastructure via the Responses API. This supports streaming results, optional approval flows for sensitive operations, and integration with OpenAI connectors (e.g., Google Calendar).
2.  **Streamable HTTP MCP servers (`MCPServerStreamableHttp`):** For connecting to HTTP servers where the network connection is managed by the user, offering low latency.
3.  **HTTP with SSE MCP servers (`MCPServerSse`):** Similar to Streamable HTTP but specifically for servers implementing HTTP with Server-Sent Events (SSE).
4.  **stdio MCP servers (`MCPServerStdio`):** Used for local subprocesses, communicating over stdin/stdout, useful for quick proofs of concept.

The page also details features like **Tool filtering** (static or dynamic based on context) to control which tools are exposed, the ability for MCP servers to provide **dynamic prompts** to generate agent instructions, and **caching** of tool lists to reduce latency. Tracing is automatically captured for MCP activity.

**Regarding your query:**

The page focuses on the **Model Context Protocol (MCP)** and its implementation within the **OpenAI Agents SDK** for providing tools and context to agents. It mentions concepts related to agent infrastructure like **tool use** and **function calling** (via MCP tools).

However, it **does not explicitly detail or define**:
*   **agent\_infrastructure** (as a general concept)
*   **agent memory** or **agentic memory**
*   **agent frameworks** (LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK are mentioned only in the context of the SDK being discussed or as potential external frameworks, but not defined or compared)
*   **structured outputs** (though tool results can have structured content, the general concept isn't the focus)
*   **agent orchestration**

No answer found

---

## Full Content

Model context protocol (MCP) - OpenAI Agents SDK
[Skip to content](#model-context-protocol-mcp)
# Model context protocol (MCP)
The[Model context protocol](https://modelcontextprotocol.io/introduction)(MCP) standardises how applications expose tools and
context to language models. From the official documentation:
> > MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI> applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCPprovides a standardized way to connect AI models to different data sources and tools.
> The Agents Python SDK understands multiple MCP transports. This lets you reuse existing MCP servers or build your own to expose
filesystem, HTTP, or connector backed tools to an agent.
## Choosing an MCP integration
Before wiring an MCP server into an agent decide where the tool calls should execute and which transports you can reach. The
matrix below summarises the options that the Python SDK supports.
|What you need|Recommended option|
Let OpenAI's Responses API call a publicly reachable MCP server on the model's behalf|**Hosted MCP server tools**via[`HostedMCPTool`](../ref/tool/#agents.tool.HostedMCPTool)|
Connect to Streamable HTTP servers that you run locally or remotely|**Streamable HTTP MCP servers**via[`MCPServerStreamableHttp`](../ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp)|
Talk to servers that implement HTTP with Server-Sent Events|**HTTP with SSE MCP servers**via[`MCPServerSse`](../ref/mcp/server/#agents.mcp.server.MCPServerSse)|
Launch a local process and communicate over stdin/stdout|**stdio MCP servers**via[`MCPServerStdio`](../ref/mcp/server/#agents.mcp.server.MCPServerStdio)|
The sections below walk through each option, how to configure it, and when to prefer one transport over another.
## 1. Hosted MCP server tools
Hosted tools push the entire tool round-trip into OpenAI's infrastructure. Instead of your code listing and calling tools, the[`HostedMCPTool`](../ref/tool/#agents.tool.HostedMCPTool)forwards a server label (and optional connector metadata) to the Responses API. The
model lists the remote server's tools and invokes them without an extra callback to your Python process. Hosted tools currently
work with OpenAI models that support the Responses API's hosted MCP integration.
### Basic hosted MCP tool
Create a hosted tool by adding a[`HostedMCPTool`](../ref/tool/#agents.tool.HostedMCPTool)to the agent's`tools`list. The`tool\_config`dict mirrors the JSON you would send to the REST API:
```
`[](#__codelineno-0-1)importasyncio[](#__codelineno-0-2)[](#__codelineno-0-3)fromagentsimportAgent,HostedMCPTool,Runner[](#__codelineno-0-4)[](#__codelineno-0-5)asyncdefmain()-&gt;None:[](#__codelineno-0-6)agent=Agent([](#__codelineno-0-7)name=&quot;Assistant&quot;,[](#__codelineno-0-8)tools=[[](#__codelineno-0-9)HostedMCPTool([](#__codelineno-0-10)tool\_config={[](#__codelineno-0-11)&quot;type&quot;:&quot;mcp&quot;,[](#__codelineno-0-12)&quot;&quot;server\_label&quot;&quot;:&quot;gitmcp&quot;,[](#__codelineno-0-13)&quot;&quot;server\_url&quot;&quot;:&quot;https://gitmcp.io/openai/codex&quot;,[](#__codelineno-0-14)&quot;&quot;require\_approval&quot;&quot;:&quot;never&quot;,[](#__codelineno-0-15)}[](#__codelineno-0-16))[](#__codelineno-0-17)],[](#__codelineno-0-18))[](#__codelineno-0-19)[](#__codelineno-0-20)result=awaitRunner.run(agent,&quot;Which language is this repository written in?&quot;)[](#__codelineno-0-21)print(result.final\_output)[](#__codelineno-0-22)[](#__codelineno-0-23)asyncio.run(main())`
```
The hosted server exposes its tools automatically; you do not add it to`mcp\_servers`.
### Streaming hosted MCP results
Hosted tools support streaming results in exactly the same way as function tools. Pass`stream=True`to`Runner.run\_streamed`to
consume incremental MCP output while the model is still working:
```
`[](#__codelineno-1-1)result=Runner.run\_streamed(agent,&quot;Summarise this repository&#39;s top languages&quot;)[](#__codelineno-1-2)asyncforeventinresult.stream\_events():[](#__codelineno-1-3)ifevent.type==&quot;&quot;run\_item\_stream\_event&quot;&quot;:[](#__codelineno-1-4)print(f&quot;Received:{event.item}&quot;)[](#__codelineno-1-5)print(result.final\_output)`
```
### Optional approval flows
If a server can perform sensitive operations you can require human or programmatic approval before each tool execution. Configure`require\_approval`in the`tool\_config`with either a single policy (`"always"`,`"never"`) or a dict mapping tool names to
policies. To make the decision inside Python, provide an`on\_approval\_request`callback.
```
`[](#__codelineno-2-1)fromagentsimportMCPToolApprovalFunctionResult,MCPToolApprovalRequest[](#__codelineno-2-2)[](#__codelineno-2-3)SAFE\_TOOLS={&quot;&quot;read\_project\_metadata&quot;&quot;}[](#__codelineno-2-4)[](#__codelineno-2-5)defapprove\_tool(request:MCPToolApprovalRequest)-&gt;MCPToolApprovalFunctionResult:[](#__codelineno-2-6)ifrequest.data.nameinSAFE\_TOOLS:[](#__codelineno-2-7)return{&quot;approve&quot;:True}[](#__codelineno-2-8)return{&quot;approve&quot;:False,&quot;reason&quot;:&quot;Escalate to a human reviewer&quot;}[](#__codelineno-2-9)[](#__codelineno-2-10)agent=Agent([](#__codelineno-2-11)name=&quot;Assistant&quot;,[](#__codelineno-2-12)tools=[[](#__codelineno-2-13)HostedMCPTool([](#__codelineno-2-14)tool\_config={[](#__codelineno-2-15)&quot;type&quot;:&quot;mcp&quot;,[](#__codelineno-2-16)&quot;&quot;server\_label&quot;&quot;:&quot;gitmcp&quot;,[](#__codelineno-2-17)&quot;&quot;server\_url&quot;&quot;:&quot;https://gitmcp.io/openai/codex&quot;,[](#__codelineno-2-18)&quot;&quot;require\_approval&quot;&quot;:&quot;always&quot;,[](#__codelineno-2-19)},[](#__codelineno-2-20)on\_approval\_request=approve\_tool,[](#__codelineno-2-21))[](#__codelineno-2-22)],[](#__codelineno-2-23))`
```
The callback can be synchronous or asynchronous and is invoked whenever the model needs approval data to keep running.
### Connector-backed hosted servers
Hosted MCP also supports OpenAI connectors. Instead of specifying a`server\_url`, supply a`connector\_id`and an access token. The
Responses API handles authentication and the hosted server exposes the connector's tools.
```
`[](#__codelineno-3-1)importos[](#__codelineno-3-2)[](#__codelineno-3-3)HostedMCPTool([](#__codelineno-3-4)tool\_config={[](#__codelineno-3-5)&quot;type&quot;:&quot;mcp&quot;,[](#__codelineno-3-6)&quot;&quot;server\_label&quot;&quot;:&quot;&quot;google\_calendar&quot;&quot;,[](#__codelineno-3-7)&quot;&quot;connector\_id&quot;&quot;:&quot;&quot;connector\_googlecalendar&quot;&quot;,[](#__codelineno-3-8)&quot;authorization&quot;:os.environ[&quot;&quot;GOOGLE\_CALENDAR\_AUTHORIZATION&quot;&quot;],[](#__codelineno-3-9)&quot;&quot;require\_approval&quot;&quot;:&quot;never&quot;,[](#__codelineno-3-10)}[](#__codelineno-3-11))`
```
Fully working hosted tool samples—including streaming, approvals, and connectors—live in[`examples/hosted\_mcp`](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp).
## 2. Streamable HTTP MCP servers
When you want to manage the network connection yourself, use[`MCPServerStreamableHttp`](../ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp). Streamable HTTP servers are ideal when you control the
transport or want to run the server inside your own infrastructure while keeping latency low.
```
`[](#__codelineno-4-1)importasyncio[](#__codelineno-4-2)importos[](#__codelineno-4-3)[](#__codelineno-4-4)fromagentsimportAgent,Runner[](#__codelineno-4-5)fromagents.mcpimportMCPServerStreamableHttp[](#__codelineno-4-6)fromagents.model\_settingsimportModelSettings[](#__codelineno-4-7)[](#__codelineno-4-8)asyncdefmain()-&gt;None:[](#__codelineno-4-9)token=os.environ[&quot;&quot;MCP\_SERVER\_TOKEN&quot;&quot;][](#__codelineno-4-10)asyncwithMCPServerStreamableHttp([](#__codelineno-4-11)name=&quot;Streamable HTTP Python Server&quot;,[](#__codelineno-4-12)params={[](#__codelineno-4-13)&quot;url&quot;:&quot;http://localhost:8000/mcp&quot;,[](#__codelineno-4-14)&quot;headers&quot;:{&quot;Authorization&quot;:f&quot;Bearer{token}&quot;},[](#__codelineno-4-15)&quot;timeout&quot;:10,[](#__codelineno-4-16)},[](#__codelineno-4-17)cache\_tools\_list=True,[](#__codelineno-4-18)max\_retry\_attempts=3,[](#__codelineno-4-19))asserver:[](#__codelineno-4-20)agent=Agent([](#__codelineno-4-21)name=&quot;Assistant&quot;,[](#__codelineno-4-22)instructions=&quot;Use the MCP tools to answer the questions.&quot;,[](#__codelineno-4-23)mcp\_servers=[server],[](#__codelineno-4-24)model\_settings=ModelSettings(tool\_choice=&quot;required&quot;),[](#__codelineno-4-25))[](#__codelineno-4-26)[](#__codelineno-4-27)result=awaitRunner.run(agent,&quot;Add 7 and 22.&quot;)[](#__codelineno-4-28)print(result.final\_output)[](#__codelineno-4-29)[](#__codelineno-4-30)asyncio.run(main())`
```
The constructor accepts additional options:
* `client\_session\_timeout\_seconds`controls HTTP read timeouts.
* `use\_structured\_content`toggles whether`tool\_result.structured\_content`is preferred over textual output.
* `max\_retry\_attempts`and`retry\_backoff\_seconds\_base`add automatic retries for`list\_tools()`and`call\_tool()`.
* `tool\_filter`lets you expose only a subset of tools (see[Tool filtering](#tool-filtering)).## 3. HTTP with SSE MCP servers
If the MCP server implements the HTTP with SSE transport, instantiate[`MCPServerSse`](../ref/mcp/server/#agents.mcp.server.MCPServerSse). Apart from the transport, the API is identical to the Streamable HTTP server.
```
`[](#__codelineno-5-1)fromagentsimportAgent,Runner[](#__codelineno-5-2)fromagents.model\_settingsimportModelSettings[](#__codelineno-5-3)fromagents.mcpimportMCPServerSse[](#__codelineno-5-4)[](#__codelineno-5-5)workspace\_id=&quot;demo-workspace&quot;[](#__codelineno-5-6)[](#__codelineno-5-7)asyncwithMCPServerSse([](#__codelineno-5-8)name=&quot;SSE Python Server&quot;,[](#__codelineno-5-9)params={[](#__codelineno-5-10)&quot;url&quot;:&quot;http://localhost:8000/sse&quot;,[](#__codelineno-5-11)&quot;headers&quot;:{&quot;X-Workspace&quot;:workspace\_id},[](#__codelineno-5-12)},[](#__codelineno-5-13)cache\_tools\_list=True,[](#__codelineno-5-14))asserver:[](#__codelineno-5-15)agent=Agent([](#__codelineno-5-16)name=&quot;Assistant&quot;,[](#__codelineno-5-17)mcp\_servers=[server],[](#__codelineno-5-18)model\_settings=ModelSettings(tool\_choice=&quot;required&quot;),[](#__codelineno-5-19))[](#__codelineno-5-20)result=awaitRunner.run(agent,&quot;What&#39;s the weather in Tokyo?&quot;)[](#__codelineno-5-21)print(result.final\_output)`
```
## 4. stdio MCP servers
For MCP servers that run as local subprocesses, use[`MCPServerStdio`](../ref/mcp/server/#agents.mcp.server.MCPServerStdio). The SDK spawns the
process, keeps the pipes open, and closes them automatically when the context manager exits. This option is helpful for quick
proofs of concept or when the server only exposes a command line entry point.
```
`[](#__codelineno-6-1)frompathlibimportPath[](#__codelineno-6-2)fromagentsimportAgent,Runner[](#__codelineno-6-3)fromagents.mcpimportMCPServerStdio[](#__codelineno-6-4)[](#__codelineno-6-5)current\_dir=Path(\_\_file\_\_).parent[](#__codelineno-6-6)samples\_dir=current\_dir/&quot;&quot;sample\_files&quot;&quot;[](#__codelineno-6-7)[](#__codelineno-6-8)asyncwithMCPServerStdio([](#__codelineno-6-9)name=&quot;Filesystem Server via npx&quot;,[](#__codelineno-6-10)params={[](#__codelineno-6-11)&quot;command&quot;:&quot;npx&quot;,[](#__codelineno-6-12)&quot;args&quot;:[&quot;-y&quot;,&quot;@modelcontextprotocol/server-filesystem&quot;,str(samples\_dir)],[](#__codelineno-6-13)},[](#__codelineno-6-14))asserver:[](#__codelineno-6-15)agent=Agent([](#__codelineno-6-16)name=&quot;Assistant&quot;,[](#__codelineno-6-17)instructions=&quot;Use the files in the sample directory to answer questions.&quot;,[](#__codelineno-6-18)mcp\_servers=[server],[](#__codelineno-6-19))[](#__codelineno-6-20)result=awaitRunner.run(agent,&quot;List the files available to you.&quot;)[](#__codelineno-6-21)print(result.final\_output)`
```
## Tool filtering
Each MCP server supports tool filters so that you can expose only the functions that your agent needs. Filtering can happen at
construction time or dynamically per run.
### Static tool filtering
Use[`create\_static\_tool\_filter`](../ref/mcp/util/#agents.mcp.util.create_static_tool_filter)to configure simple allow/block lists:
```
`[](#__codelineno-7-1)frompathlibimportPath[](#__codelineno-7-2)[](#__codelineno-7-3)fromagents.mcpimportMCPServerStdio,create\_static\_tool\_filter[](#__codelineno-7-4)[](#__codelineno-7-5)samples\_dir=Path(&quot;/path/to/files&quot;)[](#__codelineno-7-6)[](#__codelineno-7-7)filesystem\_server=MCPServerStdio([](#__codelineno-7-8)params={[](#__codelineno-7-9)&quot;command&quot;:&quot;npx&quot;,[](#__codelineno-7-10)&quot;args&quot;:[&quot;-y&quot;,&quot;@modelcontextprotocol/server-filesystem&quot;,str(samples\_dir)],[](#__codelineno-7-11)},[](#__codelineno-7-12)tool\_filter=create\_static\_tool\_filter(allowed\_tool\_names=[&quot;&quot;read\_file&quot;&quot;,&quot;&quot;write\_file&quot;&quot;]),[](#__codelineno-7-13))`
```
When both`allowed\_tool\_names`and`blocked\_tool\_names`are supplied the SDK applies the allow-list first and then removes any
blocked tools from the remaining set.
### Dynamic tool filtering
For more elaborate logic pass a callable that receives a[`ToolFilterContext`](../ref/mcp/util/#agents.mcp.util.ToolFilterContext). The callable can be
synchronous or asynchronous and returns`True`when the tool should be exposed.
```
`[](#__codelineno-8-1)frompathlibimportPath[](#__codelineno-8-2)[](#__codelineno-8-3)fromagents.mcpimportMCPServerStdio,ToolFilterContext[](#__codelineno-8-4)[](#__codelineno-8-5)samples\_dir=Path(&quot;/path/to/files&quot;)[](#__codelineno-8-6)[](#__codelineno-8-7)asyncdefcontext\_aware\_filter(context:ToolFilterContext,tool)-&gt;bool:[](#__codelineno-8-8)ifcontext.agent.name==&quot;Code Reviewer&quot;andtool.name.startswith(&quot;&quot;danger\_&quot;&quot;):[](#__codelineno-8-9)returnFalse[](#__codelineno-8-10)returnTrue[](#__codelineno-8-11)[](#__codelineno-8-12)asyncwithMCPServerStdio([](#__codelineno-8-13)params={[](#__codelineno-8-14)&quot;command&quot;:&quot;npx&quot;,[](#__codelineno-8-15)&quot;args&quot;:[&quot;-y&quot;,&quot;@modelcontextprotocol/server-filesystem&quot;,str(samples\_dir)],[](#__codelineno-8-16)},[](#__codelineno-8-17)tool\_filter=context\_aware\_filter,[](#__codelineno-8-18))asserver:[](#__codelineno-8-19)...`
```
The filter context exposes the active`run\_context`, the`agent`requesting the tools, and the`server\_name`.
## Prompts
MCP servers can also provide prompts that dynamically generate agent instructions. Servers that support prompts expose two
methods:
* `list\_prompts()`enumerates the available prompt templates.
* `get\_prompt(name, arguments)`fetches a concrete prompt, optionally with parameters.
```
`[](#__codelineno-9-1)fromagentsimportAgent[](#__codelineno-9-2)[](#__codelineno-9-3)prompt\_result=awaitserver.get\_prompt([](#__codelineno-9-4)&quot;&quot;generate\_code\_review\_instructions&quot;&quot;,[](#__codelineno-9-5){&quot;focus&quot;:&quot;security vulnerabilities&quot;,&quot;language&quot;:&quot;python&quot;},[](#__codelineno-9-6))[](#__codelineno-9-7)instructions=prompt\_result.messages[0].content.text[](#__codelineno-9-8)[](#__codelineno-9-9)agent=Agent([](#__codelineno-9-10)name=&quot;Code Reviewer&quot;,[](#__codelineno-9-11)instructions=instructions,[](#__codelineno-9-12)mcp\_servers=[server],[](#__codelineno-9-13))`
```
## Caching
Every agent run calls`list\_tools()`on each MCP server. Remote servers can introduce noticeable latency, so all of the MCP
server classes expose a`cache\_tools\_list`option. Set it to`True`only if you are confident that the tool definitions do not
change frequently. To force a fresh list later, call`invalidate\_tools\_cache()`on the server instance.
## Tracing
[Tracing](../tracing/)automatically captures MCP activity, including:
1. Calls to the MCP server to list tools.
2. MCP-related information on tool calls.
![MCP Tracing Screenshot](../assets/images/mcp-tracing.jpg)
## Further reading
* [Model Context Protocol](https://modelcontextprotocol.io/)– the specification and design guides.
* [examples/mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp)– runnable stdio, SSE, and Streamable HTTP samples.
* [examples/hosted\_mcp](https://github.com/openai/openai-agents-python/tree/main/examples/hosted_mcp)– complete hosted MCP demonstrations including approvals and connectors.
