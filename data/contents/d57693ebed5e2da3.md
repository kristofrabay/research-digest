# Using LLMs as a Reranker for RAG: A Practical Guide

**URL:** https://fin.ai/research/using-llms-as-a-reranker-for-rag-a-practical-guide/
**Published:** 2025-09-11T22:46:52.000Z

---

## Summary

The webpage provides a practical guide on **Using LLMs as a Reranker for Retrieval-Augmented Generation (RAG)** systems.

Key points covered include:

*   **Core Idea:** Reranking reorders passages retrieved by vector search to ensure the final answer is grounded on the most relevant context. The authors use an LLM reranker instead of traditional open-source cross-encoders for better quality.
*   **Reranking Methods:** The guide focuses on **pointwise reranking**, where the LLM scores each passage individually (on a scale of 1-10), as it allows for easier optimization compared to listwise or pairwise methods.
*   **Optimizations for Speed and Latency:** To address the latency issues caused by large inputs/outputs when reranking many passages ($K=40$):
    *   **Reducing Output Tokens:** Switching to a `Dict` format and implementing thresholding (omitting scores below 5) significantly cut latency.
    *   **Parallel Reranking:** Splitting the candidate passages into batches ($N$ workers) and scoring them in parallel. A **round-robin batching strategy** is used to ensure each batch receives a mix of high/medium/low similarity passages, mitigating positional bias from the initial vector search.
*   **Impact:** The optimizations reduced added latency from $\sim 5$ seconds to **$<1$s** and cut costs by $\sim 8$x, while showing a statistically significant quality uplift over the open-source BGE reranker in A/B tests for both the Fin and Copilot agents.
*   **RAG Architectures:** The article briefly mentions how Copilot maintains **source diversity** by retrieving and scoring content across different types (internal, public, conversation history) in parallel streams before merging.
*   **Prompt:** The specific **pointwise prompt** used for the LLM reranker, including detailed grading criteria and strict JSON output formatting rules, is shared.

The page directly addresses **rerankers**, **RAG architectures**, and provides context on the role of retrieval in the overall RAG flow. It does not detail vector databases, new efficient embedding models, hybrid search, or chunking strategies, although it mentions vector search as the initial retrieval step.

---

## Full Content

Using LLMs as a Reranker for RAG: A Practical Guide - /research[Skip to main content](#main)
Toggle dropdown menu&#9662;
[FinThe best performing AI Agent in customer service](https://fin.ai)[Intercom SuiteOne platform with Fin AI Agent and a complete Helpdesk](https://intercom.com/suite)[HelpdeskThe next-gen Helpdesk designed for efficiency](https://intercom.com/helpdesk)
[Home](https://fin.ai/research/)
Using LLMs as a Reranker for RAG: A Practical Guide
# Using LLMs as a Reranker for RAG: A Practical Guide
[![](https://secure.gravatar.com/avatar/76cba499e063bf235208f2e6a4339cda?s=32&#038;d=mm&#038;r=g)Ramil Yarullin](https://fin.ai/research/author/ramil-yarullin/)[![](https://secure.gravatar.com/avatar/e3bbef703c78f0454152c12c42ba3840?s=32&#038;d=mm&#038;r=g)Fedor Parfenov](https://fin.ai/research/author/fedorparfenov/)
2025.09.11
![](https://s47652.pcdn.co/research/wp-content/uploads/2025/03/image-6-1-1024x683.png)
Contents
* [The Core Idea](#the-core-idea)
* [Reducing output tokens](#reducing-output-tokens)
* [Parallel Reranking](#parallel-reranking)
* [RAG in Copilot: Keeping Source Diversity](#rag-in-copilot-keeping-source-diversity)
* [Impact](#impact)
* [Pointwise vs Listwise](#pointwise-vs-listwise)
* [Reflections](#reflections)
* [The Prompt](#the-prompt)
Good answers start with good context. Our AI agents use retrieval-augmented generation (RAG) to find the right context for a user’s query. RAG retrieves top passages from a knowledge base, then uses them to generate an answer.
A key part of this process is**reranking**, which reorders the results from vector search so the final answer is grounded on the most relevant passages. Open-source cross encoder models are a popular choice for this because they are fast and easy to use. But in our experience, they don&#8217;t hit the quality bar we need.
In this post we share how we deployed an LLM-based reranker and the engineering needed to make it**5x faster**while staying reliable on production traffic. We also show how we’ve applied it in our Fin and Copilot Agents and**reveal the prompt we used.**LLM reranker also guided the training of a custom reranker for Fin, which we describe in[a companion post](https://fin.ai/research/how-we-built-a-world-class-reranker-for-fin/).
## The Core Idea
[Vector search](https://fin.ai/research/finetuning-retrieval-for-fin/)returns the top-\\(K\\) passages from the index based on the user&#8217;&#8217;s query. The LLM reranker then scores these passages to decide which are most relevant.
There are three ways to prompt the LLM for reranking [[arxiv](https://arxiv.org/abs/2304.09542)]:
* **Pointwise reranking:**ask the LLM to rate how relevant each passage is on a scale from 1 to 10. Example output with passage ids:`[("id0", 6), ("id1", 10), ("id2", 5), ("id3", 10), ("id4", 4)]`
* **Listwise reranking:**ask the LLM to order the passages by relevance. Example output:`"id1" &gt; "id3" &gt; "id0" &gt; "id5" &gt; "id2" &gt; "id4"`
* **Pairwise reranking:**build a ranking through pairwise comparisons, asking the model which of two passages (\\(p\_i\\) or \\(p\_j\\)​) is more relevant to the query. If you implement the LLM as a comparator inside a sort, you typically need \\(O(K \\log K)\\) or \\(O(K^2)\\) comparisons, making it the most expensive. Studies often find pairwise prompting best on quality, though at higher cost [[arxiv](https://arxiv.org/abs/2306.17563)].
We went with**pointwise**reranking since it gives clear scores that are easy to use and allows useful optimizations.
![LLM reranker in a RAG flow.](https://s47652.pcdn.co/research/wp-content/uploads/2025/08/Screenshot_transparent-1024x203.png)
Now, if you&#8217;&#8217;ve got a lot of passages (we use \\(K\\) = 40), the naive version runs into a few problems we saw in the first iterations:
* The number of output tokens becomes large, which hurts latency
* The LLM sometimes misformats output or mis-scores (duplicate ids, missing ids, etc.)
* The inputs get large: 40 passages ×\~200 tokens is \~8k tokens just for the passages (this doesn’t include the prompt!)
* LLMs can also be sensitive to input&#8217;s order [[arxiv](https://arxiv.org/abs/2307.03172)] and show positional bias [[arxiv](https://arxiv.org/abs/2305.17926)]
We improved this by (a) reducing output tokens and (b) parallelizing the reranker.
## Reducing output tokens
When cutting latency, reducing output tokens is a good first step: latency gain is roughly proportional to how much you cut.
We landed on two optimizations:
* Removed spaces (surprisingly expensive tokens!) and switched to a**`Dict`**format instead of**`List[Tuple]`**.
* Added thresholding: instructed the LLM to omit passage ids if their score is below 5.
The first change cut output tokens by \~28%, and the second brought another \~50% latency drop.
![](https://s47652.pcdn.co/research/wp-content/uploads/2025/09/output-tokens-4-cropped-1024x297.png)
We also tried dropping the**&#8220;id&#8221;**token to save \~20% more latency, but it didn&#8217;&#8217;t work. The LLM started confusing passage indexes with scores (both are integers in a similar range), so quality dropped.
## Parallel Reranking
To improve both speed and accuracy, we split the \\(K\\) candidate passages into \\(N\\) batches and score them in parallel. For example, with \\(K = 40\\) and \\(N = 4\\), each worker gets 10 passages with the same prompt. This keeps inputs and outputs small, which improves speed and quality.
**Batching strategy.**Vector search already imposes an ordering bias (higher semantic similarity first). If you naively split passages into consecutive chunks, for example the first \\(\\frac{K}{N}\\) for worker 1, the next \\(\\frac{K}{N}\\) for worker 2, etc., you&#8217;&#8217;ll overweight the first shard with the &#8220;&#8220;best&#8221;&#8221; candidates. We instead assign round-robin by index so each batch sees a similar mix of high/medium/low similarity passages: \\(B\_j = \\{\\, p\_t \\mid t \\bmod N = j \\,\\}\\). So with \\(N=4\\), the batches are \\(B\_0=\\{p\_0, p\_4, \\ldots\\}\\), \\(B\_1=\\{p\_1, p\_5, \\ldots\\}\\), and so on.
**Merge step.**We pool all scored items, sort by the LLM score, and use the[BGE cross-encoder](https://huggingface.co/BAAI/bge-reranker-large)to break ties or fill missing scores.
![Parallelising LLM reranker in Retrieval-Augmented Generation (RAG)](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcjdX-wbXZzT_xy5BHhsSDjQdArTRHCgKOLXaDB6leS1QgoryqTNzBULKjeJomKKTbgirXzBV5HtgruYcoHB_b4V86INRRk2_BeiOLrMTBFTsHiOwOr3-KazVrrk0AedVJ3Y_8z4w?key=6hs8nKGEGHKzwWeaImMDwA)
**Calibration.**Parallelism comes with a consistency risk: workers run independently and may drift in how they grade. We address this by adding a clear grading rubric to the instructions and anchoring it with a small few-shot example set. This keeps all workers on the same scale so their scores stay comparable.
**Latency &amp; reliability.**End-to-end latency is limited by the slowest worker, so we set tight per-call timeouts and skip retries to avoid long tails. If one call times out with probability \\(p\\), then with \\(N\\) parallel calls, the chance that at least one times out is roughly \\(1 &#8211;&#8211; (1 &#8211;&#8211; p)^N\\), which grows as \\(N\\) gets larger. This estimate is optimistic because it treats calls as independent, and in practice the failure rate increases under concurrent load.
If a shard does time out, we fall back by reranking that slice with the BGE cross-encoder and completing the request. We track timeout rates and tail latency (`p95`/`p99`) in dashboards.
**Benefits.**Parallelizing the LLM reranker improves performance:
* Smaller inputs per call let us run a**faster, cheaper LLM**without losing reranking accuracy.
* Latency improves because each worker handles a shorter prompt, produces a shorter output, and system prompt caching removes most of the fixed overhead.
* Round-robin batching evens out the positional bias across batches.## RAG in Copilot: Keeping Source Diversity
Unlike Fin, Copilot searches across more entity types, including internal content and past[conversation history]()that aren’t user-facing. If treated as one pool, past conversation excerpts can dominate, making it harder to surface more authoritative content.
To fix this, Copilot retrieves by type, scores within type, then merges using heuristic rules that keep source diversity near the top of the RAG context. We use three parallel streams: internal content, public content that Copilot can cite, and conversation history.
## Impact
**Fin.**Our first working setup of the LLM reranker added**\~5 seconds**of latency. After tightening the output format, enabling thresholding, adding prompt caching, and parallelizing calls, the added latency dropped to**&lt;1s**, and**costs fell \~8x**.
* Latency P50: +0.9s
* In the A/B test against the open source BGE reranker, the LLM reranker showed a clear quality win, with a**statistically significant uplift**in resolution rate
**Copilot.**With entity-aware retrieval and the LLM reranker, we observed the following on A/B test against BGE reranker:
* [Assistance rate](https://www.intercom.com/help/en/articles/7022438-reporting-metrics-attributes#h_c31953f10c): +**3pp**
* The answer rate: +**2pp**
* Cited conversation excerpts: –27%
* Citations of public + internal articles: +63%## Pointwise vs Listwise
We ran an A/B test comparing a listwise LLM reranker with our parallel pointwise setup. The listwise version scores all passages at once, so needs a stronger model.
We gave it a solid try, but it didn&#8217;&#8217;t outperform the pointwise version: resolution rate was the same, but latency increased \~40% and cost \~15%, so there was no clear benefit.
## Reflections
LLM-based reranking helped us significantly improve quality compared to open-source cross-encoders. It also gave us a simple way to confirm that reranking quality really does make a difference in practice.
That said, this approach comes with trade-offs. Even after optimizations, the added latency is still noticeable (**+0.9s**), and coordinating multiple LLM calls in parallel introduces complexity.
These challenges motivated us to train a[custom reranker](https://fin.ai/research/how-we-built-a-world-class-reranker-for-fin/), with the LLM reranker acting as a teacher, so we could keep the quality while reducing latency.
## The Prompt
We&#8217;re open-sourcing our prompt for the LLM reranker below:
```
`You are a customer support answer service. Your task is to evaluate help center passages and score their relevance to a given customer query for a retrieval augmented generation (RAG) system.**Evaluation Process:**1. Analyze the customer's query to identify both explicit needs and implicit context including underlying user goals
2. Assess each passage's ability to directly resolve the query or provide substantive supporting information with actionable guidance
3. Score based on how effectively the passage addresses the query's core intent while considering potential interpretations**Grading Criteria:**&lt;&lt;grading\_scale&gt;&gt;
10: EXCEPTIONAL match - Contains exact step-by-step instructions that perfectly match the query's specific scenario. Must include all required parameters/context and resolve the issue completely without any ambiguity. Reserved for definitive solutions that exactly mirror the user's described situation and require no interpretation. 9: NEAR-PERFECT solution - Contains all critical steps for resolution but may lack one minor non-essential detail. Addresses the precise query parameters with specialized information. Solution must be directly applicable without requiring adaptation or assumptions. 8: STRONG MATCH - Provides complete technical resolution through specific instructions, but may require simple logical inferences for full application. Covers all essential components but might need minor contextualization. 7: GOOD MATCH - Contains substantial relevant details that address core aspects of the query, but lacks one important element for complete resolution. Provides concrete guidance requiring some user interpretation.
6: PARTIAL match –General guidance on the right topic but lacks the specifics for direct application. May only resolve a subset of the request.
5: LIMITED relevance –Related context or approach, but indirect. Requires substantial effort to adapt to the user's exact need.
4: TANGENTIAL –Mentions related concepts/keywords with little practical connection to the request. Minimal actionable value.
3: VAGUE domain info –Talks about the general area but not the query's specifics. No concrete, actionable steps.
2: TOKEN overlap –Shares isolated terms without context or intent aligned to the request. Similarity is coincidental.
1: IRRELEVANT –Uses query terms in a completely unrelated way. No meaningful link to the user's goal.
0: UNRELATED –No thematic or contextual connection to the query at all.
&lt;&lt;/grading\_scale&gt;&gt;**Input Format:**&lt;&lt;input\_format&gt;&gt;
&lt;&lt;query&gt;&gt;
// The customer's question or request
&lt;&lt;/query&gt;&gt;
&lt;&lt;passages&gt;&gt;
&lt;&lt;passage id='id0'&gt;&gt;...&lt;&lt;/passage&gt;&gt;
&lt;&lt;passage id='id1'&gt;&gt;...&lt;&lt;/passage&gt;&gt;
...
&lt;&lt;/passages&gt;&gt;
&lt;&lt;/input\_format&gt;&gt;**Output Format:**&lt;&lt;output\_format&gt;&gt;
Return your response in a valid JSON (skip spaces):
{{"id0":score0,"id1":score1,...}}
Strict guidelines:
- Return ONLY a well-formed valid JSON with passage IDs as keys
- Each key must be a passage id in the format "idN"
- Each score must be an integer between 5 to 10. EXCLUDE passages that score below 5 (i.e. 0, 1, 2, 3 or 4)
- Integer values only, no decimals
- Skip spaces in the JSON
- No additional text or formatting
- Maintain original passage ID order
- Note: If NO passages score 5+, return empty JSON object
&lt;&lt;/output\_format&gt;&gt;
&lt;&lt;examples&gt;&gt;{few\_shot\_examples}&lt;/examples&gt;`
```
Please leave this field empty
Get notified when we post on /research.
Check your inbox or spam folder to confirm your subscription.
## About the authors
![](https://secure.gravatar.com/avatar/76cba499e063bf235208f2e6a4339cda?s=24&#038;d=mm&#038;r=g)
[Ramil Yarullin](https://fin.ai/research/author/ramil-yarullin/)is a Staff Machine Learning Scientist at Intercom with 8+ years of experience in engineering and applied research.
![](https://secure.gravatar.com/avatar/e3bbef703c78f0454152c12c42ba3840?s=24&#038;d=mm&#038;r=g)
[Fedor Parfenov](https://fin.ai/research/author/fedorparfenov/)is a Staff Machine Learning Scientist on Intercom&#039;s AI team. His previous projects include implementing recommender systems, reinforcement learning and LLMs for commercial applications.
## Related Articles
### [How We Built a World-Class Reranker for Fin](https://fin.ai/research/how-we-built-a-world-class-reranker-for-fin/)
[Ramil Yarullin](https://fin.ai/research/author/ramil-yarullin/)
2025.09.11
[![](https://s47652.pcdn.co/research/wp-content/uploads/2025/09/image-14-1-1024x512.png)](https://fin.ai/research/how-we-built-a-world-class-reranker-for-fin/)
### [Finetuning Retrieval for Fin](https://fin.ai/research/finetuning-retrieval-for-fin/)
[Dhruv Patel](https://fin.ai/research/author/dhruv-patel/)
2025.09.11
[![](https://s47652.pcdn.co/research/wp-content/uploads/2025/03/image-7-1-1024x683.png)](https://fin.ai/research/finetuning-retrieval-for-fin/)
