# New paper reveals Chain-of-Thought reasoning of LLMs a mirage

**URL:** https://www.reddit.com/r/LocalLLaMA/comments/1mkza1b/new_paper_reveals_chainofthought_reasoning_of/
**Published:** 2025-08-08T00:00:00.000Z

---

## Summary

The webpage discusses a new paper suggesting that the Chain-of-Thought (CoT) reasoning observed in Large Language Models (LLMs) might be a "mirage." The paper indicates that LLMs could be "thinking" in latent space, which effectively separates their internal reasoning process from the visible context tokens they output.

This directly relates to the user's query regarding **reasoning and planning in LLMs**, specifically **chain-of-thought reasoning**.

---

## Full Content

A new paper demonstrates that LLMs could "think" in latent space, effectively decoupling internal reasoning from visible context tokens. This ...
