# New paper reveals Chain-of-Thought reasoning of LLMs a mirage

**URL:** https://www.reddit.com/r/LocalLLaMA/comments/1mkza1b/new_paper_reveals_chainofthought_reasoning_of/
**Published:** 2025-08-08T00:00:00.000Z

---

## Summary

The webpage discusses a new paper suggesting that the Chain-of-Thought (CoT) reasoning observed in Large Language Models (LLMs) might be an illusion, as the models could be "thinking" in latent space, separate from the visible context tokens.

This directly relates to the user's query topics of **reasoning and planning**, **Chain-of-Thought**, and potentially **hallucination reduction and detection** (as the validity of CoT is being questioned).

---

## Full Content

A new paper demonstrates that LLMs could "think" in latent space, effectively decoupling internal reasoning from visible context tokens.
