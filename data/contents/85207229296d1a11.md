# Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers

**URL:** https://arxiv.org/html/2506.00054v1
**Published:** 2025-11-14T00:00:00.000Z

---

## Summary

The webpage provides a comprehensive survey of Retrieval-Augmented Generation (RAG) systems, covering architectures, enhancements, and robustness frontiers.

Here is a summary addressing the specific topics in your query:

*   **Vector databases:** The text discusses **dense retrievers** (e.g., DPR) which rely on vector representations (embeddings) for retrieval from external knowledge sources, implying the use of vector databases for storage and similarity search.
*   **Embeddings (new efficient models):** The survey covers **dense retrievers** and the use of **Query Encoder** modules to create query representations, which are essentially embeddings. It also mentions enhancements related to **Rich Answer Encoding (RAE)**, which embeds answer-aligned semantics into retriever outputs.
*   **Rerankers:** Reranking is explicitly covered as a vital enhancement. Methods like **Re2G**, **GenRT**, and **RAG-Fusion** utilize reranking layers or fusion techniques to refine the initial set of retrieved documents based on relevance before generation.
*   **RAG Architectures:** The survey systematically categorizes RAG architectures into **Retriever-centric**, **Generator-centric**, **Hybrid**, and **Robustness-oriented** designs.
*   **RAG Alternatives:** While the focus is on RAG, the text frames RAG as an enhancement to LLMs that addresses the limitations of **static, parametric knowledge storage**. The concept of **GenGround** (Generate-Then-Ground) suggests an alternative flow where a provisional answer is generated first, followed by retrieval for substantiation.
*   **Hybrid Search:** The text discusses **Hybrid and structured retrieval** approaches, such as those integrating unstructured text embeddings with structured knowledge graph embeddings (e.g., Doan et al.'s lightweight hybrid strategy), which aligns with the concept of hybrid search.
*   **Chunking Strategies:** This is addressed under **Granularity-Aware Retrieval**. Methods like **LongRAG** retrieve "compressed long-context chunks" constructed through document grouping, and **FILCO** filters irrelevant spans from retrieved passages, both relating to how documents are segmented and selected.
*   **Context Window Management:** This is covered under **Efficiency Enhancements** and **Generator-Based RAG Systems**. Techniques like **FiD-Light** (token-level passage compression) and **IB Filtering** (information bottleneck-based filtering) aim to optimize the context passed to

The user query asks for a summary covering several specific topics related to Retrieval-Augmented Generation (RAG): **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, and context window management.**

The provided webpage text is a comprehensive survey of RAG architectures and enhancements. Here is a summary of how the text addresses the user's query points:

*   **RAG Architectures:** The page extensively covers various RAG architectures, categorized into Retriever-based, Generator-based, and Hybrid RAG, detailing specific frameworks like RQ-RAG, SELF-RAG, and DRAGIN, and comparing their performance in short-form QA and multi-hop reasoning.
*   **Rerankers:** Reranking is explicitly mentioned as a category under "Enhancement Type." Methods like **FILCO** (lexical filtering), **IB Filtering** (info-theoretic), **Stochastic Filtering**, **RLT** (adaptive truncation), **ToolRerank**, **RankRAG**, and **uRAG** are listed, along with their mechanisms and performance impacts.
*   **RAG Alternatives/Enhancements:** The entire document details numerous enhancements and alternatives to standard RAG, such as adaptive retrieval (TA-ARE, FLARE), robustness modules (CRAG, RAAT), efficiency techniques (Sparse RAG, FiD-Light), and security considerations (BadRAG, TrojanRAG).
*   **Hybrid Search:** The "Hybrid" category under Enhancement Type lists methods like **M-RAG** (Semantic partitioning + dual agents) and **KRAGEN** (Knowledge graph subgraph retrieval), which represent forms of hybrid search or integration.
*   **Chunking Strategies / Context Window Management:** While not explicitly using the terms "chunking strategies" or "context window management" as dedicated sections, the text addresses related concepts:
    *   **Compression:** **Sparse RAG** (retains high-signal tokens) and **FiD-Light** (compresses passages) directly relate to managing the context provided to the generator, which is a key aspect of chunking/compression strategies.
    *   **Context Reduction:** **SEER** achieves a **9.25x context reduction** through self-training.
    *   **Long-Context Tasks:** **Sparse RAG** is noted as

---

## Full Content

Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers
This is a preprint under review at ACM TOIS. Do not redistribute the final version without permission.
# Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers
Chaitanya SharmaIndependent ResearcherUnited States
###### Abstract.
Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance large language models (LLMs) by conditioning generation on external evidence retrieved at inference time. While RAG addresses critical limitations of parametric knowledge storageâ€”such as factual inconsistency and domain inflexibilityâ€”it introduces new challenges in retrieval quality, grounding fidelity, pipeline efficiency, and robustness against noisy or adversarial inputs. This survey provides a comprehensive synthesis of recent advances in RAG systems, offering a taxonomy that categorizes architectures into retriever-centric, generator-centric, hybrid, and robustness-oriented designs. We systematically analyze enhancements across retrieval optimization, context filtering, decoding control, and efficiency improvements, supported by comparative performance analyses on short-form and multi-hop question answering tasks. Furthermore, we review state-of-the-art evaluation frameworks and benchmarks, highlighting trends in retrieval-aware evaluation, robustness testing, and federated retrieval settings. Our analysis reveals recurring trade-offs between retrieval precision and generation flexibility, efficiency and faithfulness, and modularity and coordination. We conclude by identifying open challenges and future research directions, including adaptive retrieval architectures, real-time retrieval integration, structured reasoning over multi-hop evidence, and privacy-preserving retrieval mechanisms. This survey aims to consolidate current knowledge in RAG research and serve as a foundation for the next generation of retrieval-augmented language modeling systems.
Retrieval-Augmented Generation, Query Reformulation, Context Filtering, Reranking, Multi-hop Reasoning, Hallucination Mitigation, Robustness, Dynamic Retrieval, Evaluation Benchmarks, Federated Retrieval, Faithfulness, Efficiency Optimization, Document Ranking, LLM Alignment, Open-Domain QA
â€ â€ copyright:noneâ€ â€ ccs:Information systemsÂ Retrieval models and rankingâ€ â€ ccs:Information systemsÂ Evaluation of retrieval resultsâ€ â€ ccs:Information systemsÂ Information retrieval query processingâ€ â€ ccs:Information systemsÂ Retrieval tasks and goalsâ€ â€ ccs:Information systemsÂ Document representation
## 1.Introduction
Large Language Models (LLMs) have demonstrated impressive generalization across natural language tasks, but their reliance on static, parametric knowledge remains a fundamental limitation. This restricts their ability to handle queries requiring up-to-date, verifiable, or domain-specific information, often resulting in hallucinations or factual inconsistencies> (Gao etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib20)> ; Lewis etÂ al
> .
> , [> 2020b
](https://arxiv.org/html/2506.00054v1#bib.bib41)> )
.
Retrieval-Augmented Generation (RAG) addresses this issue by coupling pretrained language models with non-parametric retrieval modules that fetch external evidence during inference. By conditioning generation on retrieved documents, RAG systems offer greater transparency, factual grounding, and adaptability to evolving knowledge bases. These properties have made RAG central to tasks such as open-domain QA, biomedical reasoning, knowledge-grounded dialogue, and long-context summarization.
However, integrating retrieval with generation introduces unique challenges: retrieval noise and redundancy can degrade output quality; misalignment between retrieved evidence and generated text can lead to hallucinations; and pipeline inefficiencies and latency make deployment costly at scale. Moreover, balancing modularity with tight retrievalâ€“generation interaction remains an open architectural trade-off.
In this survey, we first present a high-level taxonomy of RAG architectures based on where core innovations occurâ€”within the retriever, the generator, or through their joint coordination (Section[3](https://arxiv.org/html/2506.00054v1#S3)). We begin with a background on RAGâ€™s mathematical formulation and components (Section[2.2](https://arxiv.org/html/2506.00054v1#S2.SS2)), and then explore advances in retrieval strategies, filtering, and control mechanisms (Section[4](https://arxiv.org/html/2506.00054v1#S4)). We further analyze how RAG systems are benchmarked (Section[6](https://arxiv.org/html/2506.00054v1#S6)), compare prominent frameworks (Section[5](https://arxiv.org/html/2506.00054v1#S5)), and conclude with open research challenges and future directions (Section[7](https://arxiv.org/html/2506.00054v1#S7)).
## 2.Background and foundations of retrieval-augmented generation
Retrieval-Augmented Generation (RAG) is a framework that augments large language models (LLMs) with external knowledge access via document retrieval. It builds on the intuition that generating grounded and verifiable responses requires not only parametric knowledge stored in model weights, but also non-parametric access to a dynamic evidence corpus. This section outlines the core components of RAG systems and presents the mathematical formulation that underpins their design.
![Refer to caption](extracted/6477147/RAG_architecture.jpg)Figure 1.Retrieval-Augmented Generation (RAG) workflow.A user query is processed by the retriever, which may perform query expansion before retrieving documents from external knowledge sources (e.g., databases, APIs, or document stores). Retrieved documents are re-ranked by relevance, and the Top-K are passed to the generator as factual context. The generator synthesizes a response conditioned on both the query and retrieved content. An optional post-processing step (e.g., ranking, rewriting, or fact-checking) may further refine the output, enhancing factual consistency, real-time adaptability, and overall response quality in large language models (LLMs).
### 2.1.Components of a RAG System
At a high level, a RAG system consists of three modules:
Query Encoder:Encodes the inputxð‘¥xitalic\_xinto a query representationqð‘žqitalic\_q, which is used to retrieve relevant documents. This can be either a neural encoder or a rule-based template.
Retriever:Given the queryqð‘žqitalic\_q, the retriever fetches a ranked list of documentsd1,d2,â€¦,dksubscriptð‘‘1subscriptð‘‘2â€¦subscriptð‘‘ð‘˜d\_{1},d\_{2},\\ldots,d\_{k}italic\_d start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_d start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , â€¦, italic\_d start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPTfrom a corpusð’žð’ž\\mathcal{C}caligraphic\_C. Retrievers may be sparse (e.g., BM25> (Robertson etÂ al
> .
> , [> 2009
](https://arxiv.org/html/2506.00054v1#bib.bib55)> )
), dense (e.g., DPR> (Karpukhin etÂ al
> .
> , [> 2020
](https://arxiv.org/html/2506.00054v1#bib.bib37)> )
), hybrid, or generative.
Generator:The generator conditions on the inputxð‘¥xitalic\_xand the retrieved documentsdisubscriptð‘‘ð‘–d\_{i}italic\_d start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPTto produce the final outputyð‘¦yitalic\_y. This is typically a pretrained transformer model (e.g., T5> (Raffel etÂ al
> .
> , [> 2020
](https://arxiv.org/html/2506.00054v1#bib.bib52)> )
, BART> (Lewis etÂ al
> .
> , [> 2020a
](https://arxiv.org/html/2506.00054v1#bib.bib40)> )
, GPT> (Brown etÂ al
> .
> , [> 2020
](https://arxiv.org/html/2506.00054v1#bib.bib6)> )
).
### 2.2.Mathematical Formulation
Formally, the generation process in Retrieval-Augmented Generation (RAG) can be expressed as modeling the conditional distribution:
(1)||Pâ¢(yâˆ£x)=âˆ‘dâˆˆð’žPâ¢(yâˆ£x,d)â‹…Pâ¢(dâˆ£x)ð‘ƒconditionalð‘¦ð‘¥subscriptð‘‘ð’žâ‹…ð‘ƒconditionalð‘¦ð‘¥ð‘‘ð‘ƒconditionalð‘‘ð‘¥P(y\\mid x)=\\sum\_{d\\in\\mathcal{C}}P(y\\mid x,d)\\cdot P(d\\mid x)italic\_P ( italic\_y âˆ£italic\_x ) = âˆ‘start\_POSTSUBSCRIPT italic\_d âˆˆcaligraphic\_C end\_POSTSUBSCRIPT italic\_P ( italic\_y âˆ£italic\_x , italic\_d ) â‹…italic\_P ( italic\_d âˆ£italic\_x )||
where:
* â€¢xð‘¥xitalic\_xis the input (e.g., a question or prompt),
* â€¢dð‘‘ditalic\_dis a retrieved document from corpusð’žð’ž\\mathcal{C}caligraphic\_C,
* â€¢yð‘¦yitalic\_yis the generated response.
In practice, the summation is approximated by retrieving the top-kð‘˜kitalic\_kdocumentsd1,â€¦,dksubscriptð‘‘1â€¦subscriptð‘‘ð‘˜d\_{1},\\ldots,d\_{k}italic\_d start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , â€¦, italic\_d start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPT, yielding:
(2)||Pâ¢(yâˆ£x)â‰ˆâˆ‘i=1kPâ¢(yâˆ£x,di)â‹…Pâ¢(diâˆ£x)ð‘ƒconditionalð‘¦ð‘¥superscriptsubscriptð‘–1ð‘˜â‹…ð‘ƒconditionalð‘¦ð‘¥subscriptð‘‘ð‘–ð‘ƒconditionalsubscriptð‘‘ð‘–ð‘¥P(y\\mid x)\\approx\\sum\_{i=1}^{k}P(y\\mid x,d\_{i})\\cdot P(d\_{i}\\mid x)italic\_P ( italic\_y âˆ£italic\_x ) â‰ˆâˆ‘start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_k end\_POSTSUPERSCRIPT italic\_P ( italic\_y âˆ£italic\_x , italic\_d start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ) â‹…italic\_P ( italic\_d start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT âˆ£italic\_x )||
This decomposition reflects two key probabilities:
* â€¢Pâ¢(diâˆ£x)ð‘ƒconditionalsubscriptð‘‘ð‘–ð‘¥P(d\_{i}\\mid x)italic\_P ( italic\_d start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT âˆ£italic\_x ): the relevance score of documentdisubscriptð‘‘ð‘–d\_{i}italic\_d start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPTgiven the inputxð‘¥xitalic\_x, often derived from a retriever or reranker.
* â€¢Pâ¢(yâˆ£x,di)ð‘ƒconditionalð‘¦ð‘¥subscriptð‘‘ð‘–P(y\\mid x,d\_{i})italic\_P ( italic\_y âˆ£italic\_x , italic\_d start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ): the probability of generating outputyð‘¦yitalic\_yconditioned onxð‘¥xitalic\_xand documentdisubscriptð‘‘ð‘–d\_{i}italic\_d start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT, modeled by the language model.
Variants of RAG differ in how they estimate and combine these components. Some use a fixed retriever and let the generator handle noisy inputs, while others jointly optimize retrieval and generation to maximize downstream utility.
## 3.Taxonomy of RAG Architectures
To contextualize recent advances in Retrieval-Augmented Generation (RAG), we propose a taxonomy that categorizes existing systems based on their architectural focusâ€”retriever-centric, generator-centric, hybrid, and robustness-oriented designs. This classification highlights key design patterns and illustrates how different frameworks tackle the core challenges of retrieval, grounding, and reliability.
![Refer to caption](extracted/6477147/RAG_Taxonomy.jpg)Figure 2.Figure 2: Taxonomy of Retrieval-Augmented Generation (RAG) Systems.This taxonomy categorizes RAG frameworks based on their primary architectural orientationâ€”retriever-based, generator-based, hybrid, and robustness-focused designs. Retriever-based models are further grouped into query-centric, retriever-centric, and granularity-aware approaches, while generator-based models include faithfulness-aware decoding, context compression, and retrieval-guided generation. Hybrid systems are organized by retrieval dynamics (e.g., multi-round, utility-driven), and robustness-oriented models address challenges such as noise, hallucination, and adversarial vulnerabilities. This structure highlights the diverse innovations shaping the RAG landscape.
### 3.1.Retriever-Based RAG Systems
Retriever-based Retrieval-Augmented Generation (RAG) systems delegate architectural responsibility primarily to the retriever, treating the generator as a passive decoder. These systems operate under the premise that the fidelity and relevance of the retrieved context are the most critical factors for generating accurate and grounded outputs. Innovations in this space typically fall into one of three design patterns: input-side query enhancement, retriever-side adaptation, and retrieval granularity optimization.
Query-Driven Retrieval:A prominent strategy focuses on refining and structuring user intent before retrieval to maximize alignment with relevant corpus segments. This includes decomposition, rewriting, generative reformulation, and the incorporation of structured priors to guide retrieval. Notable examples include RQ-RAG (Refine Query for RAG)> (Chan etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib7)> )
, which decomposes multi-hop queries into latent sub-questions, and GMR (Generative Multi-hop Retrieval)> (Lee etÂ al
> .
> , [> 2022
](https://arxiv.org/html/2506.00054v1#bib.bib39)> )
, which uses a generative LLM to autoregressively formulate complex multi-hop queries. RAG-Fusion> (Rackauckas, [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib51)> )
further improves recall by combining results from multiple reformulated queries through reciprocal rank fusion> (Cormack etÂ al
> .
> , [> 2009
](https://arxiv.org/html/2506.00054v1#bib.bib14)> )
. Structured approaches have also emerged: KRAGEN (Knowledge Retrieval Augmented Generation ENgine)> (Matsumoto etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib44)> )
introduces graph-of-thoughts prompting to decompose complex queries into subproblems, retrieving relevant subgraphs to guide multi-hop reasoning. Additionally, LQR (Layered Query Retrieval)> (Huang etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib26)> )
implements hierarchical planning over multi-hop questions, while Sparse Context Selection> (Zhu etÂ al
> .
> , [> 2025
](https://arxiv.org/html/2506.00054v1#bib.bib91)> )
emphasizes efficient sparse reformulations for both recall and speed.
Retriever-Centric Adaptation:Another line of work modifies the retriever itself through architectural enhancements or task-specific learning. Re2G (Retrieve, Rerank,
Generate)> (Glass etÂ al
> .
> , [> 2022
](https://arxiv.org/html/2506.00054v1#bib.bib21)> )
blends symbolic and neural retrieval via reranking layers, while SimRAG (Self-Improving RAG)> (Xu etÂ al
> .
> , [> 2025
](https://arxiv.org/html/2506.00054v1#bib.bib74)> )
employs self-training over synthetic QA pairs to improve domain generalization. Frameworks like RankRAG> (Yu etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib84)> )
and uRAG (unified RAG)> (Salemi and Zamani, [> 2024b
](https://arxiv.org/html/2506.00054v1#bib.bib58)> )
emphasize retriever versatilityâ€”either by unifying reranking and generation within a single backbone or by training general-purpose retrievers across varied downstream tasks. Additionally, systems such as ToolRerank> (Zheng etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib89)> )
dynamically adapt retrieval strategies based on query semantics, optimizing tool selection in specialized retrieval settings. Relatedly, SEER (Self-Aligned Evidence Extraction for RAG)> (Zhao etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib88)> )
focuses on post-retrieval adaptation, advancing corpus-based evidence extraction by aligning evidence selection with faithfulness, helpfulness, and conciseness criteria, thereby improving evidence quality for open-domain and temporally sensitive queries.
Granularity-Aware Retrieval:This pattern addresses retrieval precision by optimizing the unit of retrievalâ€”from full documents to fine-grained, semantically aligned segments. LongRAG> (Jiang etÂ al
> .
> , [> 2024a
](https://arxiv.org/html/2506.00054v1#bib.bib32)> )
exemplifies this by retrieving compressed long-context chunks, constructed through document grouping, to better exploit long-context language models. Similarly, FILCO (Filter Context)> (Wang etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib70)> )
enhances retrieval granularity by filtering irrelevant or low-utility spans from retrieved passages before generation, improving the faithfulness and efficiency of RAG outputs. In parallel, the Sufficient Context analysis framework> (Joren etÂ al
> .
> , [> 2025
](https://arxiv.org/html/2506.00054v1#bib.bib35)> )
offers a complementary lens, evaluating whether retrieved contexts contain enough information to support accurate generation, thereby highlighting the importance of granular retrieval quality for system robustness.
Each of these patterns anchors its innovation in the retriever, preserving modularity and interpretability. However, they also introduce trade-offs in latency, redundancy, and sensitivity to ambiguous or underspecified queries. Section[4](https://arxiv.org/html/2506.00054v1#S4)elaborates on how downstream enhancementsâ€”such as reranking, adaptive filtering, and utility-based context selectionâ€”further address these limitations.
### 3.2.Generator-Based RAG Systems
Generator-based RAG systems concentrate architectural innovation on the decoding process, assuming the retrieved content is sufficiently relevant and shifting the burden of factual grounding and integration to the language model. These systems enhance output quality through mechanisms for self-verification, compression, and controlled generation. We identify three recurring design patterns within this category: faithfulness-aware decoding, context compression and utility filtering, and retrieval-conditioned generation control.
Faithfulness-Aware Decoding:To reduce hallucinations and improve factual grounding, several systems embed mechanisms for self-reflection, verification, or correction during generation. SELF-RAG (Self-Reflective RAG)> (Asai etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib2)> )
introduces a critiqueâ€“generate loop, allowing the model to assess and revise its outputs before finalization. SelfMem> (Cheng etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib10)> )
builds on this by incorporating a self-memory module that enables the model to revisit and refine prior generations. INFO-RAG> (Xu etÂ al
> .
> , [> 2024d
](https://arxiv.org/html/2506.00054v1#bib.bib77)> )
treats the LLM as a denoising module trained with contrastive objectives. Collectively, these systems decouple output faithfulness from retrieval fidelity, enabling recovery even when retrieval is suboptimal.
Context Compression and Utility Filtering:To address context window limitations, several systems optimize retrieval inputs into denser or more structured forms. FiD-Light> (HofstÃ¤tter etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib25)> )
, a streamlined variant of the Fusion-in-Decoder (FiD) architecture> (Izacard and Grave, [> 2021
](https://arxiv.org/html/2506.00054v1#bib.bib28)> )
, improves decoding efficiency by compressing encoder outputs across retrieved passages and pruning cross-passage attention without modifying retrieval mechanisms. xRAG> (Cheng etÂ al
> .
> , [> 2024b
](https://arxiv.org/html/2506.00054v1#bib.bib11)> )
projects document embeddings directly into the modelâ€™s representation space, minimizing token overhead through modality fusion. Rich Answer Encoding (RAE)> (Huang etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib27)> )
enhances retrieval relevance by embedding answer-aligned semantics into retriever outputs rather than relying on token overlap. GenRT> (Xu etÂ al
> .
> , [> 2024c
](https://arxiv.org/html/2506.00054v1#bib.bib76)> )
further refines retrieval utility by reranking and dynamically truncating retrieved lists, retaining only the most contextually valuable candidates for generation. Complementing these designs, an information bottleneck-based filtering approach> (Zhu etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib90)> )
selectively preserves evidence most informative for generation. Together, these strategies advance decoding efficiency and context quality, particularly for long-form and multi-hop RAG tasks.
Retrieval-Guided Generation:A third strategy modulates generation based on retrieval metadata, task-specific cues, or agentic decision-making. AU-RAG (Agent-based Universal RAG)> (Jang and Li, [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib29)> )
exemplifies this by using an agent to decide dynamically between retrieved and parametric knowledge across diverse data environments. RAG-Ex> (Sudhi etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib63)> )
perturbs retrieval context to analyze how variability influences model behavior and reliance on external evidence.R2AG(Retrieval information into
RAG)> (Ye etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib83)> )
extends this by recursively reranking candidates during generation, dynamically prioritizing evidence based on the evolving answer state. In high-stakes domains, Confidence-Calibrated RAG> (Ozaki etÂ al
> .
> , [> 2025
](https://arxiv.org/html/2506.00054v1#bib.bib49)> )
shows that document ordering and prompt structure affect output certainty, highlighting the need for calibration alongside factual accuracy.
These architectures are particularly suited to domains where factual correctness, reasoning transparency, or structured output formats are essentialâ€”such as biomedical QA, finance, and enterprise workflows. While they leave the retriever fixed, many of their techniques are complementary to retrieval-side enhancements and can be layered atop other RAG variants. SectionÂ 4.2 further explores compression, reranking, and decoding control strategies in these systems.
### 3.3.Hybrid RAG Systems
Hybrid RAG systems tightly couple the retriever and generator, moving beyond modular architectures to treat retrieval and generation as co-adaptive reasoning agents. These systems emphasize iterative feedback, utility-aware coordination, and dynamic control over retrieval actions, particularly in open-domain, multi-hop, and evolving knowledge contexts. We identify three dominant architectural patterns: iterative or multi-round retrieval, utility-driven joint optimization, and retrieval-aware generation control.
Iterative or Multi-Round Retrieval:These systems interleave retrieval and generation across multiple reasoning steps, allowing for evidence refinement and progressive answer construction. IM-RAG (Inner Monologue RAG)> (Yang etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib81)> )
simulates an â€œinner monologueâ€ by alternating between generation and retrieval phases, supporting multi-step reasoning. Generate-Then-Ground (GenGround)> (Shi etÂ al
> .
> , [> 2024b
](https://arxiv.org/html/2506.00054v1#bib.bib60)> )
follows a similar philosophy, generating a provisional answer first and then retrieving supporting evidence to substantiate or revise itâ€”improving factuality and interpretability in multi-hop settings. G-Retriever> (He etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib23)> )
retrieves graph-structured subcomponents as generation unfolds, enhancing complex reasoning over textual graphs.
Utility-Driven Joint Optimization:Several frameworks seek to align retriever outputs with their downstream utility for generation through joint objectives or reinforcement learning. Stochastic RAG> (Zamani and Bendersky, [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib85)> )
treats retrieval as an expected utility maximization problem, updating both retriever and generator end-to-end using REINFORCE-based gradients. M-RAG> (Wang etÂ al
> .
> , [> 2024b
](https://arxiv.org/html/2506.00054v1#bib.bib71)> )
applies multi-agent reinforcement learning, coordinating distributed retrievers and generators via shared memory and task-specific roles. MedGraphRAG> (Wu etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib73)> )
integrates knowledge graphs into the joint learning loop, facilitating domain-specific reasoning with structured priors. These systems improve factuality and answer consistency, particularly in biomedical and enterprise domains.
Dynamic Retrieval Triggering:A growing class of systems dynamically controls when and how to retrieve, conditioned on generation uncertainty, task complexity, or intermediate outputs. DRAGIN (Dynamic Retrieval Augmented
Generation based on the Information Needs
of LLMs)> (Su etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib62)> )
triggers retrieval at the token level using entropy-based confidence signals, while FLARE (Forward-Looking Active REtrieval augmented
generation ()> (Jiang etÂ al
> .
> , [> 2023b
](https://arxiv.org/html/2506.00054v1#bib.bib33)> )
selectively retrieves based on low-confidence predictions during sentence generation. SELF-ROUTE> (Li etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib42)> )
dynamically routes tasks between retrieval and generation modules based on model self-assessed difficulty, and AU-RAG> (Jang and Li, [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib29)> )
leverages agentic decision-making to mediate between diverse retrieval sources and procedural knowledge. TA-ARE (Time-Aware Adaptive REtrieval)> (Zhang etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib87)> )
introduces a retrieval trigger classifier that adaptively determines when retrieval is necessary and adjusts the granularity of evidence based on query needs. A related approach, CRAG (Corrective RAG)> (Yan etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib80)> )
, evaluates retrieved evidence quality before generation and dynamically decides whether to proceed with generation, re-trigger retrieval, or decompose the input into simpler sub-queries. This corrective mechanism positions CRAG within the hybrid class, as it tightly coordinates retrieval assessment with adaptive generation pathways under uncertainty.
These architectures reflect a broader trend toward treating retrieval as a controllable, contextualized act rather than a fixed preprocessing step. Their strength lies in adaptivity, coordination, and the capacity to handle under-specified or evolving queries. However, they introduce new challenges in training stability, latency, and system transparencyâ€”especially when retrieval is performed mid-decoding. These trade-offs, as well as efficiency-oriented enhancements like pipelining and reranking, are further explored in SectionÂ 4.
### 3.4.Robustness and Security-Oriented RAG Systems
Robustness- and security-oriented RAG systems are designed to preserve output quality in the face of noisy, irrelevant, or adversarially manipulated retrieval contexts. Unlike models that optimize retrieval or generation under ideal assumptions, these systems explicitly address worst-case scenariosâ€”such as hallucination under misleading evidence, retrieval failures, or corpus poisoning. We identify three major design strategies in this category: noise-adaptive training, hallucination-aware decoding constraints, and adversarial robustness.
Noise-Adaptive Training Objectives:These systems aim to make RAG outputs resilient to degraded or spurious input evidence by training under perturbed, irrelevant, or misleading contexts. RAAT> (Fang etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib19)> )
classifies retrieved passages into relevant, irrelevant, or counterfactual categories and introduces an adversarial training objective to maximize worst-case performance. Bottleneck Noise Filtering> (Zhu etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib90)> )
applies information bottleneck theory to identify the intersection of useful and noisy information, compressing retrieved context into minimal, high-utility representations. These approaches are particularly effective in retrieval-heavy pipelines where context precision cannot be guaranteed.
Hallucination-Aware Decoding Constraints:To mitigate factual inaccuracies in generation, several systems introduce decoding-time constraints or architectures designed to enforce grounding. RAGTruth> (Niu etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib47)> )
provides benchmarks and evaluation protocols for hallucination detection, guiding system-level design. Structured retrieval-based approaches have also been explored: one method> (HofstÃ¤tter etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib25)> )
retrieves executable templates (e.g., JSON workflows) to constrain output generation, minimizing reliance on generative interpolation and reducing domain-specific hallucination. RAG-Ex> (Sudhi etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib63)> )
simulates retrieval variability by injecting perturbed documents during training, improving robustness to inconsistent or adversarial context. In high-stakes domains such as healthcare, Confidence-Calibrated RAG> (Ozaki etÂ al
> .
> , [> 2025
](https://arxiv.org/html/2506.00054v1#bib.bib49)> )
explores how document ordering and prompt design affect both answer accuracy and model certainty.
Adversarial Robustness and Security:Emerging work also highlights new vulnerabilities. BadRAG> (Xue etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib79)> )
and TrojanRAG> (Cheng etÂ al
> .
> , [> 2024a
](https://arxiv.org/html/2506.00054v1#bib.bib9)> )
demonstrate that adversarially poisoned passages can serve as semantic backdoors, triggering specific behaviors in LLM outputs even when base models remain unmodified. These attacks rely on stealthy corpus manipulations that are hard to detect and pose significant threats in open-domain or API-exposed RAG systems.
Collectively, these systems complement retrieval- and generation-oriented architectures by offering essential safety guarantees in real-world deployments. Their robustness strategiesâ€”ranging from retrieval verification and context compression to constrained generationâ€”are modular and often integrable into existing RAG pipelines.
## 4.Enhancements in RAG
Recent advancements in Retrieval-Augmented Generation (RAG) increasingly focus on targeted enhancements across the retrievalâ€“generation pipeline. Beyond architectural baselines, these enhancements address key limitations in retrieval quality, context integration, computational efficiency, robustness to perturbations, and ranking precision. This section delineates five core areas of optimizationâ€”retrieval, filtering, efficiency, robustness, and rerankingâ€”each contributing to the development of more reliable and performant RAG systems, and collectively summarized in Table[1](https://arxiv.org/html/2506.00054v1#S4.T1), which compares representative methods based on their mechanisms, strengths, limitations, and ideal use cases.
### 4.1.Retrieval Enhancement
RAG systems have increasingly adopted smarter retrieval strategies to mitigate inefficiencies such as redundant lookups, irrelevant context, and computational overhead. These improvements can be categorized into four major families: adaptive retrieval, multi-source retrieval, query refinement, and hybrid or structured retrieval. Each addresses a distinct bottleneck in the retrieval pipeline, offering trade-offs in latency, scalability, and faithfulness.
Adaptive retrievaldynamically adjusts when to retrieve based on model uncertainty or predictive confidence. TA-ARE replaces static thresholds with a learned estimator, reducing redundant retrievals by 14.9% in short-form tasks. DRAGIN takes this further by applying retrieval at the token level, using entropy signals to detect knowledge gaps and triggering retrieval through a self-attentive query formulation process. Though it improves multi-hop QA precision, DRAGIN introduces notable inference costs, mitigated through adaptive frequency thresholds. FLARE proactively anticipates knowledge needs before uncertainty arises, improving faithfulness but requiring careful thresholding to avoid excessive retrieval.
Multi-source retrievaltargets adaptability across evolving corpora or specialized domains. AU-RAG introduces agent-based retrieval, dynamically selecting sources based on metadata heuristics. This improves domain coverage but necessitates hierarchical pipelines to manage source prioritization. SimRAG enhances retrieval precision using self-supervised learning on synthetic QA pairs, filtered via round-trip consistency. While it achieves 1.2â€“8.6% accuracy gains across datasets, it risks overfitting, mitigated by human-in-the-loop validation.
Query refinementtechniques enhance retrieval relevance by modifying ambiguous or underspecified queries. RQ-RAG uses perplexity-driven decomposition and rewriting to improve relevance, especially in multi-fact scenarios. However, this incurs inference overhead, mitigated through selective refinement based on query ambiguity.R2AGimproves post-retrieval alignment by injecting retrieval metadata into prompts, bridging the retrieverâ€“generator semantic gap. Though effective, it adds computational cost, addressed by only enabling metadata prompting when retrieval scores fall below a relevance threshold.
Hybrid and structured retrievalapproaches improve coherence by integrating unstructured and structured sources. M-RAG clusters knowledge into semantic partitions, with dual agents selecting and refining content. It reduces noise but introduces latency, mitigated by dynamic partition expansion. KRAGEN retrieves subgraphs from knowledge graphs, using Graph-of-Thoughts prompting for relational reasoning. This reduces hallucinations by 20â€“30%, though it increases memory overhead, controlled via selective node expansion.
Extending hybrid retrieval designs, the Dual-Pathway KG-RAG framework> (Xu etÂ al
> .
> , [> 2024a
](https://arxiv.org/html/2506.00054v1#bib.bib75)> )
combines structured retrieval from knowledge graphs with unstructured corpus retrieval in parallel, enhancing factual consistency and reducing hallucinations by 18% in biomedical QA tasks. Similarly, Graph RAG> (Edge etÂ al
> .
> , [> 2025
](https://arxiv.org/html/2506.00054v1#bib.bib17)> )
constructs entity-centric graphs from retrieved passages and uses community summarization to scale RAG to large corpora, improving multi-hop QA recall by 6.4 points compared to baseline retrieval. Likewise, Customer Service QA> (Xu etÂ al
> .
> , [> 2024b
](https://arxiv.org/html/2506.00054v1#bib.bib78)> )
integrates RAG with knowledge graphs constructed from issue-tracking tickets, achieving a 77.6% improvement in retrieval MRR and a 28.6% reduction in resolution time when deployed at LinkedInâ€™s customer service team.
In a complementary direction, Doan et al.> (Doan etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib16)> )
propose a lightweight hybrid retrieval strategy that combines unstructured text embeddings with structured knowledge graph embeddings without requiring complex retriever re-training, achieving up to 13.1% improvements in retrieval correctness and ranking precision in domain-specific RAG deployments.
### 4.2.Enhancing Context Relevance through Filtering
Despite advances in retrieval models, RAG systems often integrate irrelevant, redundant, or semantically noisy documents that degrade generation quality. Filtering techniques aim to reduce hallucinations and improve answer relevance by selecting only contextually appropriate content. These methods vary in supervision, granularity, and efficiency, and can be categorized into three groups: lexical/statistical filters, information-theoretic optimizers, and self-supervised passage scoring.
Lexical filterssuch as FILCO apply word overlap and statistical relevance scoring. Using STRINC and CXMI metrics, FILCO removes low-relevance passages and reduces hallucinations by up to 64%, improving EM by +8.6. However, its reliance on lexical similarity limits its adaptability across domains and query styles.
Information-theoretic methodslike IB Filtering> (Zhu etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib90)> )
use principles from the information bottleneck framework to retain only high-utility input features while discarding noise. Though computation-heavy, IB Filtering improves EM by +3.2 with a 2.5% compression ratio, offering a balance between precision and conciseness. Similarly, Stochastic Filtering models retrieval as an expected utility maximization problem and re-ranks passages based on marginal value, achieving consistent retrieval effectiveness gains with minimal retriever changes.
Self-supervised methodslike SEER and RAG-Ex use internal feedback signals to filter noisy retrievals. SEER applies label-free training and generates pseudo-relevance judgments, improving F1 by 13.5% and achieving a 9.25Ã— reduction in context length. RAG-Ex perturbs retrieved passages and compares generation outcomes, selecting those that maximize semantic consistency. It aligns with human-assessed faithfulness 76.9% of the time and is model-agnostic, though computationally expensive due to multiple inference passes.
Collectively, these methods balance retrieval compression, answer faithfulness, and domain adaptability. While lexical filters are efficient, self-supervised models provide deeper semantic filtering and support long-form reasoning.
### 4.3.Efficiency Enhancements
While Retrieval-Augmented Generation (RAG) significantly enhances factual consistency in large language models (LLMs) by integrating external document retrieval, it introduces new inefficiencies. These include increased memory overhead, latency from retrieval-processing pipelines, and redundancy in passage selection. This section synthesizes key research efforts aimed at improving retrieval efficiency across four areas: sparse retrieval and context selection, inference acceleration, caching and redundancy reduction, and retrieval faithfulness.
Sparse context selection and retrieval-aware generationtechniques aim to reduce the input length and improve semantic alignment without sacrificing output quality. Sparse RAG addresses this by filtering low-relevance content before self-attention, retaining only high-signal tokens via parallel encoding. While it builds on Fusion-in-Decoder (FiD), it improves efficiency by avoiding dense input concatenation. However, it may discard useful context under suboptimal retrieval, requiring fine-tuning to maintain robustness.R2AGtakes a complementary approach by embedding retrieval representations directly into the LLMâ€™s context space, enhancing semantic alignment. Unlike prompt-based methods (e.g., REPLUG> (Shi etÂ al
> .
> , [> 2024a
](https://arxiv.org/html/2506.00054v1#bib.bib59)> )
),R2AGbypasses explicit concatenation, reducing redundant processing. Both approaches enhance efficiency at different stages but require retriever fine-tuning and increase model complexity.
Inference accelerationstrategies focus on reducing decoding latency in autoregressive models by minimizing redundant token processing. FiD-Light achieves this through token-level passage compression, which lowers decoding time while preserving key information. Though effective, aggressive filtering can marginally reduce retrieval precision. Speculative Pipelining> (Wang etÂ al
> .
> , [> 2025
](https://arxiv.org/html/2506.00054v1#bib.bib72)> )
further reduces latency by overlapping retrieval and generation. It incrementally processes top-kð‘˜kitalic\_kcandidates before retrieval completes, lowering time-to-first-token (TTFT) by 20â€“30%. However, it risks speculative hallucinations unless controlled by fallback mechanisms and selective decoding checkpoints. This line of work opens the door for future speculative decoding architecturesâ€”discussed in SectionÂ 8â€”that balance responsiveness and reliability in low-latency applications.
Caching and redundancy reductiontechniques aim to eliminate recomputation overhead in repetitive or high-throughput workloads. RAGCache> (Jin etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib34)> )
introduces a hierarchical caching system that stores key-value tensors from prior retrievals. PGDSF extends this with prefix-aware eviction that prioritizes frequent and important documents. While these methods significantly improve efficiency in common-query settings, their impact diminishes on long-tail distributions and introduces cache complexity.
Retrieval faithfulness and answer relevancemethods go beyond lexical similarity to ensure that retrieved documents are factually aligned with the generated output. Rich Answer Encoding (RAE) addresses this using a Retriever-as-Answer Classifier (RAC) and Dense Knowledge Similarity (DKS), which rescore documents based on their plausibility. RAE reduces hallucinations and improves grounding but requires retriever retraining, increasing cost.
Taken together, these optimization strategies enhance efficiency across the RAG pipeline: Sparse RAG andR2AGimprove alignment between retrieved documents and generation; FiD-Light and Speculative Pipelining reduce latency during inference; RAGCache and PGDSF minimize recomputation in high-throughput environments; and RAE advances retrieval faithfulness. Collectively, they represent a move toward more scalable, accurate, and computationally efficient RAG systems.
### 4.4.Enhancing Robustness
RAG systems improve factual accuracy in language models by retrieving external information. However, they remain vulnerable to retrieval noise, hallucinations, and adversarial attacks. While past research has addressed these challenges separatelyâ€”such as noise resilience, hallucination control, and retrieval securityâ€”a unified perspective is essential. This section groups robustness techniques into three areas: noise mitigation, hallucination reduction, and security defenses.
Empirical studies further support this need for a unified view; a recent study identifies seven recurrent failure points in operational RAG systems, spanning retrieval errors, context consolidation failures, hallucinated outputs, and incomplete answers> (Barnett etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib5)> )
.
Noise mitigationstrategies target irrelevant, misleading, or adversarial content that can degrade RAG accuracy. However, recent work challenges the assumption that all retrieval noise is detrimental; Cuconasu et al.> (Cuconasu etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib15)> )
demonstrate that carefully positioned random documents can paradoxically improve LLM reasoning and answer quality by promoting evidence selection behaviors. Two contrasting approaches address this: RAAT and CRAG. RAAT uses adversarial pretraining to expose models to subtle and counterfactual retrieval noise, improving F1/EM scores by 20â€“30%. Its high training cost limits it to static, high-stakes domains. CRAG filters low-confidence retrievals at inference time and works well in real-time systems, reducing retrieval errors by 12â€“18%. However, it struggles with â€œsoft noise,â€ where superficially relevant content misleads the model.
Hallucination reductiontechniques such as Structured RAG> (Ayala and Bechard, [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib3)> )
and IM-RAG aim to improve the faithfulness of generated content. Structured RAG constrains retrieval to verified corpora, lowering hallucination rates by 30â€“40% with minimal compute cost. Its drawback is poor adaptability, requiring manual updates. IM-RAG uses iterative retrieval refinement, achieving +5.3 F1 / +7.2 EM on HotPotQA. Though more accurate in evolving domains, it is computationally intensive and slower at inference.
Security defensesfocus on adversarial threats such as data poisoning and backdoor attacks. Research into BadRAG shows that poisoning just 0.04% of a corpus can lead to a 98.2% attack success rate and 74.6% system failure. Defenses like cryptographic document signing or adversarial filtering are only partially effective. TrojanRAG embeds backdoors in retrieval embeddings, bypassing traditional sanitization. Stronger mitigationsâ€”secure training and integrity validationâ€”are needed but require proactive design. Beyond adversarial attacks, privacy vulnerabilities in RAG systems have also been identified; Zeng et al.> (Zeng etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib86)> )
show that both retrieval databases and pretraining corpora can be exploited through structured prompting, although retrieval can paradoxically help reduce memorization leakage by acting as a grounding mechanism.
### 4.5.Enhancements and Optimizations in Reranking
Reranking plays a vital role in improving the relevance and faithfulness of Retrieval-Augmented Generation (RAG) outputs. While initial retrieval stages often return noisy results, reranking refines document ordering before passage selection and generation, reducing hallucinations and improving response accuracy. Recent work advances reranking across three key areas: adaptive reranking, unified pipelines, and fusion-based reranking.
Adaptive rerankingmethods dynamically adjust the number of documents reranked based on query complexity. RLT> (Meng etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib45)> )
uses ranked list truncation to improve MRR/nDCG while reducing retrieval noise by 15%. ToolRerank further adapts reranking depth based on familiarity with seen vs. unseen tools, boosting recall by 12% in hierarchical retrieval tasks. These methods optimize computation by avoiding unnecessary reranking in low-complexity scenarios.
Unified reranking pipelinescombine retrieval, document ranking, and generation within a single architecture. RankRAG fine-tunes a language model to jointly score documents and generate answers, improving MRR@10 by 7.8% while reducing latency. uRAG extends this to multiple tasksâ€”like QA and fact verificationâ€”using shared reranking logic and user-feedback signals, improving cross-task generalization by 8% MRR@10. These approaches eliminate the overhead of separate ranking modules and increase retrieval consistency.
Fusion-based rerankingstrategies aggregate evidence from multiple query variants to improve answer robustness. RAG-Fusion generates multiple subqueries and applies reciprocal rank fusion, improving answer accuracy by 9%.R2AGrefines these rankings iteratively, reducing irrelevant retrievals by 15% through recursive feedback. These models are especially effective for multi-hop and ambiguous tasks.
Reranking methods significantly boost the efficiency and faithfulness of RAG systems. Future work may explore hybrid approaches combining adaptive truncation with fusion-based aggregation, as well as domain-adaptive reranking for enterprise scalability. As RAG expands to more tasks and domains, reranking will remain essential to enabling context-aware, trustworthy generation.
Table 1.Summary of RAG System Enhancements.This table categorizes enhancements across five dimensionsâ€”retrieval, filtering, efficiency, robustness, and reranking. Each entry specifies the enhancement type, method, mechanism, key strengths, known limitations, and ideal use cases.
Enhancement Type|Category|Method|Mechanism|Strengths|Limitations|Best Use Case|
Retrieval|Adaptive|TA-ARE|Dynamic confidence estimation|Reduces redundant retrieval|Estimator latency|Short-form QA|
Adaptive|DRAGIN|Token-level entropy-based triggers|Improves multi-hop QA precision|High inference cost|Multi-hop QA|
Adaptive|FLARE|Preemptive uncertainty detection|Enhances faithfulness|Risk of over-retrieval|Long-form generation|
Multi-source|AU-RAG|Agent-based source selection|High domain adaptability|Source management overhead|Evolving corpora|
Multi-source|SimRAG|Synthetic QA + round-trip filtering|Cross-domain accuracy gains|Overfitting risk|Specialized domains|
Query|RQ-RAG|Perplexity-based query rewriting|Improves query clarity and relevance|Additional inference steps|Multi-fact queries|
Query|R2AG|Retrieval-aware prompt injection|Enhances factual grounding|Prompt expansion overhead|Low-confidence queries|
Hybrid|M-RAG|Semantic partitioning + dual agents|Reduces retrieval noise|Partition latency|Context-heavy reasoning|
Hybrid|KRAGEN|Knowledge graph subgraph retrieval|Improves structured reasoning|Memory and compute intensive|Biomedical, graph-based tasks|
Filtering|Lexical|FILCO|STRINC + CXMI scoring|+8.6 EM, 64% hallucination reduction|Query-style bias|Structured QA|
Info-Theoretic|IB Filtering|Bottleneck-based compression|+3.2 EM, 2.5% compression|Computation overhead|High-precision QA|
Info-Theoretic|Stochastic Filtering|Utility-maximizing re-ranking|Improves effectiveness|Needs custom scoring|Lightweight retrieval tasks|
Self-Supervised|SEER|Pseudo-relevance via self-training|+13.5% F1, 9.25x context reduction|High training cost|Open-domain QA|
Self-Supervised|RAG-Ex|Generation perturbation comparison|76.9% human-aligned faithfulness|Multiple inference passes|Faithful generation|
Efficiency|Sparse Selection|Sparse RAG|Retains high-signal tokens|Reduces memory, improves relevance|May discard useful docs|Long-context tasks|
Sparse Selection|R2AG|Context-aware retrieval injection|Enhances coherence, lowers redundancy|Retriever fine-tuning needed|Knowledge-intensive QA|
Inference Acceleration|FiD-Light|Compresses passages|Faster decoding|Slight loss in recall|Low-latency applications|
Caching|Speculative Pipelining|Overlaps retrieval and generation|20â€“50% TTFT reduction|Risk of hallucination|Real-time applications|
Caching|RAGCache|Hierarchical cache w/ PGDSF|Eliminates recomputation|Cache complexity in long-tail|High-throughput workloads|
Retrieval Quality|RAE|Retriever-as-answer scorer|Boosts grounding and precision|Requires scoring/retraining|Factual QA|
Robustness|Noise Mitigation|RAAT|Adversarial training|+20â€“30% F1/EM|High training cost|Offline pretraining|
Noise Mitigation|CRAG|Inference-time filtering|+12â€“18% precision gain|Ineffective on â€œsoftâ€ noise|Real-time support|
Hallucination Control|Structured RAG|Curated corpus retrieval|30â€“40% hallucination reduction|Low adaptability|Static domains|
Hallucination Control|IM-RAG|Iterative retrieval refinement|+5.3 F1 / +7.2 EM|Inference latency|Multi-hop QA|
Security|BadRAG|Adversarial retrieval poisoning|Demonstrates corpus-level threat|Needs stronger filtering|Security evaluation|
Security|TrojanRAG|Embedding-level backdoor|Persistent attack vector|Requires secure training|Security-sensitive pipelines|
Reranking|Adaptive|RLT|Dynamic list truncation|+15% noise reduction|Heuristic tuning needed|Real-time QA|
Adaptive|ToolRerank|Familiarity-aware reranking|+12% recall for unseen tools|Complexity for unseen/frequent tools|Tool-aware retrieval|
Unified Pipeline|RankRAG|Joint rerank + generate|+7.8% MRR@10|Domain-specific tuning|End-to-end QA systems|
Unified Pipeline|uRAG|Shared reranking engine|+8% MRR@10, task generalization|Higher setup cost|Multi-task enterprise RAG|
Fusion-based|RAG-Fusion|Reciprocal rank fusion|+9% accuracy|Query explosion risk|Complex multi-hop QA|
Fusion-based|R2AG|Recursive reranking refinement|15% irrelevant retrieval reduction|Higher latency|Iterative reasoning|
## 5.Comparative Analysis
To assess the empirical effectiveness of design innovations in Retrieval-Augmented Generation (RAG), this section presents a comparative analysis of representative frameworks across three key evaluation settings: short-form question answering, multi-hop reasoning, and robustness under retrieval perturbations. Results are reported as relative improvements over both raw and retrieval-augmented baselines, normalized for model and dataset variability. Additionally, we review ablation studies from the literature to disentangle the contributions of specific components such as retrieval triggers, filtering layers, reranking mechanisms, and robustness modules. These insights offer a clearer understanding of which enhancements most significantly impact performance, faithfulness, and efficiency across diverse RAG configurations.
Table 2.Comparative Performance of Retrieval-Augmented Generation Frameworks Across Multi-Hop and Short-Form QA Benchmarks.This table reports relative performance improvements achieved by each RAG framework over two baselines: (i) the raw backbone language model (B) and (ii) the same model augmented with a standard retrieval module (B+R). Results are shown across multi-hop benchmarks (HotpotQA> (Yang etÂ al
> .
> , [> 2018
](https://arxiv.org/html/2506.00054v1#bib.bib82)> )
, 2Wiki> (Ho etÂ al
> .
> , [> 2020
](https://arxiv.org/html/2506.00054v1#bib.bib24)> )
, MuSiQue> (Trivedi etÂ al
> .
> , [> 2022
](https://arxiv.org/html/2506.00054v1#bib.bib68)> )
) and short-form QA datasets (PopQA> (Mallen etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib43)> )
, TriviaQA> (Joshi etÂ al
> .
> , [> 2017
](https://arxiv.org/html/2506.00054v1#bib.bib36)> )
, ARC-Challenge> (Clark etÂ al
> .
> , [> 2018
](https://arxiv.org/html/2506.00054v1#bib.bib13)> )
, NQ> (Kwiatkowski etÂ al
> .
> , [> 2019
](https://arxiv.org/html/2506.00054v1#bib.bib38)> )
), with metrics including F1, Exact Match (EM), and Accuracy (Acc). Frameworks are grouped by architectural category: retriever-based, generator-based, and hybrid. A â€œâ€“â€indicates that the corresponding score was not reported in the original publication. Backbone LLMs referenced in this table include LLaMA 2> (Touvron etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib67)> )
, LLaMA 3> (Grattafiori etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib22)> )
, GPT-3.5/4> (OpenAI etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib48)> )
, Vicuna> (Chiang etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib12)> )
, Mistral> (Jiang etÂ al
> .
> , [> 2023a
](https://arxiv.org/html/2506.00054v1#bib.bib30)> )
, Mixtral> (Jiang etÂ al
> .
> , [> 2024b
](https://arxiv.org/html/2506.00054v1#bib.bib31)> )
, Gemini> (Reid etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib54)> )
, and Gemma> (Team etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib65)> )
.
|Framework|Backbone|HotpotQA|2Wiki|MusiQue|PopQA|TriviaQA|ARC-Challenge|NQ|
||B/B+R|B/B+R|B/B+R|B/B+R|B/B+R|B/B+R|B/B+R|
Retriever-Based RAG|
RQ-RAG|LLaMA2-7B|8.485/2.749 (F1)|1.8/1.396 (F1)|12.9/4.635 (F1)|2.884/0.434 (Acc)|-/-|2.133/1.379 (Acc)|-/-|
SimRAG|LLaMA3-8B|-/-|-/-|-/-|-/-|-/-|0.145/- (Acc)|-/-|
SimRAG|Gemma2-27B|-/-|-/-|-/-|-/-|-/-|0.034/- (Acc)|-/-|
SEER|LLaMA2-7B-Chat|0.104/0.037 (F1)|-/-|-/-|-/-|-/-|-/-|-/-|
RankRAG|LLaMA3-8B|-/0.079 (F1)|-/0.323 (F1)|-/-|-/-|-/-|-/-|-/-|
RankRAG|LLaMA3-70B|-/0.242 (F1)|-/0.376 (F1)|-/-|-/-|-/-|-/-|-/-|
LQR|LLaMA3-8B|2.081/0.516 (F1)|0.706/0.141 (F1)|2.922/0.841 (F1)|-/-|-/-|-/-|-/-|
LongRAG|GPT-4o|0.517/- (EM)|-/-|-/-|-/-|-/-|-/-|-/-|
LongRAG|Gemini-1.5-Pro|0.696/- (EM)|-/-|-/-|-/-|-/-|-/-|-/-|
FILCO|LLaMA2-7B|-/0.057 (EM)|-/-|-/-|-/-|-/0.056 (EM)|-/-|-/0.298 (EM)|
Re2G|BART Large|-/-|-/-|-/-|-/-|0.251/- (Acc)|-/-|0.144/-|
Generator-Based RAG|
xRAG|Mistral-7B|0.26/-0.122 (EM)|-/-|-/-|-/-|0.152/-0.002 (EM)|-/-|0.293/-0.085 (EM)|
xRAG|Mixtral-8x7B|0.207/-0.087 (EM)|-/-|-/-|-/-|0.043/0.054 (EM)|-/-|0.126/0.047 (EM)|
INFO-RAG|LLaMA2-7B|0.182/- (EM)|-/-|0.163/- (EM)|-/-|-/-|-/-|-/-|
INFO-RAG|LLaMA2-13B|0.222/- (EM)|-/-|0.358/- (EM)|-/-|-/-|-/-|-/-|
INFO-RAG|LLaMA2-13B-chat|0.011/- (EM)|-/-|0.018/- (EM)|-/-|-/-|-/-|-/-|
SELF-RAG|LLaMA2-7B|-/-|-/-|-/-|2.735/0.437 (Acc)|1.177/0.562 (Acc)|2.092/0.404 (Acc)|-0.505/- (Acc)|
SELF-RAG|LLaMA2-13B|-/-|-/-|-/-|2.796/0.221 (Acc)|0.8/0.474 (Acc)|-/-|-/-|
FiD-Light|FiD+DPR|-/-|-/-|-/-|-/-|0.185/- (EM)|-/-|0.27/- (EM)|
R2AG|LLaMA2-7B|3.231/- (F1)|34.52/4.445 (F1)|-/-|-/-|-/-|-/-|0.824/- (Acc)|
Hybrid RAG|
DRAGIN|LLaMA2-7B-chat|0.218/0.338 (F1)|0.311/0.148 (F1)|-/-|-/-|-/-|-/-|-/-|
DRAGIN|LLaMA2-13B-chat|0.368/0.144 (F1)|0.445/0.169 (F1)|-/-|-/-|-/-|-/-|-/-|
DRAGIN|Vicuna-13B-v1.5|0.279/0.179 (F1)|0.575/0.371 (F1)|-/-|-/-|-/-|-/-|-/-|
FLAREdirect|GPT-3.5|-/-|0.622/0.223 (F1)|-/-|-/-|-/-|-/-|-/-|
FLAREinstruct|GPT-3.5|-/-|0.353/0.02 (F1)|-/-|-/-|-/-|-/-|-/-|
GenGround|GPT-3.5|0.236/0.093 (F1)|0.219/0.122 (F1)|0.359/0.361 (F1)|-/-|-/-|-/-|-/-|
Stochastic RAG|FiD-Light (T5-Base)|0.066/- (F1)|-/-|-/-|-/-|-/0.036 (EM)|-/-|-/0.013 (EM)|
Stochastic RAG|FiD-Light (T5-XL)|0.065/- (F1)|-/-|-/-|-/-|-/0.016 (EM)|-/-|-/0.037 (EM)|
CRAG|LLaMA2-7B|-/-|-/-|-/-|3.034/0.471 (Acc)|-/-|1.514/0.173 (Acc)|0.045/- (Acc)|
Self-CRAG|LLaMA2-7B|-/-|-/-|-/-|3.204/0.533 (Acc)|-/-|2.083/0.439 (Acc)|-/-|
TA-ARE|GPT-3.5|-/-|-/-|-/-|-/-|-/-|-/-|-/-|
TA-ARE|GPT-4|-/-|-/-|-/-|-/-|-/-|-/-|-/-|
TA-ARE|LLaMA2-7B|-/-|-/-|-/-|-/-|-/-|-/-|-/-|
### 5.1.Comparative Analysis of Framework Performance on Short-Form QA
This section presents a comparative analysis of Retrieval-Augmented Generation (RAG) frameworks in short-form question answering, emphasizing their relative improvements over raw large language model (LLM) baselines and retrieval-augmented baselines. As shown in Table[2](https://arxiv.org/html/2506.00054v1#S5.T2), these comparisons focus on relative gains (e.g., a value of 2.7 indicates a 270% improvement) rather than absolute performance metrics, which normalize for variations in backbone architectures, prompting strategies, and evaluation protocols. This approach enables a meaningful comparison across diverse experimental setups.
Among generator-based RAG systems primarily optimized for accuracy, SELF-RAG consistently demonstrates substantial gains across multiple datasets. It achieves over a 270% improvement from the raw LLM baseline on PopQA> (Mallen etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib43)> )
and over 200% on ARC-Challenge> (Clark etÂ al
> .
> , [> 2018
](https://arxiv.org/html/2506.00054v1#bib.bib13)> )
, illustrating the effectiveness of deep context integration for enhancing short-form factual recall. FiD-Light, although also a generator-side enhancement, adopts a different optimization philosophy centered on lightweight, efficient fusion of retrieved documents during decoding, yielding more moderate improvements of 18â€“27% across TriviaQA> (Joshi etÂ al
> .
> , [> 2017
](https://arxiv.org/html/2506.00054v1#bib.bib36)> )
and NQ> (Kwiatkowski etÂ al
> .
> , [> 2019
](https://arxiv.org/html/2506.00054v1#bib.bib38)> )
. R2AG, another generator-based approach, shows promising gains, with over 80% improvement from the baseline on NQ, further validating the benefits of integrating retrieval signals within generation. We note that generator-based frameworks primarily designed for efficiency, such as xRAG, are discussed separately due to their distinct optimization focus.
Retriever-based frameworks such as RQ-RAG and SimRAG also demonstrate notable gains. RQ-RAG achieves a 288% improvement on PopQA and over 210% on ARC-Challenge, reaffirming the importance of retrieval quality in evidence-centric QA. SimRAG also shows strong improvements on ARC, although gains are more modest (approximately 14%). Additionally, retriever-side re-ranking approaches like FILCO deliver moderate but meaningful gains, with 5â€“30% improvements across NQ and TriviaQA, further highlighting the incremental value of retrieval refinement strategies.
Hybrid frameworks exhibit a more heterogeneous pattern. CRAG and Self-CRAG achieve impressive gains, with Self-CRAG delivering a 320% improvement on PopQA and a 208% improvement on ARC-Challenge, suggesting that combining retrieval refinement with generation adaptation can be highly effective when well aligned. However, TA-ARE, despite achieving a significant 28Ã—\\timesÃ—improvement over raw baselines on RetrievalQA, occasionally underperforms relative to the standard retrieval baseline, indicating that retrieval frequency reduction strategies, while efficient, may introduce trade-offs. Stochastic RAG frameworks, meanwhile, display relatively modest gains (typically under 4%), reflecting that introducing retrieval randomness increases diversity without consistently boosting short-form QA accuracy.
Efficiency-focused generator-based systems such as xRAG exhibit mixed results. While xRAG achieves 10â€“29% improvements over raw LLM baselines on datasets such as NQ and TriviaQA, its gains over retrieval baselines are marginal or occasionally negative. This suggests that while resource-efficient designs are promising for scaling RAG systems, further optimization is needed to maintain competitive factual accuracy in short-form tasks.
Finally, robustness-oriented frameworks such as RAAT demonstrate strong performance, with a 116% improvement from the raw baseline and over 27% gain compared to retrieval on RAG-Benchâ€”a robustness-focused variant of NQ, WebQ, and TriviaQA. Although evaluated under challenging retrieval noise conditions, RAATâ€™s results suggest that robustness-driven retrieval strategies can effectively complement factual QA objectives.
Overall, retrieval- and generation-enhanced frameworks deliver substantial relative gains in short-form QA, while hybrid and efficiency-focused approaches offer promising but variable results depending on dataset and retrieval complexity. These findings underscore the critical role of retrieval optimization and generation-adaptive strategies in advancing retrieval-augmented short-form question answering.
### 5.2.Comparative Analysis of Framework Performance on Multi-Hop QA
A comparative evaluation of various Retrieval-Augmented Generation (RAG) frameworks reveals distinct patterns in their ability to enhance multi-hop question answering, assessed through improvements over both raw large language models (LLMs) and standard retrieval-augmented baselines. Similar to the previous section, this analysis focuses on relative gains rather than absolute scores to normalize for architectural and experimental variations. The results, summarized in Table[2](https://arxiv.org/html/2506.00054v1#S5.T2), enable a consistent comparison of framework contributions across diverse multi-hop QA settings.
Among retrieval-based RAG systems, models such as RQ-RAG, RankRAG, LQR, and LongRAG demonstrate substantial relative gains. Notably, RQ-RAG achieves over an 800% improvement from its raw LLM baseline on HotpotQA> (Yang etÂ al
> .
> , [> 2018
](https://arxiv.org/html/2506.00054v1#bib.bib82)> )
, and a 275% improvement over standard retrieval, highlighting the effectiveness of sophisticated query decomposition techniques in multi-hop settings. Similarly, LQR achieves a 292% improvement from the raw baseline and an 84% improvement over retrieval in the MuSiQue dataset> (Trivedi etÂ al
> .
> , [> 2022
](https://arxiv.org/html/2506.00054v1#bib.bib68)> )
, suggesting that intelligent retrieval-ranking substantially boosts multi-hop reasoning. LongRAG also exhibits strong performance, improving by over 50% from the raw LLM baseline on HotpotQA, further emphasizing the value of extended retrieval for complex question answering. These patterns collectively affirm that optimizing retrieval quality remains a dominant driver of performance gains in multi-hop RAG applications.
Generator-based RAG frameworks, including R2AG, INFO-RAG, and xRAG, display more varied relative improvements. R2AG shows consistent strong gains, improving by over 300% relative to the baseline on HotpotQA, demonstrating the benefits of tightly integrating retrieval signals into the generation process. In contrast, INFO-RAG exhibits more modest improvements, with relative gains around 16â€“35% across different backbones and datasets, suggesting that while generator-side augmentations enhance output faithfulness, their standalone effect may be limited without concurrent retrieval refinement. xRAG, while improving from raw baselines by approximately 20â€“26%, shows negative or marginal gains compared to the retrieval baseline in some settings, indicating that extreme context compression, although efficient, may compromise the modelâ€™s ability to utilize retrieved evidence effectively for complex multi-hop reasoning.
Hybrid RAG frameworks, such as DRAGIN, FLARE, GenGround, and Stochastic RAG, present a diverse range of outcomes. DRAGIN frameworks achieve moderate improvements, typically ranging between 22â€“44% over raw LLMs and 14â€“34% over retrieval baselines, reflecting the incremental gains from dynamically adapting retrieval to evolving information needs. FLAREdirect stands out, achieving a 62% improvement from the raw LLM and a 22% improvement over standard retrieval on 2Wiki> (Ho etÂ al
> .
> , [> 2020
](https://arxiv.org/html/2506.00054v1#bib.bib24)> )
, suggesting that model-guided active retrieval significantly strengthens multi-hop evidence gathering. GenGround reports relatively smaller improvements (13â€“36% from the baseline) but is evaluated against already-strong baselines, which partially accounts for the more conservative gains. Stochastic RAG frameworks offer consistent yet modest gains (6%), indicating that introducing randomness into retrieval can modestly diversify and enhance evidence coverage without destabilizing performance.
Overall, retrieval-based RAG frameworks demonstrate the most consistent and substantial improvements across multi-hop QA tasks, particularly when retrieval quality, ranking, and query decomposition are optimized. Generator-based adaptations, while beneficial in specific cases, often require complementary retrieval-side enhancements to realize their full potential. Hybrid frameworks offer promising but more variable results, underscoring the challenge of harmonizing retrieval and generation strategies dynamically. These findings highlight retrieval optimization as a critical lever for advancing complex reasoning capabilities in RAG systems.
### 5.3.Comparative Robustness Analysis: Framework Gains Over Retrieval-Only Baselines
To assess robustness in Retrieval-Augmented Generation (RAG) systems, we report incremental improvements each framework achieves over its retrieval-augmented LLM baseline. This isolates the added value of mechanisms such as critique, reranking, and filtering, independent of the baseline retrieval gain. Evaluations span multiple datasets and focus on gains in precision, recall, and FactScore. By standardizing on relative improvements, the analysis enables fair comparisons across models with differing backbone architectures. A summary of these results is provided in Table[3](https://arxiv.org/html/2506.00054v1#S5.T3).
Among hybrid systems, the most substantial gains in factual consistency are observed. Self-CRAG yields the highest FactScore improvementâ€”+0.456 on the Biography dataset> (Min etÂ al
> .
> , [> 2023
](https://arxiv.org/html/2506.00054v1#bib.bib46)> )
â€”significantly surpassing other frameworks, most of which reportâ‰¤\\leqâ‰¤0.05 gains. The multi-sentence compositional nature of the Biography task likely benefits from Self-CRAGâ€™s feedback-based reranking and correction loop, which aligns generation with retrieved evidence. Comparable improvements are evident with Self-RAG and CRAG, reporting +0.372 and +0.252 gains on the same dataset, underscoring the importance of evidence-aware generation refinement. On 2Wiki, Flare-Direct improves both precision and recall by +21.6%, while Flare-Instructâ€”despite using the same retrieval backboneâ€”offers negligible gains, illustrating how prompt design alone can meaningfully impact robustness in multi-hop settings. In contrast, Stochastic RAG shows only marginal FactScore gains (â‰¤\\leqâ‰¤+0.008) on Fever, suggesting that entropy-driven retrieval without subsequent verification may be insufficient to ensure factual reliability.
Generator-based systems present more variable, task-dependent performance. SELF-RAG, evaluated on ASQA> (Stelmakh etÂ al
> .
> , [> 2022
](https://arxiv.org/html/2506.00054v1#bib.bib61)> )
, achieves sizable improvements in precision (+22â€“30%) and recall (+16â€“19%), though its FactScore gains remain modest (+0.03â€“0.04), implying improved evidence usage without equivalent advances in factual accuracy. DRAGIN similarly improves precision and recall by +9â€“22% on HotPotQA, leveraging entropy-based token-level triggers suited for multi-hop reasoning. However, lacking reported FactScore, its contribution to factual consistency remains indeterminate. Other generator-oriented systems, including GenRT and Rich Answer Encoding, achieve smaller recall gains (â‰¤\\leqâ‰¤+0.1) on datasets such as TriviaQA, KILT-WoW> (Petroni etÂ al
> .
> , [> 2021
](https://arxiv.org/html/2506.00054v1#bib.bib50)> )
, and MSMARCO> (Bajaj etÂ al
> .
> , [> 2018
](https://arxiv.org/html/2506.00054v1#bib.bib4)> )
. These modest improvements suggest better document selection but limited post-retrieval validation, constraining their robustness impact.
Retriever-based systems exhibit consistent yet comparatively modest gains. Re2G reports +17.8% precision and +15.9% recall on TriviaQA, reflecting the benefits of retrieval-aware prompt optimization. FILCO, by contrast, improves precision by +3.25% on Fever but fails to enhance recall or FactScore, indicating that filtering irrelevant context improves selectivity, but without downstream verification, its robustness contribution is limited. Not all frameworks report all three metrics across datasets; while relative improvement facilitates normalization, incomplete coverageâ€”particularly of FactScoreâ€”may obscure the full extent of a systemâ€™s capabilities.
In sum, Self-CRAG on Biography delivers the strongest FactScore gain (+0.456), while SELF-RAG on ASQA achieves the best precision (+29.56%) and recall (+18.81%) improvements. Flare-Direct, outperforming Flare-Instruct by over 20% on 2Wiki, highlights the sensitivity of robustness to prompt design. At the lower end, Stochastic RAG on FEVER> (Thorne etÂ al
> .
> , [> 2018
](https://arxiv.org/html/2506.00054v1#bib.bib66)> )
records the smallest impact (â‰¤\\leqâ‰¤+0.008 FactScore), reinforcing the necessity of combining retrieval strategies with downstream verification to enhance factual fidelity.
Collectively, these findings affirm that retrieval alone is insufficient for robust generation. The most effective frameworks tightly couple retrieval, generation, and verification in iterative loops, ensuring that generation is guided by critique and alignment rather than treated as a terminal step.
Table 3.Comparative Robustness Analysis of RAG Frameworks Across Architectures.Relative improvements in precision, recall, and FactScore over retrieval-augmented baselines across multiple datasets. A dash (â€“) denotes missing values in the original paper.Taxonomy|Framework|Backbone|Dataset|Precision|Recall|FactScore|
Retriever-based RAG|Re2G|KGI0|NQ|0.096984|0.074569|â€“|
Re2G|KGI1|TriviaQA|0.177981|0.159062|â€“|
Re2G|KGI2|Fever|0.120986|0.073732|â€“|
FILCO|RAG|Fever|3.25|â€“|â€“|
Generation-based RAG|SELF-RAG|LLaMA2-7B|ASQA|22.06897|15.95|0.041026|
SELF-RAG|LLaMA2-7B|ASQA|29.56522|18.80556|0.034839|
Rich Answer Encoding|RAG|MSMARCO|â€“|0.086957|â€“|
Rich Answer Encoding|RAG|KILT-WoW|â€“|0.107293|â€“|
DRAGIN|LLaMA2-13B|HotPotQA|0.185934|0.09893|â€“|
DRAGIN|VICUNA-13B|HotPotQA|0.222447|0.105114|â€“|
GenRT|RAG|NQ|â€“|0.023232|â€“|
GenRT|RAG|TriviaQA|â€“|0.026239|â€“|
Hybrid RAG|CRAG|LLaMA2-7B|Biography|â€“|â€“|0.251689|
Self-CRAG|LLaMA2-7B|Biography|â€“|â€“|0.456081|
Flare-Instruct|GPT-3.5|2Wiki|0.010288|0.019417|â€“|
Flare-Direct|GPT-3.5|2Wiki|0.216049|0.215534|â€“|
Stochastic RAG|FiD-Light (T5-Base)|Fever|â€“|â€“|0.008685|
Stochastic RAG|FiD-Light (T5-XL)|Fever|â€“|â€“|0.00355|
### 5.4.Ablation Studies
Ablation studies serve as a crucial methodological lens for disentangling the contributions of individual components in Retrieval-Augmented Generation (RAG) frameworks. Across the surveyed literature, these studies primarily target retrieval triggers, filtering layers, reranking strategies, compression modules, and corrective mechanisms, offering empirical insights into performance, efficiency, and robustness.
Adaptive Retrieval and Query Reformulation.Frameworks such as TA-ARE, FLARE, and IM-RAG demonstrate that retrieval adaptivity is central to long-form and multi-hop reasoning. Ablating dynamic query triggers (e.g., forward-looking or self-reflective prompts) consistently results in degraded factual accuracy and increased hallucinations, confirming the value of retrieval-awareness throughout the generation process.
Filtering, Reranking, and Evidence Quality.Systems like SEER, CRAG, and Re2G show that context filtering, reranking, and correction layers significantly influence downstream performance. Ablations reveal that removing context evaluators or decomposing mechanisms leads to verbosity and reduced grounding fidelity. Notably, reranking-truncation co-designs (e.g., in GenRT and ToolRerank) outperform static top-kð‘˜kitalic\_kapproaches by improving answer faithfulness and retrieval precision.
Compression and Efficiency Trade-offs.FiD-Light and RAGCache demonstrate that passage compression and caching can substantially reduce latency without compromising accuracy. Ablating vector sparsity or caching mechanisms (e.g., speculative pipelining or prefix-aware replacement) increases inference time up to 4Ã—\\timesÃ—, underscoring the operational significance of architectural optimization in production RAG systems.
Robustness and Security.Studies like BadRAG and TrojanRAG emphasize that security-focused ablations reveal novel vulnerabilities. Even lightweight retrieval poisoning or trigger crafting can steer model outputs, while mitigation strategies (e.g., summarization, distance thresholds) offer partial resilience but require further study.
Synthesis.Ablation studies consistently reinforce that high-performing RAG frameworks are modular, with complementary retrieval, filtering, and generation components. Performance degradation in ablation settings not only validates novel modules but also guides design toward more interpretable, efficient, and secure RAG pipelines.
## 6.Evaluation and Benchmarking of RAG Systems
Retrieval-Augmented Generation (RAG) systems introduce unique challenges for evaluation due to their hybrid architecture combining a retriever and a generator. Accurate evaluation demands assessing multiple interdependent components, including retrieval relevance, faithfulness of generated responses, and overall answer utility. In this section, we synthesize recent advancements in automated evaluation frameworks, retrieval quality assessment techniques, and benchmark construction to provide a comprehensive overview of evaluation practices in RAG systems.
### 6.1.Evaluation Dimensions
The core dimensions> (Saad-Falcon etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib56)> )
used to evaluate RAG systems include:
1. (1)
Context Relevance:Measures how pertinent the retrieved documents are to the input query.
2. (2)
Answer Faithfulness:Assesses whether the generated output remains grounded in the retrieved evidence.
3. (3)
Answer Relevance:Evaluates whether the output adequately addresses the user query.
These dimensions are interdependent: poor context relevance often cascades into reduced faithfulness and answer relevance, underscoring the need for joint evaluation. Frameworks such as ARES and RAGAS have formalized these dimensions, incorporating both automated judgment and reference-free evaluation.
### 6.2.Automated Evaluation Frameworks
ARES> (Saad-Falcon etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib56)> )
introduces an LLM-based judge system that uses few-shot prompted language models to generate synthetic datasets. These judges are trained on three classification tasks corresponding to the core dimensions and use prediction-powered inference (PPI) to align model-based scoring with human judgment. ARES shows significant improvements in accuracy and annotation efficiency, outperforming RAGAS> (Es etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib18)> )
by up to 59.3 percentage points in context relevance.
RAGAS employs a modular framework that decomposes generated answers into atomic factual statements, then evaluates each against the retrieved context using LLMs. This structure provides high-resolution feedback, revealing which parts of an answer are hallucinated.
These frameworks automate the evaluation of faithfulness, grounding, and contextual relevanceâ€”enabling scalable, reference-free analysis of RAG performance.
### 6.3.Evaluating Retrieval Quality
eRAG> (Salemi and Zamani, [> 2024a
](https://arxiv.org/html/2506.00054v1#bib.bib57)> )
challenges traditional relevance label techniques by applying the RAG generator to each retrieved document individually. The performance of each document, assessed via downstream task metrics, serves as a relevance label. This method provides a retrieval-aware, document-level granularity and has shown significantly improved correlation with actual RAG performance.
INFO-RAG introduces an unsupervised training paradigm that improves the LLMâ€™s ability to refine retrieved information under three scenarios: redundant, noisy, or insufficient context. By viewing the LLM as an â€œinformation refiner,â€ it enables the model to extract relevant content, reject misinformation, and infer missing detailsâ€”enhancing retrieval robustness without supervised relevance labels.
uRAG proposes a unified retrieval system that serves multiple RAG models across diverse downstream tasks. It introduces a shared reranker trained on feedback signals (e.g., EM, accuracy) from various black-box LLMs, treating each LLM as a user of the search engine. uRAGâ€™s training protocol enables evaluation and optimization of retrieval based on downstream task performance, offering retrieval diagnostics grounded in actual utility rather than surface similarity.
### 6.4.Benchmarking RAG Capabilities
As RAG systems mature, a growing suite of benchmarks has emerged to evaluate them across dimensions like robustness, factuality, adaptivity, and domain sensitivity. These benchmarks not only reflect the evolving needs of real-world RAG deployments but also shape future directions by surfacing recurrent failure modes and task-specific limitations.
Robustness to retrieval noiseis a core requirement in operational RAG systems. RGB> (Chen etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib8)> )
evaluates four fundamental capacitiesâ€”noise robustness, negative rejection, information integration, and counterfactual resistanceâ€”revealing consistent weaknesses in LLMs when handling distracting or misleading context. Complementing this, RAG-Bench> (Fang etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib19)> )
introduces a noise-centric benchmark simulating three retrieval corruption typesâ€”relevant-but-incomplete, irrelevant, and counterfactualâ€”and applies adaptive adversarial training to improve model tolerance. These benchmarks enable fine-grained analysis of how retrieval perturbations degrade end-task performance and inform robust retrieval-policy design.
Faithfulness and hallucination detectionbenchmarks have taken center stage in evaluating generation quality. RAGTruth> (Niu etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib47)> )
provides nearly 18,000 annotated examples from QA, summarization, and data-to-text generation, offering both response- and span-level hallucination labels across four types: subtle vs. evident, and conflict vs. baseless information. Uniquely, it supports training hallucination detectors and benchmarking span-level detection precision and recallâ€”tasks not addressed by coarse-grained metrics. This makes it foundational for measuring factual integrity in RAG outputs.
Reasoning and retrieval chainingare central to multi-hop question answering, where evidence spans multiple documents. MultiHop-RAG> (Tang and Yang, [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib64)> )
targets this challenge through linked question-answer pairs, bridge entities, and explicit multi-hop query types, enabling systematic assessment of retrieval chaining, evidence linking, and document-level reasoningâ€”all key bottlenecks in complex RAG workflows.
Adaptive retrieval and necessity estimationare benchmarked in RetrievalQA> (Zhang etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib87)> )
, which mixes queries requiring external retrieval with those answerable via the base LLM alone. This design tests whether models can intelligently toggle retrieval based on query uncertainty, supporting the development of resource-efficient, retrieval-aware systems that avoid introducing unnecessary context.
Domain-specific evaluationis exemplified by MIRAGE> (Ozaki etÂ al
> .
> , [> 2025
](https://arxiv.org/html/2506.00054v1#bib.bib49)> )
, a benchmark tailored to medical RAG. It contains 7,663 questions sourced from five clinical and biomedical QA datasets and incorporates real-world evaluation constraints: zero-shot generalization, multiple-choice formats, retrieval necessity assessment, and question-only retrieval. This multi-faceted setup tests reliability under high-stakes conditions where factual errors can be consequential.
Cross-corpus and federated retrievalare explored in FeB4RAG> (Wang etÂ al
> .
> , [> 2024a
](https://arxiv.org/html/2506.00054v1#bib.bib69)> )
, a benchmark constructed from 16 BEIR sub-collections. It evaluates federated retrieval through 790 conversational queries with LLM-graded relevance judgments and quantifies the impact of resource selection and result merging strategies. This benchmark surfaces key risks in multi-source RAG pipelines, especially retrieval inconsistency and hallucination amplification due to poor corpus coordination.
Evaluation infrastructure and reproducibilityare addressed by BERGEN> (Rau etÂ al
> .
> , [> 2024
](https://arxiv.org/html/2506.00054v1#bib.bib53)> )
, a benchmarking library designed to unify assessment across RAG components. It offers modular templates for measuring retrieval precision, generation faithfulness, and their interplay across datasets and model configurations. BERGEN facilitates consistent and extensible RAG benchmarking in both academic and applied settings.
Table 4.Emerging Benchmarks for Evaluating Retrieval-Augmented Generation (RAG) Systems.This table summarizes recent benchmarks developed to assess key aspects of RAG systems, including robustness, multi-hop reasoning, medical-domain adaptation, and federated retrieval. These benchmarks differ in evaluation granularityâ€”ranging from query-level to document-levelâ€”and employ varied annotation methods such as manual labeling, programmatic perturbation, and LLM-based scoring. Distinctive features, such as noise stress-testing (RGB), zero-shot medical QA (MIRAGE), and federated source merging (FeB4RAG), support targeted evaluations of both retriever components and full RAG pipelines.
|Benchmark|Evaluation Focus|Granularity|Annotation Type|Unique Features|Evaluation Target|
RGB|Robustness (noise, integration, hallucination)|Query-context pair|None|Stress tests for noise, contradiction, and multi-source fusion|Full pipeline|
MultiHop-RAG|Multi-hop reasoning and retrieval chaining|Document-level|Manual + derived|Linked multi-hop queries and bridge-entity chaining|Full pipeline|
RAGTruth|Hallucination detection and factuality evaluation|Response-level (yes/no), span-level (exact)|Human-labeled|18,000+ examples, 4 hallucination types, span-level F1|Generator|
MIRAGE|Medical domain QA under real-world constraints|Query-level|Dataset-native|Zero-shot, multi-choice, question-only retrieval (MEDRAG)|Full pipeline|
FeB4RAG|Federated retrieval evaluation|Document + resource|LLM-labeled|Measures retrieval + merging across 16 BEIR sources|Retriever|
RetrievalQA|Adaptive retrieval necessity detection|Query-level|Derived|Queries with and without need for retrieval|Retriever|
RAG-Bench|Retrieval robustness to noise|Query-level|Programmatic|Irrelevant, incomplete, and counterfactual retrieval noise|Full pipeline|
BERGEN|Retrieval, generation, and joint evaluation|Query-context and document-level|Configurable (task-dependent)|Unified benchmarking library across datasets and models|Full pipeline|
This section outlines the rapidly evolving landscape of RAG evaluation and benchmarking. Future RAG development hinges not only on improving generation quality but also on designing principled, scalable, and interpretable evaluation strategies that reflect real-world usage and complexities. To advance the field meaningfully, the community must prioritize the creation of standardized, efficient, and adaptive evaluation protocols that can serve both research and production contexts.
## 7.Future Directions
As Retrieval-Augmented Generation (RAG) systems continue to evolve, a number of unresolved challenges remain that limit their deployment in dynamic, open-ended, and high-stakes applications. These challenges span retrieval efficiency, semantic misalignment, hallucination control, generalization, and trust. Based on the synthesis of contemporary research gaps, we outline five interrelated future directions that represent promising trajectories for advancing the field.
### 7.1.Retrieval Adaptivity and Semantic Alignment
Current RAG architectures often rely on static retrieval policies and fixed embedding transformations, limiting their adaptability to complex or evolving user queries. Future systems must support dynamically calibrated retrieval strategies that adjust depth, modality, and source selection in response to task difficulty and contextual cues. This calls for co-optimized retrieverâ€“generator pipelines that leverage reinforcement signals, uncertainty estimates, or semantic control layers to align evidence retrieval with generative intent in real time.
### 7.2.Robustness under Noise and Adversarial Conditions
Despite recent advances in noise filtering and adversarial training, RAG systems remain vulnerable to retrieval perturbations, misleading content, and corpus-level poisoning attacks. Future work should move toward retrieval-aware adversarial defenses that incorporate noise-aware loss functions, retrieval-type-specific regularization, and semantic provenance filtering. This includes evaluation protocols that stress-test systems against contextually plausible yet misleading passages and group-triggered semantic attacks, as exemplified by recent backdoor threat models.
### 7.3.Multi-Hop Reasoning and Structured Compositionality
Many knowledge-intensive tasks require aggregating evidence across multiple retrieval steps and reasoning over entity or schema-level structures. Current models exhibit limited capacity for compositional inference or procedural synthesis. Future RAG systems should support multi-turn retrievalâ€“generation loops, structured subgoal decomposition, and graph-augmented reasoning pipelines that maintain discourse coherence and entity consistency across long-range dependencies.
### 7.4.Cross-Domain Generalization and Temporal Adaptivity
RAG performance often degrades in the face of domain shifts, novel schema, or temporal drift. Addressing this will require pretraining retrieval modules on diverse proxy tasks, developing meta-retrievers capable of adapting to unseen query distributions, and incorporating recency-aware document scoring. Additionally, the design of temporally evolving benchmarks and evaluation suites will be necessary to assess the robustness of RAG systems under realistic, time-sensitive knowledge conditions.
### 7.5.Explainability, Personalization, and Trust Calibration
As RAG systems are increasingly integrated into user-facing applications, demands for interpretability, personalization, and secure behavior intensify. Future architectures should expose transparent interfaces for explaining retrieval decisions and generation provenance, while supporting privacy-preserving personalization through user-clustered retrieval, memory-efficient modeling, or differential privacy mechanisms. Furthermore, integrating retrieval calibration signalsâ€”such as factual salience, source trustworthiness, or hallucination riskâ€”can enhance user trust and system accountability.
## References
* (1)
* Asai etÂ al.(2024)Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024.Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In*The Twelfth International Conference on Learning Representations*.[https://openreview.net/forum?id=hSyW5go0v8](https://openreview.net/forum?id=hSyW5go0v8)
* Ayala and Bechard (2024)Orlando Ayala and Patrice Bechard. 2024.Reducing hallucination in structured outputs via Retrieval-Augmented Generation. In*Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)*, YiÂ Yang, Aida Davani, Avi Sil, and Anoop Kumar (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 228â€“238.[doi:10.18653/v1/2024.naacl-industry.19](https://doi.org/10.18653/v1/2024.naacl-industry.19)
* Bajaj etÂ al.(2018)Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018.MS MARCO: A Human Generated MAchine Reading COmprehension Dataset.arXiv:1611.09268Â [cs.CL][https://arxiv.org/abs/1611.09268](https://arxiv.org/abs/1611.09268)
* Barnett etÂ al.(2024)Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024.Seven Failure Points When Engineering a Retrieval Augmented Generation System. In*Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI*(Lisbon, Portugal)*(CAIN â€™24)*. Association for Computing Machinery, New York, NY, USA, 194â€“199.[doi:10.1145/3644815.3644945](https://doi.org/10.1145/3644815.3644945)
* Brown etÂ al.(2020)TomÂ B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, DanielÂ M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020.Language models are few-shot learners. In*Proceedings of the 34th International Conference on Neural Information Processing Systems*(Vancouver, BC, Canada)*(NIPS â€™20)*. Curran Associates Inc., Red Hook, NY, USA, Article 159, 25Â pages.
* Chan etÂ al.(2024)Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024.RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation. In*First Conference on Language Modeling*.[https://openreview.net/forum?id=tzE7VqsaJ4](https://openreview.net/forum?id=tzE7VqsaJ4)
* Chen etÂ al.(2024)Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024.Benchmarking large language models in retrieval-augmented generation. In*Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence**(AAAIâ€™24/IAAIâ€™24/EAAIâ€™24)*. AAAI Press, Article 1980, 9Â pages.[doi:10.1609/aaai.v38i16.29728](https://doi.org/10.1609/aaai.v38i16.29728)
* Cheng etÂ al.(2024a)Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Haodong Zhao, Ping Yi, Zhuosheng Zhang, and Gongshen Liu. 2024a.TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models.[https://openreview.net/forum?id=RfYD6v829Y](https://openreview.net/forum?id=RfYD6v829Y)
* Cheng etÂ al.(2023)Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2023.Lift yourself up: retrieval-augmented text generation with self-memory. In*Proceedings of the 37th International Conference on Neural Information Processing Systems*(New Orleans, LA, USA)*(NIPS â€™23)*. Curran Associates Inc., Red Hook, NY, USA, Article 1899, 20Â pages.
* Cheng etÂ al.(2024b)Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. 2024b.xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token. In*The Thirty-eighth Annual Conference on Neural Information Processing Systems*.[https://openreview.net/forum?id=6pTlXqrO0p](https://openreview.net/forum?id=6pTlXqrO0p)
* Chiang etÂ al.(2023)Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, JosephÂ E. Gonzalez, Ion Stoica, and EricÂ P. Xing. 2023.Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%\* ChatGPT Quality.[https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)
* Clark etÂ al.(2018)Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge.arXiv:1803.05457Â [cs.AI][https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457)
* Cormack etÂ al.(2009)GordonÂ V. Cormack, Charles LÂ A Clarke, and Stefan Buettcher. 2009.Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In*Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval*(Boston, MA, USA)*(SIGIR â€™09)*. Association for Computing Machinery, New York, NY, USA, 758â€“759.[doi:10.1145/1571941.1572114](https://doi.org/10.1145/1571941.1572114)
* Cuconasu etÂ al.(2024)Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024.The Power of Noise: Redefining Retrieval for RAG Systems. In*Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval*(Washington DC, USA)*(SIGIR â€™24)*. Association for Computing Machinery, New York, NY, USA, 719â€“729.[doi:10.1145/3626772.3657834](https://doi.org/10.1145/3626772.3657834)
* Doan etÂ al.(2024)NguyenÂ Nam Doan, Aki HÃ¤rmÃ¤, Remzi Celebi, and Valeria Gottardo. 2024.A Hybrid Retrieval Approach for Advancing Retrieval-Augmented Generation Systems. In*Proceedings of the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024)*, Mourad Abbas and AbedÂ Alhakim Freihat (Eds.). Association for Computational Linguistics, Trento, 397â€“409.[https://aclanthology.org/2024.icnlsp-1.41/](https://aclanthology.org/2024.icnlsp-1.41/)
* Edge etÂ al.(2025)Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, RobertÂ Osazuwa Ness, and Jonathan Larson. 2025.From Local to Global: A Graph RAG Approach to Query-Focused Summarization.arXiv:2404.16130Â [cs.CL][https://arxiv.org/abs/2404.16130](https://arxiv.org/abs/2404.16130)
* Es etÂ al.(2024)Shahul Es, Jithin James, Luis EspinosaÂ Anke, and Steven Schockaert. 2024.RAGAs: Automated Evaluation of Retrieval Augmented Generation. In*Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations*, Nikolaos Aletras and Orphee DeÂ Clercq (Eds.). Association for Computational Linguistics, St. Julians, Malta, 150â€“158.[https://aclanthology.org/2024.eacl-demo.16/](https://aclanthology.org/2024.eacl-demo.16/)
* Fang etÂ al.(2024)Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. 2024.Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training. In*Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 10028â€“10039.[doi:10.18653/v1/2024.acl-long.540](https://doi.org/10.18653/v1/2024.acl-long.540)
* Gao etÂ al.(2023)Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023.Retrieval-augmented generation for large language models: A survey.*arXiv preprint arXiv:2312.10997*2 (2023), 1.
* Glass etÂ al.(2022)Michael Glass, Gaetano Rossiello, MdÂ FaisalÂ Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022.Re2G: Retrieve, Rerank, Generate. In*Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, Marine Carpuat, Marie-Catherine deÂ Marneffe, and IvanÂ Vladimir MezaÂ Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 2701â€“2715.[doi:10.18653/v1/2022.naacl-main.194](https://doi.org/10.18653/v1/2022.naacl-main.194)
* Grattafiori etÂ al.(2024)Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak,
Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, CristianÂ Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, EricÂ Michael Smith, Filip Radenovic, Francisco GuzmÃ¡n, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee,
GeorgiaÂ Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, ImanolÂ Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer vanÂ der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu,
Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, KalyanÂ Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens vanÂ der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher,
Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, MiteshÂ Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Ã‡elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava,
Pratik Dubal, Praveen Krishnan, PunitÂ Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, RicardoÂ Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, SeohyunÂ Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang,
Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, VÃ­tor Albiero, Vladan Petrovic,
Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, XiaoqingÂ Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, ZacharieÂ Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya 
