# "9 Chunking Strategies for RAG Models"

**URL:** https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR
**Published:** 2025-05-12T00:00:00.000Z

---

## Summary

The webpage provides information on **Chunking Strategies for RAG Models** and discusses several related advanced RAG concepts mentioned in the user query through comments and related posts.

Here is a summary addressing the user query components:

*   **Chunking Strategies:** The main post details **9 Chunking Strategies**: Fixed-Size, Sentence-Based, Semantic-Based, Recursive, Sliding-Window, Hierarchical, Topic-Based, Modality-Specific, and Agentic Chunking. Another post mentions **section-aware chunking** and **semantic chunking** (splitting by idea). A third post discusses **contextual summaries** added to chunks.
*   **Embeddings (new efficient models):** One related post mentions **Fine Tuned Embeddings** for domain-specific improvements and **Instruction Embedding Models** for zero-shot adaptation.
*   **Rerankers:** One related post explicitly mentions **Rerankers** (like Cohere Rerank 3.5) used to reorder top candidates from the vector database for better precision.
*   **RAG Architectures:** The page discusses several architectures:
    *   The main topic is **RAG**.
    *   Related posts mention **Agentic RAG**, **Graph RAG**, and **CAG (Cache Augmented Generation)** as an alternative. Another post mentions 10 RAG architectures including Standard, Agentic, and Multi-Modal.
*   **RAG Alternatives:** **CAG (Cache Augmented Generation)** is mentioned as the opposite of RAG, preloading context instead of retrieving it.
*   **Hybrid Search:** One related post mentions **Hybrid Search** (combining keyword search like BM25 with embeddings) and another mentions **Hybrid retrieval (BM25 + vectors)** as a best practice.
*   **Vector Databases:** While not detailed, vector databases are implied as the storage mechanism for embeddings in RAG systems, and one post mentions **Vector DB comparisons** in a cheat sheet.

**Vector Databases:** Mentioned as part of the RAG system where embeddings are stored, with comparisons noted in a linked cheat sheet.

---

## Full Content

&quot;9 Chunking Strategies for RAG Models&quot; | Cornellius Y. posted on the topic | LinkedIn
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedInâ€™s[User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement),[Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and[Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).
``````````````[Skip to main content](#main-content)
# "9 Chunking Strategies for RAG Models"
This title was summarized by AI from the post below.
[![View profile for Cornellius Y.]()](https://id.linkedin.com/in/cornellius-yudha-wijaya?trk=public_post_feed-actor-image)
[Cornellius Y.](https://id.linkedin.com/in/cornellius-yudha-wijaya?trk=public_post_feed-actor-name)
Data Scientist &amp; AI Engineer | Data Insight | Helping Orgs Scale with Data
7mo
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
Building the RAG pipeline is easy, but making it useful in real life is hard. ğŸ’»One of the aspects that is often overlooked while building them is the ğ‚ğ¡ğ®ğ§ğ¤ğ¢ğ§ğ ğ’ğ­ğ«ğšğ­ğğ ğ².ğŸ”§
Chunking is essential as it segments large documents into manageable units. Organizing text into smaller chunks allows the system to retrieve only the most relevant parts during inference, improving the efficiency of responses. Chunking basically ensures the RAG model uses relevant data and improves output quality without overwhelming it with irrelevant information.
Here are nine different chunking strategies that you should know:
â†³ğ…ğ¢ğ±ğğ-ğ’ğ¢ğ³ğ ğ‚ğ¡ğ®ğ§ğ¤ğ¢ğ§ğ Divides text into uniform-length segments (e.g., by word or token count), ensuring predictable and simple chunk sizes but risking semantic fragmentation.
â†³ğ’ğğ§ğ­ğğ§ğœğ-ğğšğ¬ğğ ğ‚ğ¡ğ®ğ§ğ¤ğ¢ğ§ğ Splits text into complete sentences, preserving semantic meaning, though chunk sizes may vary and depend on sentence length.
â†³ğ’ğğ¦ğšğ§ğ­ğ¢ğœ-ğğšğ¬ğğ ğ‚ğ¡ğ®ğ§ğ¤ğ¢ğ§ğ Groups text based on semantic similarity (using embeddings), ensuring contextual relevance but at the cost of higher computational resources.
â†³ğ‘ğğœğ®ğ«ğ¬ğ¢ğ¯ğğ‚ğ¡ğ®ğ§ğ¤ğ¢ğ§ğ Iteratively breaks down text until size limits are met, ensuring compliance with size constraints but potentially leading to over-fragmentation.
â†³Â ğ’ğ¥ğ¢ğğ¢ğ§ğ -ğ–ğ¢ğ§ğğ¨ğ° ğ‚ğ¡ğ®ğ§ğ¤ğ¢ğ§ğ Uses overlapping chunks to preserve context across boundaries, though it increases redundancy and computational overhead.
â†³ğ‡ğ¢ğğ«ğšğ«ğœğ¡ğ¢ğœğšğ¥ğ‚ğ¡ğ®ğ§ğ¤ğ¢ğ§ğ Utilizes a documentâ€™s natural structure (e.g., sections and paragraphs) to create nested chunks, preserving logical flow but requiring well-structured documents.
â†³ğ“ğ¨ğ©ğ¢ğœ-ğğšğ¬ğğ ğ‚ğ¡ğ®ğ§ğ¤ğ¢ğ§ğ Clusters text into thematic groups based on topic modeling, improving thematic consistency but potentially introducing ambiguity from overlapping topics.
â†³ğŒğ¨ğğšğ¥ğ¢ğ­ğ²-ğ’ğ©ğğœğ¢ğŸğ¢ğœ ğ‚ğ¡ğ®ğ§ğ¤ğ¢ğ§ğ Separates mixed-content documents (e.g., text, images, tables) into modality-specific chunks, ensuring integrity but adding complexity to the processing pipeline.
â†³ğ€ğ ğğ§ğ­ğ¢ğœğ‚ğ¡ğ®ğ§ğ¤ğ¢ğ§ğ Uses AI agents to dynamically identify chunk boundaries, offering flexibility and human-like chunking decisions, but at a higher cost and processing time.
Want to learn how to implement the Chunking Strategy for your RAG system? See the comment section!
â™»ï¸Repost to Your Network.
ğŸ””Follow[Cornellius Y.](https://id.linkedin.com/in/cornellius-yudha-wijaya?trk=public_post-text)More Tips Like This
* ![diagram]()````
[![]()![]()![]()99](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&amp;trk=public_post_social-actions-reactions)``````````````[12 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&amp;trk=public_post_social-actions-comments)
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
[![Shivani Virdi, graphic]()](https://www.linkedin.com/in/shivanivirdi?trk=public_post_comment_actor-image)
[Shivani Virdi](https://www.linkedin.com/in/shivanivirdi?trk=public_post_comment_actor-name)
AI Engineering | Founder @ NeoSage | ex-Microsoft â€¢AWS â€¢Adobe | Teaching 70K+ How to Build Production-Grade GenAI Systems
7mo
* [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&amp;trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=COMMENT&amp;_f=guest-reporting)
Getting chunking right is such an intricate thing; there are so many tradeoffs to think of, but it all starts with understanding your data and prepping it well for chunking!
[
Like
](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&amp;trk=public_post_comment_like)[
Reply
](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&amp;trk=public_post_comment_reply)[3Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&amp;trk=public_post_comment_reactions)4Reactions
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&trk=public_post_feed-cta-banner-cta)
``
## More Relevant Posts
* [](https://www.linkedin.com/posts/aukik-aurnab_2023-was-about-building-rag-systems-that-activity-7381217977100066816-eVZs)
[![View profile for Aukik Aurnab]()](https://bd.linkedin.com/in/aukik-aurnab?trk=public_post_feed-actor-image)
[Aukik Aurnab](https://bd.linkedin.com/in/aukik-aurnab?trk=public_post_feed-actor-name)
2mo
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/aukik-aurnab_2023-was-about-building-rag-systems-that-activity-7381217977100066816-eVZs&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
2023 was about building RAG systems that worked.
2025 is about making them think, reason and retrieve like humans.
Hereâ€™s whatâ€™s changing and why itâ€™s reshaping AI infrastructure 1. Semantic Chunking
Forget token limits. The future is meaning based chunking.
Instead of cutting by size, we split text by idea.
It improves retrieval accuracy by 20â€“30% and reduces hallucinations.
2. HyDE (Hypothetical Document Embeddings)
When a user asks a vague question, the system first imagines a detailed answer, embeds that, and then retrieves the real documents.
Itâ€™s like search powered by imagination.
3. Rerankers (like Cohere Rerank 3.5)
Your vector database gets you close. The reranker gets you right.
It reorders the top 100 candidates based on context.
Precision up. Hallucinations down.
4. Fine Tuned Embeddings
Generic embeddings are so 2024.
Teams now fine tune models for their domain such as finance, legal or biotech and see 5â€“10% better retrieval.
5. Hybrid Search
Keyword plus semantic equals the perfect match.
Combining BM25 with embeddings balances precision and recall.
6. Graph RAG
Instead of flat chunks, think knowledge graphs.
Nodes, edges, and relationships that enable multi hop reasoning and factual grounding.
7. Agentic RAG
RAG that plans, reasons and retrieves iteratively from various sources
Agents break complex questions into steps, re query, verify and self correct.
Basically, RAG meets ReAct.
8. Filtered Semantic Search
Add filters like date, author or domain to embeddings.
Retrieve only whatâ€™s relevant right now.
Thatâ€™s how enterprise RAGs keep latency low and results fresh.
9. CAG (Cache Augmented Generation)
The opposite of RAG.
Preload the full context, compute caches once, and generate instantly.
No retrieval, no database. Perfect for static knowledge bases.
10. Instruction Embedding Models
Tell your embedding model what to care about, literally.
â€œRetrieve scientific papers aboutâ€¦â€
â€œEmbed this as a legal argumentâ€¦â€
Zero shot adaptation, no fine tuning needed.
11. Context Compression
Less is more.
Before feeding documents to your LLM, distill them down to what truly matters.
You save tokens, time, and improve focus.
Every one of these innovations reduces hallucinations, improves recall or speeds up generation.
````
[![]()![]()![]()18](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/aukik-aurnab_2023-was-about-building-rag-systems-that-activity-7381217977100066816-eVZs&amp;trk=public_post_social-actions-reactions)``````````````[2 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/aukik-aurnab_2023-was-about-building-rag-systems-that-activity-7381217977100066816-eVZs&amp;trk=public_post_social-actions-comments)
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/aukik-aurnab_2023-was-about-building-rag-systems-that-activity-7381217977100066816-eVZs&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/aukik-aurnab_2023-was-about-building-rag-systems-that-activity-7381217977100066816-eVZs&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/aukik-aurnab_2023-was-about-building-rag-systems-that-activity-7381217977100066816-eVZs&trk=public_post_feed-cta-banner-cta)
``
* [](https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387167056141418496-6q_A)
[![View profile for Panchangam Rohith]()](https://in.linkedin.com/in/panchangam-rohith-2a787a226?trk=public_post_feed-actor-image)
[Panchangam Rohith](https://in.linkedin.com/in/panchangam-rohith-2a787a226?trk=public_post_feed-actor-name)
1mo
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387167056141418496-6q_A&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
Step 1: How I Solved the Hidden Bottleneck in My RAG System â€”Smart Document Parsing ğŸ§ ğŸ“„When I started building my Agentic Graph RAG system, I assumed the toughest part would be tuning embeddings or optimizing retrieval.
I was wrong.
The real challenge hit me earlier â€”getting PDFs to talk sense.
What looked like a simple â€œjust extract the textâ€ task turned into a deep dive into the chaos of scattered columns, broken tables, and unreadable scans. Thatâ€™s when I realized:
Most RAG systems fail long before retrieval â€”right at the data-ingestion step.
The Problem I Faced
PDFs are a nightmare for AI systems:
Text is scattered across columns
Tables lose their structure
Scanned documents need OCR
Images and charts mixed with text
I tried the usual tools - PyMuPDF, PDFMiner. Each one solved ONE problem but created three more. I was building a Frankenstein pipeline just to read documents.
What Actually Worked
I switched to LlamaParse and it changed everything.
Instead of chaining 5 different tools, I made ONE API call. That's it.
LlamaParse uses LLMs to understand the document like a human would. It handles text, tables, images, and even scanned PDFs automatically.
Here's the difference:
Before: PyMuPDF â†’PDFMiner â†’[Unstructured.io](https://www.linkedin.com/redir/redirect?url=http://Unstructured.io&amp;urlhash=ZNpn&amp;trk=public_post-text)â†’ Tesseract â†’Custom code to merge everything (fragile mess)
After: LlamaParse API call â†’Clean, structured text (just works)
Why This Matters
Bad parsing = Bad knowledge extraction = Bad answers
Your fancy LLM and vector database won't save you if the input is garbage.
I learned this the hard way after debugging why my system kept giving weird answers. Turned out, my table parsing was breaking the data structure.
The Real Lesson
Don't overcomplicate the basics. Sometimes paying for a good API beats building a complex system yourself.
My parsing pipeline went from 200 lines of fragile code to 20 lines that actually work.
This is Step 1 of building my Agentic Graph RAG system. Next up: adding contextual summaries to make retrieval smarter.
What's your experience with document parsing? Any horror stories? ğŸ˜…[LlamaIndex](https://www.linkedin.com/company/llamaindex?trk=public_post-text)[Jerry Liu](https://www.linkedin.com/in/jerry-liu-64390071?trk=public_post-text)[#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/ai&amp;trk=public_post-text)[#RAG](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/rag&amp;trk=public_post-text)[#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/llm&amp;trk=public_post-text)[#DocumentAI](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/documentai&amp;trk=public_post-text)P.S. - If you're building something similar, happy to share what I learned. DM me!
[![Get your data LLM-ready | Unstructured]()
Get your data LLM-ready | Unstructuredunstructured.io
](https://www.linkedin.com/redir/redirect?url=https://unstructured.io/&amp;urlhash=vSX0&amp;trk=public_post_feed-article-content)
````
[![]()![]()14](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387167056141418496-6q_A&amp;trk=public_post_social-actions-reactions)``````````````
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387167056141418496-6q_A&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387167056141418496-6q_A&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387167056141418496-6q_A&trk=public_post_feed-cta-banner-cta)
``
* [](https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387361972251627520-tlqF)
[![View profile for Panchangam Rohith]()](https://in.linkedin.com/in/panchangam-rohith-2a787a226?trk=public_post_feed-actor-image)
[Panchangam Rohith](https://in.linkedin.com/in/panchangam-rohith-2a787a226?trk=public_post_feed-actor-name)
1mo
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387361972251627520-tlqF&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
Step 2: Why My RAG System Was Giving Terrible Answers (and How I Fixed It)
In my last post, I shared how Iâ€™m parsing PDFs for my Agentic Graph RAG â€”a production-grade retrieval-augmented generation system ğŸ‘‰check out the step 1 :[https://lnkd.in/gpnAvePX](https://www.linkedin.com/redir/redirect?url=https://lnkd.in/gpnAvePX&amp;urlhash=Tydr&amp;trk=public_post-text)Todayâ€™s post is about the next (painful) lesson:
Why parsing alone wasnâ€™t enough.
ğŸ§©The Problem I Discovered
My system could read documents perfectly.
But when someone asked a question, it would return completely irrelevant chunks of text.
Example:
Question: â€œWhat are the benefits of using knowledge graphs?â€
Returned: A random paragraph that mentioned â€œgraphâ€ once but was actually about data visualization.
The retrieval was technically working â€”it just wasnâ€™t context-aware.
ğŸ”What Was Actually Happening
When you split a document into chunks, you lose context.
Itâ€™s like reading a paragraph from the middle of a book with no title, no introduction, no clue about whatâ€™s happening.
Thatâ€™s exactly what my vector database was storing.
A chunk that says:
â€œThis approach improved accuracy by 40%â€
â€¦is meaningless without knowing:
What approach?
Accuracy of what?
Compared to what baseline?
ğŸ§ The Fix: Contextual Retrieval
Before storing each chunk, I introduced contextual summaries â€”inspired by[Anthropic](https://www.linkedin.com/company/anthropicresearch?trk=public_post-text)â€™s contextual retrieval approach and implemented using[Cohere](https://ca.linkedin.com/company/cohere-ai?trk=public_post-text)â€™s Command R+ model.
So instead of storing just the raw text:
â€œThis approach improved accuracy by 40%â€
I now store:
â€œThis approach improved accuracy by 40%â€ + Context Summary: â€œDiscussion of knowledge graph-based retrieval systems showing performance improvements over traditional vector search in academic benchmarking.â€
Now, my vector database doesnâ€™t just retrieve matching words â€”it retrieves meaning.
âš¡Why Cohere Command R+?
After testing multiple LLMs, Command R+ nailed it because:
It follows instructions precisely (critical for consistent contextual summaries)
Itâ€™s fast enough for batch-processing hundreds of chunks
It produces concise, high-quality context snapshots
Itâ€™s not the cheapest, but the jump in retrieval relevance and precision made it absolutely worth it.
ğŸ“ˆThe Real Lesson
Raw text chunks are like puzzle pieces without the picture on the box.
Adding context gives your RAG system that picture.
I compared results between:
RAG (without contextual retrieval) âŒRAG (with contextual retrieval) âœ…â€¦and the contextual version delivered significantly higher accuracy and semantic relevance across test queries.
ğŸ”œStep 3: Building the Knowledge Graph Layer
Next, Iâ€™ll be connecting these contextual chunks into structured entities and relationships â€”the step that turns simple â€œretrievalâ€ into true understanding.
Have you tried contextual chunking or experimented with RAG enhancement techniques? Iâ€™d love to hear what worked for you.
[![View profile for Panchangam Rohith]()](https://in.linkedin.com/in/panchangam-rohith-2a787a226?trk=public_post_reshare_feed-actor-image)
[Panchangam Rohith](https://in.linkedin.com/in/panchangam-rohith-2a787a226?trk=public_post_reshare_feed-actor-name)
Full Stack Developer Intern @Lyzr (Rapid Prototyping Team) | Turning AI Ideas into Working Products
1mo
Step 1: How I Solved the Hidden Bottleneck in My RAG System â€”Smart Document Parsing ğŸ§ ğŸ“„When I started building my Agentic Graph RAG system, I assumed the toughest part would be tuning embeddings or optimizing retrieval.
I was wrong.
The real challenge hit me earlier â€”getting PDFs to talk sense.
What looked like a simple â€œjust extract the textâ€ task turned into a deep dive into the chaos of scattered columns, broken tables, and unreadable scans. Thatâ€™s when I realized:
Most RAG systems fail long before retrieval â€”right at the data-ingestion step.
The Problem I Faced
PDFs are a nightmare for AI systems:
Text is scattered across columns
Tables lose their structure
Scanned documents need OCR
Images and charts mixed with text
I tried the usual tools - PyMuPDF, PDFMiner. Each one solved ONE problem but created three more. I was building a Frankenstein pipeline just to read documents.
What Actually Worked
I switched to LlamaParse and it changed everything.
Instead of chaining 5 different tools, I made ONE API call. That's it.
LlamaParse uses LLMs to understand the document like a human would. It handles text, tables, images, and even scanned PDFs automatically.
Here's the difference:
Before: PyMuPDF â†’PDFMiner â†’[Unstructured.io](https://www.linkedin.com/redir/redirect?url=http://Unstructured.io&amp;urlhash=ZNpn&amp;trk=public_post_reshare-text)â†’ Tesseract â†’Custom code to merge everything (fragile mess)
After: LlamaParse API call â†’Clean, structured text (just works)
Why This Matters
Bad parsing = Bad knowledge extraction = Bad answers
Your fancy LLM and vector database won't save you if the input is garbage.
I learned this the hard way after debugging why my system kept giving weird answers. Turned out, my table parsing was breaking the data structure.
The Real Lesson
Don't overcomplicate the basics. Sometimes paying for a good API beats building a complex system yourself.
My parsing pipeline went from 200 lines of fragile code to 20 lines that actually work.
This is Step 1 of building my Agentic Graph RAG system. Next up: adding contextual summaries to make retrieval smarter.
What's your experience with document parsing? Any horror stories? ğŸ˜…[LlamaIndex](https://www.linkedin.com/company/llamaindex?trk=public_post_reshare-text)[Jerry Liu](https://www.linkedin.com/in/jerry-liu-64390071?trk=public_post_reshare-text)[#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/ai&amp;trk=public_post_reshare-text)[#RAG](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/rag&amp;trk=public_post_reshare-text)[#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/llm&amp;trk=public_post_reshare-text)[#DocumentAI](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/documentai&amp;trk=public_post_reshare-text)P.S. - If you're building something similar, happy to share what I learned. DM me!
[![Get your data LLM-ready | Unstructured]()
Get your data LLM-ready | Unstructuredunstructured.io
](https://www.linkedin.com/redir/redirect?url=https://unstructured.io/&amp;urlhash=vSX0&amp;trk=public_post_reshare_feed-article-content)
````
[![]()2](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387361972251627520-tlqF&amp;trk=public_post_social-actions-reactions)``````````````
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387361972251627520-tlqF&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387361972251627520-tlqF&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387361972251627520-tlqF&trk=public_post_feed-cta-banner-cta)
``
* [](https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387693836829061120-Y6-K)
[![View profile for Panchangam Rohith]()](https://in.linkedin.com/in/panchangam-rohith-2a787a226?trk=public_post_feed-actor-image)
[Panchangam Rohith](https://in.linkedin.com/in/panchangam-rohith-2a787a226?trk=public_post_feed-actor-name)
1mo
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387693836829061120-Y6-K&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
Step 3: From contextual chunks to ontology generation. This is where things get really interesting.
What I realized
Vector search is great for â€œfind similar text.â€ But it completely misses relationships.
Example:
Vector search: â€œEinsteinâ€ is similar to â€œphysicistâ€ âœ“What it misses: Einstein FOUNDED relativity theory â†’which INFLUENCED quantum mechanics â†’which LED TO modern physics
Those arrows â€”those connections â€”are what make answers intelligent.
The challenge I faced
How do you automatically extract entities and relationships from text without manually defining everything?
I tried rule-based extraction first. Total disaster. Each document type broke it in new ways. Then I realized: why not let an LLM understand the text and extract the structure itself?
Enter: LLM-powered Ontology Generation
I used[Groq](https://www.linkedin.com/company/groq?trk=public_post-text)(openai/gpt-oss-120b) to read my enhanced chunks and extract:
Entities (people, methods, concepts)
Relationships (causes, improves, contradicts, etc.)
Properties (dates, values, categories)
The magic? Itâ€™s not rule-based â€”the LLM understands the context.
However, due to token limits, I later switched to Cohere Command-R+, one of the most advanced NLP models today, to handle larger, more complex document sets.
Why Groq?
Speed. This system analyzes hundreds of chunks in real time. Groqâ€™s LPU hardware makes Llama 3 lightning-fast â€”subsecond responses.
For 500 chunks:
Regular LLM API: 15â€“20 mins
Groq: Under 3 mins
What the output looks like
(Knowledge Graph) --ENABLES--&gt;&gt; (Retrieval Systems)
(Retrieval Systems) --ACHIEVES--&gt;&gt; (40% Accuracy Improvement)
(Graph Traversal) --COMPARED\_TO--&gt;&gt; (Vector Search)
Now, when someone asks â€œWhat improves retrieval accuracy?â€, the system doesnâ€™t just find mentions â€”it traverses relationships to understand context.
The insight
Text is 1D. Knowledge graphs are multi-dimensional.
You can now search by:
Direct mentions
Relationships
Concept paths
Properties (date, author, domain)
This is what separates basic RAG from intelligent retrieval.
Next: Storing everything in Neo4j and making it queryable â€”where the real magic begins.
Anyone else working with knowledge graphs? Whatâ€™s been your biggest challenge?
step 2:[https://lnkd.in/gSA\_GsyD](https://www.linkedin.com/redir/redirect?url=https://lnkd.in/gSA_GsyD&amp;urlhash=pwc3&amp;trk=public_post-text)[Groq](https://www.linkedin.com/company/groq?trk=public_post-text)[Neo4j](https://www.linkedin.com/company/neo4j?trk=public_post-text)[Lyzr AI](https://www.linkedin.com/company/lyzr-platform?trk=public_post-text)[Siva Surendira](https://www.linkedin.com/in/sivasurend?trk=public_post-text)[Qdrant](https://de.linkedin.com/company/qdrant?trk=public_post-text)[Jerry Liu](https://www.linkedin.com/in/jerry-liu-64390071?trk=public_post-text)[#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/ai&amp;trk=public_post-text)[#KnowledgeGraphs](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/knowledgegraphs&amp;trk=public_post-text)[#GraphRAG](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/graphrag&amp;trk=public_post-text)[#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/llm&amp;trk=public_post-text)[#Cohere](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/cohere&amp;trk=public_post-text)[#Groq](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/groq&amp;trk=public_post-text)[#BuildingInPublic](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/buildinginpublic&amp;trk=public_post-text)
[![View profile for Panchangam Rohith]()](https://in.linkedin.com/in/panchangam-rohith-2a787a226?trk=public_post_reshare_feed-actor-image)
[Panchangam Rohith](https://in.linkedin.com/in/panchangam-rohith-2a787a226?trk=public_post_reshare_feed-actor-name)
Full Stack Developer Intern @Lyzr (Rapid Prototyping Team) | Turning AI Ideas into Working Products
1mo
Step 1: How I Solved the Hidden Bottleneck in My RAG System â€”Smart Document Parsing ğŸ§ ğŸ“„When I started building my Agentic Graph RAG system, I assumed the toughest part would be tuning embeddings or optimizing retrieval.
I was wrong.
The real challenge hit me earlier â€”getting PDFs to talk sense.
What looked like a simple â€œjust extract the textâ€ task turned into a deep dive into the chaos of scattered columns, broken tables, and unreadable scans. Thatâ€™s when I realized:
Most RAG systems fail long before retrieval â€”right at the data-ingestion step.
The Problem I Faced
PDFs are a nightmare for AI systems:
Text is scattered across columns
Tables lose their structure
Scanned documents need OCR
Images and charts mixed with text
I tried the usual tools - PyMuPDF, PDFMiner. Each one solved ONE problem but created three more. I was building a Frankenstein pipeline just to read documents.
What Actually Worked
I switched to LlamaParse and it changed everything.
Instead of chaining 5 different tools, I made ONE API call. That's it.
LlamaParse uses LLMs to understand the document like a human would. It handles text, tables, images, and even scanned PDFs automatically.
Here's the difference:
Before: PyMuPDF â†’PDFMiner â†’[Unstructured.io](https://www.linkedin.com/redir/redirect?url=http://Unstructured.io&amp;urlhash=ZNpn&amp;trk=public_post_reshare-text)â†’ Tesseract â†’Custom code to merge everything (fragile mess)
After: LlamaParse API call â†’Clean, structured text (just works)
Why This Matters
Bad parsing = Bad knowledge extraction = Bad answers
Your fancy LLM and vector database won't save you if the input is garbage.
I learned this the hard way after debugging why my system kept giving weird answers. Turned out, my table parsing was breaking the data structure.
The Real Lesson
Don't overcomplicate the basics. Sometimes paying for a good API beats building a complex system yourself.
My parsing pipeline went from 200 lines of fragile code to 20 lines that actually work.
This is Step 1 of building my Agentic Graph RAG system. Next up: adding contextual summaries to make retrieval smarter.
What's your experience with document parsing? Any horror stories? ğŸ˜…[LlamaIndex](https://www.linkedin.com/company/llamaindex?trk=public_post_reshare-text)[Jerry Liu](https://www.linkedin.com/in/jerry-liu-64390071?trk=public_post_reshare-text)[#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/ai&amp;trk=public_post_reshare-text)[#RAG](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/rag&amp;trk=public_post_reshare-text)[#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/llm&amp;trk=public_post_reshare-text)[#DocumentAI](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/documentai&amp;trk=public_post_reshare-text)P.S. - If you're building something similar, happy to share what I learned. DM me!
[![Get your data LLM-ready | Unstructured]()
Get your data LLM-ready | Unstructuredunstructured.io
](https://www.linkedin.com/redir/redirect?url=https://unstructured.io/&amp;urlhash=vSX0&amp;trk=public_post_reshare_feed-article-content)
````
[![]()2](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387693836829061120-Y6-K&amp;trk=public_post_social-actions-reactions)``````````````
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387693836829061120-Y6-K&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387693836829061120-Y6-K&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/panchangam-rohith-2a787a226_get-your-data-llm-ready-unstructured-activity-7387693836829061120-Y6-K&trk=public_post_feed-cta-banner-cta)
``
* [](https://www.linkedin.com/posts/ushorus-technologies_rag-isnt-a-silver-bullet-its-a-scalpel-activity-7382512768056205312-xfh1)
[![View organization page for Horus Technology]()](https://www.linkedin.com/company/ushorus-technologies?trk=public_post_feed-actor-image)
[Horus Technology](https://www.linkedin.com/company/ushorus-technologies?trk=public_post_feed-actor-name)
9 followers
2mo
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/ushorus-technologies_rag-isnt-a-silver-bullet-its-a-scalpel-activity-7382512768056205312-xfh1&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
RAG isnâ€™t a silver bullet. Itâ€™s a scalpel.
Retrieval-Augmented Generation shines in document AIâ€”but only in the right jobs. Hereâ€™s the decision play we use with enterprise teams:
Use RAG whenâ€¦
â€¢You need cross-document answers (contracts + emails + policies).
â€¢Youâ€™re summarizing narratives or explaining policy impacts.
â€¢You require citations back to source pages for trust.
â€¢The schema is fluid and you care more about reasoning than exact fields.
Donâ€™t use RAG whenâ€¦
â€¢You need deterministic, field-level accuracy (KYC, claims, compliance forms).
â€¢Outputs must match a strict schema or trigger downstream automations.
â€¢Millisecond latency or ultra-low cost is the priority.
Best-practice guardrails
â€¢Hybrid retrieval (BM25 + vectors), section-aware chunking, versioned indexes.
â€¢Validator layer: JSON schema + rule checks before anything hits your systems.
â€¢Governance: citation required, PII masking, cost caps, human-in-the-loop.
Result: fewer hallucinations, faster reviews, and a stack that scales with confidence.
Want our RAG Decision Tree (one-pager + checklist)? Comment â€œRAGKIT.â€
* ![No alternative text description for this image]()````
[![]()1](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/ushorus-technologies_rag-isnt-a-silver-bullet-its-a-scalpel-activity-7382512768056205312-xfh1&amp;trk=public_post_social-actions-reactions)``````````````
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/ushorus-technologies_rag-isnt-a-silver-bullet-its-a-scalpel-activity-7382512768056205312-xfh1&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/ushorus-technologies_rag-isnt-a-silver-bullet-its-a-scalpel-activity-7382512768056205312-xfh1&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/ushorus-technologies_rag-isnt-a-silver-bullet-its-a-scalpel-activity-7382512768056205312-xfh1&trk=public_post_feed-cta-banner-cta)
``
* [](https://www.linkedin.com/posts/akopytko_can-llms-really-build-knowledge-graphs-activity-7383460831495376896-ymSw)
[![View profile for Aga Kopytko]()](https://pl.linkedin.com/in/akopytko?trk=public_post_feed-actor-image)
[Aga Kopytko](https://pl.linkedin.com/in/akopytko?trk=public_post_feed-actor-name)
2mo
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/akopytko_can-llms-really-build-knowledge-graphs-activity-7383460831495376896-ymSw&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
ğŸ•¸ï¸Can LLMs Really Build Knowledge Graphs We Can Trust?
Thereâ€™s a growing trend: â€œLetâ€™s use LLMs to build knowledge graphs.â€
It sounds like the perfect shortcut - take unstructured data, prompt an LLM, and get a ready-to-use graph.
Butâ€¦ are we sure those graphs are trustworthy?
Before that, letâ€™s pause for a second:
ğŸ’¡Why build knowledge graphs at all?
Because they solve one of AIâ€™s biggest weaknesses - lack of structure and reasoning. Graphs let us connect facts, entities, and relationships in a way thatâ€™s transparent, queryable, and explainable. They give context, memory, and logic - everything that raw text or embeddings alone canâ€™t provide.
Yet, hereâ€™s the catch when using LLMs to build them:
ğŸ”¹Short context window - LLMs can only â€œseeâ€ a limited amount of data at once, losing consistency across larger corpora.
ğŸ”¹Hallucinations - when context runs out or ambiguity appears, models confidently invent facts or relations that never existed.
ğŸ”¹Lack of provenance - LLM outputs donâ€™t preserve why or how a link was made. Without traceability, you canâ€™t audit or explain your graph.
ğŸ”¹Temporal instability - the same prompt can yield different graphs tomorrow, because stochastic generation â‰ deterministic structure.
ğŸ”¹Scalability &amp; cost - large-scale graph construction requires persistent context and reasoning, which LLMs werenâ€™t designed for.
Building knowledge graphs isnâ€™t just data extraction - itâ€™s engineering meaning. It demands consistency, provenance, and explainability, not just text generation.
LLMs can assist in this process, but they shouldnâ€™t be the architect.
The next step is finding a way to make graphs both trustworthy and instant - without compromising one for the other.
* ![No alternative text description for this image]()View C2PA information![]()````
[![]()![]()![]()241](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/akopytko_can-llms-really-build-knowledge-graphs-activity-7383460831495376896-ymSw&amp;trk=public_post_social-actions-reactions)``````````````[65 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/akopytko_can-llms-really-build-knowledge-graphs-activity-7383460831495376896-ymSw&amp;trk=public_post_social-actions-comments)
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/akopytko_can-llms-really-build-knowledge-graphs-activity-7383460831495376896-ymSw&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/akopytko_can-llms-really-build-knowledge-graphs-activity-7383460831495376896-ymSw&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/akopytko_can-llms-really-build-knowledge-graphs-activity-7383460831495376896-ymSw&trk=public_post_feed-cta-banner-cta)
``
* [](https://www.linkedin.com/posts/espenremman_digitalassistant-knowledgegraph-generativeai-activity-7383699302239395840-pTpp)
[![View profile for Espen Remman]()](https://no.linkedin.com/in/espenremman?trk=public_post_feed-actor-image)
[Espen Remman](https://no.linkedin.com/in/espenremman?trk=public_post_feed-actor-name)
2moEdited
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/espenremman_digitalassistant-knowledgegraph-generativeai-activity-7383699302239395840-pTpp&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
Knowledge graphs should be a part of every knowledge systems.
Today we can get better help in building these graphs than ever before.
These graphs are crucial for todays high performing state-of-the-art generative AI-based digital assistants.[#digitalassistant](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/digitalassistant&amp;trk=public_post-text)[#knowledgegraph](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/knowledgegraph&amp;trk=public_post-text)[#generativeAI](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/generativeai&amp;trk=public_post-text)[#llm](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/llm&amp;trk=public_post-text)[#languagetechnology](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/hashtag/languagetechnology&amp;trk=public_post-text)[Chronos.ai](https://no.linkedin.com/company/chronos-as?trk=public_post-text)
[![View profile for Aga Kopytko]()](https://pl.linkedin.com/in/akopytko?trk=public_post_reshare_feed-actor-image)
[Aga Kopytko](https://pl.linkedin.com/in/akopytko?trk=public_post_reshare_feed-actor-name)
Founder &amp; CTO at Smabbler | Building Galaxia - the explainable intelligence system that understands, remembers, and explains | Semantic Hypergraph â€¢Enterprise Knowledge Graphs â€¢Explainable AI
2mo
ğŸ•¸ï¸Can LLMs Really Build Knowledge Graphs We Can Trust?
Thereâ€™s a growing trend: â€œLetâ€™s use LLMs to build knowledge graphs.â€
It sounds like the perfect shortcut - take unstructured data, prompt an LLM, and get a ready-to-use graph.
Butâ€¦ are we sure those graphs are trustworthy?
Before that, letâ€™s pause for a second:
ğŸ’¡Why build knowledge graphs at all?
Because they solve one of AIâ€™s biggest weaknesses - lack of structure and reasoning. Graphs let us connect facts, entities, and relationships in a way thatâ€™s transparent, queryable, and explainable. They give context, memory, and logic - everything that raw text or embeddings alone canâ€™t provide.
Yet, hereâ€™s the catch when using LLMs to build them:
ğŸ”¹Short context window - LLMs can only â€œseeâ€ a limited amount of data at once, losing consistency across larger corpora.
ğŸ”¹Hallucinations - when context runs out or ambiguity appears, models confidently invent facts or relations that never existed.
ğŸ”¹Lack of provenance - LLM outputs donâ€™t preserve why or how a link was made. Without traceability, you canâ€™t audit or explain your graph.
ğŸ”¹Temporal instability - the same prompt can yield different graphs tomorrow, because stochastic generation â‰ deterministic structure.
ğŸ”¹Scalability &amp; cost - large-scale graph construction requires persistent context and reasoning, which LLMs werenâ€™t designed for.
Building knowledge graphs isnâ€™t just data extraction - itâ€™s engineering meaning. It demands consistency, provenance, and explainability, not just text generation.
LLMs can assist in this process, but they shouldnâ€™t be the architect.
The next step is finding a way to make graphs both trustworthy and instant - without compromising one for the other.
* ![No alternative text description for this image]()View C2PA information![]()````
[![]()![]()5](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/espenremman_digitalassistant-knowledgegraph-generativeai-activity-7383699302239395840-pTpp&amp;trk=public_post_social-actions-reactions)``````````````
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/espenremman_digitalassistant-knowledgegraph-generativeai-activity-7383699302239395840-pTpp&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/espenremman_digitalassistant-knowledgegraph-generativeai-activity-7383699302239395840-pTpp&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/espenremman_digitalassistant-knowledgegraph-generativeai-activity-7383699302239395840-pTpp&trk=public_post_feed-cta-banner-cta)
``
* [](https://www.linkedin.com/posts/vasylharasymiv_the-idea-is-that-llm-produces-a-draft-of-activity-7383984982719000577-0VhV)
[![View profile for Vasyl Harasymiv]()](https://www.linkedin.com/in/vasylharasymiv?trk=public_post_feed-actor-image)
[Vasyl Harasymiv](https://www.linkedin.com/in/vasylharasymiv?trk=public_post_feed-actor-name)
2mo
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/vasylharasymiv_the-idea-is-that-llm-produces-a-draft-of-activity-7383984982719000577-0VhV&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
The idea is that LLM produces a draft of a graph and then an experienced LLM ontologist curates it, covering key gaps that LLM lacks knowledge of (e.g. corporate lingo, private structures) and key business assumptions.
[![View profile for Aga Kopytko]()](https://pl.linkedin.com/in/akopytko?trk=public_post_reshare_feed-actor-image)
[Aga Kopytko](https://pl.linkedin.com/in/akopytko?trk=public_post_reshare_feed-actor-name)
Founder &amp; CTO at Smabbler | Building Galaxia - the explainable intelligence system that understands, remembers, and explains | Semantic Hypergraph â€¢Enterprise Knowledge Graphs â€¢Explainable AI
2mo
ğŸ•¸ï¸Can LLMs Really Build Knowledge Graphs We Can Trust?
Thereâ€™s a growing trend: â€œLetâ€™s use LLMs to build knowledge graphs.â€
It sounds like the perfect shortcut - take unstructured data, prompt an LLM, and get a ready-to-use graph.
Butâ€¦ are we sure those graphs are trustworthy?
Before that, letâ€™s pause for a second:
ğŸ’¡Why build knowledge graphs at all?
Because they solve one of AIâ€™s biggest weaknesses - lack of structure and reasoning. Graphs let us connect facts, entities, and relationships in a way thatâ€™s transparent, queryable, and explainable. They give context, memory, and logic - everything that raw text or embeddings alone canâ€™t provide.
Yet, hereâ€™s the catch when using LLMs to build them:
ğŸ”¹Short context window - LLMs can only â€œseeâ€ a limited amount of data at once, losing consistency across larger corpora.
ğŸ”¹Hallucinations - when context runs out or ambiguity appears, models confidently invent facts or relations that never existed.
ğŸ”¹Lack of provenance - LLM outputs donâ€™t preserve why or how a link was made. Without traceability, you canâ€™t audit or explain your graph.
ğŸ”¹Temporal instability - the same prompt can yield different graphs tomorrow, because stochastic generation â‰ deterministic structure.
ğŸ”¹Scalability &amp; cost - large-scale graph construction requires persistent context and reasoning, which LLMs werenâ€™t designed for.
Building knowledge graphs isnâ€™t just data extraction - itâ€™s engineering meaning. It demands consistency, provenance, and explainability, not just text generation.
LLMs can assist in this process, but they shouldnâ€™t be the architect.
The next step is finding a way to make graphs both trustworthy and instant - without compromising one for the other.
* ![No alternative text description for this image]()View C2PA information![]()````
[![]()![]()3](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/vasylharasymiv_the-idea-is-that-llm-produces-a-draft-of-activity-7383984982719000577-0VhV&amp;trk=public_post_social-actions-reactions)``````````````
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/vasylharasymiv_the-idea-is-that-llm-produces-a-draft-of-activity-7383984982719000577-0VhV&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/vasylharasymiv_the-idea-is-that-llm-produces-a-draft-of-activity-7383984982719000577-0VhV&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/vasylharasymiv_the-idea-is-that-llm-produces-a-draft-of-activity-7383984982719000577-0VhV&trk=public_post_feed-cta-banner-cta)
``
* [](https://www.linkedin.com/posts/leslieannsheppard_why-build-knowledge-graphs-at-all-because-activity-7384043746839060480-maDh)
[![View profile for Leslie Sheppard]()](https://www.linkedin.com/in/leslieannsheppard?trk=public_post_feed-actor-image)
[Leslie Sheppard](https://www.linkedin.com/in/leslieannsheppard?trk=public_post_feed-actor-name)
2mo
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/leslieannsheppard_why-build-knowledge-graphs-at-all-because-activity-7384043746839060480-maDh&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
â€œWhy build knowledge graphs at all?
Because they solve one of AIâ€™s biggest weaknesses - lack of structure and reasoning. Graphs let us connect facts, entities, and relationships in a way thatâ€™s transparent, queryable, and explainable. ğŸš¨They give context, memory, and logic - everything that raw text or embeddings alone canâ€™t provide.â€
[![View profile for Aga Kopytko]()](https://pl.linkedin.com/in/akopytko?trk=public_post_reshare_feed-actor-image)
[Aga Kopytko](https://pl.linkedin.com/in/akopytko?trk=public_post_reshare_feed-actor-name)
Founder &amp; CTO at Smabbler | Building Galaxia - the explainable intelligence system that understands, remembers, and explains | Semantic Hypergraph â€¢Enterprise Knowledge Graphs â€¢Explainable AI
2mo
ğŸ•¸ï¸Can LLMs Really Build Knowledge Graphs We Can Trust?
Thereâ€™s a growing trend: â€œLetâ€™s use LLMs to build knowledge graphs.â€
It sounds like the perfect shortcut - take unstructured data, prompt an LLM, and get a ready-to-use graph.
Butâ€¦ are we sure those graphs are trustworthy?
Before that, letâ€™s pause for a second:
ğŸ’¡Why build knowledge graphs at all?
Because they solve one of AIâ€™s biggest weaknesses - lack of structure and reasoning. Graphs let us connect facts, entities, and relationships in a way thatâ€™s transparent, queryable, and explainable. They give context, memory, and logic - everything that raw text or embeddings alone canâ€™t provide.
Yet, hereâ€™s the catch when using LLMs to build them:
ğŸ”¹Short context window - LLMs can only â€œseeâ€ a limited amount of data at once, losing consistency across larger corpora.
ğŸ”¹Hallucinations - when context runs out or ambiguity appears, models confidently invent facts or relations that never existed.
ğŸ”¹Lack of provenance - LLM outputs donâ€™t preserve why or how a link was made. Without traceability, you canâ€™t audit or explain your graph.
ğŸ”¹Temporal instability - the same prompt can yield different graphs tomorrow, because stochastic generation â‰ deterministic structure.
ğŸ”¹Scalability &amp; cost - large-scale graph construction requires persistent context and reasoning, which LLMs werenâ€™t designed for.
Building knowledge graphs isnâ€™t just data extraction - itâ€™s engineering meaning. It demands consistency, provenance, and explainability, not just text generation.
LLMs can assist in this process, but they shouldnâ€™t be the architect.
The next step is finding a way to make graphs both trustworthy and instant - without compromising one for the other.
* ![No alternative text description for this image]()View C2PA information![]()````
[![]()8](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/leslieannsheppard_why-build-knowledge-graphs-at-all-because-activity-7384043746839060480-maDh&amp;trk=public_post_social-actions-reactions)``````````````
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/leslieannsheppard_why-build-knowledge-graphs-at-all-because-activity-7384043746839060480-maDh&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/leslieannsheppard_why-build-knowledge-graphs-at-all-because-activity-7384043746839060480-maDh&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/leslieannsheppard_why-build-knowledge-graphs-at-all-because-activity-7384043746839060480-maDh&trk=public_post_feed-cta-banner-cta)
``
* [](https://www.linkedin.com/posts/naresh-edagotti-6a71a1233_rag-activity-7382352050564591616-r4J2)
[![View profile for Naresh Edagotti]()](https://in.linkedin.com/in/naresh-edagotti-6a71a1233?trk=public_post_feed-actor-image)
[Naresh Edagotti](https://in.linkedin.com/in/naresh-edagotti-6a71a1233?trk=public_post_feed-actor-name)
2mo
* [Report this post](https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/posts/naresh-edagotti-6a71a1233_rag-activity-7382352050564591616-r4J2&amp;trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&amp;guestReportContentType=POST&amp;_f=guest-reporting)
ğ“ğ¡ğğ”ğ¥ğ­ğ¢ğ¦ğšğ­ğğ‘ğ€ğ†ğ‚ğ¡ğğšğ­ğ’ğ¡ğğğ­ğ¢ğ¬ğ¡ğğ«ğ! Most people are still struggling to connect LLMs with external data the right way. Thatâ€™s where Retrieval-Augmented Generation (RAG) comes in and this visual guide makes it super simple.
ğŸ“ŒWhat youâ€™ll get in this cheat sheet:
â€¢RAG basics explained in one flow
â€¢10 powerful RAG architectures (Standard â†’Agentic â†’Multi-Modal &amp; more)
â€¢Best practices for chunking, retrieval, and embeddings
â€¢Common challenges (like hallucination &amp; latency) + practical fixes
â€¢Popular tools &amp; vector DB comparisons
Whether youâ€™re just starting or scaling production AI systems, this guide helps you choose the right RAG setup without confusion.
ğŸ‘‰Swipe to explore the full cheat sheet and save it for reference.
â•Follow[Naresh Edagotti](https://in.linkedin.com/in/naresh-edagotti-6a71a1233?trk=public_post-text)for more content that makes complex AI topics feel simple.
````
[![]()![]()![]()256](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/naresh-edagotti-6a71a1233_rag-activity-7382352050564591616-r4J2&amp;trk=public_post_social-actions-reactions)``````````````[24 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/naresh-edagotti-6a71a1233_rag-activity-7382352050564591616-r4J2&amp;trk=public_post_social-actions-comments)
[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/naresh-edagotti-6a71a1233_rag-activity-7382352050564591616-r4J2&amp;trk=public_post_like-cta)[Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/naresh-edagotti-6a71a1233_rag-activity-7382352050564591616-r4J2&amp;trk=public_post_comment-cta)````
Share
* Copy
* LinkedIn
* Facebook
* X
To view or add a comment,[sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/naresh-edagotti-6a71a1233_rag-activity-7382352050564591616-r4J2&trk=public_post_feed-cta-banner-cta)
``
![](https://media.licdn.com/dms/image/v2/D5616AQGWPk9EpFYYZw/profile-displaybackgroundimage-shrink_200_800/B56ZoqF5UkG0AU-/0/1761642780946?e=2147483647&amp;v=beta&amp;t=6p9VFWiXytFeEgneF04NpVDchHJlqZypEZ3PDrOUkao)
![Cornellius Y.]()
43,636 followers
* [2,727 Posts](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/in/cornellius-yudha-wijaya/recent-activity/&amp;trk=public_post_follow-posts)
* [3 Articles](https://www.linkedin.com/today/author/cornellius-yudha-wijaya?trk=public_post_follow-articles)
[View Profile](https://id.linkedin.com/in/cornellius-yudha-wijaya?trk=public_post_follow-view-profile)[Connect](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/feed/update/urn:li:activity:7327889013090975745&amp;trk=public_post_follow)
## More from this author
* [
![]()
### From Biology with Data Science
Cornellius Y.6y
](https://www.linkedin.com/pulse/from-biology-data-science-cornellius-yudha-wijaya?trk=public_post)
* [
![]()
### Learning Time Series Stationary Properties
Cornellius Y.6y
](https://www.linkedin.com/pulse/learning-time-series-stationary-properties-cornellius-yudha-wijaya?trk=public_post)
* [
![]()
### Learning about the Time Series Building Block
Cornellius Y.6y
](https://www.linkedin.com/pulse/learning-time-series-building-block-cornellius-yudha-wijaya?trk=public_post)
## Explore related topics
* [How to Improve AI Using Rag Techniques](https://www.linkedin.com/top-content/artificial-intelligence/ai-prompt-improvement/how-to-improve-ai-using-rag-techniques/)
* [How to Build Intelligent Rag Systems](https://www.linkedin.com/top-content/artificial-intelligence/understanding-ai-systems/how-to-build-intelligent-rag-systems/)
* [How to Streamline RAG Pipeline Integration Workflows](https://www.linkedin.com/top-content/project-management/optimizing-workflow-processes/how-to-streamline-rag-pipeline-integration-workflows/)
* [How to Use RAG Architecture for Better Information Retrieval](https://www.linkedin.com/top-content/artificial-intelligence/retrieval-augmented-generation-guide/how-to-use-rag-architecture-for-better-information-retrieval/)
* [How to Improve RAG Retrieval Methods](https://www.linkedin.com/top-content/artificial-intelligence/retrieval-augmented-generation-guide/how-to-improve-rag-retrieval-methods/)
## Explore content categories
* [Career](https://www.linkedin.com/top-content/career/)
* [Productivity](https://www.linkedin.com/top-content/productivity/)
* [Finance](https://www.linkedin.com/top-content/finance/)
* [Soft Skills &amp; Emotional Intelligence](https://www.linkedin.com/top-content/soft-skills-emotional-intelligence/)
* [Project Management](https://www.linkedin.com/top-content/project-management/)
* [Education](https://www.linkedin.com/top-content/education/)
* [Technology](https://www.linkedin.com/top-content/technology/)
* [Leadership](https://www.linkedin.com/top-content/leadership/)
* [Ecommerce](https://www.linkedin.com/top-content/ecommerce/)
* [User Experience](https://www.linkedin.com/top-content/user-experience/)Show moreShow less
![]()## Sign in to view more content
Create your free account or sign in to continue your search
Sign in
## Welcome back
````````````````````
Email or phone
Password
Show
[Forgot password?](https://www.linkedin.com/uas/request-password-reset?trk=public_post_contextual-sign-in-modal_sign-in-modal_forgot_password)Sign in
or
By clicking Continue to join or sign in, you agree to LinkedInâ€™s[User Agreement](https://www.linkedin.com/legal/user-agreement?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_user-agreement),[Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_privacy-policy), and[Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=public_post_contextual-sign-in-modal_sign-in-modal_auth-button_cookie-policy).
New to LinkedIn?[Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&trk=public_post_contextual-sign-in-modal_sign-in-modal_join-link)
or
New to LinkedIn?[Join now](https://www.linkedin.com/signup/cold-join?session_redirect=https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR&trk=public_post_contextual-sign-in-modal_join-link)
By clicking Continue to join or sign in, you agree to LinkedInâ€™s[User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement),[Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and[Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).
``
