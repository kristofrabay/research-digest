# DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation

**URL:** https://arxiv.org/html/2505.07233v2
**Published:** 2024-01-01T00:00:00.000Z

---

## Summary

The user query asks for a summary of information related to: **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies.**

This webpage describes **DynamicRAG**, a novel Retrieval-Augmented Generation (RAG) framework.

Here is a summary of the relevant topics covered in the text:

*   **Rerankers:** The paper focuses heavily on the reranker component of RAG systems, proposing a **DynamicRAG** framework where the reranker dynamically adjusts both the order and the number ($k$) of retrieved documents based on the query. The reranker is modeled as an agent optimized through Reinforcement Learning (RL), using the Large Language Model's (LLM) output quality as a reward signal.
*   **RAG Architectures:** The paper details the standard RAG phases (Retrieval, Encoding, Generation) and introduces DynamicRAG as an improved architecture that uses LLM output feedback to optimize the reranker, contrasting it with traditional RAG systems that use static ranking thresholds.
*   **RAG Alternatives/Improvements:** DynamicRAG itself is an improvement over existing RAG methods, achieving state-of-the-art results compared to baselines like Reward-RAG, RankRAG, and others.

**Topics NOT explicitly covered in detail:**

*   **Vector databases:** Not mentioned.
*   **Embeddings (new efficient models):** The text mentions that the retrieval phase can use "dense retrieval with embeddings," but it does not discuss specific new or efficient embedding models.
*   **Hybrid search:** Not mentioned.
*   **Chunking strategies:** Not mentioned.

**Summary:**

The webpage details the **DynamicRAG** framework, which significantly improves **RAG architectures** by introducing a **reranker** optimized via Reinforcement Learning using feedback from the generator's output quality. This allows the reranker to dynamically adjust the number and order of retrieved documents. The paper compares DynamicRAG against various **RAG alternatives** and baselines, showing superior performance. However, the text does not discuss **vector databases**, specific **embeddings (new efficient models)**, **hybrid search**, or **chunking strategies**.

The user query asks for a summary covering several topics related to Retrieval-Augmented Generation (RAG), including: **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, and chunking strategies.**

The provided web page text focuses on a specific RAG architecture called **DynamicRAG**, which leverages Large Language Model (LLM) outputs as feedback for **dynamic reranking**.

Here is a summary of the relevant points from the text concerning the user's query:

*   **Rerankers:** The paper heavily focuses on reranking. It compares different Reranker and Generator model sizes (e.g., 7B vs 13B) and finds that a larger Reranker can enhance performance. It also introduces a method where the Reranker and Generator share parameters, leading to improved performance. The core of DynamicRAG is using the LLM response quality as a reward signal to optimize the reranker via Reinforcement Learning (RL) and Direct Preference Optimization (DPO).
*   **RAG Architectures:** The paper introduces **DynamicRAG**, a novel RL framework that optimizes reranking dynamically based on feedback. It is compared against **Vanilla-RAG** and shows significant performance improvements. Ablation studies confirm the critical importance of retrieval and the beneficial effect of reranking and RL in the architecture.
*   **Embeddings/Retrieval:** The text mentions using different **retrievers** (DPR, Contriever, MonoT5) and shows that DynamicRAG's performance improves as the underlying retriever models get better, demonstrating robustness. The initial retrieval step uses the top 45 documents retrieved by **Contriever-MS MARCO**.
*   **Efficiency:** The DynamicRAG model is shown to be highly efficient, requiring only two LLM calls in one evaluation scenario and demonstrating approximately **17x superior throughput** compared to a sequential scoring approach like RankRAG, as it processes documents contextually rather than sequentially.

**Topics not explicitly covered in detail:**

*   **Vector databases:** Not mentioned.
*   **Embeddings (new efficient models):** While retrievers are used (which rely on embeddings), the text does not detail new efficient embedding models themselves, only the performance impact of using different existing retrievers.
*   **RAG alternatives:** The text discusses comparisons to Vanilla-RAG

---

## Full Content

DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation
# DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation
Jiashuo Sun,
Xianrui Zhong,
Sizhe Zhou,
Jiawei Han
University of Illinois Urbana-Champaign
{jiashuo5, hanj}@illinois.eduCorresponding author.
###### Abstract
Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks.
A crucial but often under-explored component of these systems is the reranker. Since irrelevant documents in RAG systems can mislead the generator, the reranker plays a vital role in refining retrieved documents to enhance generation quality and explainability.
However, it is challenging to determine the appropriate number of documents (kð‘˜kitalic\_k) that the reranker should select: too few may result in missing critical information, while too many introduce noise and inefficiencies.
Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions.
In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query.
We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality.
Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results among models of same parameter sizes. The model, data and code are available at[https://github.com/GasolSun36/DynamicRAG](https://github.com/GasolSun36/DynamicRAG).
## 1Introduction
Retrieval-augmented generation (RAG) systems have emerged as a powerful approach for combining the strengths of large language models (LLMs) with external knowledge retrieval.
This integration has proven highly effective for addressing knowledge-intensive tasks and incorporating up-to-date information into LLMs, leading to notable performance improvements> [
[> 17
](https://arxiv.org/html/2505.07233v2#bib.bib17)> , [> 24
](https://arxiv.org/html/2505.07233v2#bib.bib24)> , [> 14
](https://arxiv.org/html/2505.07233v2#bib.bib14)> ]
.
RAG systems often suffer from two critical challenges: misleading irrelevant retrieved documents that can distort the generation process, and the â€™lost-in-the-middleâ€™ phenomenon where important information gets buried within long lists of retrieved candidates. A crucial, yet often underappreciated, component that addresses these issues is the reranker, which assesses the relevance of retrieved documents. The reranker is critical for improving the quality of generated text and enhancing explainability, thereby serving as an indispensable part of the RAG framework> [
[> 33
](https://arxiv.org/html/2505.07233v2#bib.bib33)> , [> 42
](https://arxiv.org/html/2505.07233v2#bib.bib42)> ]
In RAG systems, the rerankerâ€™s primary role is to refine the Top-Nð‘Nitalic\_Ndocuments retrieved by the retriever, selecting thekð‘˜kitalic\_kmost relevant ones to enhance the answer quality. However, determining the optimal k remains a challenging problem, as highlighted in previous studies> [
[> 36
](https://arxiv.org/html/2505.07233v2#bib.bib36)> , [> 60
](https://arxiv.org/html/2505.07233v2#bib.bib60)> ]
. Akð‘˜kitalic\_kthat is too small risks omitting critical information, leading to degraded generation quality, while a largerkð‘˜kitalic\_kmay introduce irrelevant content, increasing noise and potentially misleading the generator. Furthermore, incorporating excessively long contexts can reduce both efficiency and effectiveness, further complicating the balance between relevance and performance in RAG systems. Striking the right balance requires adaptive strategies that can dynamically adjustkð‘˜kitalic\_kbased on query complexity and document diversity, as shown in Figure[1](https://arxiv.org/html/2505.07233v2#S1.F1).
![Refer to caption](x1.png)Figure 1:Illustration of our dynamic reranker framework. (a) It represents a RAG system without a reranker, where the system primarily focuses on training LLMs. (b) It represents a RAG system with a reranker, where the reranker is trained to filter the Top-Nð‘Nitalic\_Ndocuments to a fixed Top-Kð¾Kitalic\_K, which remains constant for all queries. (c) In contrast, it represents our dynamic reranker, where both the reranker and the generator are trained simultaneously. The dynamic reranker adapts to the difficulty of each query by dynamically determining the value ofkð‘˜kitalic\_k.
Recent work has demonstrated the effectiveness of LLM-based rerankers> [
[> 9
](https://arxiv.org/html/2505.07233v2#bib.bib9)> , [> 51
](https://arxiv.org/html/2505.07233v2#bib.bib51)> , [> 26
](https://arxiv.org/html/2505.07233v2#bib.bib26)> , [> 23
](https://arxiv.org/html/2505.07233v2#bib.bib23)> , [> 30
](https://arxiv.org/html/2505.07233v2#bib.bib30)> ]
, which leverage large language modelsâ€™ capabilities to assess document relevance and improve ranking quality. Other works have also used the understanding capabilities of LLMs through sliding window mechanisms to achieve optimal re-ranking results> [
[> 51
](https://arxiv.org/html/2505.07233v2#bib.bib51)> , [> 5
](https://arxiv.org/html/2505.07233v2#bib.bib5)> ]
.
While these studies demonstrate the effectiveness of LLM-based rerankers, they typically rely on fixed document selection thresholds (kð‘˜kitalic\_k) and fail to dynamically adapt to varying query complexity and retrieval quality. Existing approaches primarily exploit LLMsâ€™ internal knowledge to score documents independently, overlooking a key insight: the actual generation quality when using different numbers of documents provides direct feedback about the optimalkð‘˜kitalic\_k. This natural reward signal enables us to apply reinforcement learning, where the LLMâ€™s generation quality serves as the reward for selecting the right number of documents. This supervisory signalâ€”derived from the downstream generation task itselfâ€”offers a more principled approach to document selection than static thresholds or isolated relevance scoring. For instance, the quality of an LLMâ€™s responseâ€”given a query and a ranked set of documentsâ€”serves as a direct indicator of document relevance.
Based on these insights, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. In DynamicRAG, the reranker is modeled as an agent optimized through reinforcement learning (RL), with rewards derived from the evaluated quality of LLM outputs. The entire training process consists of two stages. First, we adopt behavior cloning by collecting expert trajectories and training the reranker via supervised fine-tuning (SFT). This provides the reranker with a basic understanding of the dynamic reranking task while reducing the complexity of the action space. Second, we treat the generator as an interactive environment that provides feedback, enabling the reranker to explore, collect trajectories, and update itself through reinforcement learning.
We comprehensively evaluate DynamicRAG on knowledge-intensive tasks across seven datasets, including general QA (NQ> [
[> 25
](https://arxiv.org/html/2505.07233v2#bib.bib25)> ]
, TriviaQA> [
[> 19
](https://arxiv.org/html/2505.07233v2#bib.bib19)> ]
), multi-hop reasoning (HotpotQA> [
[> 57
](https://arxiv.org/html/2505.07233v2#bib.bib57)> ]
, 2WikimQA> [
[> 15
](https://arxiv.org/html/2505.07233v2#bib.bib15)> ]
), long-form generation (ASQA> [
[> 49
](https://arxiv.org/html/2505.07233v2#bib.bib49)> ]
, ELI5> [
[> 10
](https://arxiv.org/html/2505.07233v2#bib.bib10)> ]
), and fact verification (FEVER> [
[> 53
](https://arxiv.org/html/2505.07233v2#bib.bib53)> ]
). Additionally, we assess the recall results of DynamicRAGâ€™s reranker on NQ and HotpotQA. Experimental results show that DynamicRAG significantly outperforms existing fine-tuned and prompting-based approaches, achieving state-of-the-art (SOTA) performance among models of same size while requiring substantially less training data.
## 2Preliminaries
### 2.1Retrieval Augmentation Generation
#### 2.1.1Retrieval Phase
The first step in the RAG framework is to retrieve relevant documents or passages from a large corpus. This is typically done using an information retrieval (IR) system. Given a queryðªðª\\mathbf{q}bold\_q, the IR system selects a set of relevant documentsD1,D2,â€¦,Dksubscriptð·1subscriptð·2â€¦subscriptð·ð‘˜D\_{1},D\_{2},\\dots,D\_{k}italic\_D start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , â€¦, italic\_D start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPTbased on some retrieval method, such as BM25, dense retrieval with embeddings, or a hybrid approach.
Formally, the retrieval process can be represented as:
|D=Retrieverâ¢(ðª,ð’ž),ð·Retrieverðªð’žD=\\texttt{Retriever}(\\mathbf{q},\\mathcal{C}),italic\_D = Retriever ( bold\_q , caligraphic\_C ) ,||(1)|
whereð’žð’ž\\mathcal{C}caligraphic\_Cis the document corpus, andDð·Ditalic\_Drepresents the set of retrieved documents.
To quantify the relevance scoresâ¢(Di,ðª)ð‘ subscriptð·ð‘–ðªs(D\_{i},\\mathbf{q})italic\_s ( italic\_D start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT , bold\_q )of each documentDisubscriptð·ð‘–D\_{i}italic\_D start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPTwith respect to the queryðªðª\\mathbf{q}bold\_q, we define:
|sâ¢(Di,ðª)=Scoreâ¢(Di,ðª),ð‘ subscriptð·ð‘–ðªScoresubscriptð·ð‘–ðªs(D\_{i},\\mathbf{q})=\\text{Score}(D\_{i},\\mathbf{q}),italic\_s ( italic\_D start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT , bold\_q ) = Score ( italic\_D start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT , bold\_q ) ,||(2)|
whereScoreâ¢(â‹…)Scoreâ‹…\\text{Score}(\\cdot)Score ( â‹…)is a function specific to the retrieval method employed (e.g., BM25 score, cosine similarity for dense embeddings).
#### 2.1.2Encoding Phase
Once the relevant documents have been retrieved, both the queryðªðª\\mathbf{q}bold\_qand the documents{D1,D2,â€¦,Dk}subscriptð·1subscriptð·2â€¦subscriptð·ð‘˜\\{D\_{1},D\_{2},\\dots,D\_{k}\\}{ italic\_D start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , â€¦, italic\_D start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPT }are encoded into fixed-size vectors using a neural encoder such as BERT> [
[> 8
](https://arxiv.org/html/2505.07233v2#bib.bib8)> ]
, RoBERTa> [
[> 31
](https://arxiv.org/html/2505.07233v2#bib.bib31)> ]
, or other transformer-based models. The goal is to obtain a dense representation of both the query and documents.
Letðªencsubscriptðªenc\\mathbf{q}\_{\\text{enc}}bold\_q start\_POSTSUBSCRIPT enc end\_POSTSUBSCRIPTandðƒisubscriptðƒð‘–\\mathbf{D}\_{i}bold\_D start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPTrepresent the encoded query and the encoding of documentDisubscriptð·ð‘–D\_{i}italic\_D start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT, respectively:
|ðªenc=Encodeâ¢(ðª),ðƒi=Encodeâ¢(Di)fori=1,2,â€¦,k.formulae-sequencesubscriptðªencEncodeðªformulae-sequencesubscriptðƒð‘–Encodesubscriptð·ð‘–forð‘–12â€¦ð‘˜\\begin{split}\\mathbf{q}\_{\\text{enc}}&amp;&amp;=\\text{Encode}(\\mathbf{q}),\\\\
\\mathbf{D}\_{i}&amp;&amp;=\\text{Encode}(D\_{i})\\quad\\text{for}\\quad i=1,2,\\dots,k.\\end{split}start\_ROW start\_CELL bold\_q start\_POSTSUBSCRIPT enc end\_POSTSUBSCRIPT end\_CELL start\_CELL = Encode ( bold\_q ) , end\_CELL end\_ROW start\_ROW start\_CELL bold\_D start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_CELL start\_CELL = Encode ( italic\_D start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT ) for italic\_i = 1 , 2 , â€¦, italic\_k . end\_CELL end\_ROW||(3)|
#### 2.1.3Generation Phase
After encoding, the next phase is to generate an answer or response based on the query and retrieved documents. The generation model takes both the query and the retrieved documents as input and generates a responsey^^ð‘¦\\hat{y}over^ start\_ARG italic\_y end\_ARG. The generation process can be framed as maximizing the conditional probability:
|y^=argmaxyâ¢pâ¢(yâˆ£ðª,D1,D2,â€¦,Dk),^ð‘¦subscriptargmaxð‘¦ð‘conditionalð‘¦ðªsubscriptð·1subscriptð·2â€¦subscriptð·ð‘˜\\hat{y}=\\text{argmax}\_{y}\\,p(y\\mid\\mathbf{q},D\_{1},D\_{2},\\dots,D\_{k}),over^ start\_ARG italic\_y end\_ARG = argmax start\_POSTSUBSCRIPT italic\_y end\_POSTSUBSCRIPT italic\_p ( italic\_y âˆ£bold\_q , italic\_D start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , â€¦, italic\_D start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPT ) ,||(4)|
wherepâ¢(yâˆ£ðª,D1,D2,â€¦,Dk)ð‘conditionalð‘¦ðªsubscriptð·1subscriptð·2â€¦subscriptð·ð‘˜p(y\\mid\\mathbf{q},D\_{1},D\_{2},\\dots,D\_{k})italic\_p ( italic\_y âˆ£bold\_q , italic\_D start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , â€¦, italic\_D start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPT )is the likelihood of generating the outputyð‘¦yitalic\_ygiven the queryðªðª\\mathbf{q}bold\_qand the retrieved documents.
To incorporate the encoded representations, we model the conditional probability as:
|pâ¢(yâˆ£ðª,D1,â€¦,Dk)=Decodeâ¢(ðª,D1,â€¦,Dk).ð‘conditionalð‘¦ðªsubscriptð·1â€¦subscriptð·ð‘˜Decodeðªsubscriptð·1â€¦subscriptð·ð‘˜p(y\\mid\\mathbf{q},D\_{1},\\dots,D\_{k})=\\text{Decode}(\\mathbf{q},D\_{1},\\dots,D\_{k%
}).italic\_p ( italic\_y âˆ£bold\_q , italic\_D start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , â€¦, italic\_D start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPT ) = Decode ( bold\_q , italic\_D start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , â€¦, italic\_D start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPT ) .||(5)|
Finally, the generation model is trained to maximize the likelihood of generating the correct response given the query and the retrieved documents. The loss function can be expressed as:
|â„’=âˆ’logâ¡pâ¢(yâˆ£ðª,D1,D2,â€¦,Dk),â„’ð‘conditionalð‘¦ðªsubscriptð·1subscriptð·2â€¦subscriptð·ð‘˜\\mathcal{L}=-\\log p(y\\mid\\mathbf{q},D\_{1},D\_{2},\\dots,D\_{k}),caligraphic\_L = - roman\_log italic\_p ( italic\_y âˆ£bold\_q , italic\_D start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , â€¦, italic\_D start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPT ) ,||(6)|
whereyð‘¦yitalic\_yis the ground-truth response andðªðª\\mathbf{q}bold\_q,D1,â€¦,Dksubscriptð·1â€¦subscriptð·ð‘˜D\_{1},\\dots,D\_{k}italic\_D start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , â€¦, italic\_D start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPTare the input query and retrieved documents.
### 2.2Learning from Environment
We model the Reranker as a player and the generator as an environmentð’¢ð’¢\\mathcal{G}caligraphic\_G. We can formalize the rerankerâ€™s task in the environment as a partially observable Markov decision process(â„,ð’®,ð’œ,ð’¯,â„)â„ð’®ð’œð’¯â„(\\mathcal{I},\\mathcal{S},\\mathcal{A},\\mathcal{T},\\mathbb{R})( caligraphic\_I , caligraphic\_S , caligraphic\_A , caligraphic\_T , blackboard\_R ), whereâ„â„\\mathcal{I}caligraphic\_Iis the instruction space,ð’®ð’®\\mathcal{S}caligraphic\_Sis the state space,ð’œð’œ\\mathcal{A}caligraphic\_Ais the action space,ð’¯:ð’®Ã—ð’œâ†’ð’®:ð’¯â†’ð’®ð’œð’®\\mathcal{T}:\\mathcal{S}\\times\\mathcal{A}\\to\\mathcal{S}caligraphic\_T : caligraphic\_S Ã—caligraphic\_A â†’caligraphic\_Sis the deterministic state transition function, andâ„:ð’®Ã—ð’œâ†’â„›:â„â†’ð’®ð’œâ„›\\mathbb{R}:\\mathcal{S}\\times\\mathcal{A}\\to\\mathcal{R}blackboard\_R : caligraphic\_S Ã—caligraphic\_A â†’caligraphic\_Ris the reward function. In our design, we exclude explicit observationsð’ªð’ª\\mathcal{O}caligraphic\_Ohere since we focus on the overall reward of the rerankerâ€™s complete episode in the environment, rather than step-wise rewards. We leave observation-based optimization for future work.
Given a task instructionið‘–iitalic\_iin environmentð’¢ð’¢\\mathcal{G}caligraphic\_G, the Reranker generates an action sequencea1,a2,â€¦,aTsubscriptð‘Ž1subscriptð‘Ž2â€¦subscriptð‘Žð‘‡a\_{1},a\_{2},\\dots,a\_{T}italic\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_a start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , â€¦, italic\_a start\_POSTSUBSCRIPT italic\_T end\_POSTSUBSCRIPTbased on its policyÏ€Î¸subscriptðœ‹ðœƒ\\pi\_{\\theta}italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT, where each actionatâˆ¼Ï€Î¸(â‹…|ð’¢,i,a1,â€¦,atâˆ’1)a\_{t}\\sim\\pi\_{\\theta}(\\cdot|\\mathcal{G},i,a\_{1},\\dots,a\_{t-1})italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT âˆ¼italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( â‹…| caligraphic\_G , italic\_i , italic\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , â€¦, italic\_a start\_POSTSUBSCRIPT italic\_t - 1 end\_POSTSUBSCRIPT )is determined by the history of previous actions. The trajectory is represented as:
|Ï„=(a1,â€¦,aT)âˆ¼Ï€Î¸â¢(Ï„|ð’¢,i)ðœsubscriptð‘Ž1â€¦subscriptð‘Žð‘‡similar-tosubscriptðœ‹ðœƒconditionalðœð’¢ð‘–\\tau=(a\_{1},\\dots,a\_{T})\\sim\\pi\_{\\theta}(\\tau|\\mathcal{G},i)italic\_Ï„ = ( italic\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , â€¦, italic\_a start\_POSTSUBSCRIPT italic\_T end\_POSTSUBSCRIPT ) âˆ¼italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_Ï„ | caligraphic\_G , italic\_i )||(7)|
|Ï€Î¸â¢(Ï„|ð’¢,i)=âˆt=1TÏ€Î¸â¢(at|ð’¢,i,htâˆ’1)subscriptðœ‹ðœƒconditionalðœð’¢ð‘–superscriptsubscriptproductð‘¡1ð‘‡subscriptðœ‹ðœƒconditionalsubscriptð‘Žð‘¡ð’¢ð‘–subscriptâ„Žð‘¡1\\pi\_{\\theta}(\\tau|\\mathcal{G},i)=\\prod\_{t=1}^{T}\\pi\_{\\theta}(a\_{t}|\\mathcal{G}%
,i,h\_{t-1})italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_Ï„ | caligraphic\_G , italic\_i ) = âˆstart\_POSTSUBSCRIPT italic\_t = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | caligraphic\_G , italic\_i , italic\_h start\_POSTSUBSCRIPT italic\_t - 1 end\_POSTSUBSCRIPT )||(8)|
whereTð‘‡Titalic\_Tis the number of steps in interaction, andhtâˆ’1=(a1,â€¦,atâˆ’1)subscriptâ„Žð‘¡1subscriptð‘Ž1â€¦subscriptð‘Žð‘¡1h\_{t-1}=(a\_{1},\\dots,a\_{t-1})italic\_h start\_POSTSUBSCRIPT italic\_t - 1 end\_POSTSUBSCRIPT = ( italic\_a start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , â€¦, italic\_a start\_POSTSUBSCRIPT italic\_t - 1 end\_POSTSUBSCRIPT )represents the history of action sequences up to steptâˆ’1ð‘¡1t-1italic\_t - 1. The final rewardrâ¢(ð’¢,i,Ï„)âˆˆ[0,1]ð‘Ÿð’¢ð‘–ðœ01r(\\mathcal{G},i,\\tau)\\in[0,1]italic\_r ( caligraphic\_G , italic\_i , italic\_Ï„ ) âˆˆ[ 0 , 1 ]is computed based on the quality of the generatorâ€™s response.
## 3DynamicRAG
In this section, we propose DynamicRAG. Unlike traditional RAG systems that rely on static ranking methods, DynamicRAG introduces a dynamic reranking mechanism and leverages feedback from LLM output to further refine the reranker, thereby achieving overall optimization of the RAG system.
The DynamicRAG framework consists of three key components: (1) a frozen retriever, (2) a trainable dynamic reranker, and (3) a trainable generator that is optimized to effectively leverage the rerankerâ€™s dynamically selectedkð‘˜kitalic\_kdocuments. The retriever retrieves relevant documents from a large corpus, while the reranker dynamically determines both the order and the number of documents to be passed to the generator to produce an answer. The generator then produces the final output based on the rerankerâ€™s selected documents. By iteratively training the reranker and generator, DynamicRAG achieves improvements in the overall efficiency and effectiveness of the RAG system.
### 3.1Dynamic Reranking
Traditional reranking approaches rely on static ranking models that determine the relevance of retrieved documents independently of the generation task. These models typically operate within a fixed-length input framework, where a list comprisingnð‘›nitalic\_ndocuments serves as the input, and the output is a reordered sequence containing the samenð‘›nitalic\_ndocuments. This inherent limitation prevents these static models from dynamically adapting to the specific needs of the generation process, particularly in terms of the number and arrangement of the selected documents. In contrast, DynamicRAG uses feedback from the generator to guide the reranking process, allowing it to dynamically adjust both the order and the number of documents selected. This enables the reranker to optimize the input of the generator, maximizing the likelihood of producing high-quality output (In this paper, we only consider the list-wise ranking).
![Refer to caption](x2.png)Figure 2:Illustration of the training paradigm for our method. We treat Dynamic Reranker as an Agent, which interacts with the Environment, generating Top-K docs and receiving rewards to optimize itself.
To formalize this operation, the reranking process can be expressed as a single function that directly outputs a reordered subset of the initially retrieved documents. Given a queryðªðª\\mathbf{q}bold\_qand a set of retrieved documentsD={D1,D2,â€¦,Dk}ð·subscriptð·1subscriptð·2â€¦subscriptð·ð‘˜D=\\{D\_{1},D\_{2},\\dots,D\_{k}\\}italic\_D = { italic\_D start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_D start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , â€¦, italic\_D start\_POSTSUBSCRIPT italic\_k end\_POSTSUBSCRIPT }, the reranker computes:
|D^=RerankerÎ¸râ¢(ðª,D),^ð·subscriptRerankersubscriptðœƒð‘Ÿðªð·\\hat{D}=\\texttt{Reranker}\_{\\theta\_{r}}(\\mathbf{q},D),over^ start\_ARG italic\_D end\_ARG = Reranker start\_POSTSUBSCRIPT italic\_Î¸ start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ( bold\_q , italic\_D ) ,||(9)|
whereD^âŠ‚D^ð·ð·\\hat{D}\\subset Dover^ start\_ARG italic\_D end\_ARG âŠ‚italic\_Dis the selected and reordered subset of documents, andÎ¸rsubscriptðœƒð‘Ÿ\\theta\_{r}italic\_Î¸ start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPTrepresents the parameters of the reranker model. This formulation encapsulates both the scoring and selection processes, producing a subsetD^^ð·\\hat{D}over^ start\_ARG italic\_D end\_ARGwith dynamically adjusted order and size. This dynamic adjustment enables the model to exhibit greater flexibility in accommodating diverse queries and the specific requirements of the generation task.
### 3.2Training with Interacting
In this section, we outline the training process for the Reranker, where the architectural overview of which is schematically depicted in Figure[2](https://arxiv.org/html/2505.07233v2#S3.F2). The training begins with behavior cloning, which equips the Reranker with the basic ability to adjust both the order and the number of selected documents. Building upon this foundational model, we further refine the Rerankerâ€™s performance through interactions with the environment. This interaction allows the Reranker to collect feedback from multiple trajectories and enhance its decision-making policy. The complete training procedure is illustrated in Figure[3](https://arxiv.org/html/2505.07233v2#S3.F3)and is presented in detail in Algorithm[1](https://arxiv.org/html/2505.07233v2#alg1).
#### 3.2.1Behavioral Cloning with Expert Trajectories
Behavioral cloning> [
[> 56
](https://arxiv.org/html/2505.07233v2#bib.bib56)> , [> 61
](https://arxiv.org/html/2505.07233v2#bib.bib61)> ]
is used to supervised fine-tuning the Reranker by mimicking expert trajectories, allowing the model to learn the fundamental actions required for effective ranking. In this stage, the Reranker focuses on learning how to predict the correct intermediate actionsatsubscriptð‘Žð‘¡a\_{t}italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPTbased on the given task and context.
To achieve this, the Reranker is trained on a dataset of expert demonstrations, denoted asDesubscriptð·ð‘’D\_{e}italic\_D start\_POSTSUBSCRIPT italic\_e end\_POSTSUBSCRIPT. This enables the model to acquire basic instruction-following capabilities and leverage prior knowledge. The training objective is to maximize the likelihood of the expertâ€™s document selection decisions:
|ð’¥Bâ¢Câ¢(Î¸)=ð”¼(q,D,kâˆ—)âˆ¼ð’Ÿeâ¢[logâ¡Ï€Î¸â¢(k|q,D)],subscriptð’¥ðµð¶ðœƒsubscriptð”¼similar-toð‘žð·superscriptð‘˜subscriptð’Ÿð‘’delimited-[]subscriptðœ‹ðœƒconditionalð‘˜ð‘žð·\\mathcal{J}\_{BC}(\\theta)=\\mathbb{E}\_{(q,D,k^{\*})\\sim\\mathcal{D}\_{e}}\\left[\\log%
\\pi\_{\\theta}(k|q,D)\\right],caligraphic\_J start\_POSTSUBSCRIPT italic\_B italic\_C end\_POSTSUBSCRIPT ( italic\_Î¸ ) = blackboard\_E start\_POSTSUBSCRIPT ( italic\_q , italic\_D , italic\_k start\_POSTSUPERSCRIPT âˆ—end\_POSTSUPERSCRIPT ) âˆ¼caligraphic\_D start\_POSTSUBSCRIPT italic\_e end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT [ roman\_log italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_k | italic\_q , italic\_D ) ] ,||(10)|
whereqð‘žqitalic\_qdenotes the query,D={d1,d2,â€¦,dN}ð·subscriptð‘‘1subscriptð‘‘2â€¦subscriptð‘‘ð‘D=\\{d\_{1},d\_{2},...,d\_{N}\\}italic\_D = { italic\_d start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT , italic\_d start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT , â€¦, italic\_d start\_POSTSUBSCRIPT italic\_N end\_POSTSUBSCRIPT }represents the set of retrieved documents,kâˆ—superscriptð‘˜k^{\*}italic\_k start\_POSTSUPERSCRIPT âˆ—end\_POSTSUPERSCRIPTis the expert-demonstrated optimal number of documents, andÏ€Î¸â¢(k|q,D)subscriptðœ‹ðœƒconditionalð‘˜ð‘žð·\\pi\_{\\theta}(k|q,D)italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_k | italic\_q , italic\_D )represents the conditional probability of selectingkð‘˜kitalic\_kdocuments from the candidate setDð·Ditalic\_Dgiven the queryqð‘žqitalic\_q.
![Refer to caption](x3.png)Figure 3:Illustration of our training framework. During the training phase, we have a total of six steps. First, we retrieve the Top-Nð‘Nitalic\_Ndocuments based on the given question. Then, we use an expert model to score each document and filter a subset of data for behavior cloning by the dynamic reranker. Next, we use the trained dynamic reranker to sample multiple different trajectories. The responses generated by the generator serve as rewards to evaluate the quality of the trajectories, and we select the trajectory pairs with the highest and lowest rewards as the training data for DPO. During the inference phase, DynamicRAG only require two LLM inferences.
#### 3.2.2Optimizing the Reranker via Exploration and Feedback
After the initial training with behavioral cloning, the Reranker requires further refinement to align its actions with the goal of maximizing response quality. This is achieved through an interactive learning process, in which the Reranker continually interacts with the environment to gather feedback, progressively improving its action policy.
The primary objective is to train a policyÏ€Î¸subscriptðœ‹ðœƒ\\pi\_{\\theta}italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPTthat maximizes the expected reward across all trajectoriesÏ„ðœ\\tauitalic\_Ï„for a given environmentð’¢ð’¢\\mathcal{G}caligraphic\_Gand user instructionið‘–iitalic\_i:
|ð’¥exploreâ¢(Î¸)=ð”¼ð’¢,iâˆ¼ð”¾â¢ð”¼Ï„âˆ¼Ï€Î¸â¢(Ï„|ð’¢,i)â¢[â„â¢(ð’¢,i,Ï„)],subscriptð’¥exploreðœƒsubscriptð”¼similar-toð’¢ð‘–ð”¾subscriptð”¼similar-toðœsubscriptðœ‹ðœƒconditionalðœð’¢ð‘–delimited-[]â„ð’¢ð‘–ðœ\\mathcal{J}\_{\\text{explore}}(\\theta)=\\mathbb{E}\_{\\mathcal{G},i\\sim\\mathbb{G}}%
\\mathbb{E}\_{\\tau\\sim\\pi\_{\\theta}(\\tau|\\mathcal{G},i)}[\\mathbb{R}(\\mathcal{G},i%
,\\tau)],caligraphic\_J start\_POSTSUBSCRIPT explore end\_POSTSUBSCRIPT ( italic\_Î¸ ) = blackboard\_E start\_POSTSUBSCRIPT caligraphic\_G , italic\_i âˆ¼blackboard\_G end\_POSTSUBSCRIPT blackboard\_E start\_POSTSUBSCRIPT italic\_Ï„ âˆ¼italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_Ï„ | caligraphic\_G , italic\_i ) end\_POSTSUBSCRIPT [ blackboard\_R ( caligraphic\_G , italic\_i , italic\_Ï„ ) ] ,||(11)|
whereâ„â¢(ð’¢,i,Ï„)â„ð’¢ð‘–ðœ\\mathbb{R}(\\mathcal{G},i,\\tau)blackboard\_R ( caligraphic\_G , italic\_i , italic\_Ï„ )quantifies the response quality. However, optimizing this objective is challenging due to the complexity of the action space and the inherent inefficiencies in sampling. To address this, we adopt the Direct Preference Optimization (DPO) framework> [
[> 44
](https://arxiv.org/html/2505.07233v2#bib.bib44)> ]
, which simplifies the reward optimization with pairwise comparisons of trajectories.
Formally, given a set of sampled trajectories{Ï„i}i=1Nsuperscriptsubscriptsubscriptðœð‘–ð‘–1ð‘\\{\\tau\_{i}\\}\_{i=1}^{N}{ italic\_Ï„ start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT } start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_N end\_POSTSUPERSCRIPTwith rewards{ri}i=1Nsuperscriptsubscriptsubscriptð‘Ÿð‘–ð‘–1ð‘\\{r\_{i}\\}\_{i=1}^{N}{ italic\_r start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT } start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_N end\_POSTSUPERSCRIPT, we identify the trajectory pair(Ï„+,Ï„âˆ’)superscriptðœsuperscriptðœ(\\tau^{+},\\tau^{-})( italic\_Ï„ start\_POSTSUPERSCRIPT + end\_POSTSUPERSCRIPT , italic\_Ï„ start\_POSTSUPERSCRIPT - end\_POSTSUPERSCRIPT )such that:
|Ï„+=argâ¡maxÏ„iâ¡ri,Ï„âˆ’=argâ¡minÏ„iâ¡ri.formulae-sequencesuperscriptðœsubscriptsubscriptðœð‘–subscriptð‘Ÿð‘–superscriptðœsubscriptsubscriptðœð‘–subscriptð‘Ÿð‘–\\tau^{+}=\\arg\\max\_{\\tau\_{i}}r\_{i},\\quad\\tau^{-}=\\arg\\min\_{\\tau\_{i}}r\_{i}.italic\_Ï„ start\_POSTSUPERSCRIPT + end\_POSTSUPERSCRIPT = roman\_arg roman\_max start\_POSTSUBSCRIPT italic\_Ï„ start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT italic\_r start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT , italic\_Ï„ start\_POSTSUPERSCRIPT - end\_POSTSUPERSCRIPT = roman\_arg roman\_min start\_POSTSUBSCRIPT italic\_Ï„ start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT italic\_r start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT .||(12)|
The whole training objective is then defined as:
|ð’¥DPOâ¢(Î¸)=ð”¼(Ï„+,Ï„âˆ’)â¢[logâ¡Ïƒâ¢(Î²â¢(logâ¡Ï€Î¸â¢(Ï„+)âˆ’logâ¡Ï€Î¸â¢(Ï„âˆ’)))],subscriptð’¥DPOðœƒsubscriptð”¼superscriptðœsuperscriptðœdelimited-[]ðœŽð›½subscriptðœ‹ðœƒsuperscriptðœsubscriptðœ‹ðœƒsuperscriptðœ\\mathcal{J}\_{\\text{DPO}}(\\theta)=\\mathbb{E}\_{(\\tau^{+},\\tau^{-})}\\Big{[}\\log%
\\sigma\\big{(}\\beta(\\log\\pi\_{\\theta}(\\tau^{+})-\\log\\pi\_{\\theta}(\\tau^{-}))\\big{%
)}\\Big{]},caligraphic\_J start\_POSTSUBSCRIPT DPO end\_POSTSUBSCRIPT ( italic\_Î¸ ) = blackboard\_E start\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT + end\_POSTSUPERSCRIPT , italic\_Ï„ start\_POSTSUPERSCRIPT - end\_POSTSUPERSCRIPT ) end\_POSTSUBSCRIPT [ roman\_log italic\_Ïƒ ( italic\_Î² ( roman\_log italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT + end\_POSTSUPERSCRIPT ) - roman\_log italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT - end\_POSTSUPERSCRIPT ) ) ) ] ,||(13)|
|Ï€Î¸â¢(Ï„+)=Ï€Î¸â¢(Ï„+|ð’¢,i)Ï€refâ¢(Ï„+|ð’¢,i),Ï€Î¸â¢(Ï„âˆ’)=Ï€Î¸â¢(Ï„âˆ’|ð’¢,i)Ï€refâ¢(Ï„âˆ’|ð’¢,i).formulae-sequencesubscriptðœ‹ðœƒsuperscriptðœsubscriptðœ‹ðœƒconditionalsuperscriptðœð’¢ð‘–subscriptðœ‹refconditionalsuperscriptðœð’¢ð‘–subscriptðœ‹ðœƒsuperscriptðœsubscriptðœ‹ðœƒconditionalsuperscriptðœð’¢ð‘–subscriptðœ‹refconditionalsuperscriptðœð’¢ð‘–\\pi\_{\\theta}(\\tau^{+})=\\frac{\\pi\_{\\theta}(\\tau^{+}|\\mathcal{G},i)}{\\pi\_{\\text{%
ref}}(\\tau^{+}|\\mathcal{G},i)},\\quad\\pi\_{\\theta}(\\tau^{-})=\\frac{\\pi\_{\\theta}(%
\\tau^{-}|\\mathcal{G},i)}{\\pi\_{\\text{ref}}(\\tau^{-}|\\mathcal{G},i)}.italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT + end\_POSTSUPERSCRIPT ) = divide start\_ARG italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT + end\_POSTSUPERSCRIPT | caligraphic\_G , italic\_i ) end\_ARG start\_ARG italic\_Ï€ start\_POSTSUBSCRIPT ref end\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT + end\_POSTSUPERSCRIPT | caligraphic\_G , italic\_i ) end\_ARG , italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT - end\_POSTSUPERSCRIPT ) = divide start\_ARG italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ end\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT - end\_POSTSUPERSCRIPT | caligraphic\_G , italic\_i ) end\_ARG start\_ARG italic\_Ï€ start\_POSTSUBSCRIPT ref end\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT - end\_POSTSUPERSCRIPT | caligraphic\_G , italic\_i ) end\_ARG .||(14)|
This objective encourages the Reranker to assign higher probabilities to trajectories with greater rewards.
By iteratively combining environment feedback and RL optimization, the Reranker transitions from a basic policy to a robust, task-specific system capable of generating high-quality responses.
#### 3.2.3Reward Function Design
To evaluate the quality of the generated responsey^^ð‘¦\\hat{y}over^ start\_ARG italic\_y end\_ARGin relation to the ground-truth answerygâ¢tsubscriptð‘¦ð‘”ð‘¡y\_{gt}italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPTand the contribution of reranked documents{Di}i=1Ksuperscriptsubscriptsubscriptð·ð‘–ð‘–1ð¾\\{D\_{i}\\}\_{i=1}^{K}{ italic\_D start\_POSTSUBSCRIPT italic\_i end\_POSTSUBSCRIPT } start\_POSTSUBSCRIPT italic\_i = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_K end\_POSTSUPERSCRIPT, we employ a multi-dimensional reward function. This function integrates five key aspects of quality: Exact Match (EM), Semantic Similarity (SS), Textual Fluency (TF), Length Penalty (LP) and LLM-Based Evaluation (LLM-Eval). These dimensions collectively provide a holistic assessment of the generated output.
Formally, the reward function is defined as:
|râ¢(ð’¢,i,Ï„)=Î±â‹…EM+Î²â‹…SS+Î³â‹…TF+Î»â‹…LP+Î´â‹…LLM-Evalð‘Ÿð’¢ð‘–ðœâ‹…ð›¼EMâ‹…ð›½SSâ‹…ð›¾TFâ‹…ðœ†LPâ‹…ð›¿LLM-Evalr(\\mathcal{G},i,\\tau)=\\alpha\\cdot\\text{EM}+\\beta\\cdot\\text{SS}+\\gamma\\cdot%
\\text{TF}+\\lambda\\cdot\\text{LP}+\\delta\\cdot\\text{LLM-Eval}italic\_r ( caligraphic\_G , italic\_i , italic\_Ï„ ) = italic\_Î± â‹…EM + italic\_Î² â‹…SS + italic\_Î³ â‹…TF + italic\_Î» â‹…LP + italic\_Î´ â‹…LLM-Eval||(15)|
whereEM,SS,TF,LP, andLLM-Evalare computed as functions of(ygâ¢t,y^)subscriptð‘¦ð‘”ð‘¡^ð‘¦(y\_{gt},\\hat{y})( italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPT , over^ start\_ARG italic\_y end\_ARG )andÎ±ð›¼\\alphaitalic\_Î±,Î²ð›½\\betaitalic\_Î²,Î³ð›¾\\gammaitalic\_Î³,Î»ðœ†\\lambdaitalic\_Î», andÎ´ð›¿\\deltaitalic\_Î´are weighting coefficients for each quality dimension.
The individual components are defined as follows:
* â€¢Exact Match (EM):Measures whethery^^ð‘¦\\hat{y}over^ start\_ARG italic\_y end\_ARGmatchesygâ¢tsubscriptð‘¦ð‘”ð‘¡y\_{gt}italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPTexactly:
|ExactMatchâ¢(ygâ¢t,y^)={1ifâ¢y^=ygâ¢t,0otherwise.ExactMatchsubscriptð‘¦ð‘”ð‘¡^ð‘¦cases1if^ð‘¦subscriptð‘¦ð‘”ð‘¡0otherwise\\text{ExactMatch}(y\_{gt},\\hat{y})=\\begin{cases}1&amp;&amp;\\text{if }\\hat{y}=y\_{gt},\\\\
0&amp;&amp;\\text{otherwise}.\\end{cases}ExactMatch ( italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPT , over^ start\_ARG italic\_y end\_ARG ) = { start\_ROW start\_CELL 1 end\_CELL start\_CELL if over^ start\_ARG italic\_y end\_ARG = italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPT , end\_CELL end\_ROW start\_ROW start\_CELL 0 end\_CELL start\_CELL otherwise . end\_CELL end\_ROW||(16)|
* â€¢Semantic Similarity (SS):Assesses the alignment betweeny^^ð‘¦\\hat{y}over^ start\_ARG italic\_y end\_ARGandygâ¢tsubscriptð‘¦ð‘”ð‘¡y\_{gt}italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPTusing BERTScore> [
[> 62
](https://arxiv.org/html/2505.07233v2#bib.bib62)> ]
:
|SemanticSimilarityâ¢(ygâ¢t,y^)=BERTScoreâ¢(ygâ¢t,y^).SemanticSimilaritysubscriptð‘¦ð‘”ð‘¡^ð‘¦BERTScoresubscriptð‘¦ð‘”ð‘¡^ð‘¦\\text{SemanticSimilarity}(y\_{gt},\\hat{y})=\\text{BERTScore}(y\_{gt},\\hat{y}).SemanticSimilarity ( italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPT , over^ start\_ARG italic\_y end\_ARG ) = BERTScore ( italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPT , over^ start\_ARG italic\_y end\_ARG ) .||(17)|
* â€¢Textual Fluency (TF):Evaluates fluency using ROUGE> [
[> 28
](https://arxiv.org/html/2505.07233v2#bib.bib28)> ]
metrics:
|TextualFluencyâ¢(ygâ¢t,y^)=ROUGEâ¢(ygâ¢t,y^).TextualFluencysubscriptð‘¦ð‘”ð‘¡^ð‘¦ROUGEsubscriptð‘¦ð‘”ð‘¡^ð‘¦\\text{TextualFluency}(y\_{gt},\\hat{y})=\\text{ROUGE}(y\_{gt},\\hat{y}).TextualFluency ( italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPT , over^ start\_ARG italic\_y end\_ARG ) = ROUGE ( italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPT , over^ start\_ARG italic\_y end\_ARG ) .||(18)|
* â€¢Length Penalty (LP):Encourages concise answers by penalizing longer responses:
|LengthPenaltyâ¢(y^)=11+lenâ¢(y^).LengthPenalty^ð‘¦11len^ð‘¦\\text{LengthPenalty}(\\hat{y})=\\frac{1}{1+\\text{len}(\\hat{y})}.LengthPenalty ( over^ start\_ARG italic\_y end\_ARG ) = divide start\_ARG 1 end\_ARG start\_ARG 1 + len ( over^ start\_ARG italic\_y end\_ARG ) end\_ARG .||(19)|
* â€¢LLM-Based Evaluation (LLM-Eval):Uses LLM-based scoring to assess alignment with task requirements, whereð’«ð’«\\mathcal{P}caligraphic\_Pdenotes the scoring prompt, detailed in Appendix[C.3](https://arxiv.org/html/2505.07233v2#A3.SS3):
|LLM-Evalâ¢(ygâ¢t,y^)=LLMScoreâ¢(ð’«,ygâ¢t,y^).LLM-Evalsubscriptð‘¦ð‘”ð‘¡^ð‘¦LLMScoreð’«subscriptð‘¦ð‘”ð‘¡^ð‘¦\\text{LLM-Eval}(y\_{gt},\\hat{y})=\\text{LLMScore}(\\mathcal{P},y\_{gt},\\hat{y}).LLM-Eval ( italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPT , over^ start\_ARG italic\_y end\_ARG ) = LLMScore ( caligraphic\_P , italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPT , over^ start\_ARG italic\_y end\_ARG ) .||(20)|
Table 1:The DynamicRAG results for different datasets among different backbone models. Results are directly from the original paper. Best results are inboldand the second results areunderlined. \* denotes that FLARE is based on the more powerful 175B text-davinci-003 model, which we currently do not have access to.
||Extra Data|NQ|TriviaQA|HotpotQA|2WikimQA|ASQA|FEVER|ELI5|
Metrics|for Training|EM|EM/Acc|EM|EM|EM|Acc|Rg|
Baseline Without Retrieval|
GPT-3.5-Turbo> [
[> 40
](https://arxiv.org/html/2505.07233v2#bib.bib40)> ]
|N/A|38.6|70.7/74.3|29.9|23.9|68.3|82.7|27.5|
GPT-4> [
[> 39
](https://arxiv.org/html/2505.07233v2#bib.bib39)> ]
|N/A|40.3|73.3/78.4|34.5|29.8|71.9|87.7|30.3|
GPT-4o> [
[> 38
](https://arxiv.org/html/2505.07233v2#bib.bib38)> ]
|N/A|40.0|74.0/79.2|36.1|33.3|74.1|86.3|30.2|
Baseline With Retrieval|
IRCoT> [
[> 55
](https://arxiv.org/html/2505.07233v2#bib.bib55)> ]
|N/A|-|-|17.4|-|-|-|-|
ReAct> [
[> 58
](https://arxiv.org/html/2505.07233v2#bib.bib58)> ]
|N/A|-|-|35.1|-|-|62.0|-|
RA-DIT> [
[> 29
](https://arxiv.org/html/2505.07233v2#bib.bib29)> ]
|âˆ¼similar-to\\simâˆ¼1,129k|43.5|72.8/-|36.6|-|-|86.9|-|
FLARE\*> [
[> 18
](https://arxiv.org/html/2505.07233v2#bib.bib18)> ]
|N/A|-|-|-|51.0|41.3|-|-|
Reward-RAG> [
[> 36
](https://arxiv.org/html/2505.07233v2#bib.bib36)> ]
|Unknown|42.2|75.6/80.4|-|-|-|89.8|-|
LLaMA2-7B> [
[> 54
](https://arxiv.org/html/2505.07233v2#bib.bib54)> ]
|N/A|17.9|-/42.5|16.6|17.9|19.0|30.0|15.6|
w/ Reranker|N/A|20.6|-/49.6|18.9|18.3|21.1|35.6|16.7|
LLaMA2-7B-SFT|âˆ¼similar-to\\simâˆ¼130k|29.1|53.7/59.1|27.1|18.9|23.8|40.6|18.6|
Self-RAG (LLaMA2-7B)> [
[> 1
](https://arxiv.org/html/2505.07233v2#bib.bib1)> ]
|âˆ¼similar-to\\simâˆ¼150k|36.4|-/66.4|-|-|30.0|-|-|
DRAGIN> [
[> 50
](https://arxiv.org/html/2505.07233v2#bib.bib50)> ]
|N/A|-|-|23.2|22.0|-|-|-|
Smart-RAG (LLaMA2-7B)> [
[> 12
](https://arxiv.org/html/2505.07233v2#bib.bib12)> ]
|âˆ¼similar-to\\simâˆ¼218k|-|-|-|-|26.6|-|-|
Ours (LLaMA2-7B)|âˆ¼similar-to\\simâˆ¼150k|38.7|59.6/70.5|29.4|23.1|41.1|51.2|22.6|
LLaMA2-13B> [
[> 54
](https://arxiv.org/html/2505.07233v2#bib.bib54)> ]
|N/A|23.6|-/47.0|17.7|18.7|20.5|30.2|19.9|
w/ Reranker|N/A|26.5|-/53.2|20.4|18.8|23.6|37.1|20.3|
LLaMA2-13B-SFT|âˆ¼similar-to\\simâˆ¼130k|32.5|60.1/66.2|27.9|19.1|28.4|45.8|20.1|
Self-RAG (LLaMA2-13B)> [
[> 1
](https://arxiv.org/html/2505.07233v2#bib.bib1)> ]
|âˆ¼similar-to\\simâˆ¼150k|-|-/69.3|-|-|31.7|-|-|
Ours (LLaMA2-13B)|âˆ¼similar-to\\simâˆ¼150k|39.1|62.3/72.6|30.1|25.0|46.4|77.2|23.3|
LLaMA3-8B> [
[> 13
](https://arxiv.org/html/2505.07233v2#bib.bib13)> ]
|N/A|36.4|-/57.4|26.1|24.6|24.9|34.6|24.0|
w/ Reranker|N/A|37.5|-/64.5|28.7|25.3|29.8|49.7|23.7|
LLaMA3-8B-SFT|âˆ¼similar-to\\simâˆ¼130k|39.1|67.5/74.2|31.5|27.1|46.8|82.1|22.9|
Auto-RAG (LLaMA3-8B-Instruct)> [
[> 59
](https://arxiv.org/html/2505.07233v2#bib.bib59)> ]
|Unknown|37.9|60.9/-|-|-|30.0|-|-|
ChatQA-1.5 (LLaMA3-8B)> [
[> 32
](https://arxiv.org/html/2505.07233v2#bib.bib32)> ]
|âˆ¼similar-to\\simâˆ¼442k|42.4|81.0/87.6|33.4|26.8|-|90.9|-|
RankRAG (LLaMA3-8B)> [
[> 60
](https://arxiv.org/html/2505.07233v2#bib.bib60)> ]
|âˆ¼similar-to\\simâˆ¼470k|50.6|82.9/89.5|35.3|31.4|-|93.8|-|
Ours (LLaMA3-8B)|âˆ¼similar-to\\simâˆ¼150k|48.4|78.3/87.4|36.7|34.2|56.3|91.4|24.6|
## 4Experiment
### 4.1Datasets and Evaluation Metrics
We evaluate DynamicRAGâ€™s performance on comprehensive knowledge-intensive question-answering tasks, spanning over seven datasets and covering different types of challenges: Natural Questions> [
[> 25
](https://arxiv.org/html/2505.07233v2#bib.bib25)> ]
, TriviaQA> [
[> 19
](https://arxiv.org/html/2505.07233v2#bib.bib19)> ]
, HotpotQA> [
[> 57
](https://arxiv.org/html/2505.07233v2#bib.bib57)> ]
, 2WikimQA> [
[> 15
](https://arxiv.org/html/2505.07233v2#bib.bib15)> ]
, FEVER> [
[> 53
](https://arxiv.org/html/2505.07233v2#bib.bib53)> ]
, ASQA> [
[> 49
](https://arxiv.org/html/2505.07233v2#bib.bib49)> ]
, and ELI5> [
[> 10
](https://arxiv.org/html/2505.07233v2#bib.bib10)> ]
. For the first five datasets (NQ, TriviaQA, HotpotQA, 2WikimQA, and ASQA), we follow prior studies> [
[> 1
](https://arxiv.org/html/2505.07233v2#bib.bib1)> , [> 60
](https://arxiv.org/html/2505.07233v2#bib.bib60)> ]
and adopt exact match as the evaluation metric. For TriviaQA and FEVER, we used accuracy, while for ELI5, we employed ROUGE-L scores to assess performance> [
[> 60
](https://arxiv.org/html/2505.07233v2#bib.bib60)> , [> 41
](https://arxiv.org/html/2505.07233v2#bib.bib41)> ]
.
### 4.2Baselines
##### Baselines without Retrieval
We evaluate publicly available close-sourced LLMs, including GPT-3.5-turbo, GPT-4, and GPT-4o. These models represent state-of-the-art LLMs that are not augmented with external retrieval information. The system prompts and instruction formats are shown in Appendix[C.3](https://arxiv.org/html/2505.07233v2#A3.SS3).
##### Baselines with Retrieval
We compare our method against several retrieval-augmented baselines. The baselines are categorized into four groups as follows:RAG-based Baselines: This group includes approaches such as IRCoT> [
[> 55
](https://arxiv.org/html/2505.07233v2#bib.bib55)> ]
, ReAct> [
[> 58
](https://arxiv.org/html/2505.07233v2#bib.bib58)> ]
, FLARE> [
[> 18
](https://arxiv.org/html/2505.07233v2#bib.bib18)> ]
, RA-DIT> [
[> 29
](https://arxiv.org/html/2505.07233v2#bib.bib29)> ]
and Reward-RAG> [
[> 36
](https://arxiv.org/html/2505.07233v2#bib.bib36)> ]
, which leverage agent-like strategies for retrieval-augmented generation.LLaMA2-7B-based Baselines: This category consists of standard retrieval-augmented baselines such as Vanilla-RAG, as well as its enhanced versions with additional components like reranker, SFT, Self-RAG> [
[> 1
](https://arxiv.org/html/2505.07233v2#bib.bib1)> ]
, DRAGIN> [
[> 50
](https://arxiv.org/html/2505.07233v2#bib.bib50)> ]
and Smart-RAG> [
[> 12
](https://arxiv.org/html/2505.07233v2#bib.bib12)> ]
.LLaMA2-13B-based Baselines: Similar to the LLaMA2-7B group, this set includes Vanilla-RAG, Reranker, SFT, and Self-RAG> [
[> 1
](https://arxiv.org/html/2505.07233v2#bib.bib1)> ]
, providing a larger-scale comparison using the LLaMA2-13B model.LLaMA3-8B-based Baselines: In this group, we consider models based on LLaMA3-8B, including Vanilla-RAG and its variations with Reranker and SFT. Additionally, we compare our models with more advanced retrieval-augmented methods such as Auto-RAG> [
[> 59
](https://arxiv.org/html/2505.07233v2#bib.bib59)> ]
, ChatQA-1.5> [
[> 32
](https://arxiv.org/html/2505.07233v2#bib.bib32)> ]
, and RankRAG> [
[> 60
](https://arxiv.org/html/2505.07233v2#bib.bib60)> ]
.
### 4.3Implementation Details
##### Training data and settings
Our training data comprises 150k diverse instruction-output pairs, drawn from Alpaca> [
[> 52
](https://arxiv.org/html/2505.07233v2#bib.bib52)> ]
, KILT> [
[> 41
](https://arxiv.org/html/2505.07233v2#bib.bib41)> ]
, ASQA> [
[> 49
](https://arxiv.org/html/2505.07233v2#bib.bib49)> ]
, and OpenBookQA> [
[> 35
](https://arxiv.org/html/2505.07233v2#bib.bib35)> ]
. We employ three models as the base LMs for our dynamic reranker and generator: LLaMA2-7B, LLaMA2-13B, and LLaMA3-8B. For the retriever model, we use the off-the-shelf Contriever-MS MARCO> [
[> 16
](https://arxiv.org/html/2505.07233v2#bib.bib16)> ]
as the default retriever, retrieving up to 45 documents for LLaMA3 and 20 documents for LLaMA2 per input, tailored to their respective context window sizes. Unless otherwise specified, the retriever and generator share the parameters. Additional training details can be found in the Appendix[C](https://arxiv.org/html/2505.07233v2#A3).
##### Inference settings
For the dynamic reranker, we set the temperature to 0.2 to enhance output diversity. For the generator, we use a temperature of 0 to ensure output stability and reproducibility. By default, we use the top 45 documents from Contriever-MS MARCO> [
[> 16
](https://arxiv.org/html/2505.07233v2#bib.bib16)> ]
as input to the reranker. In contrast, all baseline methods use the top 10 documents from Contriever-MS MARCO as input to ensure a fair comparison.
Table 2:The performance of different Reranker models. Results are directly from the original paper. Best results are inboldand the second results areunderlined.
||Training Data|NQ|HotpotQA|Avg.|
Metric|R@5|R@10|R@20|R@5|R@10|R@20|
Close-Source Models|
GPT-3.5-Turbo> [
[> 40
](https://arxiv.org/html/2505.07233v2#bib.bib40)> ]
|Unknown|77.8|82.5|85.7|52.1|56.6|62.4|69.5|
GPT-4> [
[> 39
](https://arxiv.org/html/2505.07233v2#bib.bib39)> ]
|Unknown|79.3|83.2|85.1|53.2|57.0|61.0|69.8|
Open-Source Rerank Models|
BM25> [
[> 46
](https://arxiv.org/html/2505.07233v2#bib.bib46)> ]
|N/A|38.0|50.7|60.1|57.5|63.0|67.5|56.1|
Contriever> [
[> 16
](https://arxiv.org/html/2505.07233v2#bib.bib16)> ]
|Unknown|73.6|80.2|84.8|53.1|58.7|62.4|68.8|
monoT5> [
[> 37
](https://arxiv.org/html/2505.07233v2#bib.bib37)> ]
|âˆ¼similar-to\\simâˆ¼503k|75.6|80.9|84.9|54.8|60.2|63.3|70.0|
RankLLaMA> [
[> 34
](https://arxiv.org/html/2505.07233v2#bib.bib34)> ]
|âˆ¼similar-to\\simâˆ¼503k|77.8|83.1|86.0|57.1|62.1|64.8|71.8|
ChatQA-1.5 (LLaMA3-8B)> [
[> 32
](https://arxiv.org/html/2505.07233v2#bib.bib32)> ]
|N/A|68.2|75.7|82.0|37.4|45.0|53.6|60.3|
RankRAG (LLaMA3-8B)> [
[> 60
](https://arxiv.org/html/2505.07233v2#bib.bib60)> ]
|âˆ¼similar-to\\simâˆ¼50k|80.3|84.0|86.3|57.6|61.8|65.2|72.5|
Open-Source Generative Models|
GENRE> [
[> 4
](https://arxiv.org/html/2505.07233v2#bib.bib4)> ]
|âˆ¼similar-to\\simâˆ¼406k|61.4|-|-|34.0|-|-|-|
Re3eval> [
[> 48
](https://arxiv.org/html/2505.07233v2#bib.bib48)> ]
|âˆ¼similar-to\\simâˆ¼240k|65.4|-|-|44.2|-|-|-|
SEAL> [
[> 2
](https://arxiv.org/html/2505.07233v2#bib.bib2)> ]
|Unknown|68.2|-|-|51.0|-|-|-|
DynamicRAG (LLaMA3-8B)|âˆ¼similar-to\\simâˆ¼20k|79.3|83.7|86.8|59.1|63.7|67.2|73.7|
### 4.4Main Results
#### 4.4.1Comparison against baselines with and without retrieval
Table[1](https://arxiv.org/html/2505.07233v2#S3.T1)presents a comprehensive comparison of our proposed DynamicRAG approach against various baseline models, categorized into those without retrieval and those incorporating retrieval mechanisms. For baseline models that do not utilize the retrieval, such as GPT-3.5-Turbo, GPT-4, and GPT-4o, our approach significantly outperforms them across multiple datasets. Notably, in the NQ dataset, our method achieves an EM score of 48.4 with LLaMA3-8B, surpassing GPT-4o. These results highlight the effectiveness of our retrieval-augmented approach compared to models without retrieval capabilities. When compared to agent-based baselines such as IRCoT, ReAct, and Reward-RAG, our DynamicRAG framework consistently achieves state-of-the-art performance across various datasets. Moreover, our approach achieves superior performance compared to other retrieval-based models such as RankRAG and ChatQA-1.5, despite using significantly less training data (âˆ¼similar-to\\simâˆ¼150k examples vs.âˆ¼similar-to\\simâˆ¼470k for RankRAG). Second, the effectiveness of our method is consistent across different backbone sizes (7B, 13B, 8B), showing scalability and robustness.
#### 4.4.2Comparison with baselines for reranking performance
The Table[2](https://arxiv.org/html/2505.07233v2#S4.T2)presents a comparison of different reranker models categorized into three groups: close-sourced models, open-sourced rerank models, and open-sourced generative models. Our LLaMA3-8B-based model, DynamicRAG, demonstrates competitive performance while utilizing only 20k training samples, achieving results comparable to RankRAG, which requires 50k training samples. Furthermore, our model significantly surpasses other open-sourced models, such as monoT5, RankLLaMA, and generative models like Re3eval, across key ranking metrics (R@5, R@10, and R@20). This highlights the effectiveness of our approach in efficiently utilizing limited training data without compromising performance.
Additionally, our approach adopts a list-wise reranking strategy, which contributes to superior overall ranking efficiency compared to other models that primarily rely on point-wise methods. Notably, we leverage the quality of generated responses as a signal for reranking, which significantly enhances model performance, particularly when compared to traditional information retrieval-based models.
### 4.5Ablation Studies
We perform various ablation studies to understand the importance of different factors in DynamicRAG.
#### 4.5.1Impact of Reranker and Generator Size
The Table[4.5.1](https://arxiv.org/html/2505.07233v2#S4.SS5.SSS1)presents results from different Reranker and Generator configurations in the DynamicRAG, evaluating model variants with LLaMA2-7B, LLaMA2-13B, and LLaMA3-8B, where performance improves as model size increases. The 13B Reranker and 13B Generator configuration outperforms the 7B-13B setup in both NQ and HotpotQA, with the average EM score rising from 33.1 to 34.3. Switching to an 8B Reranker with a 13B Generator results in a slight further increase in the average EM score to 34.8, suggesting that a larger Reranker can enhance performance, even with a fixed Generator size. The model where both the Reranker and Generator share parameters (denoted by \*) achieves the EM of 39.1 on NQ and 30.1 on HotpotQA, yielding an average EM score of 34.6. This improvement indicates that sharing parameters allows the Reranker and Generator to better complement each other, as their tasks can mutually enhance performance.
(a)Results of different models and sizes for Reranker and Generator. \* denotes we share the parameters of the Reranker and Generator. We use Exact Match as the metric.
|DynamicRAG|NQ|HotpotQA|Avg.|
Reranker|Generator|EM|EM|
LLaMA2-7B|LLaMA2-13B|37.6|28.6|33.1|
LLaMA2-13B|LLaMA2-13B|38.7|29.8|34.3|
LLaMA2-13B\*|LLaMA2-13B\*|39.1|30.1|34.6|
LLaMA3-8B|LLaMA2-13B|39.4|30.2|34.8|
(b)The impact of different key components in DynamicRAG among different benchmarks. We use Exact Match as the metric.
||NQ|HotpotQA|ASQA|Avg.|
|EM|EM|EM||
DynamicRAG|48.4|36.7|56.3|47.1|
w/o Retrieval|25.0|25.6|15.7|22.1|
w/o Reranking|36.4|27.2|39.8|34.5|
w/o RL|44.6|29.6|45.5|39.9|
#### 4.5.2Effect of Key Components on DynamicRAG Performance
The Table[4.5.1](https://arxiv.org/html/2505.07233v2#S4.SS5.SSS1)shows the performance of DynamicRAG under various ablation conditions, where key components such as retrieval, reranking, reinforcement learning, and iterative training are removed. Evaluation on NQ, HotpotQA, and ASQA reveals that removing retrieval causes a significant drop in performance, with EM scores falling to 25.0 on NQ, 25.6 on HotpotQA, and 15.7 on ASQA, leading to an average EM of just 22.1. This emphasizes the critical importance of retrieval in supplying relevant information. Excluding reranking results in a degradation in performance, especially on NQ (EM = 36.4), indicating that reranking has a beneficial effect. Removing RL also hinders performance across all datasets, with the average EM decreased to 39.9, particularly on NQ and ASQA (44.6, 45.5), suggesting that RL significantly benefits performance.
#### 4.5.3Performance with different retrievers
We conducted experiments to compare the performance of Vanilla-RAG and DynamicRAG on three different benchmarks: NQ, HotpotQA, and ASQA, using different retrievers (DPR> [
[> 20
](https://arxiv.org/html/2505.07233v2#bib.bib20)> ]
, Contriever, MonoT5), as shown in Figure[4](https://arxiv.org/html/2505.07233v2#S4.F4). It can be observed that as the retriever models improve, both approaches exhibit better performance on downstream tasks. This also demonstrates the robustness of our model.
![Refer to caption](x4.png)(a)Performance on NQ.
![Refer to caption](x5.png)(b)Performance on HotpotQA.
![Refer to caption](x6.png)(c)Performance on ASQA.
Figure 4:Performance with different retrievers between Vanilla-RAG and DynamicRAG.
### 4.6Model Analysis
#### 4.6.1Efficiency
##### From LLM-Calling Perspective
![Refer to caption](x7.png)Figure 5:Comparison of different RAG models in terms of efficiency and effectiveness. The x-axis represents the number of LLM calls, while the y-axis denotes the average performance on the NQ benchmark. Models closer to the top-left corner achieve better overall performance.
We evaluate the maximum number of LLM calls required by different RAG models to generate an answer when the retriever returns 20 documents. It is evident that the closer a model is to the top-left corner, the better it performs, as both effectiveness and efficiency are optimized. Our model is positioned in the top-left corner, demonstrating superior performance compared to other models. Specifically, our model can generate answers with only two LLM calls when the retriever returns 20 documents.
##### From Token Perspective
We empirically evaluated computational efficiency against existing methodologies, notably RankRAG. Our architecture processes Question + Top-20 documents concurrently, producing Top-k documents through a single Reranker pass before generating the final answer via Question + Top-k document integration. In contrast, RankRAGâ€™s methodology necessitates separate processing of Question + individual document pairs, requiring 20 distinct forward passes for a corpus of 20 documents. Assumingk=10ð‘˜10k=10italic\_k = 10for both approaches, Table[3](https://arxiv.org/html/2505.07233v2#S4.T3)presents mean runtime metrics (averaged across three experimental iterations):
Table 3:Comparative analysis of processing latency from token-input perspective.Input|Time (Seconds)|
Question + Top-10 Docs (Vanilla-RAG)|0.57|
Question + Top-20 Docs (4k context window) to rank|0.75|
(Question + Single Doc)Ã—\\timesÃ—20 for reranking|13.00|
Question + Top-k Docs (avgk=12ð‘˜12k=12italic\_k = 12)|0.61|
Results demonstrate that contextual integration yields substantially higher computational efficiency compared to multiple LLM invocations. (As the RankRAG implementation is not publicly available, we constructed a functional equivalent utilizing LLaMA2-7Bâ€”matching the original architectureâ€™s scaleâ€”deployed within a VLLM framework on 8 A100 GPUs.) Our methodology demonstrates approximately 17Ã—\\timesÃ—superior throughput relative to RankRAGâ€™s sequential scoring approach. Furthermore, compared to standard RAG pipelines without reranking, our approach introduces minimal computational overheadâ€”specifically, a 2.3Ã—\\timesÃ—latency increase while delivering significant performance gains. For instance, on the NQ benchmark, our methodology demonstrates a 9.6 percentage point improvement over vanilla RAG implementations using LLaMA2-7B.
#### 4.6.2Reranked Document Distribution
![Refer to caption](x8.png)Figure 6:Distribution of reranked document numbers (k) on NQ and HotpotQA before and after RL training.kð‘˜kitalic\_kis truncated at 15 to ensure a fair comparison, as we restrictkâ‰¤15ð‘˜15k\\leq 15italic\_k â‰¤15during both training and sampling.
We analyzed the reranked results of DynamicRAG on NQ and HotpotQA, as shown in Figure[6](https://arxiv.org/html/2505.07233v2#S4.F6)111We selected a maximum of 15 documents for reranking to maintain fairness in comparison with existing studies, which commonly evaluate top-10 documents. Given LLaMA-7Bâ€™s 4K token context length and an average document length of approximately 200 tokens, accommodating 15 documents and their associated queries is comfortably feasible on an 80GB A100 GPU.. Before RL training, the reranked results on NQ and HotpotQA predominantly had k values of 14 and 15. This is because the model trained solely with SFT tends to favor a higher number of reranked documents, achieving a better downstream performance. However, after RL training, especially with the introduction of the length penalty, a leftward shift inkð‘˜kitalic\_kvalues can be observed, with peaks appearing at 12, 13, and 14. This indicates a tendency to output fewer reranked documents. This also proves the effectiveness of our RL training.
Due to space limitations, we defer several important analyses to the Appendices. Appendix[B](https://arxiv.org/html/2505.07233v2#A2)includes ablation studies on different retrievers, Top-Nð‘Nitalic\_Ndocument selection, reward function components, and training experiments with closed-source models. Appendix[C](https://arxiv.org/html/2505.07233v2#A3)provides comprehensive implementation details, qualitative examples demonstrating our dynamic reranking approach, and complete prompt templates.
### 4.7Related Work
#### 4.7.1Retrieval-Augmented Generation
RAG boosts LLM performance by adding external knowledge, enhancing factual accuracy and context> [
[> 26
](https://arxiv.org/html/2505.07233v2#bib.bib26)> , [> 14
](https://arxiv.org/html/2505.07233v2#bib.bib14)> ]
. Research includes retrieval-based next-token prediction> [
[> 22
](https://arxiv.org/html/2505.07233v2#bib.bib22)> , [> 45
](https://arxiv.org/html/2505.07233v2#bib.bib45)> ]
and end-to-end fine-tuning for better integration> [
[> 3
](https://arxiv.org/html/2505.07233v2#bib.bib3)> , [> 17
](https://arxiv.org/html/2505.07233v2#bib.bib17)> , [> 63
](https://arxiv.org/html/2505.07233v2#bib.bib63)> ]
.> Asai etÂ al. [
[> 1
](https://arxiv.org/html/2505.07233v2#bib.bib1)> ]
optimizes RAG using special tokens for adaptive retrieval and reflection, fine-tuning with a critic model.
In related work,> Ke etÂ al. [
[> 21
](https://arxiv.org/html/2505.07233v2#bib.bib21)> ]
optimized RAG by training a bridge model to refine the retriever-LLM connection. While sharing similarities, our approach differs in key ways: (1) We optimize the reranker by treating it as an agent that interacts with the generator to collect training trajectories. (2) We jointly train the reranker and generator, finding that shared parameters improve adaptation to downstream tasks.
#### 4.7.2LLM for Reranking
LLMs are increasingly used for passage reranking, with methods generally being point-wise (assessing individual relevance via relevance or query generation> [
[> 27
](https://arxiv.org/html/2505.07233v2#bib.bib27)> , [> 9
](https://arxiv.org/html/2505.07233v2#bib.bib9)> , [> 47
](https://arxiv.org/html/2505.07233v2#bib.bib47)> ]
), pair-wise (comparing passage pairs for relative relevance> [
[> 43
](https://arxiv.org/html/2505.07233v2#bib.bib43)> ]
), and list-wise (holistically ranking passages like Learning to Rank> [
[> 51
](https://arxiv.org/html/2505.07233v2#bib.bib51)> , [> 33
](https://arxiv.org/html/2505.07233v2#bib.bib33)> , [> 5
](https://arxiv.org/html/2505.07233v2#bib.bib5)> ]
). Recent advancements like zero-shot reranking with fine-tuned open-weight LLMs> [
[> 42
](https://arxiv.org/html/2505.07233v2#bib.bib42)> , [> 30
](https://arxiv.org/html/2505.07233v2#bib.bib30)> ]
and logit-based methods> [
[> 11
](https://arxiv.org/html/2505.07233v2#bib.bib11)> , [> 7
](https://arxiv.org/html/2505.07233v2#bib.bib7)> ]
aim to solve issues but often need specific fine-tuning or have scalability limits. Our approach treats the LLM reranker as an agent, initially using behavior cloning to imitate expert behavior, then interacting with the generator to create trajectories for further optimization.
## 5Conclusion
This work introduces DynamicRAG, a new reinforcement learning framework to optimize reranking in RAG. By modeling the reranker as an RL agent and using LLM response quality as rewards, it dynamically adjusts the order and number of retrieved documents per query. This dynamic reranking mechanism enhances both the relevance of selected documents and the overall system efficiency. Extensive evaluations on seven knowledge-intensive datasets demonstrate that DynamicRAG consistently outperforms existing fine-tuned and prompting-based approaches, achieving state-of-the-art performance.
## References
* Asai etÂ al. [2023]A.Â Asai, Z.Â Wu, Y.Â Wang, A.Â Sil, and H.Â Hajishirzi.Self-rag: Learning to retrieve, generate, and critique through self-reflection, 2023.URL[https://arxiv.org/abs/2310.11511](https://arxiv.org/abs/2310.11511).
* Bevilacqua etÂ al. [2022]M.Â Bevilacqua, G.Â Ottaviano, P.Â S.Â H. Lewis, S.Â Yih, S.Â Riedel, and F.Â Petroni.Autoregressive search engines: Generating substrings as document identifiers.In S.Â Koyejo, S.Â Mohamed, A.Â Agarwal, D.Â Belgrave, K.Â Cho, and A.Â Oh, editors,*Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022*, 2022.
* Borgeaud etÂ al. [2022]S.Â Borgeaud, A.Â Mensch, J.Â Hoffmann, T.Â Cai, E.Â Rutherford, K.Â Millican, G.Â B. Van DenÂ Driessche, J.-B. Lespiau, B.Â Damoc, A.Â Clark, D.Â DeÂ LasÂ Casas, A.Â Guy, J.Â Menick, R.Â Ring, T.Â Hennigan, S.Â Huang, L.Â Maggiore, C.Â Jones, A.Â Cassirer, A.Â Brock, M.Â Paganini, G.Â Irving, O.Â Vinyals, S.Â Osindero, K.Â Simonyan, J.Â Rae, E.Â Elsen, and L.Â Sifre.Improving language models by retrieving from trillions of tokens.In*International Conference on Machine Learning*, pages 2206â€“2240, 2022.
* Cao etÂ al. [2021]N.Â D. Cao, G.Â Izacard, S.Â Riedel, and F.Â Petroni.Autoregressive entity retrieval.In*9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021*. OpenReview.net, 2021.URL[https://openreview.net/forum?id=5k8F6UU39V](https://openreview.net/forum?id=5k8F6UU39V).
* Chao etÂ al. [2024]W.-S. Chao, Z.Â Zheng, H.Â Zhu, and H.Â Liu.Make large language model a better ranker, 2024.URL[https://arxiv.org/abs/2403.19181](https://arxiv.org/abs/2403.19181).
* Chen etÂ al. [2023]J.Â Chen, H.Â Lin, X.Â Han, and L.Â Sun.Benchmarking large language models in retrieval-augmented generation, 2023.URL[https://arxiv.org/abs/2309.01431](https://arxiv.org/abs/2309.01431).
* Chen etÂ al. [2024]S.Â Chen, B.Â J. GutiÃ©rrez, and Y.Â Su.Attention in large language models yields efficient zero-shot re-rankers, 2024.URL[https://arxiv.org/abs/2410.02642](https://arxiv.org/abs/2410.02642).
* Devlin etÂ al. [2019]J.Â Devlin, M.-W. Chang, K.Â Lee, and K.Â Toutanova.BERT: Pre-training of deep bidirectional transformers for language understanding.In J.Â Burstein, C.Â Doran, and T.Â Solorio, editors,*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
* Drozdov etÂ al. [2023]A.Â Drozdov, H.Â Zhuang, Z.Â Dai, Z.Â Qin, R.Â Rahimi, X.Â Wang, D.Â Alon, M.Â Iyyer, A.Â McCallum, D.Â Metzler, and K.Â Hui.Parade: Passage ranking using demonstrations with large language models, 2023.URL[https://arxiv.org/abs/2310.14408](https://arxiv.org/abs/2310.14408).
* Fan etÂ al. [2019]A.Â Fan, Y.Â Jernite, E.Â Perez, D.Â Grangier, J.Â Weston, and M.Â Auli.ELI5: long form question answering.In A.Â Korhonen, D.Â R. Traum, and L.Â MÃ rquez, editors,*Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers*, pages 3558â€“3567. Association for Computational Linguistics, 2019.
* GangiÂ Reddy etÂ al. [2024]R.Â GangiÂ Reddy, J.Â Doo, Y.Â Xu, M.Â A. Sultan, D.Â Swain, A.Â Sil, and H.Â Ji.FIRST: Faster improved listwise reranking with single token decoding.In Y.Â Al-Onaizan, M.Â Bansal, and Y.-N. Chen, editors,*Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing*, pages 8642â€“8652, Miami, Florida, USA, Nov. 2024. Association for Computational Linguistics.doi:10.18653/v1/2024.emnlp-main.491.URL[https://aclanthology.org/2024.emnlp-main.491/](https://aclanthology.org/2024.emnlp-main.491/).
* Gao etÂ al. [2024]J.Â Gao, L.Â Li, W.Â Li, Y.Â Fu, and B.Â Dai.Smartrag: Jointly learn rag-related tasks from the environment feedback, 2024.URL[https://arxiv.org/abs/2410.18141](https://arxiv.org/abs/2410.18141).
* Grattafiori etÂ al. [2024]A.Â Grattafiori, A.Â Dubey, A.Â Jauhri, etÂ al.The llama 3 herd of models, 2024.URL[https://arxiv.org/abs/2407.21783](https://arxiv.org/abs/2407.21783).
* Guu etÂ al. [2020]K.Â Guu, K.Â Lee, Z.Â Tung, P.Â Pasupat, and M.-W. Chang.Realm: retrieval-augmented language model pre-training.In*International Conference on Machine Learning*, 2020.
* Ho etÂ al. [2020]X.Â Ho, A.Â D. Nguyen, S.Â Sugawara, and A.Â Aizawa.Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning steps.In D.Â Scott, N.Â Bel, and C.Â Zong, editors,*Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020*, pages 6609â€“6625. International Committee on Computational Linguistics, 2020.
* Izacard etÂ al. [2022]G.Â Izacard, M.Â Caron, L.Â Hosseini, S.Â Riedel, P.Â Bojanowski, A.Â Joulin, and E.Â Grave.Unsupervised dense information retrieval with contrastive learning.*Trans. Mach. Learn. Res.*, 2022, 2022.URL[https://openreview.net/forum?id=jKN1pXi7b0](https://openreview.net/forum?id=jKN1pXi7b0).
* Izacard etÂ al. [2023]G.Â Izacard, P.Â Lewis, M.Â Lomeli, L.Â Hosseini, F.Â Petroni, T.Â Schick, J.Â Dwivedi-Yu, A.Â Joulin, S.Â Riedel, and E.Â Grave.Atlas: Few-shot learning with retrieval augmented language models.*Journal of Machine Learning Research*, pages 1â€“43, 2023.
* Jiang etÂ al. [2023]Z.Â Jiang, F.Â Xu, L.Â Gao, Z.Â Sun, Q.Â Liu, J.Â Dwivedi-Yu, Y.Â Yang, J.Â Callan, and G.Â Neubig.Active retrieval augmented generation.In H.Â Bouamor, J.Â Pino, and K.Â Bali, editors,*Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, Singapore, Dec. 2023. Association for Computational Linguistics.
* Joshi etÂ al. [2017]M.Â Joshi, E.Â Choi, D.Â S. Weld, and L.Â Zettlemoyer.Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.In R.Â Barzilay and M.Â Kan, editors,*Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers*, pages 1601â€“1611. Association for Computational Linguistics, 2017.
* Karpukhin etÂ al. [2020]V.Â Karpukhin, B.Â Oguz, S.Â Min, P.Â S.Â H. Lewis, L.Â Wu, S.Â Edunov, D.Â Chen, and W.Â Yih.Dense passage retrieval for open-domain question answering.In B.Â Webber, T.Â Cohn, Y.Â He, and Y.Â Liu, editors,*Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020*, pages 6769â€“6781. Association for Computational Linguistics, 2020.
* Ke etÂ al. [2024]Z.Â Ke, W.Â Kong, C.Â Li, M.Â Zhang, Q.Â Mei, and M.Â Bendersky.Bridging the preference gap between retrievers and llms.In L.Â Ku, A.Â Martins, and V.Â Srikumar, editors,*Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024*, pages 10438â€“10451. Association for Computational Linguistics, 2024.doi:10.18653/V1/2024.ACL-LONG.562.URL[https://doi.org/10.18653/v1/2024.acl-long.562](https://doi.org/10.18653/v1/2024.acl-long.562).
* Khandelwal etÂ al. [2020]U.Â Khandelwal, O.Â Levy, D.Â Jurafsky, L.Â Zettlemoyer, and M.Â Lewis.Generalization through memorization: Nearest neighbor language models.In*International Conference on Learning Representations (ICLR)*, 2020.
* Khramtsova etÂ al. [2024]E.Â Khramtsova, S.Â Zhuang, M.Â Baktashmotlagh, and G.Â Zuccon.Leveraging llms for unsupervised dense retriever ranking.In*Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval*, SIGIR 2024, page 1307â€“1317. ACM, July 2024.doi:10.1145/3626772.3657798.URL[http://dx.doi.org/10.1145/3626772.3657798](http://dx.doi.org/10.1145/3626772.3657798).
* Kulkarni etÂ al. [2024]M.Â Kulkarni, P.Â Tangarajan, K.Â Kim, and A.Â Trivedi.Reinforcement learning for optimizing rag for domain chatbots.*arXiv preprint arXiv:2401.06800*, 2024.
* Kwiatkowski etÂ al. [2019]T.Â Kwiatkowski, J.Â Palomaki, O.Â Redfield, M.Â Collins, A.Â P. Parikh, C.Â Alberti, D.Â Epstein, I.Â Polosukhin, J.Â Devlin, K.Â Lee, K.Â Toutanova, L.Â Jones, M.Â Kelcey, M.Â Chang, A.Â M. Dai, J.Â Uszkoreit, Q.Â Le, and S.Â Petrov.Natural questions: a benchmark for question answering research.*Trans. Assoc. Comput. Linguistics*, 7:452â€“466, 2019.
* Lewis etÂ al. [2020]P.Â Lewis, E.Â Perez, A.Â Piktus, F.Â Petroni, V.Â Karpukhin, N.Â Goyal, H.Â KÃ¼ttler, M.Â Lewis, W.-t. Yih, T.Â RocktÃ¤schel, S.Â Riedel, and D.Â Kiela.Retrieval-augmented generation for knowledge-intensive nlp tasks.In*Advances in Neural Information Processing Systems*, volumeÂ 33, pages 9459â€“9474, 2020.
* Liang etÂ al. [2023]P.Â Liang, R.Â Bommasani, T.Â Lee, D.Â Tsipras, D.Â Soylu, M.Â Yasunaga, Y.Â Zhang, D.Â Narayanan, Y.Â Wu, A.Â Kumar, B.Â Newman, B.Â Yuan, B.Â Yan, C.Â Zhang, C.Â Cosgrove, C.Â D. Manning, C.Â RÃ©, D.Â Acosta-Navas, D.Â A. Hudson, E.Â Zelikman, E.Â Durmus, F.Â Ladhak, F.Â Rong, H.Â Ren, H.Â Yao, J.Â Wang, K.Â Santhanam, L.Â Orr, L.Â Zheng, M.Â Yuksekgonul, M.Â Suzgun, N.Â Kim, N.Â Guha, N.Â Chatterji, O.Â Khattab, P.Â Henderson, Q.Â Huang, R.Â Chi, S.Â M. Xie, S.Â Santurkar, S.Â Ganguli, T.Â Hashimoto, T.Â Icard, T.Â Zhang, V.Â Chaudhary, W.Â Wang, X.Â Li, Y.Â Mai, Y.Â Zhang, and Y.Â Koreeda.Holistic evaluation of language models, 2023.URL[https://arxiv.org/abs/2211.09110](https://arxiv.org/abs/2211.09110).
* Lin [2004]C.-Y. Lin.ROUGE: A package for automatic evaluation of summaries.In*Text Summarization Branches Out*, Barcelona, Spain, July 2004. Association for Computational Linguistics.
* Lin etÂ al. [2024]X.Â V. Lin, X.Â Chen, M.Â Chen, W.Â Shi, M.Â Lomeli, R.Â James, P.Â Rodriguez, J.Â Kahn, G.Â Szilvasy, M.Â Lewis, L.Â Zettlemoyer, and W.Â Yih.RA-DIT: retrieval-augmented dual instruction tuning.In*The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net, 2024.
* Liu etÂ al. [2024a]W.Â Liu, Y.Â Zhu, and Z.Â Dou.Demorank: Selecting effective demonstrations for large language models in ranking task, 2024a.URL[https://arxiv.org/abs/2406.16332](https://arxiv.org/abs/2406.16332).
* Liu etÂ al. [2019]Y.Â Liu, M.Â Ott, N.Â Goyal, J.Â Du, M.Â Joshi, D.Â Chen, O.Â Levy, M.Â Lewis, L.Â Zettlemoyer, and V.Â Stoyanov.Roberta: A robustly optimized bert pretraining approach, 2019.
* Liu etÂ al. [2024b]Z.Â Liu, W.Â Ping, R.Â Roy, P.Â Xu, C.Â Lee, M.Â Shoeybi, and B.Â Catanzaro.Chatqa: Surpassing gpt-4 on conversational qa and rag, 2024b.URL[https://arxiv.org/abs/2401.10225](https://arxiv.org/abs/2401.10225).
* Ma etÂ al. [2023]X.Â Ma, X.Â Zhang, R.Â Pradeep, and J.Â Lin.Zero-shot listwise document reranking with a large language model, 2023.URL[https://arxiv.org/abs/2305.02156](https://arxiv.org/abs/2305.02156).
* Ma etÂ al. [2024]X.Â Ma, L.Â Wang, N.Â Yang, F.Â Wei, and J.Â Lin.Fine-tuning llama for multi-stage text retrieval.In G.Â H. Yang, H.Â Wang, S.Â Han, C.Â Hauff, G.Â Zuccon, and Y.Â Zhang, editors,*Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024*, pages 2421â€“2425. ACM, 2024.doi:10.1145/3626772.3657951.URL[https://doi.org/10.1145/3626772.3657951](https://doi.org/10.1145/3626772.3657951).
* Mihaylov etÂ al. [2018]T.Â Mihaylov, P.Â Clark, T.Â Khot, and A.Â Sabharwal.Can a suit of armor conduct electricity? A new dataset for open book question answering.In E.Â Riloff, D.Â Chiang, J.Â Hockenmaier, and J.Â Tsujii, editors,*Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018*, pages 2381â€“2391. Association for Computational Linguistics, 2018.
* Nguyen etÂ al. [2024]T.Â Nguyen, P.Â Chin, and Y.-W. Tai.Reward-rag: Enhancing rag with reward driven supervision, 2024.URL[https://arxiv.org/abs/2410.03780](https://arxiv.org/abs/2410.03780).
* Nogueira etÂ al. [2020]R.Â F. Nogueira, Z.Â Jiang, R.Â Pradeep, and J.Â Lin.Document ranking with a pretrained sequence-to-sequence model.In T.Â Cohn, Y.Â He, and Y.Â Liu, editors,*Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020*, volume EMNLP 2020 of*Findings of ACL*, pages 708â€“718. Association for Computational Linguistics, 2020.doi:10.18653/V1/2020.FINDINGS-EMNLP.63.URL[https://doi.org/10.18653/v1/2020.findings-emnlp.63](https://doi.org/10.18653/v1/2020.findings-emnlp.63).
* OpenAI etÂ al. [2024a]OpenAI, :, A.Â Hurst, A.Â Lerer, A.Â P. Goucher, A.Â Perelman, A.Â Ramesh, etÂ al.Gpt-4o system card, 2024a.URL[https://arxiv.org/abs/2410.21276](https://arxiv.org/abs/2410.21276).
* OpenAI etÂ al. [2024b]OpenAI, J.Â Achiam, S.Â Adler, S.Â Agarwal, L.Â Ahmad, I.Â Akkaya, etÂ al.Gpt-4 technical report, 2024b.URL[https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774).
* Ouyang etÂ al. [2022]L.Â Ouyang, J.Â Wu, X.Â Jiang, D.Â Almeida, C.Â L. Wainwright, P.Â Mishkin, C.Â Zhang, S.Â Agarwal, K.Â Slama, A.Â Ray, J.Â Schulman, J.Â Hilton, F.Â Kelton, L.Â Miller, M.Â Simens, A.Â Askell, P.Â Welinder, P.Â Christiano, J.Â Leike, and R.Â Lowe.Training language models to follow instructions with human feedback, 2022.URL[https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155).
* Petroni etÂ al. [2021]F.Â Petroni, A.Â Piktus, A.Â Fan, P.Â S.Â H. Lewis, M.Â Yazdani, N.Â D. Cao, J.Â Thorne, Y.Â Jernite, V.Â Karpukhin, J.Â Maillard, V.Â Plachouras, T.Â RocktÃ¤schel, and S.Â Riedel.KILT: a benchmark for knowledge intensive language tasks.In K.Â Toutanova, A.Â Rumshisky, L.Â Zettlemoyer, D.Â Hakkani-TÃ¼r, I.Â Beltagy, S.Â Bethard, R.Â Cotterell, T.Â Chakraborty, and Y.Â Zhou, editors,*Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021*, pages 2523â€“2544. Association for Computational Linguistics, 2021.
* Pradeep etÂ al. [2023]R.Â Pradeep, S.Â Sharifymoghaddam, and J.Â Lin.Rankvicuna: Zero-shot listwise document reranking with open-source large language models, 2023.URL[https://arxiv.org/abs/2309.15088](https://arxiv.org/abs/2309.15088).
* Qin etÂ al. [2024]Z.Â Qin, R.Â Jagerman, K.Â Hui, H.Â Zhuang, J.Â Wu, L.Â Yan, J.Â Shen, T.Â Liu, J.Â Liu, D.Â Metzler, X.Â Wang, and M.Â Bendersky.Large language models are effective text rankers with pairwise ranking prompting.In K.Â Duh, H.Â Gomez, and S.Â Bethard, editors,*Findings of the Association for Computational Linguistics: NAACL 2024*, pages 1504â€“1518, Mexico City, Mexico, June 2024. Association for Computational Linguistics.doi:10.18653/v1/2024.findings-naacl.97.URL[https://aclanthology.org/2024.findings-naacl.97/](https://aclanthology.org/2024.findings-naacl.97/).
* Rafailov etÂ al. [2024]R.Â Rafailov, A.Â Sharma, E.Â Mitchell, C.Â D. Manning, S.Â Ermon, and C.Â Finn.Direct preference optimization: Your language model is secretly a reward model.*Advances in Neural Information Processing Systems*, 36, 2024.
* Ram etÂ al. [2023]O.Â Ram, Y.Â Levine, I.Â Dalmedigos, D.Â Muhlgay, A.Â Shashua, K.Â Leyton-Brown, and Y.Â Shoham.In-context retrieval-augmented language models.*Transactions of the Association for Computational Linguistics*, pages 1316â€“1331, 2023.
* Robertson and Zaragoza [2009]S.Â E. Robertson and H.Â Zaragoza.The probabilistic relevance framework: BM25 and beyond.*Found. Trends Inf. Retr.*, 3(4):333â€“389, 2009.doi:10.1561/1500000019.URL[https://doi.org/10.1561/1500000019](https://doi.org/10.1561/1500000019).
* Sachan etÂ al. [2023]D.Â S. Sachan, M.Â Lewis, M.Â Joshi, A.Â Aghajanyan, W.Â tau Yih, J.Â Pineau, and L.Â Zettlemoyer.Improving passage retrieval with zero-shot question generation, 2023.URL[https://arxiv.org/abs/2204.07496](https://arxiv.org/abs/2204.07496).
* Song etÂ al. [2024]E.Â Song, S.Â Kim, H.Â Lee, J.Â Kim, and J.Â Thorne.Re3val: Reinforced and reranked generative retrieval.In Y.Â Graham and M.Â Purver, editors,*Findings of the Association for Computational Linguistics: EACL 2024, St. Julianâ€™s, Malta, March 17-22, 2024*, pages 393â€“409. Association for Computational Linguistics, 2024.URL[https://aclanthology.org/2024.findings-eacl.27](https://aclanthology.org/2024.findings-eacl.27).
* Stelmakh etÂ al. [2022]I.Â Stelmakh, Y.Â Luan, B.Â Dhingra, and M.Â Chang.ASQA: factoid questions meet long-form answers.In Y.Â Goldberg, Z.Â Kozareva, and Y.Â Zhang, editors,*Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022*, pages 8273â€“8288. Association for Computational Linguistics, 2022.
* Su etÂ al. [2024]W.Â Su, Y.Â Tang, Q.Â Ai, Z.Â Wu, and Y.Â Liu.Dragin: Dynamic retrieval augmented generation based on the information needs of large language models, 2024.URL[https://arxiv.org/abs/2403.10081](https://arxiv.org/abs/2403.10081).
* Sun etÂ al. [2023]W.Â Sun, L.Â Yan, X.Â Ma, etÂ al.Is ChatGPT good at search? investigating large language models as re-ranking agents.In H.Â Bouamor, J.Â Pino, and K.Â Bali, editors,*Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 14918â€“14937, Singapore, Dec. 2023. Association for Computational Linguistics.doi:10.18653/v1/2023.emnlp-main.923.URL[https://aclanthology.org/2023.emnlp-main.923/](https://aclanthology.org/2023.emnlp-main.923/).
* Taori etÂ al. [2023]R.Â Taori, I.Â Gulrajani, T.Â Zhang, Y.Â Dubois, X.Â Li, C.Â Guestrin, P.Â Liang, and T.Â B. Hashimoto.Stanford alpaca: An instruction-following llama model.[https://github.com/tatsu-lab/stanford\_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023.
* Thorne etÂ al. [2018]J.Â Thorne, A.Â Vlachos, C.Â Christodoulopoulos, and A.Â Mittal.FEVER: a large-scale dataset for fact extraction and verification.In M.Â A. Walker, H.Â Ji, and A.Â Stent, editors,*Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers)*, pages 809â€“819. Association for Computational Linguistics, 2018.
* Touvron etÂ al. [2023]H.Â Touvron, L.Â Martin, K.Â Stone, etÂ al.Llama 2: Open foundation and fine-tuned chat models, 2023.URL[https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).
* Trivedi etÂ al. [2023]H.Â Trivedi, N.Â Balasubramanian, T.Â Khot, and A.Â Sabharwal.Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.In A.Â Rogers, J.Â Boyd-Graber, and N.Â Okazaki, editors,*Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 10014â€“10037, Toronto, Canada, July 2023. Association for Computational Linguistics.doi:10.18653/v1/2023.acl-long.557.URL[https://aclanthology.org/2023.acl-long.557/](https://aclanthology.org/2023.acl-long.557/).
* Xi etÂ al. [2024]Z.Â Xi, Y.Â Ding, W.Â Chen, B.Â Hong, H.Â Guo, J.Â Wang, D.Â Yang, C.Â Liao, X.Â Guo, W.Â He, S.Â Gao, L.Â Chen, R.Â Zheng, Y.Â Zou, T.Â Gui, Q.Â Zhang, X.Â Qiu, X.Â Huang, Z.Â Wu, and Y.-G. Jiang.Agentgym: Evolving large language model-based agents across diverse environments, 2024.URL[https://arxiv.org/abs/2406.04151](https://arxiv.org/abs/2406.04151).
* Yang etÂ al. [2018]Z.Â Yang, P.Â Qi, S.Â Zhang, Y.Â Bengio, W.Â W. Cohen, R.Â Salakhutdinov, and C.Â D. Manning.Hotpotqa: A dataset for diverse, explainable multi-hop question answering.In E.Â Riloff, D.Â Chiang, J.Â Hockenmaier, and J.Â Tsujii, editors,*Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018*, pages 2369â€“2380. Association for Computational Linguistics, 2018.
* Yao etÂ al. [2023]S.Â Yao, J.Â Zhao, D.Â Yu, N.Â Du, I.Â Shafran, K.Â Narasimhan, and Y.Â Cao.React: Synergizing reasoning and acting in language models, 2023.URL[https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629).
* Yu etÂ al. [2024a]T.Â Yu, S.Â Zhang, and Y.Â Feng.Auto-rag: Autonomous retrieval-augmented generation for large language models, 2024a.URL[https://arxiv.org/abs/2411.19443](https://arxiv.org/abs/2411.19443).
* Yu etÂ al. [2024b]Y.Â Yu, W.Â Ping, Z.Â Liu, B.Â Wang, J.Â You, C.Â Zhang, M.Â Shoeybi, and B.Â Catanzaro.Rankrag: Unifying context ranking with retrieval-augmented generation in llms, 2024b.URL[https://arxiv.org/abs/2407.02485](https://arxiv.org/abs/2407.02485).
* Zeng etÂ al. [2023]A.Â Zeng, M.Â Liu, R.Â Lu, B.Â Wang, X.Â Liu, Y.Â Dong, and J.Â Tang.Agenttuning: Enabling generalized agent abilities for llms, 2023.URL[https://arxiv.org/abs/2310.12823](https://arxiv.org/abs/2310.12823).
* Zhang etÂ al. [2020]T.Â Zhang, V.Â Kishore, F.Â Wu, K.Â Q. Weinberger, and Y.Â Artzi.Bertscore: Evaluating text generation with BERT.In*8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020.
* Zhang etÂ al. [2024]T.Â Zhang, S.Â G. Patil, N.Â Jain, S.Â Shen, M.Â Zaharia, I.Â Stoica, and J.Â E. Gonzalez.Raft: Adapting language model to domain specific rag.*arXiv preprint arXiv:2403.10131*, 2024.
## Appendix AAppendix
### A.1Algorithm
The algorithm of our main method is shown in Algorithm[1](https://arxiv.org/html/2505.07233v2#alg1).
Algorithm 1DynamicRAG
0:Expert datasetð’Ÿesubscriptð’Ÿð‘’\\mathcal{D}\_{e}caligraphic\_D start\_POSTSUBSCRIPT italic\_e end\_POSTSUBSCRIPT, environmentð’¢ð’¢\\mathcal{G}caligraphic\_G, iterationsNð‘Nitalic\_N, Normal datasetð’Ÿtâ¢râ¢aâ¢iâ¢nsubscriptð’Ÿð‘¡ð‘Ÿð‘Žð‘–ð‘›\\mathcal{D}\_{train}caligraphic\_D start\_POSTSUBSCRIPT italic\_t italic\_r italic\_a italic\_i italic\_n end\_POSTSUBSCRIPT
1:INITIALIZE rerankerÏ€Î¸rsubscriptðœ‹subscriptðœƒð‘Ÿ\\pi\_{\\theta\_{r}}italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT end\_POSTSUBSCRIPTand generatorÏ€Î¸gsubscriptðœ‹subscriptðœƒð‘”\\pi\_{\\theta\_{g}}italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ start\_POSTSUBSCRIPT italic\_g end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT
2:STEP 1: BEHAVIORAL CLONING
3:foreach sample(ð’¢,i,Ï„)âˆˆð’Ÿeð’¢ð‘–ðœsubscriptð’Ÿð‘’(\\mathcal{G},i,\\tau)\\in\\mathcal{D}\_{e}( caligraphic\_G , italic\_i , italic\_Ï„ ) âˆˆcaligraphic\_D start\_POSTSUBSCRIPT italic\_e end\_POSTSUBSCRIPTdo
4:Update reranker via:|ð’¥Bâ¢Câ¢(Î¸r)=ð”¼â¢[âˆ‘t=1Tlogâ¡Ï€Î¸râ¢(at|ð’¢,i,htâˆ’1)]subscriptð’¥ðµð¶subscriptðœƒð‘Ÿð”¼delimited-[]superscriptsubscriptð‘¡1ð‘‡subscriptðœ‹subscriptðœƒð‘Ÿconditionalsubscriptð‘Žð‘¡ð’¢ð‘–subscriptâ„Žð‘¡1\\mathcal{J}\_{BC}(\\theta\_{r})=\\mathbb{E}\\Big{[}\\sum\_{t=1}^{T}\\log\\pi\_{\\theta\_{r%
}}(a\_{t}|\\mathcal{G},i,h\_{t-1})\\Big{]}caligraphic\_J start\_POSTSUBSCRIPT italic\_B italic\_C end\_POSTSUBSCRIPT ( italic\_Î¸ start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT ) = blackboard\_E [ âˆ‘start\_POSTSUBSCRIPT italic\_t = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT roman\_log italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ( italic\_a start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT | caligraphic\_G , italic\_i , italic\_h start\_POSTSUBSCRIPT italic\_t - 1 end\_POSTSUBSCRIPT ) ]||
5:endfor
6:STEP 2: GENERATOR TRAINING
7:foreach sample(ðª,D^,ygâ¢t)âˆˆð’Ÿtâ¢râ¢aâ¢iâ¢nðª^ð·subscriptð‘¦ð‘”ð‘¡subscriptð’Ÿð‘¡ð‘Ÿð‘Žð‘–ð‘›(\\mathbf{q},\\hat{D},y\_{gt})\\in\\mathcal{D}\_{train}( bold\_q , over^ start\_ARG italic\_D end\_ARG , italic\_y start\_POSTSUBSCRIPT italic\_g italic\_t end\_POSTSUBSCRIPT ) âˆˆcaligraphic\_D start\_POSTSUBSCRIPT italic\_t italic\_r italic\_a italic\_i italic\_n end\_POSTSUBSCRIPTdo
8:Optimize generatorÏ€Î¸gsubscriptðœ‹subscriptðœƒð‘”\\pi\_{\\theta\_{g}}italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ start\_POSTSUBSCRIPT italic\_g end\_POSTSUBSCRIPT end\_POSTSUBSCRIPTvia:|ð’¥genâ¢(Î¸g)=ð”¼â¢[âˆ‘t=1Tlogâ¡pÎ¸gâ¢(ytâˆ£ðª,D^,y&lt;t)]subscriptð’¥gensubscriptðœƒð‘”ð”¼delimited-[]superscriptsubscriptð‘¡1ð‘‡subscriptð‘subscriptðœƒð‘”conditionalsubscriptð‘¦ð‘¡ðª^ð·subscriptð‘¦absentð‘¡\\mathcal{J}\_{\\text{gen}}(\\theta\_{g})=\\mathbb{E}\\Big{[}\\sum\_{t=1}^{T}\\log p\_{%
\\theta\_{g}}(y\_{t}\\mid\\mathbf{q},\\hat{D},y\_{&lt;&lt;t})\\Big{]}caligraphic\_J start\_POSTSUBSCRIPT gen end\_POSTSUBSCRIPT ( italic\_Î¸ start\_POSTSUBSCRIPT italic\_g end\_POSTSUBSCRIPT ) = blackboard\_E [ âˆ‘start\_POSTSUBSCRIPT italic\_t = 1 end\_POSTSUBSCRIPT start\_POSTSUPERSCRIPT italic\_T end\_POSTSUPERSCRIPT roman\_log italic\_p start\_POSTSUBSCRIPT italic\_Î¸ start\_POSTSUBSCRIPT italic\_g end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ( italic\_y start\_POSTSUBSCRIPT italic\_t end\_POSTSUBSCRIPT âˆ£bold\_q , over^ start\_ARG italic\_D end\_ARG , italic\_y start\_POSTSUBSCRIPT &lt;&lt; italic\_t end\_POSTSUBSCRIPT ) ]||
9:endfor
10:STEP 3: INTERACTIVE LEARNING
11:forn=1ð‘›1n=1italic\_n = 1toNð‘Nitalic\_Ndo
12:Collect trajectories and compute rewards:|r=Î±â‹…EM+Î²â‹…SS+Î³â‹…TF+Î»â‹…LP+Î´â‹…LLM-Evalð‘Ÿâ‹…ð›¼EMâ‹…ð›½SSâ‹…ð›¾TFâ‹…ðœ†LPâ‹…ð›¿LLM-Evalr=\\alpha\\cdot\\text{EM}+\\beta\\cdot\\text{SS}+\\gamma\\cdot\\text{TF}+\\lambda\\cdot%
\\text{LP}+\\delta\\cdot\\text{LLM-Eval}italic\_r = italic\_Î± â‹…EM + italic\_Î² â‹…SS + italic\_Î³ â‹…TF + italic\_Î» â‹…LP + italic\_Î´ â‹…LLM-Eval||
13:Optimize reranker via DPO:|ð’¥DPOâ¢(Î¸r)=ð”¼â¢[logâ¡Ïƒâ¢(Î²â¢(logâ¡Ï€Î¸râ¢(Ï„+)âˆ’logâ¡Ï€Î¸râ¢(Ï„âˆ’)))]subscriptð’¥DPOsubscriptðœƒð‘Ÿð”¼delimited-[]ðœŽð›½subscriptðœ‹subscriptðœƒð‘Ÿsuperscriptðœsubscriptðœ‹subscriptðœƒð‘Ÿsuperscriptðœ\\mathcal{J}\_{\\text{DPO}}(\\theta\_{r})=\\mathbb{E}\\Big{[}\\log\\sigma\\big{(}\\beta(%
\\log\\pi\_{\\theta\_{r}}(\\tau^{+})-\\log\\pi\_{\\theta\_{r}}(\\tau^{-}))\\big{)}\\Big{]}caligraphic\_J start\_POSTSUBSCRIPT DPO end\_POSTSUBSCRIPT ( italic\_Î¸ start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT ) = blackboard\_E [ roman\_log italic\_Ïƒ ( italic\_Î² ( roman\_log italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT + end\_POSTSUPERSCRIPT ) - roman\_log italic\_Ï€ start\_POSTSUBSCRIPT italic\_Î¸ start\_POSTSUBSCRIPT italic\_r end\_POSTSUBSCRIPT end\_POSTSUBSCRIPT ( italic\_Ï„ start\_POSTSUPERSCRIPT - end\_POSTSUPERSCRIPT ) ) ) ]||
14:endfor
## Appendix BFurther Analysis
### B.1Effect of Top-N Documents on DynamicRAG Performance
The Figure[7](https://arxiv.org/html/2505.07233v2#A2.F7)presents the performance of DynamicRAG with different numbers of top-K documents (from 50 to 500) used for reranking across three benchmarks: NQ, HotpotQA, and ASQA. We adopt the same technique as> Sun etÂ al. [
[> 51
](https://arxiv.org/html/2505.07233v2#bib.bib51)> ]
, where we use sliding window to handle large document sets which allows the model to process larger corpora efficiently. First, as the number of top documents increases, the performance improves, reaching its peak around Top-100 or Top-150, with the highest EM scores recorded for NQ and HotpotQA. However, as more documents are introduced (Top-200, Top-300, Top-500), the performance starts to decline slightly, with the average EM dropping from 56.8 (Top-50) to 55.3 (Top-500). This trend indicates that beyond a certain threshold, increasing the number of documents introduces irrelevant or misleading information that negatively impacts the modelâ€™s performance. The challenge of processing and filtering through a larger set of documents, which can introduce noise and reduce the modelâ€™s ability to focus on the most relevant content.
![Refer to caption](x9.png)Figure 7:The impact of varying the number of Top-Nð‘Nitalic\_Ndocuments (Top-50, Top-100, Top-150, Top-200, Top-300, and Top-500) used for reranking on DynamicRAG performance across different benchmarks (NQ, HotpotQA, ASQA). We use Exact Match as the metric.
### B.2Impact of Different Reward Functions on DynamicRAG Performance
The Table[4](https://arxiv.org/html/2505.07233v2#A2.T4)presents an analysis of DynamicRAGâ€™s performance under various reward model configurations, incorporating key components such as EM (Exact Match), SS (Semantic Similarity), TF (Textual Fluency), LP (Length Penalty), and LLMEval (LLM-Based Evaluation). The EM component is found to be critical for open-domain QA tasks, as its removal results in a significant performance decline, particularly on benchmarks such as NQ, HotpotQA, and ASQA. However, its impact on long-text generation tasks, like ELI5, is comparatively less pronounced. Conversely, the removal of SS or TF functions yields opposing effects: removing SS and TF has a more pronounced negative impact on long-text generation tasks, while their effect on open-domain QA is relatively modest. This suggests that SS is essential for improving the modelâ€™s generalization capabilities, while TF plays a crucial role in enhancing text relevance. The LP function, when excluded, results in a slight but consistent drop in performance across all benchmarks, indicating its influence on overall model balance by regulating output length and maintaining response coherence. LLMEval, exhibiting effects similar to those of SS and TF, contributes moderately to performance optimization, underscoring its supportive role. Overall, the consistent trends across different reward function configurations highlight that the modelâ€™s success is predominantly driven by the synergy of EM, SS, TF, and LP, with LLMEval serving as a supplementary component in refining performance.
Table 4:The performance of DynamicRAG with different reward function designs across various benchmarks (NQ, HotpotQA, ASQA, ELI5). We use Exact Match as the metric for NQ, HotpotQA and ASQA and use Rouge-L as the metric for ELI5.
|Reward Function|NQ|HotpotQA|ASQA|ELI5|Avg.|
|EM|EM|EM|Rg||
DynamicRAG|48.4|36.7|56.3|24.6|41.5|
w/o EM|39.6|22.3|35.4|24.4|30.4|
w/o SS|47.7|36.0|55.6|22.6|40.5|
w/o TF|47.8|36.2|55.7|22.0|40.4|
w/o LP|48.0|36.2|55.8|24.0|41.0|
w/o LLMEval|48.1|36.3|56.0|22.7|40.8|
To further prove the robustness of our reward functions, we additionally experimented with the following configurations (Î±ð›¼\\alphaitalic\_Î±,Î²ð›½\\betaitalic\_Î²,Î³ð›¾\\gammaitalic\_Î³,Î»ðœ†\\lambdaitalic\_Î», andÎ´ð›¿\\deltaitalic\_Î´):
* â€¢Hyperparameter 1 = (0.3, 0.25, 0.2, 0.15, 0.1)
* â€¢Hyperparameter 1 = (0.4, 0.15, 0.15, 0.15, 0.15)
* â€¢Hyperparameter 1 = (0.1, 0.25, 0.2, 0.15, 0.3)
Table 5:Performance comparison across different settings and datasets|Setting|NQ|HotpotQA|ASQA|ELI5|Avg.|
Paper Setting|48.4|36.7|56.3|24.6|41.5|
Hyperparameter 1|48.5|36.6|56.2|24.3|41.4|
Hyperparameter 2|48.5|36.7|56.5|24.4|41.5|
Hyperparameter 3|48.2|36.6|56.2|24.4|41.3|
The results show that different parameter settings yield similar performance, indicating that our framework is robust and largely insensitive to these hyperparameters.
### B.3Dynamic Reranker Module Enhances Closed-Source Model RAG Performance
Many studies> [
[> 60
](https://arxiv.org/html/2505.07233v2#bib.bib60)> , [> 6
](https://arxiv.org/html/2505.07233v2#bib.bib6)> ]
have demonstrated that increasing the number of input documents introduces more irrelevant information, ultimately degrading model performance. Therefore, dynamically adjustingkð‘˜kitalic\_kremains crucial, even for strong models. In fact, since our reward mechanism leverages responses and ground-truth outputs, our approach is fully applicable to closed-source, robust models such as GPT-4o. Experimental results validating this claim are presented below:
Table 6:Performance Improvement Using Dynamic Reranker on Closed-Source Models.|Model|NQ|HotpotQA|ASQA|
GPT-4o|40.0|36.1|74.1|
GPT-4o w/ RAG Top-20|40.4|34.2|73.2|
GPT-4o w/ Dynamic Reranker|42.3|36.9|74.8|
The results show that the Dynamic Reranker module consistently improves GPT-4oâ€™s performance across all datasets, with gains of 2.3, 0.8, and 0.7 percentage points on NQ, HotpotQA, and ASQA, respectively. In contrast, the standard RAG Top-20 approach shows inconsistent results, even degrading performance on two datasets. The Dynamic Reranker module effectively addresses this issue by dynamically adjusting the number of reranked documents k, demonstrating that even powerful closed-source models like GPT-4o can benefit from our approach.
## Appendix CExperiment Details
### C.1Training Details
We train our models on 8 Nvidia A100 GPUs, each with 80GB of memory. Fine-tuning is performed for 1 epoch with an effective batch size of 4, achieved by setting a per-device batch size of 2 and accumulating gradients over 2 steps. The learning rate is configured at 1e-5, with a 10% warmup ratio and a cosine learning rate schedule. Maximum token lengths are set to 4,096 for LLaMA2 and 8,192 for LLaMA3, adhering to the training configuration. Multi-GPU distributed training is efficiently handled using DeepSpeed Stage 3, with Bfloat16 precision to optimize memory usage. FlashAttention enhances the efficiency of long-context training. For behavior cloning, we employ MonoT5> [
[> 37
](https://arxiv.org/html/2505.07233v2#bib.bib37)> ]
as the expert model, setÏ„ðœ\\tauitalic\_Ï„as 0.8 and constrain the number of documents to a maximum of 15, since many works> [
[> 1
](https://arxiv.org/html/2505.07233v2#bib.bib1)> , [> 60
](https://arxiv.org/html/2505.07233v2#bib.bib60)> ]
only use the top-10 as the input for the generator, we aim to obtain a relatively fair comparison, so we do not setkð‘˜kitalic\_ktoo large. ForÎ±ð›¼\\alphaitalic\_Î±,Î²ð›½\\betaitalic\_Î²,Î³ð›¾\\gammaitalic\_Î³,Î»ðœ†\\lambdaitalic\_Î», andÎ´ð›¿\\deltaitalic\_Î´in reward functions, we simply use (0.2, 0.2, 0.2, 0.2, 0.2). For sampling trajectories, we use temperature to 1.0 and top p to 0.9. For DPO training, we used the following configuration: a learning rate of 5e-6, 2 epochs, a 10% warmup ratio, and a cosine learning rate schedule. The batch size was set to 1, and accumulating gradients over 4 steps. For inference, we leverage the same A100 GPUs and utilize vLLMs to accelerate inference time.
#### C.1.1Retriever Setting
To construct the training data, we retrieved the top 45 documents using Contriever-MS MARCO from official 2018 English Wikipedia embeddings. These documents were used to create training datasets for both the reranker and generator components. Unlike other works, we do not use retrieval results from external sources, such as Google Search. Instead, all evaluations are strictly based on retrieval results from the same retriever used for training data construction, ensuring consistency and comparability.
#### C.1.2Training Dataset
We utilized the Alpaca and specific KILT benchmark datasets, including NQ, FEVER, HotpotQA, ELI5, and TriviaQA, as well as ASQA and OpenBookQA, totaling approximately 150k data instances. And we employ 20k for cold-start reranker training, 100k for supervised fine-tuning of the generator, and 30k for DPO training.
#### C.1.3Evaluation Setting
For ELI5, we set the maximum token length to 256 to accommodate the benchmarkâ€™s requirement for long-answer responses. For all other benchmarks, the maximum token length is set to 50. Table[7](https://arxiv.org/html/2505.07233v2#A3.T7)shows the list of the instructions used during evaluations.
Table 7:Full list of instructions used during our evaluations. We use the same prompt when eval Open-domain QA (NQ, TriviaQA, HotpotQA, 2WikimQA, ASQA.)
|Dataset|Instruction|
ARC|Please answer the following questions and directly output the answer options.|
FEVER|Please answer the question with â€œSUPPORTSâ€, â€œREFUTESâ€ or â€œNEIâ€ based on what you know.|
ELI5|Please answer the question with a paragraph.|
Open-domain QA|Please answer the question with a short phrase.|
### C.2Qualitative Examples
Tables[8](https://arxiv.org/html/2505.07233v2#A3.T8)and[9](https://arxiv.org/html/2505.07233v2#A3.T9)present two distinct examples illustrating the effectiveness of our approach. In the first example, the dynamic reranker produces a reordered sequence and selects a different number of retrieved documents compared to the expert model, yet it successfully generates the correct answer. This demonstrates the rerankerâ€™s superior ability to identify the most relevant information. In the second example, no additional documents are selected. This suggests that the reranker recognizes the query as sufficiently straightforward for the generator, eliminating the need for external information and thereby enhancing the overall efficiency of the generation process.
Table 8:Case Study for DynamicRAG. We compare our method with Vanilla-RAG with Reranker (monoT5). The initial retrieved content is the same for both methods. We take Top-40 documents as the input.|Question|What is the name of the American Neoclassical new-age music group that released the "Ambience" collection in 2001?|
GPT-4o|The Ambient. âœ—|
Vanilla-RAGw/Reranker|(Reranker Results:)|
|Doc 2: Pure Moods Pure Moods was the first United States release of a series of compilation albums of new-age music released by Virgin Records. The original was titled "Moods â€“A contemporary Soundtrack" and released in the UK in 1991. This was followed by "Moods 2" in 1992. The series focuses on the genres of new-age,ambient, world music, and to a lesser extent, downtempo, trip-hop and smooth jazz.|
|Doc 8: Christina Lux, Alquimia and Gleisberg; and several compilations of meditative and world-influenced New Age music including "Musical Healing" in 2001 and "Perfect Balance" in 2006. In 2006, the Gordon brothers released "Soothing Sanctuary",ambientmusic with nature sounds, a re-version of the earlier release "Sanctuary" (so stated in the liner notes) with new arrangements and remastering.|
|Doc 14: number that expanded to 200 as of March 2010 with the release of "Shaman", a duo recording featuring Jean Derome and Joane HÃ©tu. Presently DAME has a catalogue of over 450 titles. Ambiances MagnÃ©tiques began in 1982 when RenÃ© Lussier, a guitarist, and Jean Derome, who plays saxophone and flute, performed live as a duo in Montreal under the nameAmbiancesMagnÃ©tiques|
|(Total 40 Documents)|
|Response:Ambient âœ—|
DynamicRAG|(Reranker Results:)|
|Doc 5: over the fact that the album was not released as of May 2018, in the groupâ€™s 2018Mannheim SteamrollerChristmas Tour announcement, founder Chip Davis announced that Exotic Spaces would be officially released upon the start of the tour, being sold at all concert locations as well as on www.mannheimsteamroller.com and Amazon.com. "Billboard"s Top New Age Albums chart became the New Age Albums chart in June 2009.Mannheim Steamrolleris an American Neoclassical new-age music group founded by Chip Davis that is known primarily for its "Fresh Aire" series of albums, which blend classical music with elements of|
|Doc 17: Neoclassical new-age music
content: Neoclassical new-age music Within the broad movement of new-age music, neoclassical new-age music, or instrumental pop, is influenced by and sometimes also based upon early, baroque or classical music, especially in terms of melody and composition.|
|Doc 3: to an increase CD sales and eventually awards with a Western Canadian Music Awards (WCMA) (best dance) and a Juno Award nomination (best instrumental). Canadian success and a trip to MIDEM in France led to a deal with Bay Area indie label XDOT25 which released the EP in the USA.|
|(Total 7 Documents)|
|Response:Mannheim Steamroller âœ“|
Ground Truth|Mannheim Steamroller|
Table 9:Case Study for DynamicRAG. We compare our method with Vanilla-RAG with Reranker (monoT5). The initial retrieved content is the same for both methods. We take Top-40 documents as the input.|Question|where is arachidonic acid found in the body|
GPT-4o|Cell Membranes,Brain,Liver,Platelets,Eicosanoid Production Sites.|
Vanilla-RAGw/Reranker|(Reranker Results:)|
|Doc 1: present in the phospholipids (especially phosphatidylethanolamine, phosphatidylcholine, and phosphatidylinositides) of membranes of the bodyâ€™s cells, and is abundant in thebrain, muscl
