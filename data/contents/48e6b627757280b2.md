# A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future

**URL:** https://ar5iv.labs.arxiv.org/html/2309.15402
**Published:** 2022-01-01T00:00:00.000Z

---

## Summary

The user query asks for a summary covering several topics related to reasoning and planning with Large Language Models (LLMs), including: **Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**

This webpage is a survey titled "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future," which focuses heavily on **Chain-of-Thought (CoT)** reasoning (referred to broadly as X-of-Thought or XoT).

Here is a summary of how the page addresses the concepts in the query:

*   **Reasoning LLMs & Chain-of-Thought (CoT):** The survey is fundamentally about CoT reasoning, which enhances LLMs' capabilities for complex reasoning tasks (like mathematical and commonsense reasoning) by generating step-by-step rationales. It covers the introduction of CoT, its mathematical formulation, and taxonomies based on construction (Manual, Automatic, Semi-automatic), structure (Chain, Tree, Graph), and enhancement methods.
*   **Planning with LLMs:** Planning is explicitly mentioned as a **Frontier Application** of XoT (Â§5.2). Examples provided include ToT (Tree-of-Thought) and ReAct.
*   **Self-Reflection & Hallucination Reduction/Detection & Factuality:** These concepts are strongly covered under **XoT Enhancement Methods**, particularly in the **Verify and Refine** section (Â§4.3.1). This section discusses strategies like Self-Refine, Reflexion (which uses reinforcement learning for reflection), and methods that use verification (like RCoT or FOBAR) to reduce cascading errors and factual mistakes (hallucinations). External knowledge integration (Â§4.3.3) also aims to reduce factual errors.
*   **MCTS (Monte Carlo Tree Search) for Language Models:** This is addressed under **XoT Structural Variants** (Â§4.2) in the **Tree Structure** subsection, where Tree-of-Thought (ToT) methods are discussed, which often incorporate tree search algorithms to explore reasoning paths.
*   **Inference-time compute & Test-time scaling:** While the survey discusses methods that increase complexity (like Tree and Graph structures) and methods that improve efficiency (like SoT

The webpage provides information related to several aspects of your query concerning **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, hallucination reduction and detection, and grounding/factuality**.

Here is a summary of the relevant sections:

**Reasoning LLMs & Chain-of-Thought (CoT):**
*   The document surveys **Chain of Thought (CoT) Reasoning** and its generalized form, **X-of-Thought (XoT)**.
*   **CoT Distillation (Â§5.3)** discusses methods where reasoning chains generated by larger LLMs are distilled into smaller models to enhance their reasoning capabilities.
*   **Faithfulness (Â§6.2)** addresses the issue of **hallucination** (factual mistakes and contextual inconsistencies) in CoT reasoning, mentioning techniques that use external knowledge for evaluation, reflection mechanisms for correction, and question decomposition.
*   **CoT Theory (Â§6.3)** explores the empirical and theoretical underpinnings of why CoT works, noting its reliance on semantic knowledge and its potential to reduce the complexity of in-context learning.

**Inference-Time Compute & Efficiency:**
*   Section **4.3.5 Efficiency** discusses the expensive overheads of LLM reasoning. Methods mentioned to reduce inference costs include dynamically adjusting the number of samples (self-consistency), parallel question decomposition, and selectively skipping intermediate layers during reasoning.

**Self-Reflection:**
*   In **Â§5.2 Planning**, methods like **Self-Refine** (where the model evaluates and feeds back on its own output) and **Reflexion** (which reflects on and rectifies previous errors) are discussed as techniques to improve error correction.

**Planning with LLMs & MCTS for Language Models:**
*   Section **Â§5.2 Planning** details advanced planning techniques beyond basic CoT:
    *   **Tree-of-Thought (ToT)** allows LLMs to explore multiple reasoning paths in a tree structure and self-evaluate.
    *   **Reasoning via Planning (RAP)** uses the **Monte Carlo Tree Search (MCTS)** algorithm, employing LLMs as both the world model and the reasoning agent.
    *   **Graph of Thought (GoT)** uses graph nodes for thoughts and external Graph Neural Networks for organization.
    *

---

## Full Content

[2309.15402] A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future
# A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future
Zheng Chu111footnotemark:1,
Jingchang Chen111footnotemark:1,
Qianglong Chen2,
Weijiang Yu2,
Tao He1
Haotian Wang1, Weihua Peng2, Ming Liu1â€ , Bing Qin1, Ting Liu1
1Harbin Institute of Technology, Harbin, China
2Huawei Inc., Shenzhen, China
{zchu, jcchen, the, mliu , qinb, tliu}@ir.hit.edu.cn
{chenqianglong.ai, wanght1998, weijiangyu8, pengwh.hit}@gmail.comEqual Contribution.Corresponding Author.
###### Abstract
Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing.
However, there still remains a lack of a comprehensive survey for this arena.
To this end, we take the first step and present a thorough survey of this research field carefully and widely.
We use X-of-Thought to refer to Chain-of-Thought in a broad sense.
In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT.
Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation.
Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory.
We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning111Resources are available at[https://github.com/zchuz/CoT-Reasoning-Survey](https://github.com/zchuz/CoT-Reasoning-Survey).
## 1Introduction
Pre-trained language models (PLMs) can automatically learn general representations from unlabeled text and achieve excellent performance through fine-tuning on downstream tasks.> (Devlin etÂ al., [> 2019
](#bib.bib21)> ; Raffel etÂ al., [> 2020
](#bib.bib121)> ; Radford and Narasimhan, [> 2018
](#bib.bib119)> )
.
Recently, scaling up language models significantly improves performance and brings many surprises, such as emergent abilities> (Wei etÂ al., [> 2022a
](#bib.bib160)> ; Schaeffer etÂ al., [> 2023
](#bib.bib127)> )
.
Therefore, the paradigm of natural language processing is shifting from pre-training with fine-tuning to pre-training with in-context learning.
However, as of now, large-scale language models (LLMs) still have considerable room for improvement on complex reasoning tasks, such as mathematical reasoning> (Cobbe etÂ al., [> 2021
](#bib.bib17)> ; Patel etÂ al., [> 2021
](#bib.bib114)> )
, commonsense reasoning> (Talmor etÂ al., [> 2021
](#bib.bib141)> ; Mihaylov etÂ al., [> 2018
](#bib.bib102)> )
, etc.
To leverage LLMs for addressing complex reasoning tasks,> Wei etÂ al. (
[> 2022b
](#bib.bib161)> )
extends in-context learning with step-by-step reasoning processes, first introducing the concept of chain-of-thought (CoT) prompting.> Kojima etÂ al. (
[> 2022
](#bib.bib64)> )
finds that simply adding a magic phraseLetâ€™s think step by stepin prompts enables LLMs to perform zero-shot chain-of-thought reasoning without any human annotation.
These studies have highlighted the significance of chain-of-thought in enhancing the modelâ€™s capability for complex reasoning and improving its reasoning and planning abilities.
Subsequently, a substantial of works about X-of-thought (XoT) emerges like mushrooms after the rain in the NLP community, such as automatic XoT construction> (Kojima etÂ al., [> 2022
](#bib.bib64)> ; Zhang etÂ al., [> 2023f
](#bib.bib196)> ; Xu etÂ al., [> 2023
](#bib.bib167)> )
, XoT structural variants> (Chen etÂ al., [> 2022a
](#bib.bib12)> ; Ning etÂ al., [> 2023
](#bib.bib107)> ; Lei etÂ al., [> 2023a
](#bib.bib71)> ; Yao etÂ al., [> 2023b
](#bib.bib172)> )
, etc.
Note that to distinguish it from primitive CoT, we use XoT to refer to CoT in a broad sense, which is a collective term for the use of step-by-step reasoning methods.
However, these methods and datasets have not yet undergone systematic review and analysis. To fill this gap, we propose this work to conduct a comprehensive and detailed analysis of the XoT family.
Even though there have been some surveys discussing chain-of-thought, they are limited to specific aspects, such as LLM reasoning with prompts> (Qiao etÂ al., [> 2023
](#bib.bib118)> )
and chain-of-thought prompt strategies> (Yu etÂ al., [> 2023c
](#bib.bib185)> )
.
In contrast, our survey not only provides a more thorough and comprehensive discussion of the topics theyâ€™ve already covered, but also includes additional topics and discussions, such as XoT construction, XoT structural variants and frontier application, etc.
Concretely, in this paper, we first introduce the relevant background and preliminaryÂ (Â§[2](#S2)).
Furthermore, we carefully classify the XoT series of work from multiple perspectives and complete an in-depth analysisÂ (Â§[4](#S4)), including XoT construction methodsÂ (Â§[4.1](#S4.SS1)),
XoT structure variantsÂ (Â§[4.2](#S4.SS2))
and XoT enhancement methodsÂ (Â§[4.3](#S4.SS3)).
Then, we provide practical applications of the XoT in the frontier fieldsÂ (Â§[5](#S5)).
In order to inspire the follow-up work of XoT, we offer insights into potential avenues for future research in this areaÂ (Â§[6](#S6)).
Finally, we compare and discuss existing methodsÂ (Â§[7](#S7)).
## 2Background and Preliminary
### 2.1Background
In recent years, with the continuous expansion of computing power, large-scale language models have sprung up> Brown etÂ al. (
[> 2020
](#bib.bib9)> ); OpenAI (
[> 2023
](#bib.bib109)> ); Touvron etÂ al. (
[> 2023a
](#bib.bib143)> ); Scao etÂ al. (
[> 2022
](#bib.bib126)> ); Touvron etÂ al. (
[> 2023b
](#bib.bib144)> ); Zhao etÂ al. (
[> 2023b
](#bib.bib200)> )
, and as the model size continues to grow, many new capabilities have emerged, such as in-context learning and chain-of-thought reasoning> Brown etÂ al. (
[> 2020
](#bib.bib9)> ); Wei etÂ al. (
[> 2022b
](#bib.bib161)> , [> a
](#bib.bib160)> ); Schaeffer etÂ al. (
[> 2023
](#bib.bib127)> )
.
> Brown etÂ al. (
[> 2020
](#bib.bib9)> )
finds that large-scale language models have excellent in-context learning (ICL) ability.
ICL incorporates input-output demonstrations into the prompt text.
With ICL, off-the-shelf LLMs can be employed without additional fine-tuning
while achieving comparable performance.
Nevertheless, this end-to-end approach tends to underperform when faced with complex reasoning tasks.
> Wei etÂ al. (
[> 2022b
](#bib.bib161)> )
finds that the reasoning ability of LLMs can be improved by adding step-by-step reasoning processes to the demonstration, which is known as chain-of-thought prompting.
CoT prompting enables the model to gain a more precise understanding of both the questionâ€™s intricacies and the reasoning process.
Furthermore, the model generates a sequence of reasoning steps, which grants us a transparent view of the modelâ€™s cognitive process, further enhancing interpretability.
### 2.2Preliminary
In this section, we introduce the preliminary chain-of-thought reasoning with LLMs, and we refer to the formula definition in> (Qiao etÂ al., [> 2023
](#bib.bib118)> )
.
Suppose there is a questionð’¬ð’¬\\mathcal{Q}, a promptð’¯ð’¯\\mathcal{T}and a probabilistic language modelPLâ€‹Msubscriptð‘ƒð¿ð‘€P\_{LM}. The model takes the question and prompt as inputs to give the rationaleâ„›â„›\\mathcal{R}and answerð’œð’œ\\mathcal{A}.
We first consider in-context scenarios where the demonstrations do not contain reasoning chains. We need to maximize the likelihood of Answerð’œð’œ\\mathcal{A}, as shown in EquÂ ([1](#S2.E1),[2](#S2.E2)).
||pâ€‹(ð’œ|ð’¯,ð’¬)=âˆi=1|ð’œ|pLâ€‹Mâ€‹(ai|ð’¯,ð’¬,a&lt;i)ð‘conditionalð’œð’¯ð’¬superscriptsubscriptproductð‘–1ð’œsubscriptð‘ð¿ð‘€conditionalsubscriptð‘Žð‘–ð’¯ð’¬subscriptð‘Žabsentð‘–\\displaystyle p(\\mathcal{A}\~{}|\~{}\\mathcal{T,Q})=\\prod\_{i=1}^{|\\mathcal{A}|}p\_{LM}(a\_{i}\~{}|\~{}\\mathcal{T,Q},a\_{&lt;&lt;i})||(1)|
||ð’¯Iâ€‹Câ€‹L={I,(x1,y1),â‹¯,(xn,yn)}subscriptð’¯ð¼ð¶ð¿ð¼subscriptð‘¥1subscriptð‘¦1â‹¯subscriptð‘¥ð‘›subscriptð‘¦ð‘›\\displaystyle\\mathcal{T}\_{ICL}=\\{I,(x\_{1},y\_{1}),\\cdots,(x\_{n},y\_{n})\\}||(2)|
In the chain-of-thought reasoning scenario, where the demonstrations contain reasoning process, we need to maximize the likelihood of Answerð’œð’œ\\mathcal{A}and rationaleâ„›â„›\\mathcal{R}, as shown in EquÂ ([3](#S2.E3),[4](#S2.E4),[5](#S2.E5),[6](#S2.E6)).
||pâ€‹(ð’œ|ð’¯,ð’¬)=pâ€‹(ð’œ|ð’¯,ð’¬,â„›)â€‹pâ€‹(â„›|ð’¯,ð’¬)ð‘conditionalð’œð’¯ð’¬ð‘conditionalð’œð’¯ð’¬â„›ð‘conditionalâ„›ð’¯ð’¬\\displaystyle p(\\mathcal{A}\~{}|\~{}\\mathcal{T,Q})=p(\\mathcal{A}\~{}|\~{}\\mathcal{T,Q,R})p(\\mathcal{R}\~{}|\~{}\\mathcal{T,Q})||(3)|
||pâ€‹(â„›|ð’¯,ð’¬)=âˆi=1|â„›|pLâ€‹Mâ€‹(ri|ð’¯,ð’¬,r&lt;i)ð‘conditionalâ„›ð’¯ð’¬superscriptsubscriptproductð‘–1â„›subscriptð‘ð¿ð‘€conditionalsubscriptð‘Ÿð‘–ð’¯ð’¬subscriptð‘Ÿabsentð‘–\\displaystyle p(\\mathcal{R}\~{}|\~{}\\mathcal{T,Q})=\\prod\_{i=1}^{|\\mathcal{R}|}p\_{LM}(r\_{i}\~{}|\~{}\\mathcal{T,Q},r\_{&lt;&lt;i})||(4)|
||pâ€‹(ð’œ|ð’¯,ð’¬,â„›)=âˆj=1|ð’œ|pLâ€‹Mâ€‹(ai|ð’¯,ð’¬,â„›,a&lt;j)ð‘conditionalð’œð’¯ð’¬â„›superscriptsubscriptproductð‘—1ð’œsubscriptð‘ð¿ð‘€conditionalsubscriptð‘Žð‘–ð’¯ð’¬â„›subscriptð‘Žabsentð‘—\\displaystyle p(\\mathcal{A}|\\mathcal{T,Q,R})=\\prod\_{j=1}^{|\\mathcal{A}|}p\_{LM}(a\_{i}|\\mathcal{T,Q,R},a\_{&lt;&lt;j})||(5)|
||ð’¯CoT={I,(x1,e1,y1),â‹¯,(xn,en,yn)}subscriptð’¯CoTð¼subscriptð‘¥1subscriptð‘’1subscriptð‘¦1â‹¯subscriptð‘¥ð‘›subscriptð‘’ð‘›subscriptð‘¦ð‘›\\displaystyle\\mathcal{T}\_{\\mathrm{CoT}}=\\{I,(x\_{1},e\_{1},y\_{1}),\\cdots,(x\_{n},e\_{n},y\_{n})\\}||(6)|
## 3Benchmarks
|Task|Dataset|Size|Input|Output|Rationale|Description|
|AddSub> Hosseini etÂ al. (
[> 2014
](#bib.bib44)> )
|395|Question|Number|Equation|Simple arithmetic|
|SingleEq> Koncel-Kedziorski etÂ al. (
[> 2015
](#bib.bib65)> )
|508|Question|Number|Equation|Simple arithmetic|
|MultiArith> Roy and Roth (
[> 2015
](#bib.bib123)> )
|600|Question|Number|Equation|Simple arithmetic|
|MAWPS> Koncel-Kedziorski etÂ al. (
[> 2016
](#bib.bib66)> )
|3320|Question|Number|Equation|Simple arithmetic|
|AQUA-RAT> Ling etÂ al. (
[> 2017
](#bib.bib84)> )
|100,000|Question|Option|Natural Language|Math reasoning with NL rationale|
|ASDiv> Miao etÂ al. (
[> 2020
](#bib.bib101)> )
|2305|Question|Number|Equation|Multi-step math reasoning|
|SVAMP> Patel etÂ al. (
[> 2021
](#bib.bib114)> )
|1,000|Question|Number|Equation|Multi-step math reasoning|
|GSM8K> Cobbe etÂ al. (
[> 2021
](#bib.bib17)> )
|8,792|Question|Number|Natural Language|Multi-step math reasoning|
|GSM-Hard> Gao etÂ al. (
[> 2023
](#bib.bib32)> )
|936|Question|Number|Natural Language|GSM8K with larger number|
|MathQA> Amini etÂ al. (
[> 2019
](#bib.bib3)> )
|37,297|Question|Number|Operation|Annotated based on AQUA|
|DROP> Dua etÂ al. (
[> 2019
](#bib.bib27)> )
|96,567|Question+Passage|Number+Span|Equation|Reading comprehension form|
|TheoremQA> Chen etÂ al. (
[> 2023
](#bib.bib13)> )
|800|Question+Theorem|Number|âœ—|Answer based on theorems|
|TAT-QA> Zhu etÂ al. (
[> 2021
](#bib.bib207)> )
|16,552|Question+Table+Text|Number+Span|Operation|Answer based on tables|
|FinQA> Chen etÂ al. (
[> 2021
](#bib.bib14)> )
|8,281|Question+Table+Text|Number|Operation|Answer based on tables|
|ConvFinQA> Chen etÂ al. (
[> 2022b
](#bib.bib15)> )
|3892|Question+Table+Dialog|Number|Operation|Multi-turn dialogs|
|MATH> Hendrycks etÂ al. (
[> 2021
](#bib.bib42)> )
|12500|Question|Number|Natural Language|Challenging competition math problems|
Mathematical|NumGLUE> Mishra etÂ al. (
[> 2022b
](#bib.bib104)> )
|101,835|Question+Text|Number+Span|âœ—|Multi-task benchmark|
Reasoning|LILA> Mishra etÂ al. (
[> 2022a
](#bib.bib103)> )
|133,815|Question+Text|Free-form|Program|Multi-task benchmark|
|ARC> Bhakthavatsalam etÂ al. (
[> 2021
](#bib.bib6)> )
|7787|Question|Option|âœ—|From science exam|
|OpenBookQA> Mihaylov etÂ al. (
[> 2018
](#bib.bib102)> )
|5,957|Question+Context|Option|âœ—|Open-book knowledges|
|PIQA> Bisk etÂ al. (
[> 2020
](#bib.bib8)> )
|21000|Goal+Solution|Option|âœ—|Physical commonsense knowledge|
|CommonsenseQA> Talmor etÂ al. (
[> 2019
](#bib.bib140)> )
|12247|Question|Option|âœ—|Derived from ConceptNet|
|CommonsenseQA 2.0> Talmor etÂ al. (
[> 2021
](#bib.bib141)> )
|14343|Question|Yes/No|âœ—|Gaming annotation with high quality|
|Event2Mind> Rashkin etÂ al. (
[> 2018
](#bib.bib122)> )
|25000|Event|Intent+Reaction|âœ—|Intension commonsense reasoning|
|McTaco> Zhou etÂ al. (
[> 2019
](#bib.bib204)> )
|13225|Question|Option|âœ—|Event temporal commonsense reasoning|
|CosmosQA> Huang etÂ al. (
[> 2019
](#bib.bib50)> )
|35588|Question+Paragraph|Option|âœ—|Narrative commonsense reasoning|
|ComValidation> Wang etÂ al. (
[> 2019
](#bib.bib148)> )
|11997|Statement|Option|âœ—|Commonsense verification|
Commonsense|ComExplanation> Wang etÂ al. (
[> 2019
](#bib.bib148)> )
|11997|Statement|Option/Free-form|âœ—|Commonsense explanation|
Reasoning|StrategyQA> Geva etÂ al. (
[> 2021
](#bib.bib34)> )
|2,780|Question|Yes/No|âœ—|Multi-hop commonsense reasoning|
|Last Letter Concat.> Wei etÂ al. (
[> 2022b
](#bib.bib161)> )
|-|Words|Letters|âœ—|Rule-based|
|Coin Flip> Wei etÂ al. (
[> 2022b
](#bib.bib161)> )
|-|Statement|Yes/No|âœ—|Rule-based|
|Reverse List> Wei etÂ al. (
[> 2022b
](#bib.bib161)> )
|-|List|Reversed List|âœ—|Rule-based|
Symbolic|BigBench> Srivastava etÂ al. (
[> 2022
](#bib.bib136)> )
|-|-|-|âœ—|Contains multiple symbolic reasoning datasets|
Reasoning|BigBench-Hard> Suzgun etÂ al. (
[> 2023
](#bib.bib138)> )
|-|-|-|âœ—|Contains multiple symbolic reasoning datasets|
|ReClor> Yu etÂ al. (
[> 2020
](#bib.bib182)> )
|6,138|Question+Context|Option|âœ—|Questions from GMAT and LSAT|
|LogiQA> Liu etÂ al. (
[> 2020
](#bib.bib88)> )
|8,678|Question+Paragraph|Option|âœ—|Questions from China Civil Service Exam|
|ProofWriter> Tafjord etÂ al. (
[> 2021
](#bib.bib139)> )
|20192|Question+Rule|Answer+Proof|Entailment Tree|Reasoning process generation|
|FOLIO> Han etÂ al. (
[> 2022
](#bib.bib38)> )
|1435|Conclusion+Premise|Yes/No|âœ—|First-order logic|
Logical|DEER> Yang etÂ al. (
[> 2022
](#bib.bib170)> )
|1,200|Fact|Rule|âœ—|Inductive reasoning|
Reasoning|PrOntoQA> Saparov and He (
[> 2023
](#bib.bib125)> )
|-|Question+Context|Yes/No+Proccess|First-Order Logic|Deductive reasoning|
|VCR> Zellers etÂ al. (
[> 2019
](#bib.bib187)> )
|264,720|Question+Image|Option|Natural Language|Visual commonsense reasoning|
|VisualCOMET> Park etÂ al. (
[> 2020
](#bib.bib113)> )
|1,465,704|Image+Event|Action+Intent|âœ—|Visual commonsense reasoning|
|PMR> Dong etÂ al. (
[> 2022
](#bib.bib25)> )
|15,360|Image+Background|Option|âœ—|Premise-based multi-modal reasoning|
|ScienceQA> Lu etÂ al. (
[> 2022
](#bib.bib91)> )
|21,208|Q+Image+Context|Option|Natural Language|Multi-modal reasoning with NL rationales|
|VLEP> Lei etÂ al. (
[> 2020
](#bib.bib73)> )
|28,726|Premise+Video|Option|âœ—|Video event prediction|
|CLEVRER> Yi etÂ al. (
[> 2020
](#bib.bib178)> )
|305,280|Question+Video|Option/Free-form|Program|Video temporal and causal reasoning|
|STAR> Wu etÂ al. (
[> 2021
](#bib.bib163)> )
|600,000|Question+Video|Option|âœ—|Video situated reasoning|
|NEXT-QA> Xiao etÂ al. (
[> 2021
](#bib.bib166)> )
|47,692|Question+Video|Option|âœ—|Video temporal,causal,commonsense reasoning|
Multimodal|Causal-VidQA> Li etÂ al. (
[> 2022a
](#bib.bib75)> )
|107,600|Question+Video|Free-form|Natural Language|Video causal and commonsense reasoning|
Reasoning|News-KVQA> Gupta and Gupta (
[> 2022
](#bib.bib36)> )
|1,041,352|Q+V+KG|Option|âœ—|Video reasoning with external knowledge|
Table 1:An overview of benchmarks and tasks on reasoning.
### 3.1Mathematical Reasoning
Mathematical reasoning is often used to measure the reasoning power of a model.
Early benchmarks contain simple arithmetic operations> Hosseini etÂ al. (
[> 2014
](#bib.bib44)> ); Koncel-Kedziorski etÂ al. (
[> 2015
](#bib.bib65)> ); Roy and Roth (
[> 2015
](#bib.bib123)> ); Koncel-Kedziorski etÂ al. (
[> 2016
](#bib.bib66)> )
.> Ling etÂ al. (
[> 2017
](#bib.bib84)> )
labels the reasoning process in natural language form, and> Amini etÂ al. (
[> 2019
](#bib.bib3)> )
builds on AQUA by labeling the reasoning process in program form.
Later benchmarks> (Miao etÂ al., [> 2020
](#bib.bib101)> ; Patel etÂ al., [> 2021
](#bib.bib114)> ; Cobbe etÂ al., [> 2021
](#bib.bib17)> ; Gao etÂ al., [> 2023
](#bib.bib32)> )
contain more complex and diverse questions.> (Zhu etÂ al., [> 2021
](#bib.bib207)> ; Chen etÂ al., [> 2021
](#bib.bib14)> , [> 2022b
](#bib.bib15)> )
require reasoning based on the table content.
There are also general benchmarks> Hendrycks etÂ al. (
[> 2021
](#bib.bib42)> ); Mishra etÂ al. (
[> 2022a
](#bib.bib103)> , [> b
](#bib.bib104)> )
and reading comprehension form benchmarks> Dua etÂ al. (
[> 2019
](#bib.bib27)> ); Chen etÂ al. (
[> 2023
](#bib.bib13)> )
. Recently,> Yu etÂ al. (
[> 2021a
](#bib.bib183)> )
endowed pre-trained model with the ability of mathematical reasoning by using hierarchical reasoning and knowledge.
### 3.2Commonsense Reasoning
Commonsense reasoning is the process of making inferences, judgments, and understandings based on knowledge that is generally known and commonly perceived in the everyday world.
How to acquire and understand commonsense knowledge is a major impediment to models facing commonsense reasoning.
Many benchmarks and tasks are proposed focusing on commonsense understanding> (Talmor etÂ al., [> 2019
](#bib.bib140)> , [> 2021
](#bib.bib141)> ; Bhakthavatsalam etÂ al., [> 2021
](#bib.bib6)> ; Mihaylov etÂ al., [> 2018
](#bib.bib102)> ; Geva etÂ al., [> 2021
](#bib.bib34)> ; Huang etÂ al., [> 2019
](#bib.bib50)> ; Bisk etÂ al., [> 2020
](#bib.bib8)> )
,
event temporal commonsense reasoning> (Rashkin etÂ al., [> 2018
](#bib.bib122)> ; Zhou etÂ al., [> 2019
](#bib.bib204)> )
, and commonsense verification> (Wang etÂ al., [> 2019
](#bib.bib148)> )
.
### 3.3Symbolic Reasoning
Symbolic reasoning here refers specifically to the simulation of some simple operations, which are simple for humans yet challenging for LLMs.
Last letter concatenation, coin flip, and reverse list> (Wei etÂ al., [> 2022b
](#bib.bib161)> )
are the most commonly used symbolic reasoning tasks.
In addition, the collaborative benchmark BigBench> (Srivastava etÂ al., [> 2022
](#bib.bib136)> )
and BigBench-Hard> (Suzgun etÂ al., [> 2023
](#bib.bib138)> )
also contain several symbolic reasoning datasets, such as state tracking and object counting.
### 3.4Logical Reasoning
Logical reasoning is divided into deductive reasoning, inductive reasoning, and abductive reasoning> (Yu etÂ al., [> 2023a
](#bib.bib180)> )
.
Deductive reasoning derives conclusions from general premises> (Liu etÂ al., [> 2020
](#bib.bib88)> ; Yu etÂ al., [> 2020
](#bib.bib182)> ; Tafjord etÂ al., [> 2021
](#bib.bib139)> ; Han etÂ al., [> 2022
](#bib.bib38)> )
.
Inductive reasoning derives general conclusions from special cases> Yang etÂ al. (
[> 2022
](#bib.bib170)> )
.
Abductive reasoning gives rational explanations for observed phenomena> Saparov and He (
[> 2023
](#bib.bib125)> )
.
### 3.5Multi-modal Reasoning
In the real world, reasoning also involves information in modalities other than text, with visual modalities being the most prevalent.
To this end, many benchmarks for visual multi-modal reasoning are proposed> (Zellers etÂ al., [> 2019
](#bib.bib187)> ; Park etÂ al., [> 2020
](#bib.bib113)> ; Dong etÂ al., [> 2022
](#bib.bib25)> ; Lu etÂ al., [> 2022
](#bib.bib91)> )
, and among them, ScienceQA> (Lu etÂ al., [> 2022
](#bib.bib91)> )
annotates reasoning process and is the most commonly used visual multi-modal reasoning benchmark.
Video multi-modal reasoning> (Lei etÂ al., [> 2020
](#bib.bib73)> ; Yi etÂ al., [> 2020
](#bib.bib178)> ; Wu etÂ al., [> 2021
](#bib.bib163)> ; Xiao etÂ al., [> 2021
](#bib.bib166)> ; Li etÂ al., [> 2022a
](#bib.bib75)> ; Gupta and Gupta, [> 2022
](#bib.bib36)> )
is more challenging as it introduces additional temporal information compared to visual multi-modal reasoning.
### 3.6Metrics
##### Accuracy
Accuracy is used to assess a modelâ€™s ability on classification tasks and is commonly used for multi-choice> (Ling etÂ al., [> 2017
](#bib.bib84)> ; Mihaylov etÂ al., [> 2018
](#bib.bib102)> ; Liu etÂ al., [> 2020
](#bib.bib88)> ; Lu etÂ al., [> 2022
](#bib.bib91)> )
and yes/no> (Talmor etÂ al., [> 2021
](#bib.bib141)> ; Geva etÂ al., [> 2021
](#bib.bib34)> ; Han etÂ al., [> 2022
](#bib.bib38)> )
tasks.
|Accuracy=NcorrectNtotalAccuracysubscriptNcorrectsubscriptNtotal\\mathrm{Accuracy}=\\frac{\\mathrm{N\_{correct}}}{\\mathrm{N\_{total}}}||(7)|
##### EM and F1
EM and F1 are metrics used to evaluate free form> (Mishra etÂ al., [> 2022a
](#bib.bib103)> ; Wang etÂ al., [> 2019
](#bib.bib148)> ; Yi etÂ al., [> 2020
](#bib.bib178)> )
and span extraction> (Dua etÂ al., [> 2019
](#bib.bib27)> ; Zhu etÂ al., [> 2021
](#bib.bib207)> ; Mishra etÂ al., [> 2022b
](#bib.bib104)> )
tasks.
Both are calculated at the token level.
|F1=2â‹…Pâ‹…RP+RF1â‹…2PRPR\\displaystyle\\mathrm{F1}=\\frac{2\\cdot\\mathrm{P}\\cdot\\mathrm{R}}{\\mathrm{P}+\\mathrm{R}}||(8)|
|EM=âˆ‘ð•€â€‹[A=Aâ€²]NtotalEMð•€delimited-[]ð´superscriptð´â€²subscriptNtotal\\displaystyle\\mathrm{EM}=\\frac{\\sum\\mathbb{I}[A=A^{\\prime}]}{\\mathrm{N\_{total}}}||(9)|
where P and R stand for precision and recall, and EM calculates the proportion of predictions and answers that are exactly the same.
## 4Methods
In this section, we explore X-of-thought reasoning through three different categorizations:
the construction of X-of-thoughtÂ (Â§[4.1](#S4.SS1)),
the structural variants of X-of-thoughtÂ (Â§[4.2](#S4.SS2)),
and the enhanced methods of X-of-thoughtÂ (Â§[4.3](#S4.SS3)).
{forest}
forked edges,
for tree=
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
base=left,
font=,
rectangle,
draw=hidden-black,
rounded corners,
align=left,
minimum width=4em,
edge+=darkgray, line width=1pt,
s sep=3pt,
inner xsep=2pt,
inner ysep=3pt,
line width=0.8pt,
ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
,
where level=1text width=7em,font=,,
where level=2text width=8.5em,font=,,
where level=3text width=10.5em,font=,,
where level=4text width=12em,font=,,
[
A survey of X-of-Thought, ver
[
Methods Â (Â§[4](#S4))
[
XoT
Construction Â (Â§[4.1](#S4.SS1))
[
Manual XoT
[
E.g.,Â 
Few-shot CoT> Wei etÂ al. (
[> 2022b
](#bib.bib161)> )
,
PoT> Chen etÂ al. (
[> 2022a
](#bib.bib12)> )
, leaf, text width=32.5em
]
]
[
Automatic XoT
[
E.g.,Â 
Zero-shot CoT> Kojima etÂ al. (
[> 2022
](#bib.bib64)> )
,
Auto-CoT> Zhang etÂ al. (
[> 2023f
](#bib.bib196)> )
, leaf, text width=32.5em
]
]
[
Semi-automatic XoTÂ 
[
E.g.,Â 
AutoMate CoT> Shum etÂ al. (
[> 2023
](#bib.bib135)> )
,
BoostedPrompt> Pitis etÂ al. (
[> 2023
](#bib.bib117)> )
, leaf, text width=32.5em
]
]
]
[
XoT Structural
VariantsÂ (Â§[4.2](#S4.SS2))
[
E.g.,Â 
AoT> (Sel etÂ al., [> 2023
](#bib.bib129)> )
,
ToT> (Yao etÂ al., [> 2023b
](#bib.bib172)> )
,
SoT> (Ning etÂ al., [> 2023
](#bib.bib107)> )
,
GoT> (Lei etÂ al., [> 2023a
](#bib.bib71)> )
, leaf, text width=44.6em
]
]
[
XoT Enhancement
MethodsÂ (Â§[4.3](#S4.SS3))
[
Verify and Refine
[
E.g.,Â 
VerifyCoT> Ling etÂ al. (
[> 2023
](#bib.bib85)> )
,
Self-Refine> Madaan etÂ al. (
[> 2023
](#bib.bib95)> )
, leaf, text width=32.5em
]
]
[
Question Decompose
[
E.g.,Â 
Least-to-Most Prompting> Zhou etÂ al. (
[> 2023b
](#bib.bib205)> )
, leaf, text width=32.5em
]
]
[
External Knowledge
[
E.g.,Â 
CoK> Wang etÂ al. (
[> 2023b
](#bib.bib149)> )
,
KD-CoT> Wang etÂ al. (
[> 2023c
](#bib.bib150)> )
, leaf, text width=32.5em
]
]
[
Vote and Rank
[
E.g.,Â 
Verifiers> Cobbe etÂ al. (
[> 2021
](#bib.bib17)> )
,
Self-Consistency> Wang etÂ al. (
[> 2023j
](#bib.bib158)> )
, leaf, text width=32.5em
]
]
[
Efficiency
[
E.g.,Â 
SoT> Ning etÂ al. (
[> 2023
](#bib.bib107)> )
,
ActivePrompting> Diao etÂ al. (
[> 2023
](#bib.bib23)> )
, leaf, text width=32.5em
]
]
]
]
[
Frontier
Application Â (Â§[5](#S5))
[
Tool UsingÂ (Â§[5.1](#S5.SS1))
[
E.g.,Â 
MRKL> Karpas etÂ al. (
[> 2022
](#bib.bib60)> )
,
TAML> Parisi etÂ al. (
[> 2022a
](#bib.bib111)> )
,
Toolformer> Schick etÂ al. (
[> 2023
](#bib.bib128)> )
, leaf, text width=44.6em
]
]
[
PlanningÂ (Â§[5.2](#S5.SS2))
[
E.g.,Â 
ToT> Long (
[> 2023
](#bib.bib89)> )
,
ReAct> Yao etÂ al. (
[> 2023c
](#bib.bib173)> )
,
Reflexion> Shinn etÂ al. (
[> 2023
](#bib.bib133)> )
, leaf, text width=44.6em
]
]
[
DistillationÂ (Â§[5.3](#S5.SS3))
[
E.g.,Â 
STaR> Zelikman etÂ al. (
[> 2022
](#bib.bib186)> )
,
SCoTD> Li etÂ al. (
[> 2023b
](#bib.bib77)> )
,
SCOTT> Wang etÂ al. (
[> 2023h
](#bib.bib155)> )
, leaf, text width=44.6em
]
]
]
[
Future
Directions Â (Â§[6](#S6))
[
Multi-modalÂ (Â§[6.1](#S6.SS1))
[
E.g.,Â 
MMCoT> Zhang etÂ al. (
[> 2023g
](#bib.bib197)> )
,
T-SciQ> Wang etÂ al. (
[> 2023d
](#bib.bib151)> )
,
ToMT> Hu etÂ al. (
[> 2023b
](#bib.bib47)> )
, leaf, text width=44.6em
]
]
[
FaithfulnessÂ (Â§[6.2](#S6.SS2))
[
E.g.,Â 
Rethinking and Retrievaling> He etÂ al. (
[> 2023a
](#bib.bib40)> )
,
Measure Faithful> Lanham etÂ al. (
[> 2023
](#bib.bib69)> )
, leaf, text width=44.6em
]
]
[
CoT TheoryÂ (Â§[6.3](#S6.SS3))
[
E.g.,Â 
Tang> Tang etÂ al. (
[> 2023
](#bib.bib142)> )
,
Li> Li etÂ al. (
[> 2023e
](#bib.bib83)> )
,
Feng> Feng etÂ al. (
[> 2023
](#bib.bib29)> )
,
Wu> Wu etÂ al. (
[> 2023
](#bib.bib164)> )
, leaf, text width=44.6em
]
]
]
[
BenchmarksÂ (Â§[3](#S3))
[
Mathematical
ReasoningÂ (Â§[3.1](#S3.SS1))
[
E.g.,Â 
MultiArith> Roy and Roth (
[> 2015
](#bib.bib123)> )
,
AQUA-RAT> Ling etÂ al. (
[> 2017
](#bib.bib84)> )
,
SVAMP> Patel etÂ al. (
[> 2021
](#bib.bib114)> )
,
GSM8K> Cobbe etÂ al. (
[> 2021
](#bib.bib17)> )
,
DROP> Dua etÂ al. (
[> 2019
](#bib.bib27)> )
,
LILA> Mishra etÂ al. (
[> 2022a
](#bib.bib103)> )
, leaf, text width=44.6em
]
]
[
Commonsense
ReasoningÂ (Â§[3.2](#S3.SS2))
[
E.g.,Â 
CSQA> Talmor etÂ al. (
[> 2019
](#bib.bib140)> )
,
ARC> Bhakthavatsalam etÂ al. (
[> 2021
](#bib.bib6)> )
,
PIQA> Bisk etÂ al. (
[> 2020
](#bib.bib8)> )
,
McTaco> Zhou etÂ al. (
[> 2019
](#bib.bib204)> )
,
CosmosQA> Huang etÂ al. (
[> 2019
](#bib.bib50)> )
,
StrategyQA> Geva etÂ al. (
[> 2021
](#bib.bib34)> )
, leaf, text width=44.6em
]
]
[
Symbolic
ReasoningÂ (Â§[3.3](#S3.SS3))
[
E.g.,Â 
Last Letter Concat.> Wei etÂ al. (
[> 2022b
](#bib.bib161)> )
,
Coin Flip> Wei etÂ al. (
[> 2022b
](#bib.bib161)> )
,
Reverse List> Wei etÂ al. (
[> 2022b
](#bib.bib161)> )
BigBench> Srivastava etÂ al. (
[> 2022
](#bib.bib136)> )
,
BigBench-Hard> Suzgun etÂ al. (
[> 2023
](#bib.bib138)> )
, leaf, text width=44.6em
]
]
[
Logical
ReasoningÂ (Â§[3.4](#S3.SS4))
[
E.g.,Â 
ReClor> Yu etÂ al. (
[> 2020
](#bib.bib182)> )
,
LogiQA> Liu etÂ al. (
[> 2020
](#bib.bib88)> )
,
ProofWriter> Tafjord etÂ al. (
[> 2021
](#bib.bib139)> )
,
FOLIO> Han etÂ al. (
[> 2022
](#bib.bib38)> )
,
PrOntoQA> Saparov and He (
[> 2023
](#bib.bib125)> )
, leaf, text width=44.6em
]
]
[
Multi-modal
ReasoningÂ (Â§[3.5](#S3.SS5))
[
Image:
ScienceQA> Lu etÂ al. (
[> 2022
](#bib.bib91)> )
,
VCR> Zellers etÂ al. (
[> 2019
](#bib.bib187)> )
,
VisualCOMET> Park etÂ al. (
[> 2020
](#bib.bib113)> )
Video:
CLEVRER> Yi etÂ al. (
[> 2020
](#bib.bib178)> )
,
STAR> Wu etÂ al. (
[> 2021
](#bib.bib163)> )
,
NExT-QA> Xiao etÂ al. (
[> 2021
](#bib.bib166)> )
, leaf, text width=44.6em
]
]
]
]
Figure 1:XoT Methods, Frontier Application, Future Direction, and Benchmarks.
### 4.1Construction Approach
After thorough analysis, we divide the construction of X-of-thought into three categories: 1) Manual XoT, 2) Automatic XoT, and 3) Semi-automatic XoT, described as follows.
#### 4.1.1Manual XoT
While large language models perform few-shot in-context learning via prompting, they are still limited in reasoning tasks.
In order to explore the potential reasoning ability of large language models, one standard approach is to provide different forms of thoughts in demonstrations.
> Wei etÂ al. (
[> 2022b
](#bib.bib161)> )
first propose chain-of-thought prompting (Few-shot CoT) by manually providing natural language form rationales to the demonstrations.
To further ensure certainty in the reasoning process and reduce inconsistencies between reasoning path and answers, PAL> (Gao etÂ al., [> 2023
](#bib.bib32)> )
, PoT> (Chen etÂ al., [> 2022a
](#bib.bib12)> )
and NLEP> (Zhang etÂ al., [> 2023e
](#bib.bib194)> )
leverage programming language as annotated rationales, which transforms the problem-solving into an executable Python program.
Meanwhile, to take both advantages of natural language and programming language and raise the confidence of reasoning output, MathPrompter> (Imani etÂ al., [> 2023
](#bib.bib53)> )
uses the zero-shot chain-of-thought prompting to generate multiple algebraic expressions or Python functions, which can verify each other and improve the reliability of results.
Furthermore, since the reasoning complexity of samples in demonstrations, such as chains with more reasoning steps, results in performance improvement,> Fu etÂ al. (
[> 2023a
](#bib.bib30)> )
proposes complexity-based prompting, where voting among high-complexity rationales is performed to get the final answer.
Manually constructed X-of-thought methods expand on in-context learning by adding different types of step-by-step intermediate reasoning processes to demonstrations. They allow LLMs to mimic and generate reasoning paths.
Although manual XoT methods provide greater interpretability as well as trustworthiness for human understanding and outperform on complex tasks, i.e., mathematical reasoning, commonsense reasoning, symbolic reasoning, etc., manual annotating of rationales entails significant costs and suffers from drawbacks such as difficulty in demonstration selection and task generalization.
Specifically, different tasks require different ways of demonstrations. Therefore, other works attempt to construct the reasoning path automatically, as discussed in Â Â§[4.1.2](#S4.SS1.SSS2).
#### 4.1.2Automatic XoT
Chain-of-thought prompting> (Wei etÂ al., [> 2022b
](#bib.bib161)> )
elicits the complex reasoning ability of LLMs with task-specific exemplars in a few-shot setting, which limits the scalability and generalization.
To reduce the cost of hand-crafted few-shot exemplars,> Kojima etÂ al. (
[> 2022
](#bib.bib64)> )
proposes zero-shot CoT by introducing a magic phraseLetâ€™s think step by stepafter question, which enables LLMs to generate reasoning chains in a zero-shot manner.
However, zero-shot CoT suffers from poor-quality reasoning paths, coming with many mistakes.
Since the diversity of demonstration plays a vital role in reasoning chains generation, Auto-CoT> (Zhang etÂ al., [> 2023f
](#bib.bib196)> )
generates the demonstrations automatically via clustering and representative exemplars selection, which improves the diversity and consistently matches or exceeds the performance of Few-shot CoT.
COSP> (Wan etÂ al., [> 2023
](#bib.bib145)> )
introduces the outcome entropy of the question to aid demonstration selection.> Xu etÂ al. (
[> 2023
](#bib.bib167)> )
proposes Reprompting to find the effective CoT prompt by employing Gibbs sampling iteratively.
Meanwhile, some mistakes in reasoning chains come from missing-step errors,> Wang etÂ al. (
[> 2023f
](#bib.bib153)> )
extend the zero-shot CoT into Plan-and-Solve (PS) Prompting via devising a plan to divide the entire task into smaller sub-tasks and carrying out the sub-tasks according to the plan with more detailed instructions.
LogiCoT> (Zhao etÂ al., [> 2023c
](#bib.bib201)> )
uses symbolic logic to validate the zero-shot reasoning process, thus reducing errors in reasoning.
Besides, PoT> (Chen etÂ al., [> 2022a
](#bib.bib12)> )
also explore language models, such as Codex, to generate an executable Python program to solve math problems in zero-shot setting via addingLetâ€™s write a Python program step by stepâ€¦, which mitigates errors in intermediate reasoning steps.
Some work introduces agents to solve reasoning problems. For example, Agent Instruct> (Crispino etÂ al., [> 2023a
](#bib.bib18)> )
utilizes agents to generate task-related, informative instructions, which guides LLMs to perform zero-shot reasoning.
Unlike manual XoT, automatic XoT, using zero-shot prompt engineering or sampling, is scalable and can be generalized between domains without human intervention. However, due to the lack of human alignment, automatically generated chain-of-thought encounters challenges such as poor quality, hallucinations, and factual inconsistencies. Therefore, constructing XoT in a semi-automatic way is necessary, which is introduced in Â Â§[4.1.3](#S4.SS1.SSS3).
#### 4.1.3Semi-automatic XoT
Semi-automatic XoT methods integrate the advantages of both manual and automatic construction methods.> Shao etÂ al. (
[> 2023
](#bib.bib130)> )
proposes Synthetic Prompting, which leverages a few human-annotated examples to prompt models to generate more examples through an alternated forward-backward process and selects effective demonstrations to elicit better reasoning, alleviating the lack of human alignment in AutoCoT.
Although previous work solves the problem of manual annotating, demonstration selection can also significantly affect performance.
Automate-CoT> (Shum etÂ al., [> 2023
](#bib.bib135)> )
employs reinforcement learning with a variance-reduced policy gradient strategy to estimate the significance of each example in a black-box language model, eliciting better demonstration selection.
Similarly,> Lu etÂ al. (
[> 2023b
](#bib.bib92)> )
proposes PromptPG, which utilizes policy gradient to learn to select demonstrations in tabular reasoning.> Ye and Durrett (
[> 2023
](#bib.bib176)> )
initially uses two proxy metrics to evaluate each example and then searches over examples to find demonstrations that yield the best performance in a silver-labeled development set.
Meanwhile,> Pitis etÂ al. (
[> 2023
](#bib.bib117)> )
proposes Boosted Prompting, a prompt ensembling way to improve the performance, which iteratively expands the examples when encountering the problem that the current demonstration is challenging to handle.> Zou etÂ al. (
[> 2023
](#bib.bib208)> )
introduce Meta-CoT, which automatically selects demonstrations based on the question category, eliminating the need for the task-specific prompt design.
![Refer to caption](https://ar5iv.labs.arxiv.org/html/2309.15402/assets/x1.png)Figure 2:The evolution of reasoning, from direct I/O to chain structure, then to tree and graph structure.
The semi-automatic XoT methods reduce the workload of manual labeling while introducing human alignment signals and demonstration selection strategies to enhance the capability and stability of reasoning.
Additionally, it enables cost-effective domain generalization.
However, the demonstration selection problem has not been entirely resolved and requires more effort and research.
### 4.2XoT Structural Variants
The most primitive chain-of-thought is a chain structure that describes intermediate reasoning steps in natural language.
In this section, we introduce structural variants that modify the original chain structure, including chain structure variants, tree structure variants, and graph structure variants.
##### Chain Structure
PAL> (Gao etÂ al., [> 2023
](#bib.bib32)> )
and PoT> (Chen etÂ al., [> 2022a
](#bib.bib12)> )
introduce programming languages to describe the reasoning process, thereby converting the reasoning problem into the implementation of an executable program to obtain the final answer.
Since the program execution is deterministic and performs arithmetic computations accurately, this approach shows excellent performance in mathematical reasoning.
Besides, symbol sequence is another type of thought representation.
Chain-of-Symbol> (Hu etÂ al., [> 2023a
](#bib.bib46)> )
represents the complex environments with condensed symbolic chain representations during planning, which reduces the complexity of the simulation environment.
Chain structure variants are shown in Figure[2](#S4.F2)(c,d)
Algorithm of Thought> (Sel etÂ al., [> 2023
](#bib.bib129)> )
injects algorithmic capabilities into the model, making the modelâ€™s reasoning more logical by adding examples based on algorithms.
Its absence of the huge search space of tree search> (Long, [> 2023
](#bib.bib89)> ; Yao etÂ al., [> 2023b
](#bib.bib172)> )
saves computational resources and achieves excellent performance.
##### Tree Structure
The original chain structure inherently limits the scope of exploration.
Through the incorporation of tree structures and tree search algorithms, models gain the capability to efficiently explore and backtrack during the reasoning process> (Long, [> 2023
](#bib.bib89)> ; Yao etÂ al., [> 2023b
](#bib.bib172)> )
, as shown in Figure[2](#S4.F2)(e).
Combined with self-assessment of intermediate thoughts, models can achieve global optimum solutions.
The reasoning process of ToT involves uncertainty, which can potentially lead to cascading errors. TouT> (Mo and Xin, [> 2023
](#bib.bib105)> )
introduces Monte Carlo Dropout in reasoning, taking into account the uncertainty.> Yu etÂ al. (
[> 2023b
](#bib.bib181)> )
delves into analogous problems, harnessing their solutions to elevate the intricate reasoning abilities of LLMs.
These analogous problems exhibit a tree-like structure, ultimately converging to solve the main problem.
However, the current tree-of-thought has considerable limitations on task selection and requires specific prompt designing for each task, which hinders its widespread application.
SoT> (Ning etÂ al., [> 2023
](#bib.bib107)> )
is another variant of the tree structure, which decomposes a problem into subproblems that can be processed in parallel and solved simultaneously to speed up reasoning.
However, its utility is restricted to parallel decomposable problems and is not suited for complex reasoning tasks.
##### Graph Structure
Compared to trees, graphs introduce loops and rings, which bring more complex topological relationships and allow for modeling more complex reasoning, as shown in Figure[2](#S4.F2)(f).
GoT> (Besta etÂ al., [> 2023
](#bib.bib5)> ; Lei etÂ al., [> 2023a
](#bib.bib71)> )
regards intermediate thought as nodes within a graph, combining exploration and backtracking operations, and additionally introduces aggregation and refinement operations compared to tree-of-thought.
The additional operations, aggregation and refinement elicit better reasoning in complex tasks.
Nevertheless, it faces the same dilemmas as the tree-of-thought, i.e., task limitations and poor generalizability.
Besides, it has increased reasoning costs.
Unlike GoT, which explicitly constructs a thought graph, ResPrompt> (Jiang etÂ al., [> 2023a
](#bib.bib55)> )
introduces residual connections between thoughts in the prompt text, allowing the reasoning of different steps to interact with each other.
As models transition from linear chains to hierarchical trees and intricate graphs, the interplay of thoughts becomes progressively more complex, thereby gradually enhancing the capacity to address intricate problems.
However, as the complexity of the topology increases, associated methods impose more constraints on task selection, leading to a significant reduction in their generalizability and making their application difficult.
Extending complex topology structure-based methods to general domains is a major challenge for future research.
### 4.3XoT Enhancement Methods
In this section, we present the XoT enhancement methods.
In total, we will provide an overview of five categories, which are
adding verification and refinementÂ (Â§[4.3.1](#S4.SS3.SSS1)),
question decompositionÂ (Â§[4.3.2](#S4.SS3.SSS2)),
leveraging external knowledgeÂ (Â§[4.3.3](#S4.SS3.SSS3)),
voting and rankingÂ (Â§[4.3.4](#S4.SS3.SSS4)),
and improving efficiencyÂ (Â§[4.3.5](#S4.SS3.SSS5)).
![Refer to caption](https://ar5iv.labs.arxiv.org/html/2309.15402/assets/x2.png)Figure 3:Verification and refinement reduce cascading errors in reasoning.
#### 4.3.1Verify and Refine
Chain-of-thought reasoning often tends to be hallucinatory, producing incorrect reasoning steps. Errors in intermediate reasoning steps can, in turn, trigger a cascade of errors.
Incorporating verification to obtain feedback and subsequently refining the reasoning process based on this feedback can be a highly effective strategy for mitigating this phenomenon, which is similar to the process of human reflection.
Figure[3](#S4.F3)depicts the overview of verification and refinement.
VerifyCoT> (Ling etÂ al., [> 2023
](#bib.bib85)> )
devises a Natural Program, a deductive reasoning form, which allows models to produce accurate reasoning steps, with each subsequent step strictly based on the previous steps.DiVeRSe> (Li etÂ al., [> 2022c
](#bib.bib82)> )
utilizes a voting mechanism to eliminate incorrect answers, followed by a fine-grained verification of each reasoning step independently.
SCREWS> (Shridhar etÂ al., [> 2023
](#bib.bib134)> )
thinks that the post-modification result may not necessarily be superior to the origin, so it introduces a selection module to select a better result between the origin and modification.
To facilitate knowledge-intensive tasks, Verify-and-Edit> (Zhao etÂ al., [> 2023a
](#bib.bib198)> )
incorporates external knowledge to re-reason uncertain examples, reducing factual mistakes in reasoning.
Some research efforts attempt to unearth the internal knowledge of models.
Some research efforts attempt to unearth the internal knowledge of models.
To address factual errors, some research attempts to unearth the intrinsic knowledge of LLMs.
They acquire knowledge from the model before answering the questions> (Dhuliawala etÂ al., [> 2023
](#bib.bib22)> ; Zheng etÂ al., [> 2023
](#bib.bib202)> )
.> Ji etÂ al. (
[> 2023
](#bib.bib54)> )
further verifies the correctness of intrinsic knowledge, and> Liu etÂ al. (
[> 2023b
](#bib.bib87)> )
enhances the accuracy of intrinsic knowledge acquisition through reinforcement learning.
Inconsistency is another major challenge in reasoning,> Dua etÂ al. (
[> 2022
](#bib.bib26)> )
iteratively uses previous reasoning results as prompts until the model gives a consistent answer.> Paul etÂ al. (
[> 2023
](#bib.bib115)> )
trains a critic model to provide structured feedback on the reasoning process.
Self-Refine> (Madaan etÂ al., [> 2023
](#bib.bib95)> )
performs iterative self-feedback and refinement to alleviate errors in reasoning.
Compared with Self-Refine, Reflexion> (Shinn etÂ al., [> 2023
](#bib.bib133)> )
introduces reinforcement learning for reflection, which additionally brings decision-making capability.
Meanwhile, some work introduces backward reasoning> (Yu etÂ al., [> 2023a
](#bib.bib180)> )
for verification.
RCoT> (Xue etÂ al., [> 2023
](#bib.bib168)> )
reconstructs the question according to the reasoning chains, and its inconsistency with the original question exposes errors in the reasoning process.
FOBAR> (Jiang etÂ al., [> 2023b
](#bib.bib56)> )
and Self Verification> (Weng etÂ al., [> 2022
](#bib.bib162)> )
perform verification by deducing the conditions in the question from the answer.
FOBAR infers the variables in the question, and Self Verification infers the conditions in the question.
However,> Huang etÂ al. (
[> 2023a
](#bib.bib49)> )
finds that LLMs struggle to self-correct without external feedback, and it could even lead to a performance decline.
LLM reasoning is an unsupervised process in which feedback signals from intermediate reasoning steps play a crucial role in improving reasoning.
Guidance from feedback signals can effectively reduce the hallucination phenomena in reasoning.
There is still significant research space for obtaining appropriate feedback and making accurate corrections based on that feedback.
#### 4.3.2Question Decomposition
The essence of X-of-thought reasoning lies in its step-by-step problem-solving.
However, the original chain-of-thought reasoning approach does not explicitly strip out the step-by-step reasoning process and still uses one-stage generation.
In this section, we discuss the question decomposition approach, which explicitly solves questions step-by-step. The overview is shown in Figure[4](#S4.F4).
![Refer to caption](https://ar5iv.labs.arxiv.org/html/2309.15402/assets/x3.png)Figure 4:Question decomposition solves complex questions progressively by solving simple sub-questions.
> Wang etÂ al. (
[> 2022a
](#bib.bib146)> )
iteratively acquires knowledge from the model, making progress in multi-hop QA.> Zhou etÂ al. (
[> 2023b
](#bib.bib205)> )
proposes Least-to-Most Prompting, which initially breaks down the question into sub-questions in a top-down fashion,
and subsequently, it solves a sub-question once at a time and leverages their solutions to facilitate subsequent sub-questions.
Successive Prompting> (Dua etÂ al., [> 2022
](#bib.bib26)> )
takes a similar approach to Least-to-Most Prompting, and the difference is that it takes a decomposition with interleaved sub-questions and answers rather than two-stage decomposition.
The above methods do not formulate tailored solutions for various sub-problems.
Decomposed Prompting> (Khot etÂ al., [> 2023
](#bib.bib63)> )
designs a modular shared library, each dedicated to a class of subproblems,
which can tailor more effective solutions to different classes of sub-problems.
Apart from general tasks, some works focus on question decomposition on tabular reasoning.
BINDER> (Cheng etÂ al., [> 2023
](#bib.bib16)> )
maps reasoning to a program in a neural-symbolic manner and obtains the final answer through a program executor such as Python or SQL.> Ye etÂ al. (
[> 2023
](#bib.bib177)> )
introduces DATER, which breaks down large tables into smaller ones and complex questions into simpler ones.
The former reduces irrelevant information, while the latter reduces the complexity of reasoning.
Providing direct answers to complex questions can be challenging.
By decomposing the question into simple sub-questions and solving them step-by-step, the difficulty is reduced.
Moreover, each sub-question can be traced back to a specific reasoning step, making the reasoning process more transparent and explainable.
Current work mostly uses top-down decomposition strategies, while bottom-up decomposition strategies based on backward reasoning remain to be explored in future work.
#### 4.3.3External Knowledge
![Refer to caption](https://ar5iv.labs.arxiv.org/html/2309.15402/assets/x4.png)Figure 5:Introducing external knowledge reduces factual errors in reasoning.
The parameterized knowledge within models is limited and outdated. Thus, factual mistakes often occur when facing knowledge-intensive tasks.
Introducing external knowledge can mitigate this phenomenon, as shown in Figure[5](#S4.F5).
> Lu etÂ al. (
[> 2023a
](#bib.bib90)> )
introduces multilingual dictionaries in prompts to enhance machine translation.> Li etÂ al. (
[> 2023d
](#bib.bib81)> )
proposes chain-of-knowledge (CoK-Li), which obtains structured knowledge from a knowledge base via a query generator to perform knowledge-guided reasoning.> Wang etÂ al. (
[> 2023b
](#bib.bib149)> )
(CoK-Wang) also retrieves structured knowledge from KB.
Moreover, it estimates the reasoning chains in terms of factuality and faithfulness and prompts models to rethink unreliable reasonings,
which mitigates the knowledge retrieval errors in CoK-Li.
KD-CoT> (Wang etÂ al., [> 2023c
](#bib.bib150)> )
addresses factual reasoning problems through a multi-turn QA approach.
They design a feedback-augmented retriever for retrieving relevant external knowledge in each round of QA to calibrate the reasoning process.
Other studies use the modelâ€™s own memory as external knowledge.
For example, Memory-of-Thought> (Li and Qiu, [> 2023
](#bib.bib80)> )
first performs pre-thinking to save the high-confidence thoughts into external memory, and during inference, it lets the LLM recall relevant memory to aid reasoning.
The parameterized knowledge in the model is fixed at the end of the pre-training, which leads to its shortcomings in terms of knowledge capacity and knowledge updating.
While introducing external knowledge can alleviate this to some extent, it remains an imperfect solution.
To fundamentally tackle this issue, continual learning> (Lange etÂ al., [> 2022
](#bib.bib68)> ; Wang etÂ al., [> 2023g
](#bib.bib154)> )
stands as a promising avenue for future research endeavors.
#### 4.3.4Vote and Rank
![Refer to caption](https://ar5iv.labs.arxiv.org/html/2309.15402/assets/x5.png)Figure 6:Voting and ranking reduce inconsistency by selecting final answers from multiple samplings.
Owing to the inherent stochasticity in the generation process, LLM reasoning exhibits an element of randomness and uncertainty.
This problem can be effectively alleviated through multiple sampling strategies, as shown in Figure[6](#S4.F6).
Some methods adopt ranking, such as> (Cobbe etÂ al., [> 2021
](#bib.bib17)> )
, which trains a verifier to select high-confidence reasoning chains through ranking.
Meanwhile, other methods select reasoning chains through a voting mechanism.
Self-consistency> (Wang etÂ al., [> 2023j
](#bib.bib158)> )
selects the most consistent answer by majority voting among sampled reasoning chains based on final answers.
Furthermore,> (Fu etÂ al., [> 2023a
](#bib.bib30)> )
proposes Complex CoT, which utilizes a complexity-based voting strategy that leans towards selecting answers generated by more complex reasoning chains.
However, answer-based voting mechanisms do not take into account the correctness of reasoning chains.> Miao etÂ al. (
[> 2023
](#bib.bib100)> )
takes the reasoning steps into account when voting, which can obtain both consistent answers and trustworthy reasoning processes simultaneously.
Moreover, to consider the relations between intermediate steps across chains,> Yoran etÂ al. (
[> 2023
](#bib.bib179)> )
mixes information between reasoning chains and selects the most relevant facts to perform meta-reason over multiple reasoning chains.
GRACE> (Khalifa etÂ al., [> 2023
](#bib.bib62)> )
trains a discriminator through contrastive learning and uses this discriminator to rank each intermediate reasoning step.
Previous methods sample based on the probability distribution, while Diversity-of-Thought> (Naik etÂ al., [> 2023
](#bib.bib106)> )
obtains multiple reasoning paths by prompting with different instructions.
Drawing inspiration from ensemble learning, the practice of voting and ranking following with multiple sampling serves to diminish uncertainty.
Furthermore, it has showcased substantial performance improvements compared to the single-sample approach.
Multiple sampling with voting has become a common technique in current X-of-thought studies.
Integrating reasoning chains into voting remains a significant area of research for the future.
#### 4.3.5Efficiency
LLM reasoning and manually annotated reasoning chains impose expensive overheads.> Aggarwal etÂ al. (
[> 2023
](#bib.bib1)> )
improves self-consistency by dynamically adjusting the number of samples, which can significantly reduce inference costs with marginal performance degradation.> Ning etÂ al. (
[> 2023
](#bib.bib107)> )
decomposed the questions in parallel and handled them simultaneously, reducing the reasoning time overhead. But it cannot handle complex questions.> Zhang etÂ al. (
[> 2023b
](#bib.bib190)> )
accelerates the reasoning by selectively skipping some intermediate layers and then verifies the draft in another forward pass.> Diao etÂ al. (
[> 2023
](#bib.bib23)> )
borrows ideas from active learning to annotate examples with high uncertainty, reducing the human annotating cost.
Large-scale language models have showcased immense capabilities, but they also come with substantial overhead. Balancing the trade-off between performance and overhead may require significant attention in future research endeavors.
## 5Frontier Application
### 5.1Tool Use
Despite the extensive knowledge exhibited by LLMs, it is accompanied by several challenges. These encompass the incapacity to access up-to-the-minute news, proclivity towards hallucinations when responding to queries involving out-of-domain knowledge, and the absence of sophisticated reasoning capacities like mathematical calculations or symbolic reasoning. By granting LLMs the ability to employ external tools, it becomes possible to augment the modelâ€™s reasoning capabilities and assimilate external knowledge, enabling it to engage in information retrieval and environmental interaction.
MRKL> (Karpas etÂ al., [> 2022
](#bib.bib60)> )
introduces a novel framework comprising scalable modules (referred to as experts) and a router. These experts can take the form of neural networks or symbols. However, this study primarily focuses on conceptualization and training an LLM specifically for mathematical computation while not delving into implementing other module contents. TALM> (Parisi etÂ al., [> 2022a
](#bib.bib111)> )
and Toolformer> (Schick etÂ al., [> 2023
](#bib.bib128)> )
integrate a text-centric methodology with supplementary tools to enhance the capabilities of language models. They employ a self-supervise mechanism to initiate performance enhancements, commencing with a limited set of tooltips. In a similar vein, HuggingGPT> (Shen etÂ al., [> 2023
](#bib.bib131)> )
leverages visual and speech models to process information from diverse modalities, thereby endowing LLMs with the capacity for multi-modal understanding and generation. Another question is how to select the appropriate tool.
LATM> (Cai etÂ al., [> 2023
](#bib.bib10)> )
enables the tool-making ability of LLMs to make generalized API across different tasks, and GEAR> (Lu etÂ al., [> 2023c
](#bib.bib93)> )
considers the efficiency of tool-using by using smaller models to delegate tool grounding and execution.
However, converting a user request into API format is often not straightforward. The existing approaches mentioned above have limitations in facilitating multiple invocations of the tool and rectifying query errors. To tackle this problem, ReAct> (Yao etÂ al., [> 2023c
](#bib.bib173)> )
integrates the strengths of reasoning and action to enhance and complement each other, augmenting problem-solving capability mutually. ART> (Paranjape etÂ al., [> 2023
](#bib.bib110)> )
uses a task library to select relevant tool usage and reasoning chains. MM-REACT> (Yang etÂ al., [> 2023
](#bib.bib169)> )
further utilizes vision experts to enable multi-modal reasoning and action.
The aforementioned research endeavors focus on designing tools (or APIs) to enhance the capabilities of LLMs in various domains.
Combining XoT with tools effectively addresses the challenges faced by LLMs.
X-of-thought reasoning enables models to effectively elicit, track, and update action plans while managing exceptions.
Simultaneously, action operations facilitate the modelâ€™s interaction with external sources, such as knowledge bases and environments, enabling it to gather additional information.
To assess the proficiency of tools, API-Bank> (Li etÂ al., [> 2023c
](#bib.bib78)> )
and MetaTool> (Huang etÂ al., [> 2023c
](#bib.bib52)> )
introduce comprehensive benchmarks, providing a robust foundation to evaluate the performance and effectiveness of tool-augmented LLMs.
### 5.2Planning
LLMs face challenges in providing accurate responses directly for intricate problems, necessitating the need to decompose them into sequential steps and sub-tasks. While CoT offers a straightforward approach to planning, it falls short in addressing highly complex problems and lacks the ability to evaluate and rectify errors through backtracking.
Numerous studies have extended the framework of chain-of-thought to various formats to enhance the capacity for planning further. Tree-of-Thought> (Yao etÂ al., [> 2023b
](#bib.bib172)> )
enables LLMs to consider multiple reasoning paths in a tree and self-evaluate to determine the next course of action.
In cases where global decisions are necessary, ToT allows forward or backward exploration through techniques like deep-first search or breadth-first search.
Reasoning via Planning (RAP)> (Hao etÂ al., [> 2023
](#bib.bib39)> )
also divides the problem into a tree and explores them by Monto Carlo tree search algorithm, using LLMs as both world-model and reasoning agent.
Another method, Graph of Thought (GoT)> (Yao etÂ al., [> 2023d
](#bib.bib174)> )
, employs graph nodes to represent individual thoughts and external Graph Neural Networks for organization.
LLM+P> (Liu etÂ al., [> 2023a
](#bib.bib86)> )
and LLM+DP> (Dagan etÂ al., [> 2023
](#bib.bib20)> )
facilitate the generation of Planning Domain Definition Language (PDDL)> (Gerevini, [> 2020
](#bib.bib33)> )
by LLMs.
PDDL assists in decomposing complex problems and utilizing specialized models for planning before converting the results into natural language for LLM processing.
However, it is essential to note that these methods use tree/graph/PDDL nodes to represent thoughts, which have limitations regarding their representation forms and can only handle specific planning problems.
Another technique is to improve the modelâ€™s ability to correct errors and summarize historical experience.
Self-Refine> (Madaan etÂ al., [> 2023
](#bib.bib95)> )
employs a unique approach where the output generated by the model is evaluated and provided with feedback using the same model.
Reflexion> (Shinn etÂ al., [> 2023
](#bib.bib133)> )
enables the model to reflect on and rectify errors made in previous actions, resembles reinforcement learning in textual format, and involves dividing memory into long and short-term components. However, Reflexion cannot update the plan when an out-of-plan error occurs.
AdaPlanner> (Sun etÂ al., [> 2023
](#bib.bib137)> )
introduces adaptive closed-loop plan refinement, which iterative refines the task plan based on the feedback of the environment.
ISR-LLM> (Zhou etÂ al., [> 2023c
](#bib.bib206)> )
combines Self-Refine with PDDL to achieve a better success rate in long-horizon sequential tasks. Meanwhile, LATS> (Zhou etÂ al., [> 2023a
](#bib.bib203)> )
utilizes LM-based Monte Carlo Tree Search for a more flexible planning procedure.
Planning can be flexibly combined with tools> (Ruan etÂ al., [> 2023
](#bib.bib124)> )
or agents> (Crispino etÂ al., [> 2023b
](#bib.bib19)> )
to enrich reasoning ability. ToRA> (Gou etÂ al., [> 2023
](#bib.bib35)> )
designs mathematical specialized agents with external tools, and
AutoUI> (Zhang and Zhang, [> 2023
](#bib.bib195)> )
directly interacts with the multi-modal environment instead of converting visual inputs into text, which enhances the reasoning efficiency and reduces error propagation.
Planning augmented approaches have advanced conventional sequential planning by introducing search-based, graph-based, and definition language-based methods.
On the other hand, some methods incorporate action, planning, reflection, or tools, aiming to enhance LLMsâ€™ long-term planning and error resilience capabilities.
### 5.3CoT Distillation
LLM can be self-improved by distilling reasoning steps to solve complex problems.> Huang etÂ al. (
[> 2022
](#bib.bib48)> )
employs an LLM with self-consistency to generate reasoning chains from unlabeled data.
These chains are subsequently utilized to fine-tune the model, enhancing its generalized reasoning capabilities.> Zelikman etÂ al. (
[> 2022
](#bib.bib186)> )
proposes STaR, a few-shot learning approach to improve LMâ€™s reasoning capabilities using a self-loop bootstrap strategy.
SECToR> (Zhang and Parkes, [> 2023
](#bib.bib189)> )
uses chain-of-thought to obtain arithmetic answers, then fine-tune the model to generate the answer without CoT directly.
Thought CoT is an emerging ability primarily observed in LLMs, with limited advancements in small models. However, enhancing small modelsâ€™ CoT ability is conceivable through techniques like distillation.> Magister etÂ al. (
[> 2023
](#bib.bib98)> )
demonstrates that fine-tuning T5 with reasoning chains generated by larger teacher models and utilizing an external calculator for answer resolution can substantially enhance task performance across diverse datasets.> Ho etÂ al. (
[> 2023
](#bib.bib43)> )
generates and filters multiple reasoning paths to enrich the diversity.
Numerous endeavors can be undertaken to reduce human costs using unannotated (or very few annotated) data by utilizing the self-consistency> (Wang etÂ al., [> 2023j
](#bib.bib158)> )
.> Hsieh etÂ al. (
[> 2023
](#bib.bib45)> )
employs prompts to generate answers from much fewer labeled/unlabeled data, followed by the generation of rationales that prompt the language model to provide reasoning for the given answer.
SCoTD> (Li etÂ al., [> 2023b
](#bib.bib77)> )
finds that sampling multiple reasoning chains per instance from teachers is paramount for improving the capability of students.
SCOTT> (Wang etÂ al., [> 2023h
](#bib.bib155)> )
utilizes contrastive decoding> (Li etÂ al., [> 2022b
](#bib.bib79)> ; Oâ€™Brien and Lewis, [> 2023
](#bib.bib108)> )
during rationale generation for teacher models.
Furthermore, to tackle the shortcut problem, it employs a counterfactual reasoning objective while training student models.
DialCoT> (Han etÂ al., [> 2023
](#bib.bib37)> )
decomposes reasoning steps into a multi-round dialog and selects the correct path using the PPO algorithm.> Jie etÂ al. (
[> 2023
](#bib.bib57)> ); Wang etÂ al. (
[> 2023i
](#bib.bib157)> )
add special tokens for mathematic problems. This high-level information improves the consistency of reasoning steps.
The studies above adopt a shared paradigm wherein reasoning chains are generated through LLMs possessing superior reasoning capabilities.
These reasoning chains are then distilled into smaller models.
The effectiveness of the distillation process is improved by augmenting the sampling strategy from the larger model, for example, through the utilization of multiple sampling paths, consistency, or contrastive decoding, which leads to improved diversity and accuracy in the generated reasoning chains, ultimately benefiting the distillation process to smaller models.
Itâ€™s notable that language models have intricate tradeoffs and complex balances associated with multidimensional capabilities.> Fu etÂ al. (
[> 2023b
](#bib.bib31)> )
emphasizes that increasing task-specific chain-of-thought capabilities through distillation may also adversely impact the modelsâ€™ performance in solving generalized problems.
## 6Future Directions
While chain-of-thought reasoning has showcased remarkable performance on numerous tasks, some challenges still require further exploration. In this section, we provide a concise overview of three promising avenues for future research: multi-modal X-of-thought reasoningÂ (Â§[6.1](#S6.SS1)), faithful X-of-thought reasoningÂ (Â§[6.2](#S6.SS2)), and X-of-thought reasoning theoryÂ (Â§[6.3](#S6.SS3)).
### 6.1Multi-modal CoT
The shift from text unimodal to vision-text multi-modal introduces richer information, meanwhile bringing more challenges. Some works have attempted to explore X-of-thought reasoning in multi-modal scenarios by fine-tuning multi-modal models to generate a high-quality chain of thoughts.
Multimodal-CoT> (Zhang etÂ al., [> 2023g
](#bib.bib197)> )
firstly fine-tunes multi-modal models to generate chain-of-thoughts and then reasons over the rationales to obtain final answers.
However, it suffers from the limitation of the linearity of the reasoning process and has difficulties in interacting between different modalities.
To alleviate the challenges encountered by Multimodal-CoT,> (Yao etÂ al., [> 2023d
](#bib.bib174)> )
proposes Graph-of-Thought (GoT), which models the thought processes as a graph. It parses the reasoning chains into a thought graph, which enables a more realistic representation of thought processes by capturing non-sequential information interactions.
This measure breaks the limitations of linear structure through graphical structures and further improves performance.
Furthermore,> Yao etÂ al. (
[> 2023a
](#bib.bib171)> )
proposes Hypergraph-of-Thought (HoT), replacing thought graphs with hypergraphs, which enables models with better ability of high-order multi-hop reasoning and multi-modal comparative judgment.
Meanwhile, some work takes an approach based on knowledge distillation. T-SciQ> (Wang etÂ al., [> 2023d
](#bib.bib151)> )
generates high-quality CoT rationales from LLMs as fine-tuning signals and introduces a novel data mixing strategy to produce effective samples for different questions.
The aforementioned studies explore multi-modal reasoning in small models and fine-tuning scenarios, which we regard as an initial endeavor in the realm of multi-modal chain-of-thought reasoning. We believe that video multi-modal reasoning combined with in-context learning should be the focus of future research. On the one hand, videos introduce additional temporal information with innate chaining relationships compared with images. Through chain-of-thought reasoning, the information in different frames can be naturally connected to explicitly model the temporal relationship, which is well-suited for video multi-modal reasoning. On the other hand, small models are capacity-limited and need fine-tuning to gain chain-of-thought ability. Worse still, multi-modal reasoning chains are difficult to obtain, which further exacerbates the challenge. In comparison, contemporary vision-language foundation models (VLMs)> (Alayrac etÂ al., [> 2022
](#bib.bib2)> ; Li etÂ al., [> 2023a
](#bib.bib76)> ; Wang etÂ al., [> 2022b
](#bib.bib156)> ; Huang etÂ al., [> 2023b
](#bib.bib51)> ; Peng etÂ al., [> 2023
](#bib.bib116)> ; Yu etÂ al., [> 2021b
](#bib.bib184)> )
have strong vision-language comprehension and are already capable of in-context learning with interleaved text and images. They provide a solid foundation for chain-of-thought reasoning with in-context learning.
Utilizing chain-of-thought for video reasoning remains an unexplored territory with only a few studies. CoMT> (Hu etÂ al., [> 2023b
](#bib.bib47)> )
combines fast-thinking and slow-thinking in video reasoning and introduces a tree search strategy for planning, which firstly applies CoT in video multi-modal reasoning.
Although some works have started to utilize chain-of-thought reasoning and solve multi-modal reasoning tasks, previous works only focus on how to construct high-quality fine-tuned data, and there are still several challenges remaining:
* â€¢How to unify visual and language features to elicit better multi-modal understanding.
* â€¢How to use VLMs for chain-of-thought reasoning without fine-tuning.
* â€¢How to adapt image multi-modal reasoning into video multi-modal reasoning.
### 6.2Faithfulness
Extensive research indicates that chain-of-thought reasoning can lead to hallucination phenomena, such as factual mistakes and contextual inconsistencies.
Considering that language models fundamentally belong to statistical models, and due to factors such as data noise and knowledge forgetting, hallucination phenomena are unavoidable.
Some works focus on mitigating factual mistakes.> He etÂ al. (
[> 2023a
](#bib.bib40)> )
introduces external knowledge to evaluate reasoning chains and votes to filter out chains that contain factual mistakes but without correcting them> Wang etÂ al. (
[> 2023b
](#bib.bib149)> )
adopts a similar way, with the difference that it additionally introduces a reflection mechanism to correct low-scoring reasoning.> Zhao etÂ al. (
[> 2023a
](#bib.bib198)> )
filters out low-confidence reasoning by consistency and guides models to re-reasoning based on relevant external knowledge. While the aforementioned methods work well on knowledge-intensive tasks, they fall short in addressing the challenge of contextual inconsistencies.> Zhang etÂ al. (
[> 2023d
](#bib.bib192)> )
explores the hallucination snowballing phenomena during the reasoning process.
Others aim to address the inconsistency issues.> Radhakrishnan etÂ al. (
[> 2023
](#bib.bib120)> )
observes that models are more faithful when dealing with simple questions. Thus, it improves faithfulness through question decomposition.
Faithful CoT> (Lyu etÂ al., [> 2023
](#bib.bib94)> )
initially generates symbolic reasoning chains and later deterministically executes symbolic functions, mitigating reasoning inconsistencies.> Lanham etÂ al. (
[> 2023
](#bib.bib69)> )
explores the factors that influence faithfulness, which provides an empirical perspective. It finds faithfulness varies on different tasks and decreases as the model size increases.
CoNLI> (Lei etÂ al., [> 2023b
](#bib.bib72)> )
proposes a post-editing strategy to diminish the hallucinations.
SynTra> (Jones etÂ al., [> 2023
](#bib.bib59)> )
performs prefix-tuning on a synthetic dataset designed to elicit hallucination easily, and then transfers this capability to real tasks.
Despite numerous efforts aimed at addressing the hallucination issues in large language models, these works have only mitigated the problem to some extent. There is still a long way to fully enhance the faithfulness of large language models.
We summarize the future directions as follows:
* â€¢Improving the ability to recognize hallucination phenomena in the reasoning processes.
* â€¢Improving the accuracy of external knowledge retrieval and utilization to reduce factual mistakes.
* â€¢Improving the ability to recognize and correct contextual inconsistencies and logical mistakes, which is more challenging.
* â€¢How to fundamentally eliminate hallucination phenomena from alternative approaches, e.g. specific pre-training.
### 6.3CoT Theory
Despite the impressive capability of chain-of-thought reasoning, the ability to generate chain-of-thought following instructions still lacks a comprehensive explanation.
Some work addresses from an empirical perspective and can serve as a practical guide.> Madaan and Yazdanbakhsh (
[> 2022
](#bib.bib96)> )
decomposes prompts into three components: symbols, patterns, and text, exploring the impact of CoT through counterfactual prompting.> Wang etÂ al. (
[> 2023a
](#bib.bib147)> )
analyzes the impact of demonstration selection. They find that the correctness of reasoning chains has a negligible effect, while the relevance to the question and correct reasoning order matters.> Tang etÂ al. (
[> 2023
](#bib.bib142)> )
explores the role of semantics.
They find that chain-of-thought reasoning relies heavily on semantic knowledge introduced during pre-training and performs poorly in symbolic reasoning.
Others work analyze theoretically, exploring the underlying principles and internal mechanisms.> Li etÂ al. (
[> 2023e
](#bib.bib83)> )
deconstructs chain-of-thought reasoning as a multi-step combinatorial function. They demonstrate that chain-of-thought reduces the complexity of in-context learning to tackle complex questions.> Feng etÂ al. (
[> 2023
](#bib.bib29)> )
theoretically proves that a fixed-size Transformer is sufficient for computational tasks and dynamic planning with chain-of-thought.> Merrill and Sabharwal (
[> 2023
](#bib.bib99)> )
observes that chain-of-thought can boost reasoning ability, with the extent of improvement increasing as the number of intermediate reasoning steps grows.> Wu etÂ al. (
[> 2023
](#bib.bib164)> )
leverages gradient-based feature attribution methods to explore the impact of chain-of-thought on outputs.
The results indicate that chain-of-thought exhibits robustness to perturbations and variations in the question.
In addition, there are some claims suggesting that the chain-of-thought ability stems from code data during the pre-training phase> (Madaan etÂ al., [> 2022
](#bib.bib97)> ; Zhang etÂ al., [> 2023c
](#bib.bib191)> )
, but there is currently no systematic work to substantiate this opinion.
Current research on chain-of-thought theory is still in its preliminary exploration stage. We summarize future research directions as follows:
* â€¢Explore the sources of chain-of-thought ability to achieve targeted improvements in CoT reasoning.
* â€¢Theoretically analyzing the advantages of chain-of-thought over in-context learning and exploring the boundaries of its capabilities.
## 7Discussion
### 7.1Comparison of XoT Construction
There are three main ways of constructing an X-of-thought for existing methods: (1)Manuallabeling reasoning chains.
(2)Automaticgenerating reasoning chains by models.
(3)Semi-automaticgeneration with automatic expansion on a small number of manually labeled reasoning chains.
We observe that the manual construction methods> (Wei etÂ al., [> 2022b
](#bib.bib161)> ; Gao etÂ al., [> 2023
](#bib.bib32)> )
face similar challenges to in-context learning, i.e., demonstration selection, instruction formatting, etc> (Dong etÂ al., [> 2023
](#bib.bib24)> )
.
This causes numerous difficulties in its application and hinders the transfer ability across different tasks.
Automatic construction methods> (Zhang etÂ al., [> 2023f
](#bib.bib196)> ; Chen etÂ al., [> 2022a
](#bib.bib12)> ; Xu etÂ al., [> 2023
](#bib.bib167)> )
lack the guidance of high-quality annotations, resulting in performance deficiencies.
Benefiting from the signals brought by manual annotations, semi-automatic methods> (Shum etÂ al., [> 2023
](#bib.bib135)> ; Shao etÂ al., [> 2023
](#bib.bib130)> )
can generate high-quality reasoning chains through self-bootstrapping and similar techniques, effectively addressing the challenges faced by previous approaches.
While achieving excellent performance, it allows for easy transfer across different tasks.
### 7.2Comparison between Verification/Refinement and Planning
Numerous parallels exist between planning methods and verification/refinement-based methods, as both rely on feedback from intermediate processes to adjust and refine behavior.
The distinction lies in the fact that planning methods encompass decision-making, while verification/refinement-based methods solely address intermediate errors without delving into higher-level cognitive processes.
LLM reasoning processes are often hallucinatory, causing factual and logical mistakes.
Verify and edit based methods> (Ling etÂ al., [> 2023
](#bib.bib85)> ; Zhao etÂ al., [> 2023a
](#bib.bib198)> ; Madaan etÂ al., [> 2023
](#bib.bib95)> ; Shinn etÂ al., [> 2023
](#bib.bib133)> )
verify the correctness of the reasoning process and refine reasoning step that may cause hallucinatory.
Through verification and refinement, cascading errors and hallucinatory phenomena in the reasoning process are significantly reduced.
The planning methods> (Long, [> 2023
](#bib.bib89)> ; Yao etÂ al., [> 2023b
](#bib.bib172)> , [> c
](#bib.bib173)> ; Liu etÂ al., [> 2023a
](#bib.bib86)> ; Shinn etÂ al., [> 2023
](#bib.bib133)> )
introduce a decision-making process in the reasoning.
They evaluate the intermediate reasoning steps to get feedback, and based on the feedback, they engage in exploration and backtracking to achieve superior solutions at a global level.
Their specialization lies in handling complex problems, enabling them to achieve remarkable performance, especially when confronted with intricate multi-hop reasoning and planning tasks.
### 7.3Compensate for Innate Weaknesses
LLMs have many inherent limitations when it comes to reasoning, such as the inability to access external information, arithmetic errors, and inconsistent reasoning.
These issues can be cleverly circumvented by entrusting specific responsibilities to dedicated modules or models.
In response to the modelsâ€™ limitation in accessing external information,> (Li etÂ al., [> 2023d
](#bib.bib81)> ; Wang etÂ al., [> 2023b
](#bib.bib149)> ; Lu etÂ al., [> 2023a
](#bib.bib90)> ; Schick etÂ al., [> 2023
](#bib.bib128)> ; Karpas etÂ al., [> 2022
](#bib.bib60)> ; Yoran etÂ al., [> 2023
](#bib.bib179)> )
utilizes external knowledge resources like knowledge base, search engines, and open-domain question-answering systems.
Some work introduces a calculator to address arithmetic errors> (Schick etÂ al., [> 2023
](#bib.bib128)> ; Karpas etÂ al., [> 2022
](#bib.bib60)> ; Parisi etÂ al., [> 2022b
](#bib.bib112)> )
.
Code execution is deterministic, and certain work enhances the consistency of the reasoning process by introducing code executor> (Gao etÂ al., [> 2023
](#bib.bib32)> ; Chen etÂ al., [> 2022a
](#bib.bib12)> ; Bi etÂ al., [> 2023
](#bib.bib7)> ; Imani etÂ al., [> 2023
](#bib.bib53)> )
.
We believe that employing LLMs as an agent for central planning and reasoning, delegating specific sub-tasks to dedicated sub-models, is a potential avenue for applying large models in complex scenarios in the future> (Wang etÂ al., [> 2023e
](#bib.bib152)> ; Xi etÂ al., [> 2023
](#bib.bib165)> )
.
### 7.4Other Work
In this chapter, we will list other works that represent early attempts at chain-of-thought reasoning or are designed for specific domains.
> Katz etÂ al. (
[> 2022
](#bib.bib61)> ); Zhang etÂ al. (
[> 2022
](#bib.bib193)> )
provide benchmarks and resources.
Some work has empirically demonstrated the effectiveness of chain-of-thought prompting> (Lampinen etÂ al., [> 2022
](#bib.bib67)> ; Ye and Durrett, [> 2022
](#bib.bib175)> ; Arora etÂ al., [> 2023
](#bib.bib4)> )
and> Shi etÂ al. (
[> 2023
](#bib.bib132)> )
explores multi-lingual CoT reasoning.
Other work focuses on specific domains, such as machine translation> (He etÂ al., [> 2023b
](#bib.bib41)> )
, sentiment analysis> (Fei etÂ al., [> 2023
](#bib.bib28)> )
,
sentence embeddings> (Zhang etÂ al., [> 2023a
](#bib.bib188)> )
,
summarization> (Wang etÂ al., [> 2023k
](#bib.bib159)> )
,
arithmetic> (Lee and Kim, [> 2023
](#bib.bib70)> )
,
and tabular reasoning> (Chen, [> 2023
](#bib.bib11)> ; Jin and Lu, [> 2023
](#bib.bib58)> )
,
etc.
Besides, some research utilizes specific pre-training to enhance certain capabilities, such as mathematical reasoning> (Lewkowycz etÂ al., [> 2022
](#bib.bib74)> ; Zhao etÂ al., [> 2022
](#bib.bib199)> )
.
## 8Conclusion
In this paper, we conduct an extensive survey of existing research on X-of-thought reasoning, offering a comprehensive review of the field.
We introduce the concept of generalized chain-of-thought (X-of-Thought) and examine advances in X-of-thought reasoning from various angles.
Additionally, we investigate the applications of X-of-thought in cutting-edge domains.
Furthermore, we spotlight the current challenges confronting this research and provide future prospects.
To the best of our knowledge, this survey represents the first systematic exploration of chain-of-thought reasoning.
Our objective is to furnish researchers interested in chain-of-thought reasoning with a thorough overview, with the hope that this survey will facilitate further research in this area.
## References
* Aggarwal etÂ al. (2023)Pranjal Aggarwal, Aman Madaan, Yiming Yang, and Mausam. 2023.[Letâ€™s sample step by step: Adaptive-consistency for efficient reasoning with llms](https://doi.org/10.48550/arXiv.2305.11860).*CoRR*, abs/2305.11860.
* Alayrac etÂ al. (2022)Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, JacobÂ L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and KarÃ©n Simonyan. 2022.[Flamingo: a visual language model for few-shot learning](http://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html).In*NeurIPS*.
* Amini etÂ al. (2019)Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019.[Mathqa: Towards interpretable math word problem solving with operation-based formalisms](https://doi.org/10.18653/v1/n19-1245).In*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)*, pages 2357â€“2367. Association for Computational Linguistics.
* Arora etÂ al. (2023)Simran Arora, Avanika Narayan, MayeeÂ F. Chen, LaurelÂ J. Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher RÃ©. 2023.[Ask me anything: A simple strategy for prompting language models](https://openreview.net/pdf?id=bhUPJnS2g0X).In*The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net.
* Besta etÂ al. (2023)Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2023.[Graph of thoughts: Solving elaborate problems with large language models](https://doi.org/10.48550/arXiv.2308.09687).*CoRR*, abs/2308.09687.
* Bhakthavatsalam etÂ al. (2021)Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, BhavanaÂ Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. 2021.[Think you have solved direct-answer question answering? try arc-da, the direct-answer AI2 reasoning challenge](http://arxiv.org/abs/2102.03315).*CoRR*, abs/2102.03315.
* Bi etÂ al. (2023)Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, and Huajun Chen. 2023.[When do program-of-thoughts work for reasoning?](http://arxiv.org/abs/2308.15452)
* Bisk etÂ al. (2020)Yonatan Bisk, Rowan Zellers, RonanÂ Le Bras, Jianfeng Gao, and Yejin Choi. 2020.[PIQA: reasoning about physical commonsense in natural language](https://ojs.aaai.org/index.php/AAAI/article/view/6239).In*The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 7432â€“7439. AAAI Press.
* Brown etÂ al. (2020)TomÂ B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, DanielÂ M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.[Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).In*Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*.
* Cai etÂ al. (2023)Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023.[Large language models as tool makers](http://arxiv.org/abs/2305.17126).
* Chen (2023)Wenhu Chen. 2023.[Large language models are few(1)-shot table reasoners](https://aclanthology.org/2023.findings-eacl.83).In*Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023*, pages 1090â€“1100. Association for Computational Linguistics.
* Chen etÂ al. (2022a)Wenhu Chen, Xueguang Ma, Xinyi Wang, and WilliamÂ W. Cohen. 2022a.[Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks](https://doi.org/10.48550/arXiv.2211.12588).*CoRR*, abs/2211.12588.
* Chen etÂ al. (2023)Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023.[Theoremqa: A theorem-driven question answering dataset](https://doi.org/10.48550/arXiv.2305.12524).*CoRR*, abs/2305.12524.
* Chen etÂ al. (2021)Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, BryanÂ R. Routledge, and WilliamÂ Yang Wang. 2021.[Finqa: A dataset of numerical reasoning over financial data](https://doi.org/10.18653/v1/2021.emnlp-main.300).In*Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021*, pages 3697â€“3711. Association for Computational Linguistics.
* Chen etÂ al. (2022b)Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and WilliamÂ Yang Wang. 2022b.[Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering](https://doi.org/10.18653/v1/2022.emnlp-main.421).In*Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022*, pages 6279â€“6292. Association for Computational Linguistics.
* Cheng etÂ al. (2023)Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, NoahÂ A. Smith, and Tao Yu. 2023.[Binding language models in symbolic languages](https://openreview.net/pdf?id=lH1PV42cbF).In*The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net.
* Cobbe etÂ al. (2021)Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.[Training verifiers to solve math word problems](http://arxiv.org/abs/2110.14168).*CoRR*, abs/2110.14168.
* Crispino etÂ al. (2023a)Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song, and Chenguang Wang. 2023a.Agent instructs large language models to be general zero-shot reasoners.*arXiv preprint arXiv:2310.03710*.
* Crispino etÂ al. (2023b)Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song, and Chenguang Wang. 2023b.[Agent instructs large language models to be general zero-shot reasoners](http://arxiv.org/abs/2310.03710).
* Dagan etÂ al. (2023)Gautier Dagan, Frank Keller, and Alex Lascarides. 2023.[Dynamic planning with a llm](https://api.semanticscholar.org/CorpusID:260887774).*ArXiv*, abs/2308.06391.
* Devlin etÂ al. (2019)Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.[BERT: pre-training of deep bidirectional transformers for language understanding](https://doi.org/10.18653/v1/n19-1423).In*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)*, pages 4171â€“4186. Association for Computational Linguistics.
* Dhuliawala etÂ al. (2023)Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023.Chain-of-verification reduces hallucination in large language models.*arXiv preprint arXiv:2309.11495*.
* Diao etÂ al. (2023)Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023.[Active prompting with chain-of-thought for large language models](https://doi.org/10.48550/arXiv.2302.12246).*CoRR*, abs/2302.12246.
* Dong etÂ al. (2023)Qingxiu Dong, Lei Li, Damai Dai, CeÂ Zheng, Zhiyong Wu, Baobao Chang, XuÂ Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023.[A survey for in-context learning](https://doi.org/10.48550/arXiv.2301.00234).*CoRR*, abs/2301.00234.
* Dong etÂ al. (2022)Qingxiu Dong, Ziwei Qin, Heming Xia, Tian Feng, Shoujie Tong, Haoran Meng, Lin Xu, Zhongyu Wei, Weidong Zhan, Baobao Chang, Sujian Li, Tianyu Liu, and Zhifang Sui. 2022.[Premise-based multimodal reasoning: Conditional inference on joint textual and visual clues](https://doi.org/10.18653/v1/2022.acl-long.66).In*Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022*, pages 932â€“946. Association for Computational Linguistics.
* Dua etÂ al. (2022)Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022.[Successive prompting for decomposing complex questions](https://doi.org/10.18653/v1/2022.emnlp-main.81).In*Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022*, pages 1251â€“1265. Association for Computational Linguistics.
* Dua etÂ al. (2019)Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019.[DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs](https://doi.org/10.18653/v1/N19-1246).In*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 2368â€“2378, Minneapolis, Minnesota. Association for Computational Linguistics.
* Fei etÂ al. (2023)Hao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, and Tat-Seng Chua. 2023.[Reasoning implicit sentiment with chain-of-thought prompting](https://doi.org/10.18653/v1/2023.acl-short.101).In*Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*, pages 1171â€“1182. Association for Computational Linguistics.
* Feng etÂ al. (2023)Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, DiÂ He, and Liwei Wang. 2023.[Towards revealing the mystery behind chain of thought: a theoretical perspective](https://doi.org/10.48550/arXiv.2305.15408).*CoRR*, abs/2305.15408.
* Fu etÂ al. (2023a)Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023a.[Complexity-based prompting for multi-step reasoning](https://openreview.net/pdf?id=yf1icZHC-l9).In*The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net.
* Fu etÂ al. (2023b)Yao Fu, Hao-Chun Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023b.[Specializing smaller language models towards multi-step reasoning](https://api.semanticscholar.org/CorpusID:256390607).In*International Conference on Machine Learning*.
* Gao etÂ al. (2023)Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.[PAL: Program-aided language models](https://proceedings.mlr.press/v202/gao23f.html).In*Proceedings of the 40th International Conference on Machine Learning*, volume 202 of*Proceedings of Machine Learning Research*, pages 10764â€“10799. PMLR.
* Gerevini (2020)AlfonsoÂ Emilio Gerevini. 2020.[An introduction to the planning domain definition language (PDDL): book review](https://doi.org/10.1016/j.artint.2019.103221).*Artif. Intell.*, 280:103221.
* Geva etÂ al. (2021)Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021.[Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies](https://doi.org/10.1162/tacl_a_00370).*Trans. Assoc. Comput. Linguistics*, 9:346â€“361.
* Gou etÂ al. (2023)Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.[Tora: A tool-integrated reasoning agent for mathematical problem solving](http://arxiv.org/abs/2309.17452).
* Gupta and Gupta (2022)Pranay Gupta and Manish Gupta. 2022.[Newskvqa: Knowledge-aware news video question answering](https://doi.org/10.1007/978-3-031-05981-0_1).In*Advances in Knowledge Discovery and Data Mining - 26th Pacific-Asia Conference, PAKDD 2022, Chengdu, China, May 16-19, 2022, Proceedings, Part III*, volume 13282 of*Lecture Notes in Computer Science*, pages 3â€“15. Springer.
* Han etÂ al. (2023)Chengcheng Han, Xiaowei Du, Che Zhang, Yixin Lian, Xiang Li, Ming Gao, and Baoyuan Wang. 2023.[Dialcot meets ppo: Decomposing and exploring reasoning paths in smaller language models](http://arxiv.org/abs/2310.05074).
* Han etÂ al. (2022)Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, ShafiqÂ R. Joty, AlexanderÂ R. Fabbri, Wojciech Kryscinski, XiÂ Victoria Lin, Caiming Xiong, and Dragomir Radev. 2022.[FOLIO: natural language reasoning with first-order logic](https://doi.org/10.48550/arXiv.2209.00840).*CoRR*, abs/2209.00840.
* Hao etÂ al. (2023)Shibo Hao, Yilan Gu, Haodi Ma, JoshuaÂ Jiahua Hong, Zhen Wang, DaisyÂ Zhe Wang, and Zhiting Hu. 2023.[Reasoning with language model is planning with world model](https://api.semanticscholar.org/CorpusID:258865812).*ArXiv*, abs/2305.14992.
* He etÂ al. (2023a)Hangfeng He, Hongming Zhang, and Dan Roth. 2023a.[Rethinking with retrieval: Faithful large language model inference](https://doi.org/10.48550/arXiv.2301.00303).*CoRR*, abs/2301.00303.
* He etÂ al. (2023b)Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2023b.[Exploring human-like translation strategy with large language models](https://doi.org/10.48550/arXiv.2305.04118).*CoRR*, abs/2305.04118.
* Hendrycks etÂ al. (2021)Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.[Measuring mathematical problem solving with the MATH dataset](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).In*Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual*.
* Ho etÂ al. (2023)Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023.[Large language models are reasoning teachers](https://doi.org/10.18653/v1/2023.acl-long.830).In*Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*, pages 14852â€“14882. Association for Computational Linguistics.
* Hosseini etÂ al. (2014)MohammadÂ Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014.[Learning to solve arithmetic word problems with verb categorization](https://doi.org/10.3115/v1/d14-1058).In*Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL*, pages 523â€“533. ACL.
* Hsieh etÂ al. (2023)Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, AlexanderÂ J. Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023.[Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes](https://api.semanticscholar.org/CorpusID:258461606).*ArXiv*, abs/2305.02301.
* Hu etÂ al. (2023a)Hanxu Hu, Hongyuan Lu, Huajian Zhang, Wai Lam, and Yue Zhang. 2023a.[Chain-of-symbol prompting elicits planning in large langauge models](https://doi.org/10.48550/arXiv.2305.10276).*CoRR*, abs/2305.10276.
* Hu etÂ al. (2023b)Pengbo Hu, JiÂ Qi, Xingyu Li, Hong Li, Xinqi Wang, Bing Quan, Ruiyu Wang, and YiÂ Zhou. 2023b.[Tree-of-mixed-thought: Combining fast and slow thinking for multi-hop visual reasoning](https://doi.org/10.48550/arXiv.2308.09658).*CoRR*, abs/2308.09658.
* Huang etÂ al. (2022)Jiaxin Huang, ShixiangÂ Shane Gu, LeÂ Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.[Large language models can self-improve](https://doi.org/10.48550/arXiv.2210.11610).*CoRR*, abs/2210.11610.
* Huang etÂ al. (2023a)Jie Huang, Xinyun Chen, Swaroop Mishra, HuaixiuÂ Steven Zheng, AdamsÂ Wei Yu, Xinying Song, and Denny Zhou. 2023a.Large language models cannot self-correct reasoning yet.*arXiv preprint arXiv:2310.01798*.
* Huang etÂ al. (2019)Lifu Huang, RonanÂ Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019.[Cosmos QA: machine reading comprehension with contextual commonsense reasoning](https://doi.org/10.18653/v1/D19-1243).In*Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019*, pages 2391â€“2401. Association for Computational Linguistics.
* Huang etÂ al. (2023b)Shaohan Huang, LiÂ Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, OwaisÂ Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. 2023b.[Language is not all you need: Aligning perception with language models](https://doi.org/10.48550/arXiv.2302.14045).*CoRR*, abs/2302.14045.
* Huang etÂ al. (2023c)Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, NeilÂ Zhenqiang Gong, and Lichao Sun. 2023c.[Metatool benchmark: Deciding whether to use tools and which to use](http://arxiv.org/abs/2310.03128).
* Imani etÂ al. (2023)Shima Imani, Liang Du, and Harsh Shrivastava. 2023.[Mathprompter: Mathematical reasoning using large language models](https://doi.org/10.18653/v1/2023.acl-industry.4).In*Proceedings of the The 61st Annual Meeting of the Association for Computational Linguistics: Industry Track, ACL 2023, Toronto, Canada, July 9-14, 2023*, pages 37â€“42. Association for Computational Linguistics.
* Ji etÂ al. (2023)Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023.Towards mitigating hallucination in large language models via self-reflection.*arXiv preprint arXiv:2310.06271*.
* Jiang etÂ al. (2023a)Song Jiang, Zahra Shakeri, Aaron Chan, Maziar Sanjabi, Hamed Firooz, Yinglong Xia, Bugra Akyildiz, Yizhou Sun, Jinchao Li, Qifan Wang, etÂ al. 2023a.Resprompt: Residual connection prompting advances multi-step reasoning in large language models.*arXiv preprint arXiv:2310.04743*.
* Jiang etÂ al. (2023b)Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, YuÂ Zhang, Zhenguo Li, and JamesÂ T. Kwok. 2023b.[Forward-backward reasoning in large language models for verification](https://doi.org/10.48550/arXiv.2308.07758).*CoRR*, abs/2308.07758.
* Jie etÂ al. (2023)Zhanming Jie, TrungÂ Quoc Luong, Xinbo Zhang, Xiaoran Jin, and Hang Li. 2023.[Design of chain-of-thought in math problem solving](http://arxiv.org/abs/2309.11054).
* Jin and Lu (2023)Ziqi Jin and Wei Lu. 2023.[Tab-cot: Zero-shot tabular chain of thought](https://doi.org/10.18653/v1/2023.findings-acl.651).In*Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023*, pages 10259â€“10277. Association for Computational Linguistics.
* Jones etÂ al. (2023)Erik Jones, Hamid Palangi, Clarisse SimÃµes, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, and Ece Kamar. 2023.Teaching language models to hallucinate less with synthetic tasks.*arXiv preprint arXiv:2310.06827*.
* Karpas etÂ al. (2022)EhudÂ D. Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, and Moshe Tenenholtz. 2022.[Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning](https://api.semanticscholar.org/CorpusID:248496374).*ArXiv*, abs/2205.00445.
* Katz etÂ al. (2022)Uri Katz, Mor Geva, and Jonathan Berant. 2022.[Inferring implicit relations in complex questions with language models](https://doi.org/10.18653/v1/2022.findings-emnlp.188).In*Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022*, pages 2548â€“2566. Association for Computational Linguistics.
* Khalifa etÂ al. (2023)Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and LuÂ Wang. 2023.Discriminator-guided multi-step reasoning with language models.*arXiv preprint arXiv:2305.14934*.
* Khot etÂ al. (2023)Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023.[Decomposed prompting: A modular approach for solving complex tasks](https://openreview.net/pdf?id=_nGgzQjzaRy).In*The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net.
* Kojima etÂ al. (2022)Takeshi Kojima, ShixiangÂ Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022.[Large language models are zero-shot reasoners](http://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html).In*NeurIPS*.
* Koncel-Kedziorski etÂ al. (2015)Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and SienaÂ Dumas Ang. 2015.[Parsing algebraic word problems into equations](https://doi.org/10.1162/tacl_a_00160).*Transactions of the Asso
