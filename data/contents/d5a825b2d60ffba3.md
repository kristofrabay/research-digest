# 

**URL:** https://www.arxiv.org/pdf/2407.11511v2
**Published:** 2025-08-14T00:00:00.000Z

---

## Summary

The provided web page is a survey on **Multi-Step Reasoning with Large Language Models (LLMs)**. It focuses on how LLMs, particularly through **Chain-of-Thought (CoT)** prompting, can perform complex reasoning tasks beyond simple next-token prediction.

Here is a summary of the page content relevant to your query:

*   **Reasoning LLMs and Chain-of-Thought (CoT):** The paper reviews the field of LLM reasoning, which gained traction with the CoT approach. CoT involves prompting the LLM to generate intermediate reasoning steps ("Let‚Äôs think step by step") to solve difficult problems, especially math word problems (like those in the GSM8K benchmark).
*   **Inference-Time Compute:** In-context learning, which includes CoT, occurs at **inference-time** where model parameters are not adapted. The performance gains rely on the model's existing knowledge and the prompt structure.
*   **Planning with LLMs:** The survey discusses a general three-stage pipeline for multi-step reasoning: **Generate, Evaluate, and Control** steps. Planning falls under the **Control** stage, which can involve complex strategies like ensemble methods or using **Reinforcement Learning (RL)** algorithms (including in-context RL) to explore different reasoning paths dynamically.
*   **MCTS (Monte Carlo Tree Search) for Language Models:** While MCTS is not explicitly named, the concept of tree search is mentioned under the **Control** stage (3.3.3), where algorithms like **Tree-of-Thoughts (ToT)** use search methods (like BFS/DFS) to scaffold the reasoning process, allowing the LLM to roll back and try different steps.
*   **Self-Reflection:** **Self-reflection** is explicitly mentioned as being used in many multi-step methods, often implemented via **Self-verification** (where the LLM evaluates its own steps) or as part of reinforcement learning fine-tuning loops.
*   **Hallucination Reduction and Detection / Grounding:** The text touches upon grounding in the context of robotics, where approaches like **Say-can** use external physics models to ground reasoning steps in reality, which helps reduce errors (a form of hallucination reduction). Tool-based validation (using external interpreters for generated code) also serves to ground the reasoning in verifiable logic.
*   **Factuality:** While the term "

The user query asks for a summary related to several advanced topics in Reasoning and Planning with Large Language Models (LLMs), including: **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**

The provided webpage text discusses many of these concepts in detail:

*   **Reasoning LLMs & Chain-of-Thought (CoT):** The text extensively covers various reasoning approaches, including **Chain-of-Thought (CoT)**, **Least-to-most prompting**, **Program-of-thought**, and **Program-aided-language**, which use LLMs to generate step-by-step reasoning.
*   **Planning with LLMs:** Planning is discussed in the context of **Robotic Behavior** (e.g., Say-can, Inner-monologue, Chain-of-tools) and in the control stage using search algorithms like **Tree-of-Thoughts (ToT)**, which explores a search tree with backtracking.
*   **Inference-time Compute & Control:** The text details three main approaches for controlling reasoning steps at inference time: **greedy selection** (like standard CoT), **ensemble strategy** (like Self-consistency), and **Reinforcement Learning (RL)** search, which involves external algorithms traversing a search tree.
*   **Self-Reflection & Self-Improvement:** Several methods involve the model evaluating and correcting its own output, such as **Self-debugging**, **Refiner**, **Self-correction**, **Self-improvement**, **Self-refine**, and **Reflexion** (which uses an actor, evaluator, and reflector model).
*   **Hallucination Reduction & Grounding:** The text notes that reasoning suffers from **hallucination** when not properly grounded. **Grounding** is achieved by combining LLMs with external tools or knowledge sources, such as using external models for **robotic affordances** (Say-can) or retrieving information from **Wikipedia** (ReAct).
*   **Factuality & Faithfulness:** The text addresses the issue of **faithfulness**‚Äîwhether the model followed the stated reasoning steps or arrived at the answer through an unfaithful path. Methods like **Faithful-chain-of-thought** are proposed to address this by translating

---

## Full Content

Multi-Step Reasoning with Large Language Models, a Survey
ASKE PLAAT, ANNIE WONG, SUZAN VERBERNE, JOOST BROEKENS, NIKI VAN STEIN, and THOMAS
B√ÑCK, Leiden University, Netherlands
Language models with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model
was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on
basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step
reasoning abilities on these benchmarks.
The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems,
and has expanded to other tasks in the past few years. This paper reviews the field of multi-step reasoning with LLMs. We propose a
taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of
core approaches and open problems, and we propose a research agenda for the near future.
We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve
challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many
studies in multi-step methods are using reinforcement learning for finetuning, external optimization loops, in context reinforcement
learning, and self-reflection.
CCS Concepts: ‚Ä¢ Computing methodologies ‚Üí Natural language processing.
ACM Reference Format:
Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck. 2025. Multi-Step Reasoning with Large
Language Models, a Survey. 1, 1 (August 2025), 33 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
Transformer-based Large Language Models (LLMs) that are trained on large datasets have achieved breakthrough
performance at text generation tasks that directly build on next token prediction [115, 152, 163]; they are good at natural
language understanding (GLUE, SQUAD, Xsum) [99, 119, 155, 156], translation [73, 106, 130], question answering [143],
and other language generation tasks. The success of models such as ChatGPT [102] is impressive.
Transformer-based generative language models whose size is beyond hundreds of billions parameters are not only
good at language generation, they also enable a new type of machine learning, called in-context learning [15]. In-context
learning, also known as prompt-based learning, is an emergent ability that occurs in LLMs beyond a certain size
(hundreds of billions of parameters‚Äîless, with judicious prompting) that have been finetuned for conversational
responses [163]. In-context learning is inference-time, prompt-based, few-shot learning with instructions. As opposed
to finetuning, model parameters are not adapted by in-context learning.
Language generation tasks are solved well by LLMs with prompt-based learning. On the other hand tasks that require
reasoning, such as grade school math word problems, are more difficult for LLMs [30]. Spurred-on by the impressive
Authors‚Äô address: Aske Plaat; Annie Wong; Suzan Verberne; Joost Broekens; Niki van Stein; Thomas B√§ck, LIACS, Leiden University, Netherlands.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
¬© 2025 Association for Computing Machinery.
Manuscript submitted to ACM
Manuscript submitted to ACM 1
arXiv:2407.11511v2 [cs.AI] 13 Aug 2025
2 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
performance on language tasks, much research has focused on understanding the reason for the poor performance of
LLMs on reasoning tasks, and how it can be improved. Among this research, the Chain-of-thought paper by Wei et al.
[164] stands out. This work, and later work by Kojima et al. [74], showed that adding a simple instruction to the prompt,
Let‚Äôs think step by step, can provoke an LLM to perform the required intermediate reasoning steps to answer difficult
problems in a multi-step approach. Subsequently, performance on math word benchmarks has increased markedly.
Much of the performance increase of models such as OpenAI o1, o3 [61] and DeepSeek R1 [49, 131] is attributed to
multi-step reasoning methods as reviewed here, with reinforcement learning playing an increasingly important role,
both for in-context learning and finetuning [25, 37, 168, 173].
The line of research into multi-step LLM-reasoning was started with grade school math word problems, with the
GSM8K benchmark [30]. Soon, other reasoning domains were included, such as reasoning about advanced math
problems, computer code, robotic movement, and games. Many works have been published that build on Chain-ofthought [27]. In this paper, we survey the literature using a straightforward taxonomy. We discuss papers based on
several reasoning benchmarks, as well as directly-related follow up work.
Having started with basic math word problems, multi-step LLM reasoning approaches now perform logic reasoning,
planning, combinatorial games, and robotic actions, amongst others. Some approaches do so by prompting the LLM to
generate code that is then interpreted by external tools. Many studies in multi-step methods are using reinforcement
learning, for finetuning, in context reinforcement learning, external optimization loops, and self-reflection. The main
contributions of this paper are: (1) we provide a survey of relevant approaches in multi-step reasoning with LLMs, (2)
we propose a taxonomy based on the reasoning literature (step generation, step evaluation, and control of reasoning
steps), and (3) we formulate a research agenda for reasoning with LLMs.
This survey is organized as follows. Section 2.1 provides background information on the most relevant developments
in LLMs, including in-context learning and finetuning. Of great importance are the benchmarks that are used in this
field (Section 2.4). Next, in Section 3 we provide a taxonomy of the field, where we discuss the main approaches in detail.
Then, in Section 4 we discuss our findings in a broader perspective. We also discuss the relation between multi-step
reasoning and work on self-reflection and metacognition. This section concludes with an agenda for future research.
Finally, Section 5 concludes the survey.
2 BACKGROUND, SCOPE AND SELECTION OF PAPERS
Reasoning has a long and active history in AI, in logical inference, and in other fields, such as commonsense and
analogical reasoning [81, 97]. Indeed, some of the early criticism on LLMs was that they could not reason, and that they
made obvious and basic reasoning errors, showing a lack of understanding. Berglund et al. [10] provide the example
of a model that is trained to report that ‚ÄúValentina Tereshkova was the first woman to travel to space,‚Äù but is not
able to answer the question, ‚ÄúWho was the first woman to travel to space?‚Äù pointing indeed to a lack of semantic
understanding. Other work suggests that results are less generalizable and transferable than often assumed, showing
how base-10 arithmetic skills do not transfer to base-9 arithmetic problems [169]. Different kinds of reasoning have
played an important role in LLM research.
2.1 Scope
The interest in Chain-of-though has stimulated more research into language and logic. Logic-LM combines LLMs with
symbolic logic solvers in an agentic way, where the symbolic solver is called by the LLM as a tool [104]. Xu et al. [174]
use a similar approach. Relevant logic-reasoning benchmarks are PrOntoQA [126], ProofWriter [141], FOLIO [50],
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 3
LogicalDeduction [138], and AR-LSAT [188]. The logic-LLM approaches often combine Chain-of-thought and symbolic
solvers in an agentic-LLM approach [112], or transform the problem in an intermediate formal language. The use of
LLMs for generating formal languages is further discussed in section 4.4.
More recently, LLMs are also shown to struggle with analogies and analogical reasoning [81, 97], a topic that is
related to self-reflection (see Sections 3.3.3 and 4.3.3).
The question whether LLMs can reason prompted the development of the GSM8k benchmark [30] in 2021, a
benchmark of some of easy reasoning problems (of the type: ‚ÄúQuestion: Natalia sold clips to 48 of her friends in April,
and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Answer:
72‚Äù). This benchmark was specifically designed to test if LLMs can answer basic mathematical multi-step reasoning
questions, or if they can not reason at all. Initially, performance was poor, but a year later Wei et al. [164] showed that
by providing multi-step examples in the prompt‚Äîa chain of thought‚Äîmuch better performance was possible. Many
works followed, and reasoning in language models has become an active area of study. In this survey, we focus on
prompt-based multi-step reasoning algorithms. After starting with math word problems, work has more recently been
extended to combinatorial games, puzzles, robotics, logic, and other fields of reasoning. At the end of this survey, we
discuss connections to other fields, such as self-reflection and in-context reinforcement learning.
Before we discuss the works on reasoning, this section reviews background terminology on LLMs. Our overview is
brief. Excellent recent general surveys on LLMs are, for example, Minaee et al. [96] and Zhao et al. [185]. Other works
focus on the nature of reasoning and its definition [26, 58, 180], on evaluating logical reasoning in LLMs [88, 98, 104],
on reinforcement learning in reasoning [173], on reasoning in language [180], or on Chain-of-thought itself [27].
The papers in this survey were selected as follows. We started by selecting papers on the ability to solve math word
benchmarks (as a proxy for reasoning ability), that contained the search terms reasoning and large language model in
their title or abstract, with a focus on papers that reference the Chain-of-thought paper. Although multi-step reasoning
with LLMs initially was aimed at solving math world problems, it is now wider, including benchmarks and approaches
for computer code, game play, puzzles, robot movement, and webpage navigation (see Table 2). We selected recent
papers (two years prior to the writing of the survey) that show experimental results on selected benchmark datasets.
We focus on prompt-based, in-context learning, methods based on Chain-of-thought, that are used in reasoning
LLMs such as OpenAI o1 and o3 [61, 168, 173]. We also include papers that work with external algorithms, finetuning
or supervised learning that have contributed to the approaches.
We discuss the generic training pipeline for LLMs, we discuss how in-context learning works, and we discuss
commonly used benchmarks. We start with the generic language model training pipeline.
2.2 Language Model Training Pipeline
LLMs are typically constructed in a sequence of stages, from data preparation, through training, to inference. The
training pipeline for most LLMs is quite elaborate. We will now list a brief pipeline of the most commonly used stages,
based on the survey by Minaee et al. [96].
In training an LLM, the first stage is to acquire a large, general, unlabeled, high-quality text corpus. Some considerations
on the selection of the texts are discussed in Brown et al. [15]. The next stage is pretraining the transformer model
[152] on this large corpus. This stage yields a generative language model. Pretraining is done using self-supervised
autoregressive training on the unlabeled dataset (text corpus). Then the general model is finetuned to a specific (narrow)
task. This can be done using supervised learning with a new labeled dataset consisting of prompts and answers
(supervised finetuning, SFT, for example with low-rank optimization [56, 96, 163], or reinforcement learning [25, 140],
Manuscript submitted to ACM
4 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
specific for the task at hand. A small number of the surveyed papers also address the finetuning stage. A specific form
of finetuning is instruction tuning, to improve instruction following for a certain task. Instruction tuning is supervised
by a labeled dataset of instruction prompts and corresponding outputs. Depending on the purpose of the model the next
step is alignment of the finetuned model with user expectations (preference alignment). Alignment is usually performed
to ensure that the model produces more ethically and socially acceptable answers, preventing, for example, hate speech.
The machine learning method that is commonly used in this stage Reinforcement Learning with Human Feedback [102]
or Direct Preference Optimization [117]. Optionally, model training can be optimized to improve cost-effectiveness, for
example, mixed precision training [95], quantization [64], or knowledge distillation [48, 175].
Once the model has been trained in the steps described above, it can be used further in the inference stage. Here, the
model is used to provide an answer to the prompt. The inference-time stage is post-training, no model parameters are
changed anymore [15, 36]; in-context learning, or prompt-learning, takes place in this stage. Due to its low barrier of
entry, without the need for expensive training or finetuning, this form of advanced prompt engineering has become a
popular training method. This is the stage on which most of the surveyed papers focus, using prompts for the LLM to
perform a complex multi-step reasoning task. The following section provides a brief introduction to in-context learning.
2.3 In-Context Learning
In large models, beyond hundreds of billions of parameters, a new kind of learning has emerged, that has been called
in-context learning or prompt-learning [15]. It occurs not when the model is trained, but when it is used, at inference
time. Since no parameters are changed in this stage, it is not a model training stage; in-context-learning ‚Äúlearns‚Äù inside
the context, or prompt, using information that is already encoded in the trained model parameters and the prompt,
not by training the model anymore. In-context learning is often able to give good results with few examples, so-called
few-shot learning, learning from the few examples in combination with the knowledge of the model. The large size of
the model, containing rich and general knowledge, is enabling the few-shot learning (see Dong et al. [36] for a survey).
In in-context learning, a prompt, consisting of a piece of demonstration context, is concatenated with a query
question, and is given to the language model, for text generation [89]. For example, when the task is emotion recognition
in a social media post, ‚ÄúI missed the bus today,‚Äù can be followed by ‚ÄúI felt so [___]‚Äù, and the model could answer with
‚Äúbad‚Äù. Alternatively, for translation, we could follow ‚ÄúI missed the bus today,‚Äù by ‚ÄúFrench: [___]‚Äù to request a translation
[89]. The prompt contains background information that is recognized by the model, selecting the desired model context.
In-context learning works when language models contain enough knowledge, allowing them to generalize on the (few)
examples provided in the prompt.
Contexts that contain a few examples are said to perform few-shot learning. Contexts that contain only instructions
with zero examples are said to perform zero-shot learning. In-context learning takes place at inference time, after the
computationally intensive training stages where parameters have been pretrained and finetuned, when the model
is queried by the user to provide answers. No parameters are changed anymore with in-context learning. This is
quite different from the common approach in supervised deep learning‚Äîor self-supervised deep learning‚Äîwhere large
datasets are used during training to update model parameters with backward propagation in lengthy and costly training
epochs [47]. Indeed, in-context learning takes place fully at inference time, no parameters are trained, instead, learning
now refers to adjusting the answers to the examples in the prompt and the internal knowledge acquired during training.
Common approaches to few-shot learning, such as metalearning, do include training and finetuning of parameters to
achieve generalization, and are computationally expensive (see, for example, [41] or [55, 62] for a survey). In-context
learning, in comparison, is computationally cheap, and it has become a popular research approach..
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 5
Benchmark GPT-3 175B Chain-of-thought [164]
GSM8K 15.6 46.9
ASDiv 70.3 71.3
MAWPS 72.7 87.1
SVAMP 65.7 68.9
AQuA 24.8 35.8
Table 1. Accuracy of GPT-3 and Chain-of-thought on popular Math Word Problems benchmarks [164]
Prompts provide a user-friendly interface to LLMs. The success of in-context learning tends to be quite sensitive to
the way a prompt is formulated; a new field called prompt engineering has emerged to help human users learn how
to make LLMs do what we want them to do [46, 115, 125, 163]. The current survey thus discusses advanced prompt
engineering methods.
2.4 Multi-step Reasoning Benchmarks
Progress in artificial intelligence is measured by benchmarks. Benchmarks define the goal that researchers aim to
achieve in their experiments. In natural language processing, a wide array of benchmarks exist to measure progress,
such as on question answering (for example, CommonsenseQA [142]), word prediction (for example, LAMBADA [105]),
translation (for example, WMT‚Äô22 [73]), language understanding (for example, GLUE [155, 156]), and text summarization
(for example, Xsum [99]).
The field of LLMs is quite active. We will mention relevant benchmarks for testing the multi-step reasoning abilities
of LLMs. The research on reasoning with LLMs started with math word problem benchmarks. The benchmark that is
most frequently associated with multi-step reasoning with LLMs is the dataset of grade school math word problems
GSM8K [30]. GSM8K was created with the aim of providing high quality, high diversity, moderate difficulty, problems
and solutions in natural language. It consists of 8500 human-written math problems. Language models struggled to
achieve good performance on this dataset before Chain-of-thought was introduced. An example of a math word task
follows. Problem: Beth bakes 4 trays with two dozen batches of cookies in a week. If these cookies are shared amongst
16 people equally, how many cookies does each person consume? Answer: 4 √ó 2 √ó 12/16 = 6.
Other benchmarks of similar math word problems are the SVAMP varying structures benchmarks [107], the ASDiv
dataset of diverse math problems [94], the AQuA dataset of algebraic word problems [85], and the MAWPS benchmark
[75]. Table 1 summarizes the accuracy of Chain-of-thought on these basic math word problems, against the baseline of
GPT-3 175B as LLM [164], as percentage of benchmark questions answered correctly. We see that Chain-of-thought
performs well against the baseline of GPT-3 on some benchmarks, but there is certainly room for further improvement
on others.
In addition to the initial set of math word benchmarks, further reasoning approaches have been introduced that test
performance in other fields of reasoning. Benchmarks have been developed on Advanced mathematical questions [31],
Computer code comprehension (Human evaluation, Spider [181], Transcoder [122]), Robotic movement (Alfworld [136],
Kitchen [2]), Puzzle solving (Game24 [179]), Creative writing [179]), Gaming (Checkmate problems[176], MineCraft
[39]), and Webpage navigation (WebShop [177]). These benchmarks are used by other approaches in our survey, as we
will see in more detail in Section 3.
Manuscript submitted to ACM
6 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
The scope of this survey is limited to multi-step reasoning approaches. Other studies have published more challenging
benchmarks, to study the performance of Chain-of-thought and self-reflection in LLMs in puzzles and (logic) games
[103, 123, 166].
It can be difficult to directly compare different benchmarking results, since there may be differences in the way the
experiments were conducted. Kamoi et al. [69] provide a critical analysis of benchmarking problems in LLM reasoning.
Despite these challenges, general conclusions can be drawn. Section 4.1 provides guidance on how to choose reasoning
approaches for different problem types.
3 STEP GENERATION, EVALUATION AND CONTROL
This survey examines how LLMs based on the transformer architecture can be prompted to solve multi-step reasoning
tasks. The Chain-of-thought paper shows how a simple command could prompt an LLM to perform reasoning steps,
yielding much better performance in math word problems. Since then much research has further explored this approach,
trying to build stronger general problem solvers for other types of reasoning problems.
A typical approach to solve a complex problem is to subdivide it into smaller steps and to solve those. This approach
is related to classical divide and conquer [6]. It consists of three stages. New steps are (1) generated, (2) evaluated, and
the search of the generated steps is (3) controlled in some way. The in-context reasoning approaches that we survey
follow a general three-stage pipeline [92]:
(1) Generate: prompt the model for the generation of steps,
(2) Evaluate: prompt for evaluation of the generated steps,
(3) Control: prompt for control of the number of steps that are generated and how deep ahead the reasoning process
will look.
This three-stage pipeline is the basis of our taxonomy. We will now discuss the three stages more deeply; for ease
of reference they are numbered according to the Subsection in which they are described in more detail (3.1, 3.2, 3.3).
Please also refer to Figure 1, or Table 2, for a diagram of the categories and subcategories of different approaches for the
generation, evaluation, and control of reasoning steps.1
(3.1) Generation. The first stage is to create a prompt that instructs the LLM to generate reasoning steps. The problem
must be split into substeps. This can be achieved with a problem-specific prompt that contains elements of the problem,
such as: ‚ÄúFirst calculate how many marbles Mary had originally, then how many her friend had, and finally how many
they had together.‚Äù In general, it is possible to prompt an LLM to fill in the blanks in a step-by-step fashion. In the papers
that we discuss, there are three main approaches for generating the step-by-step prompt, numbered with the Subsection
in which the approaches are described. First, the prompt may be handcrafted for the problem by the researchers: (3.1.1)
hand-written prompt. Second, the prompt or prompts may come from a source that is external to the model, such as
another model or dataset: (3.1.2) external knowledge-based prompt. Third, the model itself can be prompted to generate a
(series of) prompt(s) to analyze the problem (3.1.3) model-generated prompt. As we will see, all three approaches have
their advantages and disadvantages.
Generating the subproblem-steps is the first stage that is necessary for in-context learning to perform reasoning.
Each paper in our survey performs at least this stage of the reasoning pipeline. In some of the early papers (around
2022) it is the only stage of the pipeline that is performed.
1We show the approaches in the Figure in their main category only. Some approaches show innovations in two categories, and are shown twice. (Since all
approaches have a generation, an evaluation, and a control aspect, all could in principle occur three times, and all three columns can be found in Table 2).
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 7
Fig. 1. Taxonomy of LLM-Reasoning Approaches: Prompt Generation, Evaluation, and Control
(3.2) Evaluation. After the prompt has been generated and the model has answered it, the next stage is to evaluate the
quality of this answer. Such evaluation is often necessary to improve and perform well on the benchmark. Again, we see
three main approaches for substep evaluation. First, the steps may be evaluated by the model itself: (3.2.1) self-assessment.
Second, an external program can be used to evaluate the steps. For example, when the steps are expressed as computer
code, an external interpreter or compiler can be used to check the validity and the outcome: (3.2.2) tool-based evaluation.
Manuscript submitted to ACM
8 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
Finally, an external model can be used, LLM or otherwise. For example, in robotics, an external physics model can
determine if certain actions are physically possible: (3.2.3) external model validation.
(3.3) Control. The third stage is control. A reasoning process that consists of multiple steps is a sequential decision
process [86]. When a single chain of reasoning steps is generated, the control flow of the reasoning process is simple:
greedily evaluate the first step and then the next one, if present. The control flow of the reasoning process may also be
more intricate. Some reasoning problems can be divided into multiple subproblems. To execute, evaluate and combine
the results of all substeps, a separate controller may be needed. This controller can be a prompt or an external algorithm.
Again, we distinguish three approaches. Most papers use a (3.3.1) greedy selection approach: a single prompt with
a single chain of steps is generated, and these steps are directly executed and followed. The second approach is to
generate an (3.3.2) ensemble strategy of reasoning steps, evaluate them, combine the individual results, and present them
as the result of the ensemble. Finally, a full tree-search or a (3.3.3) reinforcement learning (RL) algorithm can be used as
scaffolding. In this case, when a step is followed and evaluated, the LLM can roll back and try a different reasoning step
[110]. Going further, a full reinforcement learning approach can be used [111, 140] to find an optimal policy for the
sequential decision process. In general a Markov Decision Process of state, action, transition, and reward function can
be specified, and step control can become a process where prompts are generated dynamically under the control of an
external RL algorithm, or as in-context reinforcement learning (ICRL) [80].
Taxonomy Table. Table 2 lists the main papers of this survey. We show the domain they work on, the type of prompt
generation, the evaluation of the result, and the control method. These three categories of approaches‚Äîindicated by
their Sections (3.1) generation, (3.2) evaluation, (3.3) control‚Äîare shown in the table as groups divided by horizontal
lines. The first group in the Table, from Scratchpad to Self-ask, focuses on creating a prompt that generates the reasoning
steps. The entries in the cells of this column are shown in bold, highlighting the focus of the approaches. The approaches
in this group are the start of the field of LLM-reasoning. The Chain-of-thought approach is especially an inspiration for
many works. The prompts are often written manually by the researchers for each problem; the steps are encoded in
one prompt, and step control is greedy. There is no specific evaluation of the steps, other than comparing results to
the benchmark. The Scratchpad approach is special in that it uses supervised learning, not prompt-learning; the work
showed that LLMs can generate internal reasoning steps by supervised learning, paving the way for in-context works.
The second group, from Self-verification to Self-taught-reasoner, focuses on evaluation of the reasoning steps in the
prompt. This column is shown in bold in the table. The approaches in this group aim to improve the Chain-of-thought
results by reducing error accumulation that occurs when multiple steps are taken in a reasoning chain. A variety of
step control methods is used by these approaches, which is discussed in more detail later. Note that not all approaches
use natural language problems. For example, the subgroup of Codex to Program-aided-language focuses on formal
languages. They generate code or math equations, typically in Python, to formalize the steps of the reasoning problem,
or as result of the task. LLMs are quite good at code generation [17], and these approaches typically achieve good
performance. The use of code also allows the approaches to call external programs such as interpreters and debuggers
to evaluate the correctness of the reasoning steps that are generated.
There is also a special subgroup, Refiner to Self-improvement, that uses finetuning in addition to prompt learning.
Here, new data is generated based on reasoning exemplars, which is then used to further train the model. The extra
data is often generated as a separate dataset, sometimes called critic or corrector.
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 9
Table 2. Taxonomy of approaches: Generation, Evaluation, and Control. Reported benchmark results: ‚Äô=‚Äô is absolute score, ‚Äô+‚Äô is
improvement to a baseline
Approach Domain (3.1) Step generation (3.2) Step evaluation (3.3) Step control Result
Scratchpad [101] math word hand-wr/superv - greedy/1prompt PolyEval +19%, Python +21%
Chain-of-thought [164] math word hand-written - greedy/1prompt GSM8K +39%, SVAMP +10%, ASDiv +2%, AQuA +11%,
MAWPS +14%, CSQA +1.8%, StrategyQA +0.2%
ZS-CoT [74] math word hand-written - greedy/1prompt MultiArith =89%, GSM8K =70%
Auto-CoT [184] math word model-generated - clustering MultiArith +0.3%, GSM8K +1%, AddSub +3.5%,
AQuA +0.7%, SingleEq +0.4%, SVAMP +0.6%,
CSQA +1%, StrategyQA +0%, Letter +0.7%, Coin +2.7%
Complexity [43] math word hand-written self-consistency greedy/1prompt GSM8K +7%, MultiArith +3%, Penguins +3%
Self-ask [113] math word ext knowledge LLM multi-hop q. Bamboogle =60%, 2Wiki =40%, Musique =15%
Self-verification [167] math word hand-written back-verify ensemble GSM8K +4%, SingleEq +2%, AddSub +4%,
MultiArith +3%, AQuA +3%, SVAMP +1%
Self-consistency [161] math word hand-written majority ensemble GSM8K +18%, SVAMP +11%, AQuA +12%,
StrategyQA +6%, ARC-c +4%
Codex [17] code - tool-based - HumanEval =70%
Self-debugging [21] code hand-written tool-based greedy Spider +9%, MBPP +12%, TransCoder +12%
Fun-search [120] code hand-written tool-based evolutionary alg cap set 8 =512
LLaMEa [150] code hand-written tool-based evolutionary alg BBOB +11%
MathPrompter [63] math hand-written tool-based ensemble MultiArith =92%
Program-of-thoughts [18] math word hand-wr, Codex Python+Consist. split reason/cmput GSM8K =71%, SVAMP =85%, ASDIV =85%,
AddSub =92%, MultiArith = 99%
Program-aided-lang [44] math word hand-wr, Codex NLP/Python ensemble GSM8K =72%, SVAMP =79%, ASDIV =79%,
SingleEQ =96%, SingleOP =94%, AddSub = 92%,
MultiArith = 99%, Penguins = 93%
Refiner [108] math word finetune critic model gen/crit feedback SVAMP =72%, GSM8K =78%
Self-correction [165] math word finetune corrector model gen/corr feedback MathProgSynth =24%, LexConstrGen =98%,
ToxicityControl =0.0%
Self-improvement [57] math word finetune self-assessment CoT/consistency GSM8K =82%, DROP =83%, ARC-c =90%,
OpenBookQA =94%, ANLI-A3 =68%
Say-can [2] robot model-generated external model greedy Kitchen =31%
Inner-monologue [60] robot hand-written various greedy TableTop =90%, Kitchen =60%
Self-taught-reasoner [182] math word finetune augmentation greedy/feedback CommonsenseQA =72%
Least-to-most [189] math word hand-written self-assessment curriculum SCAN =99%
Progressive-hint [186] math word model-generated self-assessment stable prompt AddSub +2%, MultiArith +0%, SingleEQ +2%,
SVAMP +3%, GSM8K +5%, AQuA +1%
Self-refine [92] math word model-generated self-assessment greedy/feedback Sentiment +32%, Dialog +49%, CodeOptim +8%,
CodeRead +28%, MathReason +0%,
AcronymGen +25%, ConstrainedGen +30%
Tree-of-thoughts [179] puzzles model-generated self-assessment BFS/DFS Game24 =74%, CreativeWriting , Crossword
Buffer-of-thoughts [176] math word thought template self-assessment buffer manager Game24 +11%, GeoShapes +20%, Checkmate +51%
Algorithm-of-thoughts [129] puzzles model-generated self-assessment in-context RL GSM8K =89%, StrategyQA =84%, Crossword
Beam-search [172] math word model-generated self-assessment Beam Search GSM8K +6%, AQuA +9%, StrategyQA +5%
ReAct [178] action external knowledge self-assessment reinf learning ALFWorld =34%, WebShop =10%
Reflexion [135] decision model-generated ext model reinf learning HumanEval =91%
Voyager [158] Minecraft model-generated Minecraft reinf learning 15 x faster
There are two approaches, Say-can and Inner-monologue, whose application domain is control of robot movement.
Robotic movement is constrained by the laws of physics (both in the body of the robot as in aspects of its environment).
The laws of physics are learned and used to ground the reasoning steps in reality (to reduce hallucination).
The third group, Least-to-most to Voyager, addresses step control (shown in bold in this column). Whereas in the
previous approaches the reasoning steps are written in a single, static, prompt, these approaches generate the steps in
multiple, dynamic, prompts. This allows control of the space of reasoning steps. Various search control approaches
are used, all in the form of an external algorithm that performs calls to the LLM with different prompts. The control
methods range from simple greedy and depth-first search to elaborate beam search and reinforcement learning schemes.
The last column of the Table summarizes reported benchmark results. The ‚Äô=‚Äô symbol indicates absolute scores on
the benchmarks, while ‚Äô+‚Äô indicates relative improvement in percentage points over a baseline LLM, typically GPT-3.5.
Results vary strongly, both between approaches and within a single approach between benchmarks. Also, different
LLMs were used, from early stage to more mature models, open and commercial, and the baselines differ. For some
benchmarks, such as the Creative Writing benchmark in Tree-of-thoughts, results are best reported qualitatively. The
Manuscript submitted to ACM
10 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
Input
Output
2 9 + 5 7
Target:
 
2 9 + 5 7 , C: 0
2 + 5 , 6 C: 1 # added 9 + 7 = 6 carry 1
, 8 6 C: 0 # added 2 + 5 + 1 = 8 carry 0
0 8 6
 
8 6
Fig. 2. Example of input and target for supervised learning on a long addition problem of adding two numbers. The carry is recorded
in the C: digit. Comments (after #) are not part of the learning target (adapted from [101])
source papers provide more measurement details, Section 4.1 discusses when to which approach to use for different
applications.
In conclusion, we see a diverse array of methods that often achieve high performance in reasoning on their respective
domains. To better understand the approaches, we discuss them in more detail, starting with the generation of steps.
3.1 Generation of Steps
Originally, LLMs performed poorly on math word problems such as GSM8K [30]. Different approaches were tried
unsuccessfully, for example scaling up the size of the LLM [116]. The LLM architecture, based on transformers, is
designed to produce a single token. When we prompt such an architecture to produce an answer, it does so. What we
should do instead, is to prompt it to follow intermediate steps, answer those, and thus work towards the final answer,
just as a student is taught to break down a complex problem into smaller steps. We should guide the model to explicitly
produce intermediate steps, and combine the intermediate results. This idea was used by Nye et al. [101] in Scratchpads,
a transformer model that performs multi-step computations by asking it to emit intermediate computation steps into a
scratchpad. They train the model by supervised learning (not prompt-based in-context learning). Figure 2 shows an
example. On experiments with addition, polynomial evaluation, and Python code execution, versions that produced the
intermediate steps on a scratchpad performed considerably better than versions that did not, going from 35% to 95%,
from 32% to 51%, and from 30% to 42% accuracy, respectively.
If supervised learning can produce intermediate steps, would prompt learning be able to do so too?
3.1.1 Hand-written Prompt. This question was studied by Wei et al. [164], amongst others. A basic way to instruct an
LLM to generate steps by prompt-learning is to manually write a prompt for the large language model to follow the
reasoning steps. When the LLM is prompted to rephrase information from the question as intermediate reasoning steps
in its answer, the LLM performed much better than when it was prompted to answer a math problem directly, without
reproducing information from the question in its answer in multiple steps. The example from the Chain-of-thought
paper is shown in Figure 3. Table 1 shows that the largest accuracy increase is on GSM8K, from 16% to 47%.
The idea that an LLM can be made to follow step-by-step instructions, and the performance improvement by
Chain-of-thought have caused much excitement and have opened up further research on reasoning with LLMs. In the
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 11

Input
Output
A: The answer is 27.
Q: Roger has 5 tennis balls. He 
buys two cans of 3 balls.
How many tennis balls does he 
have now?
A: The answer is 11.
Q: Lisa has 23 apples. 
If she ate 20 and bought 6 more,
How many apples does she have?

Input
Output
A: Lisa started with 23 apples.
She ate 20, so she has 23-20
=3 apples left. She bought 6,
so 3 + 6 = 9. The answer is 9.
Q: Roger has 5 tennis balls. He 
buys two cans of 3 balls.
How many tennis balls does he 
have now?
A: Rogers started with 5 balls.
2 cans of 3 balls each is 6 balls.
5 + 6 = 11. The answer is 11.
Q: Lisa has 23 apples. 
If she ate 20 and bought 6 more,
How many apples does she have?
Input
Q: Lisa has 23 apples. 
If she ate 20 and bought 6 more,
How many apples does she have?
Let‚Äôs think step by step.
 
Output
A: Lisa started with 23 apples.
She ate 20, so she has 23-20
=3 apples left. She bought 6,
so 3 + 6 = 9. 

 
Input
Output
A: Lisa started with 23 apples.
apples = 23
She ate 20 and the bought 6
eaten = 20; bought = 6
anwer = apples - eaten + bought 
Q: Roger has 5 tennis balls. He buys two cans of 3 
balls. How many tennis balls does he have now?
A: Rogers started with 5 balls.
tennis_balls = 5
2 cans of 3 balls each is
bought_balls = 2 * 3
tennis balls. The answer
answer = tennis_balls + bought_balls
Q: Lisa has 23 apples. If she ate 20 and bought 6 more,
How many apples does she have?
Fig. 3. Different chain-of-though (CoT) prompting techniques. At the top the prompts, at the bottom the answers. When shown the
longer example prompt, the LLM follows the longer example when answering the question (Few-Shot CoT [164]). Without example
answer and using Let‚Äôs think step by step results in similar answers (Zero-Shot CoT [74]). With Program-aided-language models [44]
similar reasoning can be achieved.
original paper the prompts were handwritten by the researchers for the individual types of problems, and evaluations
are conducted with benchmarks (not by an LLM).2In a later work the prompts were generated automatically by the
LLM [184], and evaluated.
Kojima et al. [74] go a step further. They show that the addition of a single text to the prompt (Let‚Äôs think step by
step) significantly improves performance. Since this text does not contain problem-related elements, it is as a form of
zero-shot learning. Figure 3 (third column) compares the approaches. Experiments further show that with this addition
to the prompt significant performance gains are also achieved on other reasoning benchmarks, including arithmetic,
symbolic, and logical reasoning (achieving 70% accuracy on GSM8K/PaLM when Self-consistency is also included).
The Chain-of-thought idea itself is inspired by earlier work where natural language steps are generated for arithmetic
reasoning [30, 85], and the use of formal languages for reasoning [3, 20, 22, 121].
3.1.2 Prompt using External Knowledge. Chain-of-thought prompts are written manually, by the researchers, an
approach that does not scale. We can also use external information about the problem to improve the prompt. Press
et al. [113] study how subproblems are related to the main problem, which they call compositional reasoning. They
study how often a model is able to answer the subproblems, but not the overall problem. This difference is called the
compositionality gap. They find that in GPT-3, as model size increases, the single-hop question-answering performance
improves faster than the multi-hop performance: while more powerful models memorize and recall more factual
knowledge, no improvement in their compositional reasoning occurs. The ability to reason does not depend on the size
of the model.
Subsequently, a method called Self-ask is proposed, that asks elicitive follow-up questions (like Chain-of-thought,
but with the follow up: prompt), that the model then answers. Self-ask can also use an external search engine to answer
2The Chain-of-thought idea is about prompt generation, not about the evaluation or the search control of the reasoning steps. Hence, in Table 2
Chain-of-thought is labeled as greedy without an evaluation.
Manuscript submitted to ACM
12 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
intermediate prompts, instead of the model. The initial subquestion is fed into the search engine, and the answer is
processed by the model, which generates another subquestion, and so on, until it produces the final answer. Self-ask
was tested on three benchmarks that were specifically designed for multi-hop questions. Although it performs only
a few percentage points better than vanilla Chain-of-thought, it showed how external knowledge can be used in a
reasoning setting.
3.1.3 Model-Generated Prompt. In addition to manually writing prompts or using external information, we can also
let the LLM itself study the problem to write the best reasoning-prompt. An example of such self-improvement is
Auto-chain-of-thought [184]. This approach builds on the observation by Kojima et al. [74] that large language models
are zero-shot reasoners. First, Auto-chain generates specific questions for a given dataset and partitions them into
clusters. Then an external algorithm uses the model to generate examples that are sampled for diversity. The constructed
demonstrations augment the in-context prompt. This approach also performed a few percentage points better than
hand-written Chain-of-thought prompts, on ten benchmarks, using GPT-3 (see Table 2).
Fu et al. [43] introduce Complexity-based prompting. Inspired by Chain-of-thought and Self-consistency, their work
specifically studies the impact of the complexity of the reasoning chain (the number of steps), and introduces a related
reasoning approach (Complexity-based prompting). They find that prompts with the largest complexity perform best,
and also that answers with the highest complexity are the best. Complexity-based prompting achieves somewhat higher
performance on three math reasoning benchmarks: GSM8K improves 7 points, MathQA 6 points, and the Penguins
benchmark from Big Bench Hard improve 3 percentage points.
We see that the initial approaches showed larger improvements than the later approaches. It is time to look at another
category of approaches, that focus on the evaluation of reasoning steps.
3.2 Evaluation of Steps
After discussing prompts for the generation of reasoning steps, the next stage in the generation/evaluation/control
pipeline is evaluation of the results of the steps. This stage focuses on reducing error accumulation of multi-step
reasoning chains. We will start with approaches where the same model performs step-generation and step-evaluation.
3.2.1 Self-Assessment. When LLMs are prompted to perform reasoning steps, they perform a sequence of steps and
predict multiple tokens. Performing a sequence of steps makes them sensitive to mistakes and vulnerable to error
accumulation (logical, factual, ethical, or otherwise) [167, 170]. Several methods have been developed to prevent error
accumulation. One approach is to create a new model to separately evaluate the results. Shen et al. [132] and Li et al.
[84] train an external verifier to check results. In contrast, Weng et al. [167] propose an automated approach using
evaluation by the same LLM, called Self-verification. They note that human reasoning also suffers from the problem of
accumulating errors, and that in human reasoning we frequently revisit our thought process to verify the accuracy
of our reasoning steps. The LLM is prompted to use the conclusion of the Chain-of-thought reasoning chain as a
condition for solving the original problem and then compare the answer, going back to the original question. The LLM
is given variations of its own conclusion and is instructed to choose the one with the highest similarity to the original
question. (Note that there can be feedback issues using an LLM to evaluate itself, for a discussion see Zheng et al. [187].)
Experiments are reported on GPT-3 [17] and on Instruct-GPT [102]. The accuracy of Chain-of-thought was improved
by a few percentage points on arithmetic and general reasoning tasks (GSM8K 65%, AQuA 48%, SVAMP 77%).
A popular related approach is Self-consistency [161]. Self-consistency is a straightforward ensemble approach (a
well-known machine learning technique to make a strong learner out of multiple weaker learners [13, 124]). Greedy
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 13

Input
Output
This means she uses 3 + 4 = 7 
eggs every day. She sells the 
remainder for $2 per egg, so in
total she sells 7 * $2 = $14 
per day. The answer is $14.
Q: If there are 3 cars in the parking lot and 2 more cars arrive, how 
many cars are in the parking lot?
A: There are 3 cars in the parking lot already. 2 more arrive. 
Now there are3 + 2 = 5 cars. The answer is 5.
‚Ä¶
Q: Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every
morning and bakes muns for her friends every day with four. She sells the
remainder for $2 per egg. How much does she make every day?
A:
Reasoning 1
She has 16 - 3 - 4 
= 9 eggs left. 
So she makes 
$2 * 9 = $18 per 
day.
The answer is $18.

Output
The answer is $18.
Reasoning 2
This means she 
sells the remainder
 for $2 * (16 - 4 - 3) 
= $26 per day.
The answer is $26.
Reasoning 3
She eats 3 so she 
has 16 - 3 = 13 left. 
Then she bakes,
so 13 - 4 = 9 eggs. 
9 eggs * $2 = $18.
The answer is $18.
Fig. 4. Self-Consistency (Adapted from [161]), showing majority voting
single-path decoding is replaced by sampling diverse reasoning paths, evaluating them, and selecting the most consistent
answer. Self-consistency asks the LLM to simply perform the same query multiple times, and takes the majority-vote of
the answers, or decoding paths. Self-consistency works since complex reasoning problems typically allow different
reasoning paths that lead to the correct answer. Figure 4 summarizes the approach. Self-consistency has been evaluated
on arithmetic reasoning, commonsense reasoning and symbolic reasoning, on a variety of LLMs, including GPT-3
[15, 24, 144, 145]. Self-consistency further improves the performance of Chain-of-thought by 10-20 percentage points,
and has been used as a baseline in many of the other approaches in this survey.
3.2.2 Tool-based Validation. Another approach to improve the accuracy of evaluating the reasoning steps is to switch
from a natural to a formal language. The advantage of a formal language is that it is less ambiguous than a natural
language. Examples are computer languages, such as Python, or mathematical equations. Using a formal language for
reasoning is a popular approach, and we discuss seven papers. Many approaches generate the steps in Python, and the
code can then be evaluated by a formal evaluator, such as a compiler, debugger, or interpreter.
LLMs have been quite successful in generating computer code from natural language prompts. Chen et al. [17]
introduced Codex, a GPT model that was trained on publicly available code in the repository GitHub. A production
version of this work was introduced under the name GitHub Copilot. Codex is able to generate correct programs from
descriptions in natural language, such as commentary strings.
The work on Codex is used as a basis for further research on reasoning in LLMs. Human programmers, when writing
code, typically follow a cycle of writing some code, executing it to look for errors, and then using the feedback to improve
the code. This step-by-step approach is followed in Self-debugging [21]. It follows the same steps of code generation,
Manuscript submitted to ACM
14 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
code execution, and code explanation. Self-debugging is able to identify mistakes in its own code by investigating the
execution results, and can also provide an explanation of the generated code, in natural language. It achieves strong
performance: the text-to-SQL Spider benchmark improves 9 points, and the C++ to Python Transcoder benchmark
improves by 12 percentage points.
Several works generate working code tuned for solving specific problems automatically, without human feedback.
Romera-Paredes et al. [120] introduced FunSearch, an approach that integrates formal methods and LLMs to enhance
mathematical reasoning and code generation. FunSearch uses a genetic approach with multiple populations of candidate
solutions (programs), which are evaluated using a function depending on the problem specification. In addition to
the evaluation function, also an initial program is given to the LLM in the first prompt. After evaluating a number of
generated programs from the starting prompt, a new prompt is created, in an iterative fashion, combining a selection of
sampled programs sorted according to their evaluation score, and the LLM is requested to generate a new program.
Another work leverages evolutionary computation methods to generate and optimize evolutionary algorithms [150]. This
approach, LLaMEA (Large Language Model Evolutionary Algorithm), utilizes LLMs to design and optimize evolutionary
algorithms. The approach uses LLMs to generate initial algorithmic structures, which are then refined through mutation
and selection. This enhances the efficiency of algorithm design, particularly in fields requiring innovative and adaptive
solutions, improving accuracy on the Black-Box Optimization Benchmark [51] (BBOB) suite by 11 pecentage points. A
key difference between FunSearch and LLaMEA is that LLaMEA uses a sample-efficient elitism strategy by iteratively
improving the best-so-far solution, requiring significantly fewer prompt evaluations than the large-population strategy
proposed in FunSearch. Evolutionary approaches let the LLM discover new algorithms, solving existing problems in
new ways, or solving entirely new problems. Another method, Evolution-of-heuristics [87], was proposed for evolving
code snippets for guided local search to solve combinatorial optimization problems, such as the Traveling Salesperson
Problem.
To improve prompt-based reasoning, Codex is used in an ensemble approach named MathPrompter [63]. This
approach generates multiple algebraic expressions or Python functions, which then solve the same math problem.
The results are compared, just like in Self-consistency and Self-verification, raising the confidence level in the results.
MathPrompter achieved state-of-the-art accuracy on the MultiArith dataset (78.7% ‚Üí 92.5%), evaluated on GPT-3 175B.
Two other approaches that use a formal language are Program-of-thought [18] and Program-aided-language [44].
Both approaches use the LLM to generate Python and then use an interpreter to evaluate the result. The approaches are
similar although Program-aided-language uses generic prompts, and has been tested on more benchmarks. Figure 3
(fourth column) illustrates the Program-aided-language approach. When the evaluation of the reasoning steps is
offloaded to the Python interpreter, decomposing the natural language problem into executable code-steps remains
the only task for the LLM. (Earlier work in math word problems showed how to decompose a problem and reach
an answer [85].) Gao et al. [44] provide extensive experimental evidence about the synergy between the neural LLM
and the symbolic interpreter. Experiments are performed on 13 mathematical, symbolic, and algorithmic reasoning
tasks, achieving more accurate results than much larger models (for example, Program-aided-language reported 72% on
GSM8K and 93% on Penguins).
3.2.3 External Model Validation. We have seen many examples of succesful prompt-based in-context reasoning and
evaluation (at inference time‚Äîwhere no parameters were changed). We will now look at reasoning approaches that
follow a more traditional parameter training approach. All approaches evaluate the output of the model and generate
corrective data. That data is then added to the training pipeline, and the model is subsequently finetuned.
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 15
Finetuning. The Refiner approach [108] uses a generator model and a critic model that provide fine-grained feedback
on reasoning errors. The generator generates multiple reasoning hypotheses, and the critic evaluates results by randomly
selecting a hypothesis for feedback. The generator model is then finetuned based on its reasoning errors. A small
supervised model is used to overcome the cold-start problem. The approach achieves 78% accuracy on GSM8K and also
works well on related problems.
Welleck et al. [165] follow a similar approach which they call Self-correction. Here, the corrector is a separate model
specialized in refining the outputs of the generator. Unlike Refiner, where the generator is finetuned based on the critic,
Self-correction finetunes the corrector to rectify errors in the hypotheses produced by the generator. Self-corrector is
not applied to math word problems, but to program synthesis, where a small corrector reduces toxicity to 0%.
A third finetuning approach is Self-improvement [57]. Here, too, the base model data is augmented by LLM-generated
rationales, and then finetuned. They achieve 82% accuracy on GSM8K and similarly high scores on question answering
and adversarial benchmarks. Noteworthy in all three finetuning approaches is that LLMs are capable of improving
themselves by training on their own generated output, and that stability problems from feedback loops are overcome.
Dataset Augmentation. The final finetuning approach that we discuss uses dataset augmentation. In Self-taughtreasoner [182], an intermediate reasoning is generated, called a rationale. Rationales are shown to be valuable across
diverse tasks such as mathematical and commonsense reasoning, code evaluation, social bias inference, and natural
language inference. First an augmentation dataset is created by attempting to solve the original dataset. Next, the
dataset is augmented using rationalizations and ground-truth answers to problems the model failed to solve. Finally,
the model is finetuned on the combined dataset. Self-taught-reasoner performs comparably (72%) to finetuning a 30
times larger model on CommonsenseQA.
Reasoning about Robot Behavior. In addition to math word problems, computer code, and common sense, prompt-based
reasoning has also been used to improve robot behavior. Language models contain a large amount of information about
the real world [2]. In theory, this should allow them to reason realistically about robotic behavior. However, the models
do not have knowledge about specific embodied aspects of a particular robot. If we could compare a Scratchpad-like
list of intermediate reasoning steps with a list of possible movements of the robot in its environment, then we could
prevent the model from suggesting impossible joint movements and actions, and prevent failures.
Say-can [2] learns a value function [67] of the behavior of a robot and its environment using temporal difference
reinforcement learning [139]. This value function is combined with prompt-based reasoning by the language model, to
constrain it from suggesting impossible actions. The goal of Say-can is to ground language in robotic affordances. In
contrast to Scratchpad, which used supervised learning, the affordance model is learned interactively by reinforcement
learning, and then applied using prompt-based learning on the LLM. The language model has high-level semantic
knowledge about the task (Say). The learned affordance function (Can) provides an environment-grounding on what is
possible. Say-can achieves a 31% success rate on 101 real-world robotic kitchen tasks.
Where Say-can learns affordances as a separate function, Inner-monologue [60] formulates robotic planning directly
as part of the language prompt, internally. The input consists of many elements: textual descriptions from InstructGPT
[15] for multi-step planning, scripted modules for object recognition, success detection, task-progress scene description,
and language-conditioned pick-and-place primitives, similar to CLIPort [137]. The language feedback that is thus
generated significantly improves performance on three benchmarks, achieving 90% accuracy on simulated and real
table top rearrangement tasks and 60% on the kitchen environment. There are many other studies into robotic behavior.
Manuscript submitted to ACM
16 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
An approach related to Inner-monologue is Chain-of-tools, which proposes a plan-execute-observe pipeline to ground
reasoning about tool behavior [133, 134].
This concludes our discussion of the second stage of the reasoning pipeline, evaluation of the reasoning steps.
3.3 Control of Steps
The third stage is control. This stage controls how many sub-steps are generated, and how deep into the future the
reasoning chain is generated. There are three main approaches: (3.3.1) greedy selection, which generates a step and
then follows it, (3.3.2) ensemble strategy, which generates a set of possible next steps, and (3.3.3) a (reinforcement
learning) search which generates multiple options for the steps, traversing a search tree with backtracking, controlling
an exponential search space [173].
3.3.1 Greedy Selection. Most earlier works on prompt-based reasoning follow the greedy approach: generate a single
prompt with a single sequence of steps and follow them. Among the greedy reasoners are Chain-of-thought, Auto-CoT,
and Zero-shot CoT. Inner Monologue and Say-Can also use greedy reasoning.
In Least-to-most prompting [189], the key idea is to break down a complex problem into simpler subproblems
and then solve these in sequence, explicitly encoding them in the prompt, related to Complexity-based prompting.
In Least-to-most the answers to previously solved subproblems help in finding the answer, as a curriculum [8]. On
symbolic manipulation, compositional generalization, and math reasoning, Least-to-most prompting generalizes well,
achieving 99% accuracy on a compositional generalization benchmark.
3.3.2 Ensemble Strategy. The second kind of reasoning control is based on an ensemble of (sequences of) reasoning
steps. For most problems, multiple different options exist for the next step. When all or some of these are generated and
evaluated, then the consensus result can be reported as the outcome of an ensemble of steps. Self-consistency [161] and
Self-verification [167] (in Section 3.2.1) are popular ensemble approaches to evaluate the results of reasoning steps, in
which greedy single-path decoding used in Chain-of-thought prompting is replaced by a diverse set of paths. Taking
this further, Chain-of-experts uses a mixture-of-experts ensemble for complex combinatorial problems [171]. PAL and
MathPrompter also use the ensemble approach. The ensemble approach is popular in reasoning with LLM.
3.3.3 Reinforcement Learning. In reasoning, often multiple valid steps are possible, but pursuing all possibilities over
multiple trajectories may lead to an infeasible number of possibilities. The third kind of reasoning control is to use
a full-fledged controller that can traverse a tree, or perform reinforcement learning to do so [67, 111, 140]. When
decomposing the problem, multiple alternative steps are generated that can be searched multiple steps into the future.
Then, backtracking can be performed, allowing alternative steps to be tried.
Where greedy and ensemble processes can be controlled with a prompt by the LLM, this third category is more
complex, and an external algorithm is used to control the reasoning process. The external algorithms call the LLM as a
subroutine prompting it to perform requested tasks. The external algorithm allows more complex reasoning control,
but we are now beyond prompt-based self-reasoning: control has been given to an algorithm that is external to the
LLM and external to prompt-learning.
We start our discussion of control strategies with depth-first and breadth-first search, then go to beam search, and then
to full reinforcement learning. A complex reasoning space can be traversed with a search algorithm. Tree-of-thoughts
[179] uses breadth-first or depth-first search to dynamically follow different reasoning steps. The evaluation part in
Tree-of-thoughts is performed with a prompt to the LLM. Together, the trio of generation, evaluation, and control allow
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 17
Input
Output
Input
Output
 Problem Distiller
 Meta Buer
Input
Output
Input
Output
Majority vote
.......... .......... ..........
Input
Output
..........
..........
(a) Input-Output
Prompting (IO)
(b) Chain of Thought
Prompting (CoT)
(c) Self Consistency
with CoT (CoT-SC)
(d) Tree of Thoughts (ToT) (e) Buer of Thoughts (BoT)
Fig. 5. Reasoning structure of Chain-of-Thought (b), Self-Consistency (c), Tree-of-Thoughts (d) and Buffer of Thoughts (e)
systematic exploration of the space of reasoning steps with look-ahead and backtracking. The authors compare their
approach to Chain-of-thought and Self-consistency on the Game of 24, Creative writing, and Mini crossword, achieving
an accuracy of 74% on a Game of 24 task. The other tasks are evaluated qualitatively. Figure 5 illustrates the different
reasoning structures.3
Another approach, Buffer-of-thoughts [176], goes a step towards metareasoning [45]. It introduces a meta-buffer that
stores high-level thought-templates. These thought-templates are derived from a variety of tasks. Figure 5 compares the
Buffer-of-thoughts approach to other approaches such as Chain-of-thought and Tree-of-thoughts. Buffer-of-thoughts
outperforms other methods in puzzles such as Game of 24 (by 11%) and checkmating (by 51%). Thought templates are
related to metacognition (thinking about thinking), which is further discussed in Section 4.3.3.
A related search method is Beam-search-for-reasoning [172]. When the space of possible reasoning paths is large,
Beam-search searches a promising part of this space. It uses self-evaluation to control exploration and to evaluate
(decode) reasoning steps. Beam search uses Program-aided-language models for math word problems [44]. Using
a Codex backbone [17], it surpasses the few-shot baselines by 6.34%, 9.56%, and 5.46% on the GSM8K, AQuA, and
StrategyQA benchmarks, respectively.
Reinforcement learning is another step in the sophistication of optimization algorithms. It learns by interactive
sampling, improving its policy based on rewards from the environment [140]. To use reinforcement learning, the
reasoning problem is formulated as a Markov Decision Process: the agent-algorithm creates a prompt (an action), to
sample a step (ùë°) and get an answer (state, reward) from the environment-model. The answer can then be used to
improve the prompt (next action), using the rewards to improve its policy of best actions for each state. The approaches
that use reinforcement learning also do so in the form of an external algorithm.
Progressive-hint-prompting (PHP) uses reinforcement learning to interactively improve prompts [186]. PHP calls the
LLM with dynamic prompts, using previously generated answers as hints, to progressively prompt the LLM towards
3A similarly named approach is Graph-of-thoughts [11]. Graph-of-thoughts allows more general reasoning graphs, providing a formal framework, where
the different elements can then be specified manually.
Manuscript submitted to ACM
18 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
the correct answers. It works as follows: (1) given a question (prompt), the LLM provides a base answer, and (2) by
combining the question and answer, the LLM is queried and obtains a subsequent answer. We (3) repeat operation
(2) until the answer becomes stable, as a regular policy-optimizing reinforcement learning algorithm. The authors
have combined PHP with Chain-of-thought and with Self-consistency. Using GPT-4, state-of-the-art performance was
achieved on grade school math questions (95%), simple math word problems (91%) and algebraic question answering
(79%).
Another approach that is motivated by improving answers from feedback, is Self-refine [92]. Like PHP, the LLM
generates an initial output and provides feedback for its answer, using the LLM to refine itself, iteratively. Self-refine
prompts the LLM in three ways: (1) for initial generation, (2) for feedback, and (3) for refinement, following a greedy
reasoning chain. Self-refine has been used with GPT-3.5 and GPT-4 as base LLMs, and has been benchmarked beyond
math word problems on dialogue response generation [4], code optimization, code readability improvement, math
reasoning, sentiment reversal, acronym generation, and constrained generation, showing substantial improvements
over the base models (typically around 30 percentage points, see Table 2).
Another approach that combines reinforcement learning and LLMs is ReAct [178]. Most works focus on reasoning
by the LLM, not on actions by an agent. The goal of ReAct is to combine progress in reasoning with action plan
generation. (Or, to put it differently, other approaches use RL to improve LLM-reasoning, ReAct uses LLMs to improve
RL agent policies.) ReAct uses Chain-of-thought prompt-learning as part of an RL framework that also uses external
knowledge sources (Wikipedia) and finetuning; for error reduction, grounding, and for reducing hallucination. The
framework allows hand-written prompts. On two interactive decision making benchmarks (Alfworld and WebShop),
ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively.
The ReAct work has been developed further. Reflexion [135] creates AI agents that learn by reflecting on failures
and enhance their results, much like humans do. Reflexion uses three language models: actor, evaluator, and reflector.
It works as follows: (1) an actor generates text and actions, (2) an evaluator model scores the outputs produced by
the actor, and (3) a self-reflection model generates verbal reinforcement cues to assist the actor to self-improve (see
Figure 6). For the actor, Chain-of-thought and ReAct can be used. Reflexion is evaluated on decision-making, reasoning,
and coding tasks. Improvements of 10-20 percentage points are reported.
The reinforcement learning approaches that we discussed so far‚ÄîReact, Self-refine, Tree-of-thoughts, Buffer-ofthoughts, Reflexion‚Äîuse external algorithms to manage state for the prompt improvement loop. A more elegant
solution would be to perform reinforcement learning fully in-context, within-prompt. Indeed, Krishnamurthy et al. [77]
explicitly asked the question whether LLMs can explore in context. This is the goal of the Algorithm-of-thoughts (AoT)
approach [129]. Following work by Demircan et al. [34], Lee et al. [80], Wang et al. [160] that showed that transformer
architectures can be pretrained and finetuned to perform in-context reinforcement learning, AoT aims to do so in
general LLMs such as GPT-4, Claude and Gemini 1.5. They achieve results comparable to Tree-of thoughts on GSM8K,
StrategyQA, and Crosswords. Achieving these results with in-context RL is a promising result for in-context learning.
Other search-like in-context algorithms are studied by Schultz et al. [127] and Kempinski et al. [71] (Game-of-thoughts).
Further work in games finds that LLMs struggle with the knowing-doing gap and the difference between using and
mentioning game concepts [29, 103, 151].
To conclude this overview of reinforcement learning approaches, we discuss an application in the games domain.
Voyager [158] is an agent for the game of Minecraft that uses an iterative prompting mechanism that generates code
for embodied control. The agent includes self-verifcation an a skill library to maximize exploration. The goal of is to
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 19
Fig. 6. Architecture of Reflexion [135], showing a close resemblance to the agent/environment structure of reinforcement learning
Fig. 7. Performance of Voyager in Minecraft [158]; Voyager performs well, reaching high scores by acquiring many tools
discover diverse items in Minecraft, a form of novelty search [38]. Voyager performs well, reaching high scores by
acquiring many tools (see Figure 7) 15 times faster than the baseline.
The applications of reinforcement learning in LLM reasoning are many, and the connections run deep [114]. Wang
et al. [159] use the similarity between RL timesteps and LLM reasoning steps to jointly train a value function together
with the LLM policy by optimizing the soft Bellman equation, achieving 85% accuracy on GSM8K and 81% on Alfworld.
Du et al. [37], Guo et al. [49] replace supervised finetuning by reinforcement learning, integrating it with reasoning,
achieving large efficiency gains in the training pipeline in the DeepSeek r1 and Kimi models.
Manuscript submitted to ACM
20 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
4 DISCUSSION
We have reviewed many reasoning approaches. It is now time to reflect on the approaches, to discuss limitations, and
to look for promising areas of future work. First we discuss how to choose an approach for an application. Next, we
discuss issues concerning hallucination, faithful reasoning, and scaling. Then we discuss what LLMs can and cannot do.
Then, we highlight connections with sequential decision processes, metacognition, and programming languages, and
we end with a research agenda.
4.1 Matching Methods and Applications
This survey has discussed different approaches, applications, and benchmarks. It is often difficult to directly compare
benchmark results between individual papers, due to differences in measurement setup. For example, Kamoi et al. [69]
critically compare self-reflection studies. We can provide, however, general guidance on what approach to use for
different types of applications.
For simple reasoning situations, that require a single sequence of reasoning steps, Chain-of-thought is typically used.
When the error rate is high, verification methods such as Self-consistency are popular. Indeed, this combination is used
in many modern reasoning LLMs.
For combinatorial puzzles and games that would traditionally require backtracking, tree-search or reinforcement
learning, solution methods need to be able to keep track of the enumeration state. However, an LLM without an
external algorithm often fails to keep track of this state correctly between calls to the model. An approach such as
Tree-of-thoughts, or ReAct, should be considered. Here, the state is managed externally, and the LLM is called for
application-dependent functional processing. Various in-context reinforcement learning methods are being developed
where state is managed by the LLM, as in, for example, Algorithm-of-thoughts. When performance is not sufficient,
finetuning or pretraining for the domain at hand can be used.
For complex problems, where a sequence of steps or external control fail to perform well, other reasoning approaches
are needed. We note that LLMs are good at generating computer code for many known problems [17]. Hence, when the
preceding approaches do not work, the LLM can be asked to generate a program for the problem. In this approach, the
LLM acts as a coding agent for its user. The LLM generates a program or a sequence of commands, which are then
executed by an external tool, such as an interpreter, a robot, a planner, or a logic solver. In robotics, vision-languageaction models have achieved impressive results [72], but also strong results in logistics [12], finance and medicine are
reported [112].
4.2 Hallucination, Faithfulness and Scaling
Reasoning by LLMs is not error-free, and many studies aim to provide deeper insight into the reasoning processes
in language models. Saparov and He [126] introduce a synthetic question/answer dataset designed to evaluate the
reasoning abilities of LLMs. The work showed that LLMs are capable of reasoning to a certain degree, but that Chainof-thought struggles with proof trees with a wide branching factor. In another study, Wang et al. [157] aim to increase
our understanding of how Chain-of-thought works. The authors find that the order of the reasoning steps is important.
Prompts should be relevant to the question, and coherent (steps should be in the correct order). Jin et al. [65] also study
the impact of reasoning step length on LLMs, again finding a strong positive correlation between the length of the
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 21
prompt and reasoning abilities. Next, we discuss works on errors in the Chain-of-thought approach, studying whether
the reasoning of the LLM is faithful, or that it gives the right answer for the wrong reason.
4.2.1 Faithfulness. Chain-of-thought approaches prompt a language model to take certain steps to solve the problem
that the prompt specifies. One can ask the question, whether those steps are indeed the steps that the model has
followed (faithful reasoning) or whether it took another road to arrive at the same answer (unfaithful reasoning). A few
studies measure the faithfulness of reasoning with LLMs. Lanham et al. [78] notes that just like organic reasoners, a
model‚Äôs reasoning may be post-hoc, it may be constructed after a certain conclusion has been found. By deliberately
adding mistakes to the chain of thought, the authors measure the faithfulness of the model. They find a wide variation
of post-hoc reasoning, with a tendency of larger models to be less faithful. Like regular LLMs, when not properly
grounded, (Chain-of-thought) reasoning suffers from hallucination [59].
Another study adds deliberate bias to the prompt. For example, in a multiple-choice setting, they always make answer
(A) the correct answer [147]. They find that a bias towards wrong answers can cause significant drops in accuracy,
and that models frequently generate Chain-of-though explanations rationalizing wrong answers. To address issues
of faithfulness, Lyu et al. [91], Xu et al. [174] propose Faithful-chain-of-thought. This approach involves two stages.
First, the natural language query is translated into a formal symbolic language. Second, the problem-solving stage
processes the formal language, and can explain the reasoning steps it has thus taken. For the symbolic language, Python,
Datalog, or PDDL is suggested. Another approach, mechanistic interpretability, studies methods to target individual
representations inside the LLM, to see if the expected behavior occurs in practice [9, 19, 118].
Faithfulness studies tell us more about how models reason. Further surveys on this topic are Chuang et al. [28], Luo
et al. [90], Mondorf and Plank [98], Paul et al. [109],
4.2.2 Scaling. The emergent abilities of LLMs have prompted research into the nature of scaling and reasoning with
LLMs, and, specifically, how reasoning capabilities can be transferred to smaller language models. Scaling laws of
LLMs are an active area of study, see for example [53, 54, 70]. Distillation of reasoning to smaller models can work
surprisingly well in situations with more explicit instructions, and given the computational cost of training LLMs,
there is much interest in transferring knowledge to small language models. Comprehensive surveys on knowledge
distillation are Gu et al. [48], Xu et al. [175]. For reasoning specifically, Magister et al. [93] have studied reasoning in
small language models, using a student model that learns from a teacher model, by finetuning. Other works focus on
prompt distillation for retrieval [33], recommendation [82], embodied agents [23], and LLM graph reasoning [183].
Another study related to Self-taught-reasoner focuses on explanation in small language models, also achieving good
results of knowledge transfer [83].
4.3 Limitations: What LLMs Can and Cannot do
The capabilities of LLMs are impressive. LLMs can be seen as large text-based surrogate models of the world (or the
world how we describe it on the internet), and thus allow reasoning about a large variety of contexts and problems.
Reasoning, such as in math word problems, were one of the capabilities that LLMs could not achieve, until recently. Let
us look more closely at what language models currently can and cannot do.
4.3.1 What Can LLMs Do? With the right prompt, LLMs are able to solve many of the problems in grade school math
word benchmarks and beyond. Prompt-based learning is able to perform reasoning tasks such as math word problems,
robotic movement gaming, and Python code generation, at inference time, without expensive parameter training.
Manuscript submitted to ACM
22 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
A taxonomy of generate-evaluate-control is able to describe the structure of the current LLM reasoning literature.
Furthermore, the accuracy of the reasoning chains can be improved with ensemble methods, and self-verification.
Hallucination can be reduced by grounding the model with external models, such as for robotic affordances, and
information retrieval from search engines and Wikipedia. Going a step further, using external control algorithms as
scaffolding (such as search or reinforcement learning), dynamic prompts can use the LLMs to perform complex and
dynamic reasoning patterns. Note that the reasoning control is now two layers away from the core LLM: an external
control algorithm, on top of in-context-learning, dynamically generating prompts for the LLM. This is reasoning with
prompts with LLMs, not by.
At this point, it is interesting to note the confluence of the two schools of classical artificial intelligence, symbolic
and connectionist.4 Search and reinforcement learning are rooted in the symbolic tradition, while LLMs are rooted in
the connectionist tradition. The literature in this survey combines the two traditions. High performance reasoning is
created with a (symbolic) searcher/learner on top of a (connectionist) LLM. In other fields similar combinations can be
seen (for example, AlphaFold [16, 66] and retrosynthesis of molecules [128]). The LLM helps ground symbolic reasoning
methods in language; symbolic methods help create prompts that let the LLM perform dynamic reasoning.
We note that benchmarks such as GSM8K have been central for the progress of the field, and that while reasoning
started with math word problems, the field has extended to robotics, autonomous agents, games, and most emphatically
computer code. Formal languages play an important role in the intermediate multi-step reasoning chains.
4.3.2 What Can LLMs Not Do? Now that grade school math word problems are largely solvable, harder reasoning
benchmarks in other domains are appearing [1]. Most of the reasoning capabilities exhibited by LLMs are due to the
representational powers of the transformer architecture, and how in-context learning is able to harness them. Prompt
engineering and prompt control play a crucial role in the kind of reasoning that we have seen in the papers. Models
can be instructed to write their own reasoning prompts; however, such Auto-GPT or Auto-CoT prompts need careful
evaluation, verification, and grounding in the real world, to prevent degeneration into a hallucinatory world of their
own. Models can also be instructed to interact with the world, and become the tool of external scaffolding that evaluates,
controls and improves the prompts [112]. Some of what we experience as reasoning by the LLM, is controlled by the
prompt or the scaffolding algorithm. Studies into in-context reinforcement learning aim to answer the question if
prompt learning is able get the LLM to create a prompt to exhibit dynamic reasoning by itself [34, 80, 103, 127].
Some studies on symbolic planning are critical on the abilities of LLMs [148], and show examples of planning
failures, arguing that LLMs are better used to improve heuristic elements of traditional planners, such as PDDL [68], to
strengthen traditional symbolic planning approaches.
Other works study the dangers of the size of LLMs. Bender et al. [7] mention the environmental risks associated with
the large computational training demands, as well as the difficulty of understanding the training data, for example in the
context of bias. Furthermore, there are ethical, legal, and copyright concerns regarding the data that LLMs are trained
on. Finally, to prevent putting too much trust in the outcome of LLMs, we should understand their failure modes better,
such as the well-publicized problems of hallucination (inventing facts that look right but are not). Here, mechanistic
interpretability can be used to explore LLM representations and understand where they go wrong [9, 19, 118].
4Reasoning and planning have been studied since the start of artificial intelligence, starting with logic and reasoning [100], search algorithms in puzzles
and board games [76, 110], robot planning [40], classical machine learning such as decision trees and support vector machines [13, 32, 42], through
knowledge representation and the semantic web [149]. Ever since the success of the connectionist approach [47, 79] (deep learning, including LLMs)
researchers have tried to join the two approaches.
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 23
Some of the names of the approaches surveyed in this paper are suggestive of self-awareness and self-reflective
capabilities. True (human) self-reflection, or metacognition, is still largely outside the capabilities of current LLMs.
LLMs can be prompted to reason, to take small steps, to self-evaluate, and their search process can be controlled by
an external algorithm. The self-reflective type of ‚Äúintelligence‚Äù is written into the prompt by the prompt engineer or
the control algorithm. We are unaware of any LLM that has been made to reflect on, or even control, its reasoning
processes, controlling how many reasoning steps it should take, or limiting its reasoning once the answer had become
good enough. True self-reflection remains future work, although some steps have been taken, as we will discuss next.
4.3.3 Reasoning towards Metacognition. Human thought exhibits the ability to reason about self, we are able to
think about our own thinking processes. Metacognition studies this phenomenon [153]. Prompted by the success of
Chain-of-thought and related works, metacognition has also been studied in the context of LLMs [146].
Many reasoning approaches highlight self-reflective aspects in their names and in how they work. The prompts that
prompt the models to reason are being improved with the outcome of the reasoning process, and in Buffer-of-thoughts
thought-templates are used that are derived from other reasoning processes. Wang and Zhao [162] study Metacognitiveprompting. Inspired by Chain-of-thought and Self-consistency, they create manually designed prompts to increase the
understanding of language models. Another work, again inspired by Chain-of-thought and Self-consistency, connects
psychology and LLMs. Didolkar et al. [35] study metacognitive capabilities of LLMs in mathematical problem solving,
both on GSM8K and on the harder MATH problems [52]. First, the model is prompted to find a skill name for each
problem instance in the dataset. For 7000 instances of GSM8K, 500 skill names were found by the model. Next, these
500 names are clustered down to 22 skills. They find that by using the names of these 22 skills in Chain-of-thought-like
prompts, more problems are solved than with standard Chain-of-Thought/Self-consistency/PAL prompts. Examples of
the 22 skill names are multiplication-and-addition, basic-arithmetic, subtraction, and algebra. Interestingly, the authors
find that the skill exemplar repository that is trained on a strong model (GPT-4), also down-translates to a weak model
(GPT-3). The performance of the weaker model benefits from the skill-name-enhanced prompts. Begus et al. [5] propose
a program for the study of metalinguistic abilities of LLMs. LLMs struggle with analogies [81, 97] and analogical
reasoning. Metacognitive reasoning with LLMs is still in its early stages.
4.4 LLMs that Generate Computer code
A persistent thread in the results in this survey is that LLMs are good at generating formal languages, such as PDDL,
logic, math equations, and Python. The agentic approach in which LLMs are combined with external tools such as
interpreters or debuggers often yields good performance [112]. LLMs can be used in different modes, which we will
now discuss.
When LLMs are used in direct mode, a prompt is provided with an instruction, and an answer. This is the mode in
which most casual users use an LLM.
LLMs can also be used in algorithmic mode, or Chain-of-thought mode, where an extra element is added to the prompt
that gives an example of which steps to take to arrive at the answer [129]. Lee et al. [80], Schultz et al. [127], Wang et al.
[160] show that the transformer architecture can be trained such that search and reinforcement learning algorithms
can be executed in-context. (LLMs pretrained on natural language are typically not good at in-context reinforcement
learning [103, 123]).
Manuscript submitted to ACM
24 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
We have seen that LLMs are not good at preserving state between calls [71]. External algorithms can be added to
manage the state, that call the LLM in an external mode [178, 179]. Tree-of-thoughts, and similar approaches, use the
LLM in a stateless functional way, with the prompt containing the instructions, not unlike a natural language variant of
a functional programming language such as LISP or Haskell.
Finally, an LLM can be used in code generation mode. The prompt specifies a problem, and the LLM is asked to
generate computer code (such as Python or PDDL). The code may contain variables that manage the state correctly.
The code can be executed directly, or first be checked and improved by a programmer. We expect that the use LLMs as
code generators, and prompt-engineering as natural-language-programming, will continue to grow.
4.5 Research Agenda
At the end of this discussion, we list promising topics for future work. Reasoning with LLMs is an active field of research.
It brings together elements of symbolic reasoning, connectionism, natural language, autonomous agents, affective
reasoning [14] and metacognition. First we discuss current topics for the field of LLM-reasoning itself, then we discuss
more general machine learning topics that are important for progress in LLM-reasoning, and finally we discuss more
longer term, fundamental topics.
‚Ä¢ In Context Reinforcement Learning‚ÄîSearch control beyond greedy search is often implemented as an external
reinforcement learning algorithm. Is it possible to incorporate the control stage of the reasoning pipeline into
one static prompt, as in-context reinforcement learning (ICRL)? Studies indicate that models can be trained for
decision making [34, 80], but general LLMs have not yet been shown to perform ICRL well [103, 123, 127, 151].
‚Ä¢ Code‚ÄîProgress in reasoning using formal languages and computer code has been quite promising. GitHub
Copilot is a success. Further integration of LLM-reasoning with software engineering tools is a promising area of
research that can have a large practical impact on how software is written.
‚Ä¢ Grounding‚ÄîReasoning in LLMs has been successfully applied in autonomous agents, robotics, and games. A
challenge is the grounding of the reasoning process in the environment. Retrieval augmented generative methods
(RAG) can help LLMs to actively find new information when the reasoning outcome is uncertain. RAG, search,
and agentic LLMs, are also active areas of research [33, 112, 154].
Generic topics in machine learning that also influence prompt-based reasoning research are:
‚Ä¢ Benchmarks‚ÄîProgress in LLMs depends on the availability of the right benchmarks. As the field has progressed
beyond math word problems, other benchmarks become prevalent, with more difficult and diverse tasks.
‚Ä¢ Faithfulness‚ÄîOur theoretical understanding of prompt-based reasoning with LLMs is incomplete. The research
on faithfulness highlights one example of our lack of understanding. In general, more insight into the working of
multi-step in-context learning in LLMs is dearly needed.
‚Ä¢ Smaller language models‚ÄîEfficiency is an important element for wide adoption of language models. Smaller
models have many advantages over larger models: a smaller environmental footprint, more accessible to people
and organizations with fewer computational resources, and the possibility to finetune them. Models may have
fewer parameters, or the parameters may be quantized with fewer bits. Unfortunately, with a smaller number of
parameters, the models also have less reasoning power. It is important for future work to investigate this trade-off.
Solutions to compensate the limited capabilities of smaller models are distillation of reasoning to small language
models and reinforcement learning approaches [131] to improve the efficiency of the finetuning/reasoning
pipeline [37, 49].
Manuscript submitted to ACM
Multi-Step Reasoning with Large Language Models, a Survey 25
For longer term future work, the following more fundamental questions are important:
‚Ä¢ Symbolic and Connectionist Computation‚ÄîHow can we further improve LLM-reasoning: how can LLMs benefit
from reasoning prompts based on the symbolic AI literature, and how can LLMs help ground symbolic reasoning
in language?
‚Ä¢ Multimodal World-Models with Embodied Grounding‚ÄîHow can an LLM maintain a unified, continually updated
representation that integrates text, vision, audio, and sensorimotor feedback, enabling it to reason over events
across modalities, and refine its world-model through closed perception‚Äìaction loops in real-world environments?
‚Ä¢ Norm-Sensitive and Value-Aligned Reasoning‚ÄîHow do we represent, adapt, and audit diverse cultural norms
and ethical values within the reasoning process, ensuring that each step of an LLM‚Äôs chain-of-thought respects
context-dependent moral constraints while remaining transparent and verifiable?
‚Ä¢ Reasoning in other languages than English‚ÄîThe majority of the research into LLM reasoning is for English.
Although there are efforts in creating benchmarks to test LLM capabilities in other languages5, these contain
mainly NLP tasks such as question answering (QA) and natural language inference (NLI). In addition, challenges
related to low-resource languages (languages for which very limited training data is available) have not been
addressed yet.
‚Ä¢ Reasoning in specialized domains‚ÄîIn an effort to guide development and evaluation of new methods, research in
AI has a strong focus on benchmarking: standardized datasets with a limited set of problems that are realistic
to evaluate. In the real world, however, reasoning problems also occur in more challenging contexts. A few
examples are: legal reasoning for the interpretation of case law or writing contracts; scientific reasoning for
advancing scientific fields; and medical reasoning for AI-assisted diagnostics.
5 CONCLUSION
Prompt-based in-context learning is an efficient machine learning method, requiring no parameter updates to the
LLM. While achieving good performance on language tasks, performance on reasoning tasks was lacking originally.
Reasoning tasks, such as math word problems, are typically solved in a step-by-step fashion. Recently prompts have
been developed that guide an LLM to ‚Äúthink step by step‚Äù (Chain-of-thought), and to evaluate and verify the step results.
The performance of multi-step reasoning with LLMs has improved greatly, and the field has progressed beyond math
word problems. Together, the surveyed methods allow the LLM to follow high-quality multi-step reasoning chains.
Python code or other formal languages have been used successfully to reduce the error in reasoning steps. Also, in the
field of autonomous agents and robotic action, good performance has been achieved by grounding reasoning answers
in the environment and the physical constraints of robotic movement.
For complex reasoning tasks a large number of reasoning steps may be generated. To control the size of the multi-step
reasoning space dynamically, external scaffolding algorithms can be used. Often, variations on search algorithms are
used, and especially reinforcement learning. The symbolic and connectionist AI traditions come together in reasoning
prompts and search algorithms that help LLM neural networks solve natural language math word and related problems.
Inference-time reasoning results can be used to augment finetuning, in a feedback loop.
LLMs hallucinate and suffer from bias, and their use poses different ethical dangers. In multi-step reasoning methods,
self-verification methods have been developed to reduce error-accumulation, yet ethical dangers may remain, especially
5https://github.com/NaiveNeuron/awesome-multilingual-llm-benchmarks
Manuscript submitted to ACM
26 Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas B√§ck
since reflection is not error-free. Additionally, retrieval augmentation methods ground LLM output directly in sources
such as Wikipedia.
Among the most popular reasoning benchmarks in this survey is GSM8K, which contains 8500 grade school math
word problems. With older LLMs such as GPT-3, reasoning approaches show an improvement of 20-50% points over
standard prompting methods, and some even more. Many current LLMs have since adopted the reasoning approaches.
The success of reasoning with LLMs has attracted more applications, and with them, benchmarks are diverging.
The field of reasoning with LLMs is quite new, and theoretical understanding is lacking in important areas, such as
faithful reasoning (models may sometimes find the right answer using an incorrect reasoning chain). There may be more
opportunities for efficiency gains by integrating the training of reasoning models, using reinforcement learning, both in
training and in context. Although prompt-based learning allows few-shot learning at inference time, the computational
needs of LLM pretraining and finetuning are still high, hence the interest in small language models. Reasoning skills
that work in large models can often be distilled to small models.
Human thought is capable of metacognition, we can think about our thinking process. Many of the names of the
approaches in this survey suggest a link to metacognition (Reflexion, Self-refine, Self-improvement, Inner-monologue).
The first preliminary experiments of language models that reason about their reasoning skills have appeared.
LLM-reasoning is an active field of research that shows great progress. Based on current limitations and open
questions we provide a research agenda highlighting opportunities for further progress with reinforcement learning
(for finetuning, in context reinforcement learning and self-reflection), for LLMs that generate code for external tools,
for multimodal world models, for value-aligned reasoning, and for small language models, among others.
ACKNOWLEDGEMENTS
We thank the anonymous reviewers for their valuable suggestions that have considerably improved the paper.
REFERENCES
[1] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and
challenges. arXiv preprint arXiv:2402.00157, 2024.
[2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan,
Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, 2022.
[3] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word
problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics, 2019.
[4] Arian Askari, Roxana Petcu, Chuan Meng, Mohammad Aliannejadi, Amin Abolghasemi, Evangelos Kanoulas, and Suzan Verberne. Self-seeding
and multi-intent self-instructing llms for generating intent-aware information-seeking dialogs. arXiv preprint arXiv:2402.11633, 2024.
[5] Gasper Begus, Maksymilian Dabkowski, and Ryan Rhodes. Large linguistic models: Investigating llms‚Äô metalinguistic abilities. IEEE Transactions
on Artificial Intelligence, 2025.
[6] Richard Bellman. Dynamic programming. science, 153(3731):34‚Äì37, 1966.
[7] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models
be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610‚Äì623, 2021.
[8] Yoshua Bengio, J√©r√¥me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international
conference on machine learning, pages 41‚Äì48, 2009.
[9] Leonard Bereska and Efstratios Gavves. Mechanistic interpretability for ai safety‚Äìa review. arXiv preprint arXiv:2404.14082, 2024.
[10] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms
trained on a is b fail to learn b is a. In International Conference on Learning Representations, 2024.
[11] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna 
