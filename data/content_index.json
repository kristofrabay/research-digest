{
  "https://dealflowagent.com/pevc": {
    "content_path": "../data/contents/46ea238a1f1f41a5.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:29.916215",
    "title": "AI Agents For Private Equity Funds - Automate Deal Origination And Due Diligence",
    "summary": "This page describes customizable AI Agents designed for Private Equity funds to automate deal origination, due diligence, and portfolio monitoring.\n\n**Key functionalities and agents mentioned:**\n\n*   **Deal Origination (James AI):** Monitors 1,000+ marketplaces, 4,000+ news sites, and social media 24/7 to surface and qualify deals based on search criteria, identify intent-to-sell signals, and prepare personalized outreach messages.\n*   **Due Diligence (Mary AI):** Analyzes financials, operations, and external risk signals across multiple sources, processes financial reports and filings, flags potential red flags, and structures due diligence findings.\n*   **Portfolio Growth & Monitoring (Kunal AI):** Monitors key financial, operational, and cultural indicators post-acquisition, benchmarks KPIs, surfaces integration friction points, and highlights automation opportunities.\n*   **General Capabilities:** AI scans filings, databases, and market signals to identify high-intent sellers, score and rank targets, and automate outreach. It can analyze thousands of pages instantly to flag risks and structure findings.\n\n**Integrations:** The process mentions ensuring seamless integration with tools like **PitchBook, CapIQ**, and internal databases.\n\nThe system focuses on using multi-agent systems to automate complex tasks in finance, specifically for M&A activities, aiming for increased efficiency and better insights.",
    "published_date": "2025-04-28T00:00:00.000Z",
    "score": null
  },
  "https://www.a.team/solutions/investment-due-diligence-agent": {
    "content_path": "data/contents/4d9a291c327a9c31.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.527218",
    "title": "AI Agents for Investment Firms",
    "summary": "The webpage describes the use of **AI Agents for Investment Firms**, focusing on how they can accelerate workflows and surface better insights in areas like **private equity**.\n\nKey applications mentioned include:\n\n*   **Investment Due Diligence:** Agents automatically extract KPIs, normalize financial statements, flag anomalies, and pre-populate investment committee materials. Benefits include accelerating decision-making, processing thousands of documents daily, identifying risks (like revenue recognition issues), and auto-generating first-draft IC memos.\n*   **Market Position Validation:** Agents analyze target companies against competitive landscapes and verify market size claims, providing real-time competitive intelligence and thesis risk assessment.\n*   **Portfolio Reporting:** Agents provide continuous monitoring across holdings, benchmark performance against peers, and identify optimization opportunities through a cross-portfolio KPI dashboard and early warning system.\n\nThe system utilizes a team of proprietary agents, including an **AI Data Engineer**, **AI Data Scientist**, and **AI Controller** (for traceability and compliance). The approach emphasizes rapid prototyping (1-3 days) and integration with existing systems.\n\n**Regarding the specific integrations mentioned in your query:** The page mentions **Secure API integrations** with existing systems and lists various AI solutions like **Market Data Agents**, but it **does not explicitly list** integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.",
    "published_date": null,
    "score": null
  },
  "https://www.v7labs.com/agents/ai-private-equity-due-diligence-agent": {
    "content_path": "../data/contents/165d5a5032c43e97.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:29.720870",
    "title": "AI Private Equity Due Diligence Agent",
    "summary": "The AI Private Equity Due Diligence Agent automates the first-pass diligence process for Private Equity, Growth Equity, and Venture Capital firms. It reads the entire virtual data room (VDR), extracts and normalizes financials, identifies commercial and legal risks based on the firm's playbook, and generates a structured first draft of the investment committee memo. This process can reduce diligence time from weeks to hours, saving up to 95% of the average time spent.\n\nThe agent handles various file types (PDFs, Excel, Word, PowerPoint) and complex tables, synthesizing findings into a comprehensive, cited diligence memo. It also offers other specialized finance agents, such as an **AI Financial Due Diligence Agent**, an **AI Investment Analysis Agent**, and a **Financial Valuation Agent**.",
    "published_date": "2025-11-06T00:00:00.000Z",
    "score": null
  },
  "https://www.v7labs.com/blog/private-equity-analysis-tools": {
    "content_path": "data/contents/067cd996b5947d4e.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.463698",
    "title": "Private Equity Analysis Tools: A Complete Guide",
    "summary": "The webpage discusses the evolution of Private Equity analysis tools, focusing heavily on the shift towards **Agentic AI** systems that can execute multi-step analytical workflows.\n\n**Regarding your specific query on 'agents\\_and\\_finance':**\n\nThe text explicitly details the role of **AI Agents** in finance and private equity analysis, covering several areas mentioned in your query:\n\n*   **Multi-agent systems for finance/Private Equity/Venture Capital:** The core theme is the move to \"Agentic AI systems that can execute multi-step analytical workflows the way a junior analyst would.\"\n*   **Due diligence automation:** Agents are shown automating data room analysis, extracting financials, normalizing data, and identifying red flags (e.g., the \"AI Financial Due Diligence Agent\").\n*   **Investment opportunity analysis:** An \"AI Investment Analysis Agent\" is mentioned that synthesizes reports and earnings calls to build investment theses and bull/bear cases.\n*   **Deal sourcing:** A \"Deal Screening Agent\" within V7 Go is described that extracts key metrics from pitch decks to flag companies meeting investment criteria.\n*   **Portfolio monitoring:** A \"Portfolio Monitoring Workflow\" using a \"Financial Reporting Agent\" is described to extract, normalize, and populate data from various financial reports.\n*   **Agents for data analysis/Financial report generation:** Agents are shown to extract, normalize, and reconcile financial data across multiple sources, and automated report generation (like IC memos) is mentioned in relation to DiligentIQ/ToltIQ.\n*   **Integrations (PitchBook, CapIQ, etc.):** The platform **Dili** is mentioned as being able to connect to third-party data providers like **PitchBook** and **Capital IQ** to run workflows over that combined data. Legacy platforms like **PitchBook**, **FactSet**, and **Capital IQ** are discussed as incumbents that AI challengers often integrate alongside.\n\nIn summary, the page strongly supports the concept of using agentic AI for nearly all aspects of finance, private equity, and venture capital analysis mentioned in your query, highlighting specific agents and platforms that perform these functions.",
    "published_date": "2025-12-19T00:00:00.000Z",
    "score": null
  },
  "https://www.v7labs.com/go/finance": {
    "content_path": "../data/contents/9eeb815e6ee496b8.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:29.685685",
    "title": "AI agent platform for private markets & finance",
    "summary": "The webpage describes an **AI agent platform for private markets and finance** (V7 Go) that focuses on automating deal analysis and due diligence.\n\nKey capabilities and features relevant to your query include:\n\n*   **Due Diligence Automation:** Processing entire data rooms, extracting precise metrics from Confidential Information Memorandums (CIMs) and financial statements with high accuracy (around 99%).\n*   **AI Agents for Data Analysis/Financial Report Generation:** Specific finance agents are listed, such as:\n    *   **10-K Disclosure Analysis Agent:** Automates analysis of SEC 10-K filings to find risks and compare year-over-year changes.\n    *   **Annual Report Analysis Agent:** Extracts financials, compares year-over-year changes, and finds risks.\n    *   **Cash Flow Forecasting Agent:** Automates forecasting by analyzing sales pipelines, AR/AP, and payroll data.\n*   **Deal Sourcing and Evaluation:** Uses LLMs to parse unstructured data (pitch decks, financials) and combines it with proprietary databases for predictive lead scoring to speed up deal flow management and identify promising targets.\n*   **Portfolio Monitoring:** Delivers AI-driven portfolio management by merging internal metrics with external data integration for real-time oversight, risk management, performance monitoring, and valuation.\n*   **Integration:** Offers flexible integration options via API and native connectors to plug into deal sourcing tools, data room systems, or financial modeling systems.\n\n**Regarding specific integrations mentioned in your query:**\n\n*   The page mentions importing files from **Xchanging, Crunchbase, and Affinity** for deal sourcing.\n*   It **does not explicitly mention** integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.",
    "published_date": "2025-04-21T00:00:00.000Z",
    "score": null
  },
  "https://digiqt.com/blog/ai-agents-for-venture-capital/": {
    "content_path": "../data/contents/95d2778a22aa70cd.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.065182",
    "title": "AI Agents in Venture Capital: Proven Growth Wins | Digiqt Blog",
    "summary": "AI Agents in Venture Capital are autonomous or semi-autonomous software entities powered by large language models and tool integrations that execute analyst-grade tasks like sourcing, triage, diligence, and reporting. They work by combining an LLM for reasoning, retrieval augmented generation (RAG) to ground answers in firm data, and tool access to perform actions in CRMs and data vendors.\n\n**Key features and capabilities mentioned include:**\n\n*   **Tool Connectors:** Native integrations with platforms like **PitchBook**, **AlphaSense**, Salesforce, Affinity, DealCloud, Crunchbase, Gmail, Outlook, and data warehouses like Snowflake.\n*   **Use Cases:** Sourcing, inbound triage, diligence acceleration, competitive intelligence, portfolio KPI collection, and LP reporting.\n*   **Multi-agent collaboration:** Specialist agents can hand off tasks to each other.\n*   **Benefits:** Faster time to insight, higher pipeline quality, expanded coverage, and cost savings.\n\nThe text explicitly mentions integrations with **PitchBook** and **AlphaSense** as examples of data vendors that AI agents connect with for research and analysis. It does not specifically detail agents for private equity, venture capital due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, or financial report generation in the context of the user's specific query terms, although the VC use cases cover many of these functions generally. The query also mentions **Preqin** and **CapIQ**, which are not explicitly listed in the text's integration examples, though they fall under the category of \"Data vendors and research.\"\n\n**Summary relevant to the query:**\n\nAI Agents in Venture Capital utilize LLMs and tool integrations to automate analyst tasks. They integrate with data vendors such as **PitchBook** and **AlphaSense** for research. While the text heavily focuses on VC applications like sourcing, diligence, and portfolio monitoring, it does not explicitly detail agents specifically for **private equity** or **due diligence automation** beyond the VC context, nor does it list **Preqin** or **CapIQ** integrations.",
    "published_date": "2025-09-21T21:00:01.000Z",
    "score": null
  },
  "https://www.hopkins.pe/": {
    "content_path": "data/contents/1b711d553a369e37.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.703682",
    "title": "AI Agent for Private Equity",
    "summary": "The webpage describes **Hopkins**, an AI Agent designed for **Private Equity** to automate and accelerate the entire diligence process, deal sourcing, and portfolio monitoring.\n\nIt functions as an AI partner for faster, smarter deal analysis, tackling tasks such as:\n\n*   **Deal Sourcing:** Continuous crawling of deal databases, press, and social media to surface thesis-fit targets.\n*   **Due Diligence & IC:** Analyzing data rooms (VDR Red-Flagger), auditing leadership track records, and finding customer references.\n*   **Document Analysis & Reporting:** Converting Confidential Information Memorandums (CIMs) into partner-ready investment PPT decks and two-page summaries.\n*   **Portfolio Monitoring:** Tracking KPIs, detecting variance, and building market maps.\n*   **Other Functions:** Exit prep, reviewing passed investments, fundraising readiness checks, and composing meeting briefs.\n\nHopkins integrates with data rooms, CRMs, and document stores to learn the investment playbook and deliver decision-ready outputs, significantly reducing human time spent on these tasks (e.g., VDR review reduced from 1 day to 20 minutes).\n\nWhile the query lists several specific platforms like PitchBook, AlphaSense, Preqin, CapIQ, and Bloomberg, the webpage **does not explicitly mention** integrations with these specific third-party data providers.",
    "published_date": "2025-07-10T00:00:00.000Z",
    "score": null
  },
  "https://businessinsider.com/ai-agent-fintech-auquans-pitch-deck-seed-round-2025-1": {
    "content_path": "../data/contents/830b4c368dde29c4.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.068939",
    "title": "This startup is bringing AI agents to banks and money managers including UBS, Blue Owl Capital, and T. Rowe Price. Here's the deck it used to raise $8 million.",
    "summary": "Auquan is a startup that uses generative AI agents to automate research and data processing tasks for finance firms, aiming to reduce the manual work typically done by junior analysts or bankers.\n\n**Key aspects related to your query:**\n\n*   **Multi-agent systems for finance:** Auquan uses an \"agent super orchestrator\" that breaks down jobs and organizes several \"mini agents\" to handle specific tasks, such as data searching, analysis, and report generation.\n*   **Due diligence automation:** Due-diligence reports are a major use case; the startup automates the creation of these reports for clients.\n*   **Investment opportunity analysis/Deal sourcing:** The technology is used across various divisions, including private-market investing and investment banking, to produce documents like investment committee memos and pitch books.\n*   **Agents for data analysis/Financial report generation:** The system automates gathering and processing data and putting that information into written templates (e.g., PowerPoint presentations or Google Docs).\n*   **Integrations:** Auquan pulls data from providers like **CapIQ** and **Pitchbook**, as well as public data sets and client internal file systems. (Note: **AlphaSense**, **Preqin**, and **Bloomberg** are not explicitly mentioned as integrations in the provided text, though FactSet is mentioned alongside CapIQ and PitchBook).\n*   **Clients:** Auquan has secured major clients including UBS, Blue Owl Capital, and T. Rowe Price.\n\nThe system is designed to mimic human analysts by accessing raw data, understanding user intent via templates, and automatically generating the required output.",
    "published_date": "2025-01-31T18:04:48.000Z",
    "score": null
  },
  "https://www.alpha-sense.com/solutions/financial-services/investment-and-corporate-banking/": {
    "content_path": "data/contents/9484e1a86ee4d864.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.396204",
    "title": "Give every deal an  agentic AI edge",
    "summary": "The webpage describes how **AlphaSense** provides an **agentic AI edge** for investment and corporate banking to transform tasks like pitch preparation, market and comps analysis, buyer discovery, and due diligence.\n\nKey functionalities mentioned include:\n*   **Pitch Book Preparation & Market Research:** Building pitches and ramping up on markets using AI-synthesized insights from expert call transcripts, exclusive broker research, and precedent deal notes.\n*   **Buyer and Target Identification:** Identifying high-value buyers faster using AI analysis of M&A activity patterns across 40M+ entities globally.\n*   **Transaction Comps & Valuation Analysis:** Sourcing accurate transaction comps and multiples quickly, including real-time trading analysis and sector-specific benchmarking.\n*   **Deal Prep & Due Diligence:** Accelerating deal execution with risk analysis, expert call insights on threats, and instant analysis of documents like CIMs and VDRs (\"Ask in Doc\" functionality).\n\nThe benefits highlighted are winning more mandates with deal-winning insights and executing deals faster by automating compilation tasks.\n\n**Regarding the specific terms in your query:**\n\n*   **Multi-agent systems for finance, private equity, venture capital:** The page focuses on **Investment and Corporate Banking** using AI/agentic tools, but does not explicitly detail multi-agent systems for PE/VC, though the capabilities overlap with due diligence and deal sourcing.\n*   **Due diligence automation:** Explicitly covered under \"Deal Prep & Due Diligence.\"\n*   **Investment opportunity analysis, deal sourcing:** Covered by buyer/target identification and market research.\n*   **Portfolio monitoring:** Not explicitly mentioned.\n*   **Agents for data analysis, financial report generation:** AI synthesis and data extraction (like GenGrid) are mentioned, which relates to data analysis, but specific \"financial report generation\" is not detailed.\n*   **PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations:** **AlphaSense** is the platform being advertised. It mentions using exclusive content, including **broker research** and **precedent deal notes**, which often come from sources like PitchBook or Preqin, but **direct integrations** with PitchBook, Preqin, CapIQ, or Bloomberg are **not explicitly listed** in the provided text.",
    "published_date": "2025-05-05T00:00:00.000Z",
    "score": null
  },
  "https://www.tensorway.com/projects/deal-sourcing-ai-agent-private-equity": {
    "content_path": "data/contents/f9056fa8c4b9f4c9.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.555137",
    "title": "How We Built  AI Agent That Analyzes 5,000 Investment Opportunities   in a Few Hours",
    "summary": "The webpage describes the development of an **Agentic AI Solution** for a **Private Equity Fund** to automate and enhance their **deal sourcing process**.\n\nThe solution is a **multi-agent architecture** designed for private equity workflows, featuring:\n*   **LLM Core:** Uses models like Azure OpenAI, Gemini, and Anthropic to process documents and evaluate opportunities.\n*   **Orchestration Layer:** A lead agent that assigns tasks to specialized sub-agents.\n*   **Knowledge Integration:** Connects to internal documents (using GraphRAG), investment guidelines, financial databases, CRM systems, and web data.\n*   **Specialized AI Agents:** Includes a **Research Agent** (for target identification and ranking), an **Evaluation Agent** (for standardized assessment), a **Report Agent** (for generating reports), a **Financial Extraction Agent** (for processing unstructured financial documents), and a **Presentation Agent** (for creating executive decks).\n\nThe key outcomes achieved include:\n*   Analyzing over **5,000 company profiles** in hours.\n*   **80% time reduction** for initial screening.\n*   **8X faster** investment deck preparation.\n*   **12X faster** data visualization.\n*   Eliminating manual effort on routine tasks, allowing professionals to focus on strategic analysis.\n\nThe system addresses challenges like high resource allocation, slow decision-making due to data volume, fragmented data sources, and lack of standardization in assessments.\n\n**Regarding your specific query:** The page details the use of **multi-agent systems for private equity, due diligence automation, investment opportunity analysis, and deal sourcing**. It mentions the integration of **financial databases** and the generation of **reports** and **investment decks**. However, it **does not explicitly mention** the use of agents for **venture capital**, **portfolio monitoring**, **data analysis** (beyond the specialized agents performing specific analysis tasks), **financial report generation** (though reports are created), or integrations with specific external platforms like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.",
    "published_date": "2000-01-01T00:00:00.000Z",
    "score": null
  },
  "https://www.alpha-sense.com/solutions/financial-services/private-equity/": {
    "content_path": "data/contents/2f3b423a1d69a69e.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.494266",
    "title": "From investment thesis to term sheet with speed — and confidence",
    "summary": "The webpage describes how **AlphaSense** uses **AI agents** to unify and accelerate **Private Equity (PE) workflows** from deal origination to portfolio monitoring.\n\nKey functionalities and areas covered by these agents include:\n\n*   **Market Landscaping & Deep Research:** Synthesizing insights from expert calls, broker research, filings, and diligence folders to create comprehensive sector analyses and structured reports.\n*   **Deeper Diligence:** Transforming diligence by uncovering missed signals across CIM analysis, VDR content, and market data simultaneously, allowing instant extraction of key risks and trends.\n*   **Deal Origination:** Drafting comprehensive investment memos using expert insights and market intelligence, spotting trends, and surfacing hidden signals of opportunity or red flags.\n*   **Portfolio Intelligence & Value Creation:** Providing always-on AI monitoring for competitive moves and industry shifts relevant to portfolio companies, including expert sentiment analysis and performance briefings.\n\nThe platform integrates **Tegus expert insights** and combs through **500M+ premium sources** to deliver instant intelligence, replacing general tools like ChatGPT for finance inquiries and accelerating validation of market dynamics. It also allows users to upload proprietary documents (CIMs, IC memos) for comparable metric extraction.",
    "published_date": null,
    "score": null
  },
  "https://ui.adsabs.harvard.edu/abs/2024arXiv241220138X/abstract": {
    "content_path": "data/contents/b312e7c6becbfd09.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.799645",
    "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework - ADS",
    "summary": "The webpage describes **TradingAgents**, a novel multi-agent LLM financial trading framework inspired by real-world trading firms. It features LLM-powered agents in specialized roles like fundamental analysts, sentiment analysts, technical analysts, and traders with varied risk profiles. The framework includes Bull and Bear researcher agents, a risk management team, and traders who synthesize insights from debates and historical data to make trading decisions. The goal is to improve trading performance by simulating a dynamic, collaborative environment.\n\n**It does not specifically mention or detail the use of multi-agent systems for:** private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, financial report generation, or integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": "2024-12-01T00:00:00.000Z",
    "score": null
  },
  "https://www.emergentmind.com/topics/multi-agent-llm-financial-trading": {
    "content_path": "../data/contents/0048f13e4d6b5759.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.145081",
    "title": "Multi-Agent LLM Financial Trading",
    "summary": "Multi-agent LLM financial trading systems use specialized agents powered by Language Models (LLMs) to collaboratively analyze market data and execute systematic trades. These systems feature:\n\n*   **Agent Specialization:** Roles include fundamental, technical, and sentiment analysts, traders, portfolio managers, and risk control agents.\n*   **Architectures:** They use hierarchical, team-based, or debate-driven structures (e.g., TradingAgents, FinCon).\n*   **Data Integration:** They process multi-modal inputs (structured data, text, charts) and use Retrieval-Augmented Generation (RAG) to interface with external knowledge bases.\n*   **Interaction Protocols:** Agents use structured debate, internal contests, and portfolio conferences to synthesize robust decisions and manage risk.\n*   **Optimization:** They employ techniques like constrained portfolio optimization (CVaR), reinforcement learning (RL), and self-critique for decision-making.\n\nWhile the page details architectures, data integration, and performance metrics for **financial trading**, it **does not mention** specific applications related to **private equity, venture capital, due diligence automation, deal sourcing, or integrations with PitchBook, AlphaSense, Preqin, or CapIQ.**",
    "published_date": "2025-10-15T00:00:00.000Z",
    "score": null
  },
  "https://openreview.net/pdf?id=pBkTqmhMOj": {
    "content_path": "data/contents/d3fdfeaef277d4dd.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.895666",
    "title": "",
    "summary": "The webpage describes the **Multiple Automated Finance Integration Agents (MAFIA)** framework, which focuses on building **self-healing, modular agentic AI systems for financial services**.\n\nThe system is designed to address concerns about reliability, auditability, and compliance when deploying autonomous AI agents in finance. The core methodology involves a pipeline where a **Financial Lending Assistant Agent** generates responses, which are then continuously monitored and corrected by a **Consumer Compliance Agent** and an **Financial Introspection Agent** that uses a rubric-based scoring system.\n\nKey aspects relevant to your query include:\n\n*   **Multi-agent systems for finance:** The framework explicitly uses multiple specialized agents (Lending Assistant, Compliance Agent, Introspection Agent, Security Agent, etc.) orchestrated to perform complex financial tasks.\n*   **Due diligence automation/Investment opportunity analysis/Deal sourcing/Portfolio monitoring:** While the primary *use case* demonstrated is a lending assistant, the architecture is presented as a general framework for integrating agents in financial institutions, suggesting applicability to these areas.\n*   **Agents for data analysis, financial report generation:** The system includes a **Knowledge Agent** connected to knowledge graphs and a **Data Interface Agent** for secure data lookup, supporting data-driven tasks.\n*   **Integrations:** The framework is designed to interface with various components, including **Foundation Model Interface Modules** (implying integration with various LLMs like GPT, Claude, Llama) and utilizes **RAG (Retrieval-Augmented Generation)** for domain-specific knowledge.\n\n**Note on Specific Tools:** The text mentions the general concept of integrating with external knowledge bases but **does not explicitly list integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.**",
    "published_date": "2025-06-21T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2510.04643": {
    "content_path": "../data/contents/5bd8221efb578a05.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.318080",
    "title": "Computer Science > Artificial Intelligence",
    "summary": "The webpage describes **QuantAgents**, a multi-agent financial system that integrates simulated trading. This system is designed to evaluate investment strategies and market scenarios without real risk. QuantAgents consists of four collaborating agents: a simulated trading analyst, a risk control analyst, a market news analyst, and a manager. The agents are incentivized based on performance in real-world markets and predictive accuracy in simulated trading. Experiments showed the framework achieved an overall return of nearly 300% over three years.\n\nWhile the paper focuses on a multi-agent system for finance and simulated trading, it **does not mention** private equity, venture capital, due diligence automation, deal sourcing, portfolio monitoring, financial report generation, or specific integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": "2025-10-06T00:00:00.000Z",
    "score": null
  },
  "https://smythos.com/developers/agent-development/multi-agent-systems-in-finance/": {
    "content_path": "data/contents/bfa23003073b4e2b.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.863656",
    "title": "SmythOS - Multi-agent Systems in Finance: Enhancing Decision-Making and Market Analysis",
    "summary": "Multi-agent systems (MAS) in finance involve multiple autonomous, specialized digital agents interacting to optimize tasks like algorithmic trading, risk management, and market analysis.\n\nKey applications of MAS in finance include:\n*   **Stock Market Simulations:** Testing trading strategies in risk-free virtual environments.\n*   **Portfolio Management:** Enabling adaptive investment strategies, exemplified by platforms like BlackRock’s Aladdin.\n*   **Automated Trading Systems:** Executing high-frequency trades and analyzing massive datasets, as seen with firms like Two Sigma.\n*   **Predicting Market Trends:** Identifying emerging patterns and forecasting reactions to economic events, utilized by institutions like JPMorgan Chase (DeepX).\n\nChallenges in implementing MAS include **computational complexity**, **data privacy concerns**, and the need for robust **security measures**.\n\nReal-world implementations show MAS outperforming traditional methods in hedge funds (using deep reinforcement learning frameworks) and algorithmic trading firms (with modular systems like MSPM).\n\nThe future of MAS in finance points toward advanced market prediction models, automated customer service agents, and enhanced fraud detection systems. The platform **SmythOS** is presented as a solution to simplify the development, monitoring, and scaling of these complex MAS in finance, offering features for agent collaboration, security, and scalability.\n\nThe provided text **does not mention** private equity, venture capital, due diligence automation, deal sourcing, portfolio monitoring (beyond general portfolio management), financial report generation, or specific integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/pdf/2512.02227": {
    "content_path": "data/contents/85482313bb88ddef.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.020686",
    "title": "",
    "summary": "The webpage describes the **FinAgent orchestration framework**, which maps components of a traditional algorithmic trading system to specialized AI agents to democratize financial intelligence and enable agentic trading.\n\nThe framework includes agents for:\n*   **Planning and Orchestration:** Managing the overall workflow.\n*   **Alpha Agents:** Proposing trading signals based on literature and training data summaries (e.g., momentum factors for stocks, microstructure factors for BTC).\n*   **Risk Agents:** Computing exposures, enforcing limits (volatility, leverage, drawdown), and generating risk gates.\n*   **Portfolio Agents:** Combining signals and risk diagnostics to determine final weights/positions using optimization tools.\n*   **Execution Agents:** Translating weights into orders, accounting for slippage and costs.\n*   **Backtest Agents:** Calculating performance metrics (like Sharpe ratio and drawdown) over the evaluation window, with strict protocols to prevent exposing raw test data to the LLM agents.\n*   **Memory Agent:** Recording states, prompts, and decisions using UUIDs for auditability and reproducibility.\n\nThe framework was tested on **stock trading** (hourly data, 7-stock universe) and **BTC trading** (minute data). In the stock test, the agentic strategy achieved a 20.42% return with a high Sharpe ratio (2.63) and low drawdown (-3.59%), outperforming the S&P 500 index (15.97%). In the BTC test, the strategy achieved an 8.39% return versus 3.80% for Buy-and-Hold, also with lower volatility and drawdown.\n\n**The page does not mention or discuss:** private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring (beyond trading portfolio management), financial report generation, or integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.**",
    "published_date": "2025-12-03T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2507.10448": {
    "content_path": "data/contents/0bd31ffb524904b4.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.927620",
    "title": "Computer Science > Computational Engineering, Finance, and Science",
    "summary": "The webpage describes **FinTeam**, a multi-agent collaborative intelligence system designed for comprehensive financial scenarios. It utilizes four specialized LLM agents—document analyzer, analyst, accountant, and consultant—trained with specific financial expertise to handle complex financial report generation tasks, ranging from macro- to micro-economics analysis and extensive data analysis. The system was evaluated on tasks derived from real online investment forums, achieving a 62.00% acceptance rate for generated financial reports, outperforming baseline models like GPT-4o and Xuanyuan.\n\nThe user query asks about **agents and finance**, specifically mentioning: multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, and Bloomberg.\n\n**Summary relative to the query:**\n\nFinTeam is a **multi-agent system for finance** focused on **financial report generation** and **data analysis** within comprehensive financial scenarios. It demonstrates superior performance in these areas compared to baseline models.\n\nHowever, the text **does not mention** private equity, venture capital, due diligence automation, deal sourcing, portfolio monitoring, or specific integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.\n\n**Conclusion:** The page partially answers the query by confirming the existence and success of a multi-agent system for financial analysis and report generation, but it lacks information on the specific private equity/VC applications and external platform integrations mentioned in the query.",
    "published_date": "2025-07-05T00:00:00.000Z",
    "score": null
  },
  "https://aws.amazon.com/blogs/industries/agentic-ai-in-financial-services-choosing-the-right-pattern-for-multi-agent-systems/": {
    "content_path": "data/contents/694b850dcf2c7c8f.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.989891",
    "title": "Agentic AI in Financial Services: Choosing the Right Pattern for Multi-Agent Systems",
    "summary": "The webpage discusses the application of **Agentic AI** and **multi-agent systems** in **Financial Services**, highlighting how they offer advantages over traditional generative AI.\n\nIt details three primary architectural patterns for these systems:\n1.  **Workflow Pattern (Sequential):** Used for methodical, step-by-step processes, exemplified by **Autonomous Claims Adjudication**.\n2.  **Swarm Pattern (Mesh):** Utilizes collaborative reasoning for emergent intelligence, exemplified by **Financial Research and Analysis**.\n3.  **Graph Pattern (Hierarchical):** Mirrors organizational structures with a coordinator agent, exemplified by **Intelligent Loan Application Processing**.\n\nA supplementary **Loop Pattern** is also mentioned for iterative refinement. The text also warns against anti-patterns like the **Large Singleton** (overloaded single agents) and **Agent Washing** (rebranding basic automation as agentic solutions).\n\n**Regarding your specific query about agents for private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg:**\n\nWhile the page extensively covers multi-agent systems for finance (including risk, compliance, loan processing, and general financial research), it **does not explicitly mention** agents tailored for **private equity, venture capital, due diligence automation, deal sourcing, or portfolio monitoring**, nor does it list integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**. The examples provided focus on insurance claims, loan processing, and general financial research/analysis.",
    "published_date": "2025-12-17T19:12:40.000Z",
    "score": null
  },
  "https://diligentiq.com/": {
    "content_path": "../data/contents/1f154dfd4a99cbe8.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.088082",
    "title": "Empower better decisions through AI-driven analysis",
    "summary": "The webpage describes **ToltIQ**, an AI-driven analysis platform designed to streamline **Private Equity Due Diligence**.\n\nWhile the user query is broad, focusing on **multi-agent systems for finance** covering areas like private equity, venture capital, due diligence automation, deal sourcing, portfolio monitoring, data analysis, financial report generation, and integrations with platforms like PitchBook, AlphaSense, Preqin, CapIQ, and Bloomberg, the webpage specifically details ToltIQ's capabilities in:\n\n*   **Private Equity Due Diligence Automation:** Uploading virtual data rooms (VDRs) and instantly querying financial, legal, technical, and market details.\n*   **Data Analysis & Reporting:** Document Summaries, Bulk Query function, Data Chat for analyzing Excel workbooks, and sample prompts for extracting financial data (like Income Statements) and analyzing risks.\n*   **Workflow Alignment:** Customizable prompt library aligned with financial, legal, and operational due diligence aspects.\n*   **Versatile Use Cases:** Supports CIM analysis, contract extraction, VDR reviews, **portfolio monitoring**, and Excel/CSV data cleanup.\n*   **Model Flexibility:** Secure access to OpenAI ChatGPT and Anthropic models.\n*   **Security:** SOC 2 Type II compliant, single-tenant architecture, Zero Data Retention policies.\n\n**Regarding specific query elements:**\n\n*   **Multi-agent systems:** The platform uses AI/LLMs to perform tasks, which aligns conceptually, but the term \"multi-agent system\" is not explicitly used.\n*   **Venture Capital, Deal Sourcing:** Mentioned generally under \"Versatile Use Cases\" (supporting all PE workflows), but not detailed.\n*   **Integrations (PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg):** The page mentions flexible integration options via secure API connections with VDRs and CRM systems, and that it operates as a complementary layer. However, **it does not explicitly list integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.**\n\n**Summary:** ToltIQ is an AI platform focused on automating and accelerating Private Equity Due Diligence through VDR processing, advanced querying, and workflow alignment, offering significant productivity gains. It supports portfolio monitoring but does not confirm specific integrations with the listed third-party financial data providers.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://www.v7labs.com/automations/private-equity-due-diligence-reports": {
    "content_path": "../data/contents/0e581a4d97fce5ce.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.089481",
    "title": "AI Due Diligence Analysis",
    "summary": "V7 Go is an AI solution designed to accelerate **private equity due diligence** by automating data extraction and analysis from comprehensive investment documents. It uses **AI agents** to process complex reports, extracting financial metrics, operational data, and risk factors, which can reduce review time from weeks to hours (an 85% faster process).\n\nThe platform is ideal for **Investment Professionals, Due Diligence Teams, and Portfolio Managers**. Key features include:\n*   **Comprehensive Data Extraction:** Automatically pulls financial metrics, operational KPIs, and market data.\n*   **Risk Factor Identification:** Identifies potential investment risks and regulatory concerns.\n*   **Investment Thesis Validation:** Cross-references data to validate investment assumptions.\n*   **Investment Committee Readiness:** Generates structured summaries and investment memos.\n*   **Security:** Offers enterprise-grade security, including SOC2 certification, encryption, and data governance.\n\nRegarding specific tools mentioned in your query:\n*   **PitchBook, Preqin, and CapIQ:** The FAQ section confirms that V7 Go **integrates** with popular investment platforms including **Pitchbook** and **Preqin** through APIs and structured data exports. (CapIQ is not explicitly mentioned, but integration with similar platforms is supported).\n*   **AlphaSense:** Not explicitly mentioned in the provided text.\n*   **Multi-agent systems:** The platform utilizes **multiple agents** tailored for specific tasks within the due diligence process, offering specialized AI solutions rather than relying solely on generic LLMs.\n*   **Financial Report Generation:** The platform supports generating structured summaries and investment memos for investment committees, which implies report generation capabilities.\n*   **Bloomberg integrations:** Not explicitly mentioned in the provided text.",
    "published_date": "2025-11-06T00:00:00.000Z",
    "score": null
  },
  "https://www.brownloop.com/blog/private-equity-automation/": {
    "content_path": "../data/contents/08d3957ccd4543ec.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.091165",
    "title": "Going Beyond the Manual with Private Equity Smart Workflow Automation",
    "summary": "The webpage discusses the adoption of automation, particularly using AI, RPA, and workflow engines, within private equity firms to streamline operations and improve decision-making.\n\nRegarding your specific query about **agents and finance**:\n\n*   The page mentions **Kairos by Brownloop** is an \"Agentic AI Framework\" deploying **specialized agents** (like the IC Memo Generator, Diligence Scorecard Builder, or Value Creation Plan Tracker) to streamline the investment process.\n*   It covers automation across key financial and investment workflows: **deal sourcing**, **due diligence automation**, **investment opportunity analysis**, **portfolio monitoring**, and **financial report generation** (specifically mentioning LP reporting and generating investment briefs).\n*   The text discusses using AI/ML to analyze massive datasets, uncover trends, assess risks, and forecast outcomes, which directly relates to data analysis in finance.\n\nHowever, the page **does not explicitly mention** the following specific data sources/platforms: **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg integrations.**\n\n**Summary:** The page details how private equity firms use AI agents and automation for finance-related tasks like due diligence, deal sourcing, portfolio monitoring, and report generation, but it does not confirm integration with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": "2025-06-24T05:51:00.000Z",
    "score": null
  },
  "https://copiawealthstudios.com/blog/why-ai-powered-due-diligence-is-the-new-normal-in-private-equity": {
    "content_path": "../data/contents/16e294ba25e97dcd.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.499947",
    "title": "Why AI-Powered Due Diligence is the New Normal in Private Equity",
    "summary": "The webpage discusses the increasing adoption and benefits of **AI-powered due diligence** in **private equity (PE)** and **venture capital (VC)**.\n\nKey points relevant to your query include:\n\n*   **AI in Due Diligence and Data Analysis:** Nearly all PE/VC firms (95%) use AI in investment decisions, with almost two-thirds applying it to due diligence and data analysis. AI automates financial modeling and preliminary screening, cutting deal evaluation time from weeks to days.\n*   **AI in Deal Origination:** AI platforms scan vast databases to identify hidden opportunities, score inbound deals, and surface targets based on patterns from past successes.\n*   **AI in Portfolio Monitoring:** AI transforms this function by providing real-time analytics via interactive dashboards, standardizing reporting, and automating anomaly detection to flag performance issues proactively.\n*   **Implementation:** The article suggests partnering with specialized AI service providers is often the most effective implementation route, as building in-house is slow and off-the-shelf solutions are often too generic. Rapid prototyping using synthetic data is recommended to demonstrate value quickly.\n\n**Regarding specific tools mentioned in your query:**\n\nThe text **does not mention** **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg integrations**. The focus is on the general capabilities of AI platforms and specialized partners rather than specific vendor integrations.\n\n**Summary based on the query:**\n\nThe page details how **multi-agent systems (implied through AI automation)** are being used in **finance (specifically private equity/venture capital)** for **due diligence automation**, **investment opportunity analysis**, **deal sourcing**, and **portfolio monitoring**. It emphasizes AI's role in **data analysis** and generating insights, though it **does not mention** specific tools like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg integrations.",
    "published_date": "2025-09-10T00:00:00.000Z",
    "score": null
  },
  "https://www.keye.co/": {
    "content_path": "../data/contents/b469349816ff4272.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.094753",
    "title": "Built by Investors. Engineered for  Alpha.",
    "summary": "Keye is an AI-powered due diligence platform built by private equity investors for private equity investors. It transforms raw deal files into structured, investor-ready outputs by automating data cuts, running real math, and surfacing insights quickly.\n\nThe platform focuses on:\n*   **Alpha Generation:** Helping users quickly identify bad deals to pass on and focus on winners.\n*   **Scaling Diligence:** Automating grunt work like data cleaning, anomaly detection, formatting, and modeling to handle more deal flow.\n*   **Accuracy and Transparency:** Ensuring 100% accurate analysis with audit-grade transparency, linking every output back to its raw source.\n\nWhile the page heavily details its capabilities in **due diligence automation** for **private equity**, it does not explicitly mention multi-agent systems, venture capital, deal sourcing, portfolio monitoring, financial report generation, or integrations with specific databases like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": null,
    "score": null
  },
  "https://www.docubridge.ai/articles/how-to-use-ai-for-due-diligence-in-private-equity": {
    "content_path": "../data/contents/dabe71461064e989.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.097270",
    "title": "How to use AI for Due Diligence in Private Equity",
    "summary": "The webpage discusses how Artificial Intelligence (AI) is transforming **Due Diligence in Private Equity** by:\n\n1.  **Automating Manual Processes:** AI accelerates financial modeling and analysis by extracting key data points from documents.\n2.  **Integrating and Analyzing Fragmented Data:** AI consolidates data from diverse sources (like financial statements, market research, CIMs, and data rooms) into a single platform for comprehensive analysis.\n3.  **Delivering Predictive Insights:** Machine learning algorithms generate insights to predict risks and opportunities, aiding strategic decision-making.\n\nThe article also outlines challenges in AI adoption (data quality, legacy system integration, security, and cost) and provides a three-step process for adoption: assessing needs, choosing the right tools (like DocuBridge, which integrates with sources like QuickBooks), and scaling gradually.\n\nWhile the page focuses heavily on AI for due diligence, automation, data analysis, and financial modeling within private equity, it **does not explicitly mention** multi-agent systems, venture capital, deal sourcing, portfolio monitoring, financial report generation, or integrations with specific platforms like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": "2025-04-21T00:00:00.000Z",
    "score": null
  },
  "https://cacm.acm.org/blogcacm/leveraging-ai-multi-agent-systems-in-financial-analysis/": {
    "content_path": "data/contents/9060bebcc7786ea6.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.178606",
    "title": "Leveraging AI Multi-Agent Systems in Financial Analysis",
    "summary": "AI-powered multi-agent systems (MAS) are transforming fundamental financial analysis by employing networks of specialized, autonomous software agents that collaborate to solve complex problems.\n\nThese systems are used for:\n*   **Automated Data Extraction and Structuring:** Agents use NLP to pull and organize financial data from reports and filings.\n*   **Real-Time Market and Sentiment Monitoring:** Agents track news and social media to gauge market mood.\n*   **Economic and Industry Trend Analysis:** Agents monitor macroeconomic indicators and sector-specific factors.\n*   **Scenario Simulation and Forecasting:** Systems simulate various market conditions to predict company performance.\n*   **Fraud Detection.**\n*   **Collaborative Decision-Making:** Agents share and reconcile findings to produce robust investment assessments.\n\nThe benefits include faster processing, wider data coverage, reduced human bias, and more accurate forecasting.\n\n**No answer found** regarding specific mentions of private equity, venture capital, due diligence automation, deal sourcing, portfolio monitoring, financial report generation, or integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": "2025-07-24T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2512.02227": {
    "content_path": "data/contents/0af2f6208bfd4400.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.211954",
    "title": "Computer Science > Multiagent Systems",
    "summary": "The webpage describes an **Orchestration Framework for Financial Agents** designed to democratize financial intelligence, moving from traditional algorithmic trading to agentic trading. The framework maps traditional algorithmic trading system components to various agents, including a planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and a memory agent.\n\nThe paper presents results from two trading examples:\n1. **Stock trading:** Achieved a return of $20.42\\%$, a Sharpe ratio of 2.63, and a maximum drawdown of $-3.59\\%$, outperforming the S\\&P 500 index return of $15.97\\%$.\n2. **BTC trading:** Achieved a return of $8.39\\%$, a Sharpe ratio of $0.38$, and a maximum drawdown of $-2.80\\%$, outperforming the BTC price increase of $3.80\\%$.\n\n**However, the webpage text does not mention or discuss:**\n*   Private equity\n*   Venture capital\n*   Due diligence automation\n*   Investment opportunity analysis\n*   Deal sourcing\n*   Portfolio monitoring (beyond the portfolio agent component)\n*   Agents for data analysis\n*   Financial report generation\n*   Specific integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.\n\nNo answer found.",
    "published_date": "2025-07-27T00:00:00.000Z",
    "score": null
  },
  "https://www.diligentiq.com/why-diligentiq": {
    "content_path": "../data/contents/b7083daf27c2291b.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.101984",
    "title": "",
    "summary": "ToltIQ is an AI-powered platform designed to streamline due diligence for Private Markets Investment Professionals, focusing on efficiency gains (35-85% productivity increase) and time savings (60-70% on review/analysis).\n\nThe platform offers:\n*   **VDR-Native Processing:** Contextual understanding of deal data linked directly to sources.\n*   **Workflow Alignment:** Customizable prompt library for financial, legal, and operational due diligence.\n*   **Technical Capabilities:** Bulk Query, Data Chat for Excel analysis, Document Summaries, and Quick Chat for general LLM access.\n*   **Flexible Design:** Model-Agnostic, supporting OpenAI ChatGPT and Anthropic models, with single-tenant architecture.\n*   **Use Cases:** CIM analysis, contract extraction, VDR reviews, portfolio monitoring, competitive analysis, and data cleanup.\n*   **Security:** SOC 2 Type II compliant, single-tenant architecture, robust encryption, and Zero Data Retention policies.\n\nWhile the platform supports various analysis tasks, including financial data extraction and risk identification, the user query specifically asks about **multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, and Bloomberg.**\n\nToltIQ focuses heavily on **due diligence automation, data analysis, and portfolio monitoring** within the private equity context. It mentions integration with VDRs and CRM systems via secure APIs, but **it does not explicitly mention agents for deal sourcing, investment opportunity analysis, or integrations with specific external data providers like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.**\n\nTherefore, while it covers several related areas, it does not fully address all components of the user's query regarding specific multi-agent structures or named external data integrations.\n\nNo answer found",
    "published_date": null,
    "score": null
  },
  "https://codal.com/insights/private-equity-automation-streamlining-pe-data-collection-due-diligence-and-deal-sourcing": {
    "content_path": "../data/contents/9a691532cff1c582.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.103765",
    "title": "Private equity automation: Streamlining PE data collection, due diligence, and deal sourcing",
    "summary": "The provided webpage discusses **Private Equity (PE) automation**, leveraging AI and machine learning to streamline processes like **data collection, due diligence, and deal sourcing**.\n\nWhile the user query specifically asks about **multi-agent systems for finance**, including areas like **private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, and Bloomberg**, the webpage focuses on the general application of AI/ML automation in PE.\n\nThe page mentions:\n*   **Use cases** that aid in PE data collection, due diligence, and deal sourcing.\n*   Automating the process of tracking investment exposure across multi-asset class portfolios.\n*   Automated data collection leading to better portfolio performance views and forecasting.\n*   Automated financial statements speeding up reconciliation and reducing risk.\n\nHowever, the text **does not explicitly mention \"multi-agent systems,\" \"venture capital,\" or specific integrations with external platforms like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.**\n\nNo answer found.",
    "published_date": "2023-12-17T06:32:00.000Z",
    "score": null
  },
  "https://www.brainforge.ai/blog/how-private-equity-firms-are-using-ai-to-transform-due-diligence-and-deal-flow": {
    "content_path": "../data/contents/e206bf4174c4fa14.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.745784",
    "title": "How Private Equity Firms Are Using AI to Transform Due Diligence and Deal Flow",
    "summary": "The webpage discusses how Private Equity (PE) firms are using AI to transform their operations, focusing heavily on **due diligence and deal flow automation**.\n\n**Key areas where AI is applied in PE operations:**\n\n1.  **Deal Origination:** Automating the screening, enrichment, and scoring of inbound opportunities, and identifying market signals for potential targets.\n2.  **Due Diligence and Underwriting (Highest ROI):** AI significantly cuts down the \"grunt work\" of financial modeling and preliminary screening, allowing analysts to shift from spending 90% of their time crunching numbers to spending 90% on strategic analysis. This can reduce financial modeling time by up to 90% and increase deal evaluation capacity by 50%.\n3.  **Portfolio and Fund Management:** Standardizing reporting across portfolio companies and automatically flagging performance anomalies.\n\n**Regarding the specific tools mentioned in the query:**\n\nThe text mentions the challenges of integrating AI with existing PE tech stacks (like Excel, iLevel, Chronograph) and suggests building a central data warehouse (like Snowflake) for clean data pipelines. However, the article **does not explicitly mention or discuss** the use of specific third-party platforms like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg integrations** in the context of AI agents or automation.\n\n**Summary relevant to the query:**\n\nThe page details how AI acts as a force multiplier in private equity, primarily by automating due diligence to speed up deal evaluation from weeks to days. It covers deal sourcing, due diligence automation, and portfolio monitoring. While it discusses the general need for data integration with existing systems, it **does not provide specific information** about multi-agent systems or the integration of **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.\n\n**No answer found** for the specific mention of those named platforms and multi-agent systems.",
    "published_date": "2025-06-04T20:51:41.000Z",
    "score": null
  },
  "https://arxiv.org/html/2510.03663v2": {
    "content_path": "data/contents/8eccee7fb17407a7.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.369439",
    "title": "UniDoc-Bench: A Unified Benchmark for Document-Centric Multimodal RAG",
    "summary": "The webpage introduces **UniDoc-Bench**, a large-scale, realistic benchmark specifically designed for **Document-Centric Multimodal Retrieval-Augmented Generation (MM-RAG)**.\n\n**Key aspects related to your query:**\n\n*   **Multimodal RAG and Document Understanding:** The benchmark addresses the limitations of current evaluations by focusing on real-world PDF documents (70k pages across 8 domains) that contain interleaved text, tables, and figures. It aims to evaluate systems that need to retrieve and reason over information across these modalities.\n*   **PDF Parsing and Chart/Table Extraction:** The data curation process involves parsing PDFs to extract text chunks, tables, and figures. Tables and figures are stored separately, and placeholders are inserted into the text to represent their location, facilitating multimodal grounding.\n*   **Report Generation with LLMs (MM-RAG):** The core purpose is to evaluate MM-RAG pipelines. The benchmark includes 1,600 human-verified Question-Answer (QA) pairs covering factual retrieval, comparison, summarization, and logical reasoning, requiring evidence from text, tables, and images.\n*   **Vision-Language Models (VLMs) and Multimodal Models:** The experiments compare various RAG paradigms, including text-only, image-only, multimodal joint retrieval (using models like GME), and multimodal text-image fusion. The results indicate that **text-image fusion RAG** (combining separate text and image retrievals) performs best, outperforming joint multimodal embedding approaches.\n*   **GPT-4V, Claude vision, Gemini:** While the text mentions using **GPT-4** and **Gemini-Pro** in the data synthesis pipeline (for QA generation and verification), it does not specifically benchmark or detail the performance of dedicated vision models like GPT-4V, Claude vision, or Gemini as the final RAG generator/reasoner, though they are implicitly involved in the broader VLM landscape discussed in related works.\n*   **Structured Document Output:** The evaluation metrics focus on answer **completeness** and **faithfulness**, which are crucial for ensuring the generated reports/answers are accurate and grounded in the retrieved multimodal evidence.\n\n**In summary, UniDoc-Bench provides a unified platform to benchmark MM-RAG systems that handle complex document understanding tasks involving text, tables, and charts, finding that fusing separate text and image retrieval is currently superior to joint\n\nThe user query asks for a summary related to **multimodal and generation** topics, specifically mentioning: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.\n\nThe provided webpage text is an appendix section from a paper, likely titled \"UniDoc-Bench: A Unified Benchmark for Document-Centric Multimodal RAG,\" which details dataset creation, QA synthesizing prompts, human annotation guidelines, and experimental results comparing different RAG systems (Text-only, Image-only, Multimodal (MM), and Text+Image fusion (T++I)).\n\nHere is a summary of how the text relates to the query:\n\n*   **Multimodal RAG & Vision-Language Models:** The entire document focuses on **Multimodal RAG** (Retrieval-Augmented Generation) over documents. It references several RAG systems (MM, T++I) and compares retrieval performance based on different modalities required for the answer (Text-only, Img-only, Text+Img). It also compares multimodal embeddings (Voyage vs. GME).\n*   **Document Understanding, PDF Parsing, Chart/Table Extraction:** Appendix B.3 explicitly mentions parsing PDFs into \"text chunks, images of figures, and images of tables\" using `unstructured`. Appendix B.4 provides detailed question templates for **Factual Retrieval, Comparison, Summarization, and Causal/Reasoning** questions specifically tailored for a finance domain, implying structured data extraction is a key component. Appendix F.1 discusses classifying images as \"content-rich\" (providing information not present in the text) or \"illustrative,\" which relates directly to extracting information from visual elements like charts/tables.\n*   **Report Generation with LLMs:** Appendix A mentions using **LLMs** for \"polishing grammar and improving readability\" and \"assisting in the evaluation of RAG outputs\" and \"synthesizing the QA pairs.\" While it doesn't detail *report generation* itself, it confirms the use of LLMs in the RAG pipeline for generation/assistance tasks.\n*   **GPT-4V, Claude vision, Gemini:** The text mentions using **Gemini-2.5-pro** in Appendix F.1 to classify images. It does not explicitly mention GPT-4V or Claude vision, though the context implies",
    "published_date": "2023-02-01T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/@shravankoninti/multimodal-rag-with-gpt-4-vision-and-langchain-60a6a13a92e4": {
    "content_path": "data/contents/a7051df8fb53dd33.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.404368",
    "title": "Multimodal RAG with GPT-4-Vision and LangChain",
    "summary": "The webpage describes a framework called **Multimodal RAG with GPT-4-Vision and LangChain**.\n\nThis framework combines:\n1.  **Multimodal RAG (Retrieval-Augmented Generation):** The ability to process and generate multiple data types (like text and images) while grounding responses in retrieved information.\n2.  **GPT-4-Vision (specifically using `gpt-4o-mini` in the example):** A multimodal model capable of processing both text and visual inputs (images).\n3.  **LangChain:** A tool used to build applications around language models.\n\nThe practical implementation detailed on the page involves:\n*   Using the `unstructured` library to **parse PDFs** to extract **text, tables, and images**.\n*   Using **GPT-4o-mini** to generate summaries for these extracted text, table, and image elements.\n*   Storing these elements and their summaries using a **MultiVectorRetriever** with a **Chroma** vector store.\n*   Setting up a chain to answer questions based on the retrieved context, which can include information derived from text, tables, and images.\n\nIn summary, the page details how to build a system that can understand and answer questions based on documents containing text, tables, and images by leveraging multimodal models and RAG techniques within the LangChain ecosystem.",
    "published_date": "2024-09-04T16:36:24.000Z",
    "score": null
  },
  "https://www.chitika.com/vision-models-pdf-parsing-rag/": {
    "content_path": "data/contents/56a4e11acfd0c40a.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.439771",
    "title": "Using Vision Models for Complex PDF Parsing in RAG Systems",
    "summary": "Vision models are revolutionizing PDF parsing in RAG systems by analyzing both text and layout to understand document structure, which overcomes the limitations of traditional text-based tools that struggle with complex elements like tables, multi-column layouts, and diagrams.\n\nKey aspects covered include:\n*   **Vision Language Models (VLMs):** These models treat PDFs as both images and text sources, blending visual and textual cues to maintain structure, align tables, and keep annotations intact, leading to richer datasets for retrieval through multimodal embeddings.\n*   **Multimodal Embeddings:** These integrate text, images, tables, and diagrams into a unified representation, preserving document structure and improving retrieval precision in RAG systems.\n*   **Benefits:** Vision-powered parsing leads to cleaner tables, intact diagrams, reliable data retrieval, faster insights, and fewer errors, benefiting industries like healthcare (clinical trial reports) and finance (financial documents).\n*   **Advanced Techniques:** Late interaction mechanisms are mentioned as a way to improve retrieval efficiency by delaying the fusion of text and image embeddings until query time.\n\nThe page focuses heavily on how vision models enhance PDF parsing within RAG workflows, particularly through structural preservation and multimodal data handling.",
    "published_date": "2025-03-18T12:01:45.000Z",
    "score": null
  },
  "https://www.youtube.com/watch?v=uLrReyH5cu0": {
    "content_path": "data/contents/d27374bbb7c1503c.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.470423",
    "title": "Multimodal RAG: Chat with PDFs (Images \\u0026 Tables) [2025]",
    "summary": "This page describes a tutorial video on building a **multimodal Retrieval-Augmented Generation (RAG)** pipeline using **LangChain** and the **Unstructured library**. This system allows you to query complex documents (like **PDFs containing text, images, and tables**) by harnessing the multimodal capabilities of advanced **LLMs** such as **GPT-4 with vision**. The process involves setting up Unstructured for parsing diverse document types and using LangChain to create a retrieval system that integrates both textual and visual data for comprehensive understanding and accurate responses.",
    "published_date": "2024-11-12T00:00:00.000Z",
    "score": null
  },
  "https://ai.google.dev/gemini-api/docs/document-processing": {
    "content_path": "data/contents/f7ad4549b6e74c6c.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.571339",
    "title": "Document understanding",
    "summary": "Gemini models can process PDF documents to perform document understanding, going beyond simple text extraction. This capability allows Gemini to:\n\n*   **Analyze and interpret content** including text, images, diagrams, charts, and tables, even in long documents (up to 1000 pages).\n*   **Extract information into structured output formats.**\n*   **Summarize and answer questions** based on both visual and textual elements.\n*   **Transcribe document content** (e.g., to HTML) while preserving layouts.\n\nThe page details how to pass PDF data either inline in the request or by using the **Files API** for larger documents, which is recommended for better latency and bandwidth usage.\n\nWhile Gemini can process other document types (like TXT, HTML, XML) via the Files API, it **only meaningfully understands the visual structure (charts, tables, formatting) of PDFs**.\n\nThe text also mentions that Gemini 3 models introduce granular control over vision processing with the `media_resolution` parameter and clarifies that **native text extracted from PDFs is not charged** for tokens.",
    "published_date": "2025-05-12T00:00:00.000Z",
    "score": null
  },
  "https://towardsdatascience.com/building-a-multimodal-rag-with-text-images-tables-from-sources-in-response/": {
    "content_path": "../data/contents/c1a0c137d1a31e16.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.694917",
    "title": "Building a Multimodal RAG That Responds with Text, Images, and Tables from Sources",
    "summary": "The webpage describes an **improved Multimodal Retrieval-Augmented Generation (RAG) pipeline** designed to reliably return text, images, and tables from complex source documents.\n\nKey aspects covered include:\n\n*   **Problem:** Standard multimodal RAG often fails because image captions/summaries lack the necessary context to distinguish between similar elements (like tables for \"producers\" vs. \"processors\") in complex documents.\n*   **Improved Pipeline Changes:**\n    1.  **Context-Aware Image Summaries:** Instead of just summarizing the image, the system extracts up to 200 characters of text immediately *before and after* the figure/table to create a contextually rich caption.\n    2.  **Text Response Guided Image Selection:** Image retrieval is performed *after* the textual response is generated, matching the best images to the generated text rather than directly matching the user query to image embeddings.\n*   **Implementation Details:** The process uses the **Adobe PDF Extract API** for reliable parsing of PDFs (including figures and tables), GPT-4o for quality checking images and generating summaries, and FAISS for indexing embeddings.\n*   **Results:** The tests show that this enhanced approach successfully retrieves highly relevant figures and tables for specific queries related to financial reports, research papers (like VectorPainter and CLIP distillation), demonstrating improved accuracy in multimodal retrieval.\n\nIn summary, the page details a method to build a more robust multimodal RAG system by focusing on **contextualizing visual elements** during ingestion and **decoupling image selection from the initial user query** during retrieval.",
    "published_date": "2025-11-03T00:00:00.000Z",
    "score": null
  },
  "https://vldb.org/cidrdb/papers/2025/p13-anderson.pdf": {
    "content_path": "../data/contents/4fb30eee230ac0f1.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.152050",
    "title": "",
    "summary": "The provided web page describes **Aryn**, an LLM-powered unstructured analytics system designed to perform complex, semantic analyses over large collections of unstructured documents, going beyond the capabilities of standard Retrieval-Augmented Generation (RAG).\n\nKey aspects relevant to your query include:\n\n*   **Document Understanding and Parsing:** Aryn uses a component called **DocParse**, which leverages **vision models** (like a custom Deformable DETR model trained on DocLayNet) to convert raw documents (like PDFs) into structured representations called **DocSets**. This process includes **chart/table extraction** and general document layout segmentation.\n*   **Multimodal Processing:** The system handles complex documents containing text, images, figures, and tables, using vision models for structure extraction and potentially multi-modal LLMs for image summarization.\n*   **Report Generation with LLMs:** One of the key use cases mentioned is **Report Generation and Business Intelligence (BI)**, where LLM-powered document pipelines generate summaries or extract structured datasets from document collections.\n*   **LLM Integration:** LLMs are used extensively throughout the system, particularly in the **Luna** query planner to translate natural language queries into executable plans (Sycamore scripts) and for semantic operators like `llmExtract` and `llmFilter`.\n*   **Comparison to RAG:** The paper explicitly shows that Aryn (Luna) achieves significantly better accuracy (67% correct vs. 6.7% for RAG) on complex analytical queries over NTSB reports, especially when the required information is spread across multiple documents, which often causes RAG systems to fail or be poisoned by boilerplate text.\n\nWhile the text discusses the underlying technology that enables multimodal analysis, document understanding, and report generation using LLMs, it **does not explicitly mention** specific commercial models like **GPT-4V, Claude vision, or Gemini** by name, although it notes that Sycamore supports LLMs from OpenAI and Anthropic.",
    "published_date": "2024-12-12T00:00:00.000Z",
    "score": null
  },
  "https://huggingface.co/learn/cookbook/en/multimodal_rag_using_document_retrieval_and_vlms": {
    "content_path": "data/contents/35c175b8c80b4450.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.714872",
    "title": "Multimodal Retrieval-Augmented Generation (RAG) with Document Retrieval (ColPali) and Vision Language Models (VLMs) - Hugging Face Open-Source AI Cookbook",
    "summary": "This webpage details how to build a **Multimodal Retrieval-Augmented Generation (RAG)** system by combining the **ColPali** retriever for document retrieval with the **Qwen2-VL** Vision Language Model (VLM).\n\nThe process demonstrated involves:\n1.  **Installing dependencies** (including `byaldi`, `pdf2image`, and `poppler-utils`).\n2.  **Loading a dataset** (IKEA assembly instructions in PDF format) and **converting the PDFs into images**.\n3.  **Initializing the ColPali Document Retrieval Model** (`vidore/colpali-v1.2`) and **indexing the documents**.\n4.  **Retrieving relevant documents** (images) based on a text query (e.g., \"How many people are needed to assemble the Malm?\").\n5.  **Initializing the Visual Language Model (VLM)**, specifically **Qwen2-VL-7B-Instruct**, for question answering.\n6.  **Assembling the system** by feeding the retrieved images and the user query to the VLM to generate an answer.\n\nThe overall goal is to create a system that enhances query responses using both text-based documents and visual data from documents, bypassing complex OCR pipelines by using a document retrieval model. The page provides a complete, reusable function (`answer_with_multimodal_rag`) to execute this pipeline.",
    "published_date": null,
    "score": null
  },
  "https://www.ibm.com/think/tutorials/build-multimodal-rag-langchain-with-docling-granite": {
    "content_path": "data/contents/9e727a3514e04b30.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.805647",
    "title": "Build an AI-powered multimodal RAG system with Docling and Granite",
    "summary": "This page describes how to build an AI-powered **multimodal Retrieval-Augmented Generation (RAG)** system using **IBM Docling** and **IBM Granite** models.\n\nThe tutorial covers:\n*   **Multimodal RAG:** Explaining that RAG can be extended beyond text to use multimodal LLMs (MLLMs) to process data like images, in addition to text. It mentions popular MLLMs like Gemini, Llama 3.2, GPT-4, and GPT-4o, and notes that the tutorial uses an IBM Granite model.\n*   **Document Preprocessing:** Using **Docling** to parse documents (like PDFs), extract text and images, chunk the text, and use a Granite MLLM to generate descriptions for the images. Tables are converted to markdown.\n*   **Vector Database Population:** Storing the text chunks, table markdown, and image descriptions (vectorized using a Granite Embeddings model) into a Milvus vector database.\n*   **RAG Pipeline:** Setting up a RAG chain using **LangChain** to retrieve relevant document chunks (including image descriptions) based on a user query and use a Granite language model to generate an answer grounded in that context.\n\nThe system specifically focuses on answering real-time user queries from unstructured data in a PDF by combining vision and text understanding.",
    "published_date": null,
    "score": null
  },
  "https://medium.com/@haliterdoan/yet-another-pdf-parsing-article-using-llms-openai-o1-vs-deepseek-r1-41fa97550b20": {
    "content_path": "../data/contents/db43e661a1aa7676.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.314079",
    "title": "Yet Another PDF Parsing Article using LLMs (OpenAI o1 vs Deepseek R1)",
    "summary": "The webpage details an experiment comparing two reasoning Large Language Models (LLMs), **OpenAI o1** and **DeepSeek R1**, for the task of **PDF parsing** to extract structured data, which is critical for **multimodal RAG** systems.\n\nThe approach involved:\n1.  **Preprocessing with Spatial Analysis:** Using `pdfplumber` to extract text blocks and their coordinates from a messy test report PDF. Spatial relationships (distance and angle) between blocks were calculated to determine alignment (rows/columns) and create contextual \"links\" stored in a markdown table.\n2.  **Leveraging Reasoning LLMs:** The LLMs were prompted with a system instruction, a **GraphQL schema** defining the desired structured output, and the markdown table of spatial links.\n\n**Conclusion:** **DeepSeek R1** significantly outperformed the much more expensive **OpenAI o1** (which cost 30 times more). DeepSeek correctly mapped all fields and produced clean, structured output, while GPT-4 mixed up some fields. The author concludes that spatial analysis preprocessing is powerful, and DeepSeek R1 is a cost-effective and accurate game-changer for unstructured data tasks like PDF parsing.",
    "published_date": "2025-01-25T09:32:17.000Z",
    "score": null
  },
  "https://build.nvidia.com/nvidia/build-an-enterprise-rag-pipeline": {
    "content_path": "../data/contents/c1a493d63a389a8f.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.158228",
    "title": "Build an Enterprise RAG Pipeline Blueprint",
    "summary": "The NVIDIA AI Blueprint for Retrieval-Augmented Generation (RAG) supports **multimodal data ingestion**, including **multimodal PDF data extraction** for text, tables, charts, and infographics. It also features **Optional Vision Language Model (VLM) Support in answer generation** and **Opt-in image captioning with vision language models (VLMs)**.\n\nWhile the blueprint explicitly mentions support for extracting data from **tables** and **charts** as part of its multimodal data ingestion capabilities, and uses models like **PaddleOCR** for potential text extraction from images/documents, it **does not explicitly mention** specific models like GPT-4V, Claude vision, or Gemini, nor does it detail a specific process for **report generation with LLMs** or **structured document output** beyond the extraction of elements like tables and charts.",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/abs/2411.04952": {
    "content_path": "../data/contents/51c1b922375081ba.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:37.237672",
    "title": "Computer Science > Computer Vision and Pattern Recognition",
    "summary": "The webpage describes **M3DocRAG**, a novel multi-modal Retrieval-Augmented Generation (RAG) framework designed for understanding multi-page and multi-document contexts.\n\nKey aspects relevant to your query include:\n\n*   **Multi-modal RAG:** It uses a multi-modal retriever and a Multi-modal Language Model (MLM) to handle various document contexts and question types (single-hop and multi-hop).\n*   **Handling Visual Information:** It preserves visual information, addressing the limitation of text-based RAG methods that rely solely on OCR and ignore visual elements like figures, charts, and tables.\n*   **Document Understanding:** It is designed to answer questions that require information across different pages or documents, which is difficult for standard MLMs that have limited context windows.\n*   **Vision Models Mentioned:** The framework achieves superior performance using models like **ColPali** and **Qwen2-VL 7B** (a vision-language model).\n\nWhile the page focuses on a specific RAG framework (M3DocRAG) and a new benchmark (M3DocVQA), it directly relates to **Vision-language models**, **multimodal RAG**, and **document understanding** (including handling charts/figures). It does not explicitly detail report generation with LLMs, PDF parsing tools, or mention GPT-4V, Claude vision, or Gemini by name, though the underlying concepts are related to the broader field you listed.",
    "published_date": "2024-11-07T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2509.11937v1": {
    "content_path": "../data/contents/8cc36beb83bccc48.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.230669",
    "title": "MMORE: Massive Multimodal Open RAG & Extraction",
    "summary": "The web page describes **MMORE (Massive Multimodal Open RAG & Extraction)**, an open-source pipeline designed for scalable ingestion, transformation, and retrieval of knowledge from heterogeneous document formats for use with Large Language Models (LLMs).\n\n**Key features and capabilities relevant to your query:**\n\n*   **Multimodal Support:** MMORE supports over fifteen file types, including text, tables, images, emails, audio, and video, processing them into a unified format for downstream LLM applications.\n*   **Document Understanding & Parsing:** It integrates extraction tools for tasks like **PDF parsing** (using tools like Surya) and handles various document formats (DOCX, PPTX, spreadsheets).\n*   **RAG (Retrieval-Augmented Generation):** It features a robust RAG pipeline with hybrid dense-sparse retrieval, supporting both interactive APIs and batch endpoints. Evaluation on PubMedQA showed that MMORE-augmented LLMs improve biomedical QA accuracy with increasing retrieval depth.\n*   **Scalability and Performance:** The architecture is modular and distributed (built on Dask), demonstrating a 3.8-fold speedup over single-node baselines in distributed mode and achieving 40% higher accuracy than Docling on scanned PDFs.\n*   **Structured Output:** The processing module standardizes heterogeneous content into a unified JSON-based format called `MultimodalSample`, which interleaves text with modality placeholders (e.g., for images/charts) to preserve context linkage.\n\nWhile the page discusses the general architecture for handling multimodal data and extraction, it does **not** specifically mention or benchmark proprietary models like **GPT-4V, Claude vision, or Gemini** for vision tasks, nor does it detail specific methods for **chart/table extraction** beyond general document understanding, although it supports spreadsheet formats and mentions layout accuracy. It focuses on the open-source pipeline infrastructure itself.",
    "published_date": "2025-05-26T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2509.02123v1": {
    "content_path": "../data/contents/fe421194b1ed9afd.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.163727",
    "title": "CMRAG: Co-modality–based document retrieval and visual question answering",
    "summary": "The webpage describes **CMRAG (Co-modality–based document retrieval and visual question answering)**, a novel Retrieval-Augmented Generation (RAG) framework designed to handle multimodal documents by simultaneously leveraging both **text and image** information.\n\nKey aspects related to your query:\n\n*   **Multimodal RAG/Vision-Language Models (VLMs):** CMRAG is a multimodal RAG approach that addresses the limitations of pure text-based RAG (which ignores images) and pure vision-based RAG (which ignores precise text semantics). It uses VLMs (like Qwen2.5-VL-7B-Instruct) for document parsing and final answer generation.\n*   **Document Understanding/PDF Parsing/Chart/Table Extraction:** The framework performs structured parsing on documents to obtain co-modality representations, including the entire page image, parsed sub-images (capturing localized elements like figures/tables), and extracted text.\n*   **Report Generation with LLMs (VLM):** The retrieved co-modality evidence (text and image) is fed into a VLM to generate the final answer, enabling cross-modal reasoning.\n*   **Performance:** Experiments show that CMRAG significantly outperforms pure-vision-based RAG, particularly when integrating parsed text with entire images, which provides complementary grounding for accurate reasoning.\n\nThe paper focuses on improving **Visual Document Question Answering (VQA)** by unifying text and image modalities within the RAG pipeline.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://www.xugj520.cn/en/archives/rag-anything-multimodal-document-processing.html": {
    "content_path": "../data/contents/0d2f15aa306704a8.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:37.163637",
    "title": "RAG-Anything: The Complete Guide to Unified Multimodal Document Processing",
    "summary": "RAG-Anything is a revolutionary **multimodal RAG system** designed for unified document processing, capable of understanding and querying complex documents containing diverse content types like text, images, tables, and formulas simultaneously.\n\nKey features relevant to your query include:\n\n*   **Multimodal Document Processing:** It handles diverse formats (PDF, Office, Images) by decomposing content into text blocks, images, tables, and formulas.\n*   **Specialized Processors:** It uses dedicated processors for visual content analysis, structured data interpretation (tables), and mathematical expression parsing.\n*   **Knowledge Graph Construction:** It transforms multimodal content into a structured semantic network to enable retrieval based on deep semantic relationships.\n*   **Modal-Aware Retrieval:** It employs a hybrid retrieval strategy (Vector-Graph Fusion) that considers content type relevance.\n\nWhile the page details a system that *processes* multimodal content, it does not explicitly mention or compare specific large language models like **GPT-4V, Claude vision, or Gemini** for the generation or understanding tasks, nor does it focus specifically on **report generation with LLMs** beyond the general RAG output. It does support **structured document output** via its structured extraction engine and knowledge graph.",
    "published_date": "2025-06-18T00:44:42.000Z",
    "score": null
  },
  "https://milvus.io/ai-quick-reference/how-is-multimodal-rag-used-in-document-understanding-systems": {
    "content_path": "../data/contents/ccaceafcadb54539.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:37.140170",
    "title": "How is multimodal RAG used in document understanding systems?",
    "summary": "Multimodal RAG (Retrieval-Augmented Generation) enhances document understanding systems by integrating multiple data types—such as text, images, tables, and diagrams—into a single framework, moving beyond traditional text-only retrieval.\n\nIt works by using encoders to convert different data formats into a shared embedding space, allowing the system to search and retrieve relevant information across modalities before synthesizing a response.\n\nPractical applications include:\n*   **Processing Scanned Invoices/Forms:** Extracting structured data (invoice numbers from text, payment terms from tables) and validating visual cues (logos, signatures).\n*   **Academic Research Analysis:** Analyzing a paper's text, equations, and figures to answer questions about methodology by linking visual elements to textual descriptions.\n\nImplementation typically involves combining separate encoders for each data type (e.g., BERT for text, ResNet for images), a fusion mechanism, and storing embeddings in a vector database (like FAISS) for retrieval, with a generator model (like GPT) producing the final answer. This requires aligning embeddings across modalities, often using vision-language models like CLIP.",
    "published_date": null,
    "score": null
  },
  "https://www.aimodels.fyi/papers/arxiv/m3docrag-multi-modal-retrieval-is-what-you": {
    "content_path": "../data/contents/e996fb91bf3982b4.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.166991",
    "title": "M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding | AI Research Paper Details",
    "summary": "The M3DocRAG framework is a new system designed for **multi-modal, multi-page, and multi-document understanding** that utilizes **retrieval-augmented generation (RAG)**. It excels at integrating information from various sources, including different **modalities** like text, images, and tables, across multiple documents to answer complex questions and perform reasoning tasks. The paper demonstrates that M3DocRAG achieves state-of-the-art performance on benchmarks for multi-document question answering and multi-modal reasoning, outperforming models that only consider a single document or modality.",
    "published_date": "2024-11-14T00:00:00.000Z",
    "score": null
  },
  "https://m3docrag.github.io/": {
    "content_path": "../data/contents/cca48fc81aa82f56.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:37.188733",
    "title": "M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding",
    "summary": "The webpage describes **M3DocRAG**, a novel **multi-modal Retrieval-Augmented Generation (RAG)** framework designed for understanding **multi-page and multi-document** contexts.\n\nKey aspects relevant to your query:\n\n*   **Multimodal RAG and Document Understanding:** M3DocRAG uses **multi-modal retrieval** and a **Multi-modal Language Model (MLM)** (like Qwen2-VL) to answer questions from documents, effectively handling visual information (charts, figures) that text-based RAG or single-page MLMs often ignore.\n*   **Handling Long Documents/Multiple Pages:** It is specifically designed to overcome the limitations of MLMs that cannot handle many long documents by retrieving relevant pages first.\n*   **Chart/Table Extraction & Report Generation:** While the focus is on VQA, the framework explicitly mentions preserving visual information, including **charts** and **tables**, as evidence modalities. The overall process leads to an answer generation component (using an MLM).\n*   **Vision-Language Models (GPT-4V, Claude vision, Gemini):** The framework utilizes MLMs for the final question-answering step, citing **Qwen2-VL 7B** as a successful component, demonstrating the application of vision-language models in this RAG pipeline.\n*   **PDF Parsing:** The framework operates on **PDF documents** and uses visual embedding extraction (with ColPali) to represent each page.\n\nIn summary, M3DocRAG is a multi-modal RAG system that integrates visual understanding (using MLMs) with efficient retrieval across many pages/documents to answer complex questions, addressing limitations in traditional document understanding pipelines.",
    "published_date": null,
    "score": null
  },
  "https://docs.aws.amazon.com/nova/latest/userguide/rag-multimodal.html": {
    "content_path": "../data/contents/e72f593e3bf1c86e.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:37.212566",
    "title": "Using Amazon Nova for Multimodal RAG",
    "summary": "Amazon Nova can be used to build **multimodal RAG systems** capable of searching documents like **PDFs, images, or videos**. This allows for RAG systems with **mixed data containing both text and images**.\n\nYou can create a multimodal RAG system using Amazon Nova in two main ways for creating the vector database:\n\n1.  **Creating a vector database using multimodal embeddings:** This involves parsing documents into text, tables, and images, and then using a multimodal embeddings model (like Titan multimodal embeddings) on the parsed content.\n2.  **Creating a vector database using text embeddings:** This involves using Amazon Nova via the Converse API to convert images into detailed text descriptions (using a specific narrator prompt) and then using a text embeddings model (like Titan Text Embeddings V2). This approach is suggested for documents like slides and infographics.\n\nAfter setting up the database, inference involves querying the database, sending the retrieved content (ideally in its **original modality**, like returning images) back to Amazon Nova, and having Amazon Nova respond to the original user query using the retrieved context. This can be done via Amazon Bedrock Knowledge bases or a custom RAG system using the Converse API.",
    "published_date": null,
    "score": null
  },
  "https://hal.science/hal-05322313/document": {
    "content_path": "../data/contents/6181ef4e0504daa8.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.170638",
    "title": "",
    "summary": "The webpage provides a comprehensive survey on **Multimodal Agentic Retrieval-Augmented Generation (MMA-RAG)**.\n\nThis paradigm represents an advancement over traditional RAG by empowering Large Language Models (LLMs) to integrate and reason over diverse data types, including **text, images, audio, and structured data**, using autonomous agents.\n\nKey aspects covered include:\n\n*   **Evolution:** Tracing the path from text-only RAG to Multimodal RAG and Agentic RAG, culminating in MMA-RAG.\n*   **Architectures:** Detailing core patterns like Hierarchical Multi-Agent Organization, Cross-Modal Retrieval and Fusion, and Agentic Capabilities (planning, tool use, iterative refinement).\n*   **Applications:** Highlighting uses in **Document Understanding** (e.g., MDocAgent), **Healthcare** (e.g., radiology report generation, clinical VQA), **Sports Analytics**, **Scientific Exploration**, and **Embodied AI**.\n*   **Challenges:** Discussing issues related to **Cross-Modal Alignment**, **Scalability/Efficiency**, and **Evaluation Fragmentation**.\n\nWhile the text discusses document understanding, PDF parsing, chart/table extraction, and report generation with LLMs as key application areas and challenges within MMA-RAG, it **does not specifically mention or survey commercial models like GPT-4V, Claude vision, or Gemini** as primary subjects of analysis, although it discusses the general capabilities these models enable. The focus is on the *frameworks* and *architectures* (like HM-RAG, MDocAgent, CAL-RAG) that implement these multimodal and agentic capabilities.",
    "published_date": "2025-10-23T00:00:00.000Z",
    "score": null
  },
  "https://community.openai.com/t/gpt-4o-vision-for-extraction-of-complex-tables/1138461": {
    "content_path": "../data/contents/0b5be6392d50ed99.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.352518",
    "title": "GPT-4o-vision for extraction of complex tables - Prompting - OpenAI Developer Community",
    "summary": "The user is asking for suggestions on well-defined prompts for **`gpt-4o vision`** to accurately extract **complex tables** (those with grouped columns and rows) from PDFs as **Markdown format**.\n\nThe provided webpage is a post on the OpenAI Developer Community where a user is seeking exactly this advice, sharing the detailed system prompt they are currently using for table extraction with `gpt-4o vision`. The post itself does not contain the suggested solutions or successful prompts from other community members.",
    "published_date": "2025-03-08T00:00:00.000Z",
    "score": null
  },
  "https://www.v7labs.com/agents/ai-chart-data-extraction-agent": {
    "content_path": "../data/contents/204b260d2cc915d5.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.374062",
    "title": "AI Chart Data Extraction Agent",
    "summary": "The webpage describes the **AI Chart Data Extraction Agent** by V7 Go, which uses computer vision to accurately extract underlying data points from various chart types (bar charts, line graphs, pie charts, scatter plots) and convert them into structured tables (CSV/Excel).\n\nKey features related to your query include:\n*   **Chart/Table Extraction:** The core function is extracting data from charts into structured tables.\n*   **Document Understanding/PDF Parsing:** It can process charts found within multi-page documents like PDF reports.\n*   **Multimodal Capability:** It leverages computer vision to read visual data from images/charts.\n*   **Structured Document Output:** It outputs data in a clean, structured format (CSV/Excel).\n\nWhile the page discusses agents that work with documents and data extraction, it **does not explicitly mention** Vision-language models (like GPT-4V, Claude vision, Gemini), multimodal RAG, or report generation with LLMs in the context of this specific chart extraction agent.\n\n**Summary based on your query:** The page details an AI agent specialized in **chart/table extraction** from visual data within documents (including PDFs) and provides **structured document output**. It does not confirm the use of specific large multimodal models (GPT-4V, Gemini) or multimodal RAG for this particular agent.",
    "published_date": "2025-10-20T00:00:00.000Z",
    "score": null
  },
  "https://towardsdatascience.com/illuminating-insights-gpt-extracts-meaning-from-charts-and-tables-a0b71c991d34/": {
    "content_path": "../data/contents/0736b731de26ba09.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.359725",
    "title": "Illuminating Insights: GPT Extracts Meaning from Charts and Tables",
    "summary": "The webpage discusses the application of **GPT-4 Vision (GPT-4V)** for interpreting and aggregating data from images, specifically focusing on **charts and tables** found in financial documents like PDFs.\n\nKey points relevant to your query:\n\n*   **Multimodality:** The article highlights the importance of integrating visual inputs (images) alongside text in Large Language Models (LLMs) as a significant new direction in AI research.\n*   **Document Understanding/PDF Parsing:** It addresses the difficulty existing methods have with complex layouts in financial statements and research reports, and demonstrates how GPT-4V can overcome this by extracting and interpreting key information directly from screenshots of these documents.\n*   **Chart/Table Extraction:** The examples show GPT-4V successfully identifying peaks in performance graphs, extracting relevant parts of tables, and even performing basic mathematical operations on extracted table data.\n*   **Report Generation with LLMs:** The core benefit demonstrated is the ability to get structured output (like sorted table data) directly from a prompt, avoiding manual data entry, scraping, and spreadsheet wrangling, thus freeing up analysts for higher-level tasks.\n*   **Vision Models:** The article specifically details the use of **GPT-4V** and shows code snippets for encoding an image and sending it to the OpenAI API for analysis.\n\nWhile the page focuses heavily on GPT-4V, it touches upon the broader concepts of **Vision-language models** and the efficiency gains in **document understanding** through visual analysis. It does not explicitly mention \"multimodal RAG,\" \"Claude vision,\" or \"Gemini\" in detail, though it notes that OpenAI, Anthropic (Claude), and Google (Gemini) are embracing multimodality.",
    "published_date": "2023-12-24T00:00:00.000Z",
    "score": null
  },
  "https://www.kadoa.com/blog/using-gpt-4-vision-for-multimodal-web-scraping": {
    "content_path": "../data/contents/9732f58200419e3e.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.176741",
    "title": "Using GPT-4 Vision for Multimodal Web Scraping",
    "summary": "The webpage discusses the use of **GPT-4 Vision (GPT-4V)**, a multimodal AI model, for **multimodal web scraping** and document processing.\n\nKey takeaways relevant to your query include:\n\n*   **Vision-Language Models (GPT-4V):** GPT-4V can understand images as input and answer questions based on them, impacting web scraping by allowing understanding of unstructured data (websites, PDFs, images) without relying solely on complex OCR.\n*   **Document Understanding/Extraction:** Experiments showed GPT-4V successfully transforming screenshots of product pages, **charts**, and **tables** into structured **JSON data**.\n*   **Report Generation with LLMs (Structured Output):** GPT-4V's **function calling/tools** feature allows for consistent extraction of structured data from images based on a pre-defined JSON schema.\n*   **Limitations:** Challenges include limited context (only processing visible screen areas), scalability issues (cost and rate limits), and limitations in spatial reasoning (inaccurate coordinates for RPA).\n\nWhile the page focuses on web scraping applications, it demonstrates capabilities relevant to **chart/table extraction** and generating **structured document output** using a multimodal model like GPT-4V. It does not explicitly detail multimodal RAG, PDF parsing, or mention Claude vision or Gemini.",
    "published_date": "2023-11-09T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/@scholarly360/gpt4-vision-for-form-and-table-understanding-b24ceb3c948b": {
    "content_path": "../data/contents/101f627b4c4c972e.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.363017",
    "title": "",
    "summary": "The webpage discusses using **GPT-4 Vision (GPT-4V)** for **form and table understanding**. It explains that GPT-4V incorporates visual capabilities, allowing it to process images and documents, unlike previous text-only models. The text provides Python code examples demonstrating how to use the OpenAI API with `gpt-4-vision-preview` to extract information from images of forms and tables. Key learnings indicate that GPT-4V works well for form understanding, but sometimes struggles with JSON responses, and its performance for table extraction via the API was noted as \"pretty bad\" compared to using the GPT-4 User Interface.",
    "published_date": "2023-11-13T07:40:22.000Z",
    "score": null
  },
  "https://community.openai.com/t/gpt-4-vision-extraction-of-tables-with-branched-rows-vertically-merged-cells/728749": {
    "content_path": "../data/contents/2aba3058d3fd5e75.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.370255",
    "title": "GPT-4-vision extraction of tables with branched rows/vertically-merged cells - Prompting - OpenAI Developer Community",
    "summary": "The user query is about **multimodal vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.**\n\nThe provided webpage is a discussion thread from the OpenAI Developer Forum titled \"GPT-4-vision extraction of tables with branched rows/vertically-merged cells.\"\n\n**Summary relevant to the query:**\n\nThe webpage discusses the challenges of using **GPT-4-vision** for **table extraction** from documents, specifically when dealing with complex structures like **branched rows** (where one row entry corresponds to multiple sub-entries in other columns, such as two Analyte/Result pairs for one Biomarker/Method pair). A user details extensive prompting attempts to get GPT-4V to extract both branches, which it often fails to do, instead ignoring one branch. The discussion also touches upon comparing GPT-4V's performance against traditional **OCR tools built for tables**, noting that GPT-4V's appeal lies in its flexibility for unknown document formats, despite prompting frustrations. Another user later mentions that **GPT-4o vision** also struggles with complex tables with grouped rows and columns.\n\n**Conclusion:** The page directly addresses **GPT-4V/GPT-4o vision** and **table extraction** (a component of document understanding), confirming difficulties with complex table structures.",
    "published_date": "2024-04-26T00:00:00.000Z",
    "score": null
  },
  "https://docs.reducto.ai/parsing/chart-extraction": {
    "content_path": "../data/contents/41237068787559b5.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.991045",
    "title": "Chart data extraction pipeline",
    "summary": "The webpage describes the **Chart data extraction pipeline** offered by the Reducto API, which uses a multi-stage process combining OCR and **vision-language models (VLMs)** to extract structured data from chart images.\n\nThe pipeline has three main stages:\n1.  **Structural Analysis:** Uses OCR for text detection (labels, titles) and layout understanding to establish coordinate system boundaries.\n2.  **Coordinate Extraction:** Uses computer vision techniques to detect precise coordinates for data elements (e.g., bar heights, line endpoints).\n3.  **Semantic Correspondence:** A fine-tuned **VLM** validates the extracted coordinates and maps them to their semantic meaning using mark prompting, handling ambiguities and transforming pixel coordinates into actual data values based on axis scales.\n\nThe pipeline adapts its processing based on the chart type (e.g., VLMs take on more responsibility for pie charts). The output is structured data in a **tabular markdown table format**.",
    "published_date": "2025-10-14T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2510.06782v1": {
    "content_path": "../data/contents/0ff124b6f9f22ae5.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.184392",
    "title": "GPT-5 Model Corrected GPT-4V’s Chart Reading Errors, Not Prompting",
    "summary": "This web page details a quantitative evaluation comparing the chart reading accuracy of large language models (LLMs), specifically **GPT-5**, **GPT-4o**, and **GPT-4V**, using difficult instances from the **CHART-6 benchmark**.\n\nThe key findings related to your query are:\n\n*   **Model Architecture Dominates:** **GPT-5** consistently and significantly **outperformed GPT-4o** (and by extension, GPT-4V) across various chart-reading tasks, suggesting that model capability is the primary factor in inference accuracy for these difficult visualization questions.\n*   **Prompting Effects are Small:** Varying the prompt conditions—using the original CHART-6 instruction, a question-only prompt, or a **GPT-5 generated chart description**—yielded only small and inconsistent effects on performance. In some cases, using the GPT-5 generated chart description even slightly decreased accuracy for GPT-5.\n*   **Chart Reading/Extraction:** The study focuses on **visual question answering** and **chart reading**, which falls under the broader category of **document understanding** and **vision-language models**. While the page discusses generating chart descriptions (a form of **report generation**), it does not specifically cover general **PDF parsing** or **multimodal RAG**. The models evaluated include **GPT-4V** and the newer **GPT-5**.",
    "published_date": "2025-09-30T00:00:00.000Z",
    "score": null
  },
  "https://cookbook.openai.com/examples/data_extraction_transformation": {
    "content_path": "../data/contents/06c198e32488f738.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.380584",
    "title": "Data Extraction and Transformation in ELT Workflows using GPT-4o as an OCR Alternative",
    "summary": "The webpage describes how to use **GPT-4o's multimodal capabilities** as an alternative to traditional OCR for **data extraction and transformation** within **ELT (Extract, Load, Transform) workflows**.\n\nSpecifically, it details a three-part process:\n\n1.  **Data Extraction from PDFs:** Converting PDF pages to images and using GPT-4o to extract structured data into JSON format, handling complex layouts and multilingual content (demonstrated with German hotel invoices).\n2.  **Data Transformation:** Taking the raw, extracted JSON and transforming it to fit a predefined target **schema** (including translating data to English and formatting dates), again using GPT-4o.\n3.  **Loading into a Database:** Segmenting the schematized JSON data into relational tables (Hotels, Invoices, Charges, Taxes) and ingesting it into an SQLite database for querying.\n\nThe overall theme aligns with **Vision-language models** being used for **document understanding** and **PDF parsing** to facilitate structured data output, which is relevant to **multimodal RAG** and **report generation** concepts mentioned in your query.",
    "published_date": "2025-03-15T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2405.07001v1": {
    "content_path": "../data/contents/4c00f9cc332213aa.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.186735",
    "title": "Computer Science > Computation and Language",
    "summary": "The webpage summarizes a research paper titled \"Evaluating Task-based Effectiveness of MLLMs on Charts.\" The study focuses on evaluating **Multimodal Large Language Models (MLLMs)**, specifically **GPT-4V**, on low-level data analysis tasks performed on charts.\n\nKey points relevant to your query include:\n*   **Vision-language models (MLLMs):** The paper evaluates 18 advanced MLLMs, including closed-source models like **GPT-4V**.\n*   **Chart Analysis:** The core focus is on data analysis tasks on charts, which falls under **document understanding** and **chart/table extraction** aspects of multimodal processing.\n*   **Effectiveness:** It reports on the capabilities and limitations of these models in this domain, finding that GPT-4V achieved the highest accuracy (56.13% initially).\n*   **Prompt Strategies:** The research proposes and tests prompt strategies (\"Chain-of-Charts\" and visual prompting) to improve performance, boosting accuracy up to 83.83%.\n\nWhile the page discusses MLLMs and chart analysis (a form of structured data extraction), it does not explicitly mention **multimodal RAG**, **PDF parsing**, **report generation with LLMs**, **Claude vision**, **Gemini**, or **structured document output** beyond chart data extraction.",
    "published_date": "2024-05-11T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2510.20296v1": {
    "content_path": "../data/contents/b930fd686cd56e74.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:39.519534",
    "title": "RAG-Stack : Co-Optimizing RAG Quality and Performance \n From the Vector Database Perspective",
    "summary": "The webpage discusses **RAG-Stack**, a blueprint for co-optimizing the **quality and performance** of Retrieval-Augmented Generation (RAG) systems, moving beyond optimizing vector databases in isolation.\n\nThe user query lists several key components and concepts related to RAG: *Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies*.\n\nHere is how the webpage addresses these topics:\n\n*   **Vector databases & RAG Architectures:** The paper positions RAG as a prominent application of vector databases and focuses on optimizing the end-to-end RAG pipeline, which includes the database.\n*   **Embeddings & Chunking Strategies:** These are explicitly mentioned as part of the **Algorithm Configuration Space** (Table 1) that influences generation quality. Specifically, the choice of **embedding model** and **document chunking strategies** are tunable knobs.\n*   **Rerankers & Hybrid Search:** **Result reranking** is listed as an algorithmic factor influencing quality. **Hybrid search** (combining vector search with keyword-based matching like BM25) is listed under the \"Retrieval method\" configuration.\n*   **New efficient models & RAG alternatives:** While the paper acknowledges the continuous emergence of new retrieval techniques and models, it focuses on a framework (**RAG-Stack**) to evaluate them rather than introducing specific new models or alternatives to RAG itself.\n\n**Summary of RAG-Stack's approach to these components:**\n\nRAG-Stack addresses the complexity by systematically exploring the vast configuration space, which includes:\n\n1.  **Algorithmic Factors (Quality-related):** Document chunking, choice of embedding models, Top-K, query rewriting, and result reranking.\n2.  **System Factors (Performance-related):** Hardware choices (CPU/GPU), software frameworks, and database indexing/quantization.\n\nThe framework uses three pillars to manage this complexity:\n*   **RAG-IR:** An abstraction layer that captures the performance-relevant attributes of the algorithm configuration (including chunking, embedding model parameters, Top-K, etc.) without capturing quality-only details.\n*   **RAG-CM:** A cost model to predict system performance based on the RAG-IR representation.\n*   **RAG-PE:** A plan exploration algorithm to efficiently search for",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/html/2506.00054v1": {
    "content_path": "data/contents/85207229296d1a11.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.489364",
    "title": "Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers",
    "summary": "The webpage provides a comprehensive survey of Retrieval-Augmented Generation (RAG) systems, covering architectures, enhancements, and robustness frontiers.\n\nHere is a summary addressing the specific topics in your query:\n\n*   **Vector databases:** The text discusses **dense retrievers** (e.g., DPR) which rely on vector representations (embeddings) for retrieval from external knowledge sources, implying the use of vector databases for storage and similarity search.\n*   **Embeddings (new efficient models):** The survey covers **dense retrievers** and the use of **Query Encoder** modules to create query representations, which are essentially embeddings. It also mentions enhancements related to **Rich Answer Encoding (RAE)**, which embeds answer-aligned semantics into retriever outputs.\n*   **Rerankers:** Reranking is explicitly covered as a vital enhancement. Methods like **Re2G**, **GenRT**, and **RAG-Fusion** utilize reranking layers or fusion techniques to refine the initial set of retrieved documents based on relevance before generation.\n*   **RAG Architectures:** The survey systematically categorizes RAG architectures into **Retriever-centric**, **Generator-centric**, **Hybrid**, and **Robustness-oriented** designs.\n*   **RAG Alternatives:** While the focus is on RAG, the text frames RAG as an enhancement to LLMs that addresses the limitations of **static, parametric knowledge storage**. The concept of **GenGround** (Generate-Then-Ground) suggests an alternative flow where a provisional answer is generated first, followed by retrieval for substantiation.\n*   **Hybrid Search:** The text discusses **Hybrid and structured retrieval** approaches, such as those integrating unstructured text embeddings with structured knowledge graph embeddings (e.g., Doan et al.'s lightweight hybrid strategy), which aligns with the concept of hybrid search.\n*   **Chunking Strategies:** This is addressed under **Granularity-Aware Retrieval**. Methods like **LongRAG** retrieve \"compressed long-context chunks\" constructed through document grouping, and **FILCO** filters irrelevant spans from retrieved passages, both relating to how documents are segmented and selected.\n*   **Context Window Management:** This is covered under **Efficiency Enhancements** and **Generator-Based RAG Systems**. Techniques like **FiD-Light** (token-level passage compression) and **IB Filtering** (information bottleneck-based filtering) aim to optimize the context passed to\n\nThe user query asks for a summary covering several specific topics related to Retrieval-Augmented Generation (RAG): **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, and context window management.**\n\nThe provided webpage text is a comprehensive survey of RAG architectures and enhancements. Here is a summary of how the text addresses the user's query points:\n\n*   **RAG Architectures:** The page extensively covers various RAG architectures, categorized into Retriever-based, Generator-based, and Hybrid RAG, detailing specific frameworks like RQ-RAG, SELF-RAG, and DRAGIN, and comparing their performance in short-form QA and multi-hop reasoning.\n*   **Rerankers:** Reranking is explicitly mentioned as a category under \"Enhancement Type.\" Methods like **FILCO** (lexical filtering), **IB Filtering** (info-theoretic), **Stochastic Filtering**, **RLT** (adaptive truncation), **ToolRerank**, **RankRAG**, and **uRAG** are listed, along with their mechanisms and performance impacts.\n*   **RAG Alternatives/Enhancements:** The entire document details numerous enhancements and alternatives to standard RAG, such as adaptive retrieval (TA-ARE, FLARE), robustness modules (CRAG, RAAT), efficiency techniques (Sparse RAG, FiD-Light), and security considerations (BadRAG, TrojanRAG).\n*   **Hybrid Search:** The \"Hybrid\" category under Enhancement Type lists methods like **M-RAG** (Semantic partitioning + dual agents) and **KRAGEN** (Knowledge graph subgraph retrieval), which represent forms of hybrid search or integration.\n*   **Chunking Strategies / Context Window Management:** While not explicitly using the terms \"chunking strategies\" or \"context window management\" as dedicated sections, the text addresses related concepts:\n    *   **Compression:** **Sparse RAG** (retains high-signal tokens) and **FiD-Light** (compresses passages) directly relate to managing the context provided to the generator, which is a key aspect of chunking/compression strategies.\n    *   **Context Reduction:** **SEER** achieves a **9.25x context reduction** through self-training.\n    *   **Long-Context Tasks:** **Sparse RAG** is noted as",
    "published_date": "2025-11-14T00:00:00.000Z",
    "score": null
  },
  "https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking": {
    "content_path": "../data/contents/8fabfae1465b16cd.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.073951",
    "title": "Optimizing RAG with Hybrid Search & Reranking",
    "summary": "The webpage focuses on **Optimizing Retrieval-Augmented Generation (RAG) with Hybrid Search and Reranking**.\n\nHere is a summary of the topics mentioned in your query that are covered in the page:\n\n*   **RAG Architectures:** The page discusses how hybrid search enhances the retrieval component in RAG systems.\n*   **Hybrid Search:** It explains what hybrid search is (combining keyword/sparse vector search, like BM25, with semantic/dense vector search) and its use cases (handling abbreviations, names, and code snippets better than pure vector search).\n*   **Rerankers:** It details the process of semantic reranking to reorder the top-k retrieved results based on relevance scores from transformer models.\n*   **Vector Databases & Embeddings:** It mentions that standard RAG uses word embeddings and vector similarity search, and discusses how different vector databases (like ChromaDB and Weaviate) support or require custom setups for hybrid search.\n*   **Chunking Strategies:** The implementation example shows the use of `RecursiveCharacterTextSplitter` for chunking documents.\n\n**Topics from your query NOT explicitly detailed:**\n\n*   **New efficient models:** The page uses existing models (like BAAI/bge-base-en-v1.5 for embeddings and Zephyr-7B-Beta for the LLM) but does not focus on *new* efficient model developments.\n*   **RAG alternatives:** The page focuses on *optimizing* RAG, not alternatives to the RAG architecture itself.\n\nIn essence, the page provides a deep dive into using **Hybrid Search** (combining keyword and vector search) and **Reranking** to improve the retrieval quality within a **RAG** pipeline, using specific **chunking** and **vector database** examples.",
    "published_date": "2025-10-21T00:00:00.000Z",
    "score": null
  },
  "https://gpt-trainer.com/blog/rag+chunking+strategy": {
    "content_path": "../data/contents/5af8601b1fbea025.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.777735",
    "title": "RAG Chunking Strategy",
    "summary": "The webpage provides a detailed overview of **Retrieval-Augmented Generation (RAG)** architectures, focusing heavily on the **chunking strategies** used during document ingestion.\n\nHere is a summary of the topics mentioned in your query that are covered on the page:\n\n*   **Vector databases & Embeddings:** The page describes using **vector search** over an embedded knowledge base, where documents are converted into **embedding vectors** using models (like SentenceTransformers) and stored in a **vector database** for efficient semantic search.\n*   **RAG Architectures:** The core RAG pipeline is detailed, involving document ingestion, chunking, embedding, indexing in a vector DB, **retrieval** (online phase), optional **reranking**, and final **generation** with the retrieved context.\n*   **Chunking Strategies:** This is the main focus. Strategies discussed include:\n    *   Fixed-Length Chunking (with overlap).\n    *   Context-Aware (Structure-Based) Chunking (using recursive splitting by delimiters like paragraphs or headings).\n    *   **Semantic Chunking** (using similarity checks between sentence embeddings to determine boundaries).\n    *   Adaptive/ML-Guided Chunking.\n    *   **Hybrid Chunking Strategies** (combining approaches or using context-enriched chunks).\n*   **Rerankers:** The optional step of **reranking** is mentioned, where a model re-scores candidate chunks retrieved by vector similarity to improve relevance before feeding them to the final LLM.\n*   **Hybrid Search:** The page mentions **hybrid retrieval** combining keyword search (lexical retrieval) with vector search to capture both exact matches and semantic matches.\n\nThe page does **not** explicitly discuss \"RAG alternatives\" in detail, though it focuses entirely on one specific, popular RAG implementation (vector-based).",
    "published_date": "2025-05-16T08:40:47.000Z",
    "score": null
  },
  "https://www.linkedin.com/posts/cornellius-yudha-wijaya_building-the-rag-pipeline-is-easy-but-making-activity-7327889013090975745-3IpR": {
    "content_path": "../data/contents/6a0fd6cd350b0012.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.159802",
    "title": "\"9 Chunking Strategies for RAG Models\"",
    "summary": "The webpage provides information on **Chunking Strategies for RAG Models** and discusses several related advanced RAG concepts mentioned in the user query through comments and related posts.\n\nHere is a summary addressing the user query components:\n\n*   **Chunking Strategies:** The main post details **9 Chunking Strategies**: Fixed-Size, Sentence-Based, Semantic-Based, Recursive, Sliding-Window, Hierarchical, Topic-Based, Modality-Specific, and Agentic Chunking. Another post mentions **section-aware chunking** and **semantic chunking** (splitting by idea). A third post discusses **contextual summaries** added to chunks.\n*   **Embeddings (new efficient models):** One related post mentions **Fine Tuned Embeddings** for domain-specific improvements and **Instruction Embedding Models** for zero-shot adaptation.\n*   **Rerankers:** One related post explicitly mentions **Rerankers** (like Cohere Rerank 3.5) used to reorder top candidates from the vector database for better precision.\n*   **RAG Architectures:** The page discusses several architectures:\n    *   The main topic is **RAG**.\n    *   Related posts mention **Agentic RAG**, **Graph RAG**, and **CAG (Cache Augmented Generation)** as an alternative. Another post mentions 10 RAG architectures including Standard, Agentic, and Multi-Modal.\n*   **RAG Alternatives:** **CAG (Cache Augmented Generation)** is mentioned as the opposite of RAG, preloading context instead of retrieving it.\n*   **Hybrid Search:** One related post mentions **Hybrid Search** (combining keyword search like BM25 with embeddings) and another mentions **Hybrid retrieval (BM25 + vectors)** as a best practice.\n*   **Vector Databases:** While not detailed, vector databases are implied as the storage mechanism for embeddings in RAG systems, and one post mentions **Vector DB comparisons** in a cheat sheet.\n\n**Vector Databases:** Mentioned as part of the RAG system where embeddings are stored, with comparisons noted in a linked cheat sheet.",
    "published_date": "2025-05-12T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/@workrelated2501/rerankers-in-rag-the-secret-ingredient-for-high-quality-retrieval-8832439e7ca8": {
    "content_path": "../data/contents/84beeb00b7b3fd4e.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.784385",
    "title": "Rerankers in RAG: The Secret Ingredient for High-Quality Retrieval 🔍✨",
    "summary": "Rerankers are a crucial component in Retrieval-Augmented Generation (RAG) systems, acting as a **second-stage filtering mechanism** to significantly improve retrieval quality beyond what initial vector search provides.\n\n**Key points about Rerankers:**\n\n*   **Purpose:** They address the limitations of single-stage retrieval (where embedding models might miss relevant documents or rank them poorly) by scoring and reordering the top $k$ candidate documents retrieved by the vector database.\n*   **Mechanism (Cross-Encoders):** Unlike the initial retrieval's bi-encoders (which encode query and document separately), rerankers are typically **cross-encoders** that process the query and document together to capture deeper, context-specific semantic relationships, resulting in a direct relevance score.\n*   **Two-Stage Paradigm:**\n    1.  **First Stage (Recall):** Fast vector search retrieves a larger set of candidates ($k$).\n    2.  **Second Stage (Precision):** Rerankers reorder these candidates, selecting the top $n$ (usually 3–5) most relevant documents to pass to the LLM.\n*   **Benefits:** Adding rerankers can improve retrieval quality by **14–30%**. They also allow the use of smaller, faster embedding models in the first stage, significantly boosting indexing throughput and reducing computational costs while maintaining high overall accuracy.\n*   **Tradeoffs:** Rerankers introduce **latency** and require additional **compute resources** due to the more intensive cross-encoding process.",
    "published_date": "2025-05-05T04:48:08.000Z",
    "score": null
  },
  "https://www.pinecone.io/learn/series/rag/rerankers/": {
    "content_path": "../data/contents/9c3eb93a556a255b.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.219521",
    "title": "Rerankers and Two-Stage Retrieval",
    "summary": "The webpage discusses **Rerankers and Two-Stage Retrieval** as a method to improve **Retrieval Augmented Generation (RAG)** pipelines when out-of-the-box RAG performance is suboptimal.\n\nKey points related to your query:\n\n*   **RAG Architectures:** It focuses on a two-stage retrieval system where the first stage uses a fast **vector search** (relying on **embeddings**/bi-encoders) to retrieve a larger set of documents, and the second stage uses a slower but more accurate **reranker** to reorder and select the most relevant documents before passing them to the LLM.\n*   **Embeddings:** It mentions that the first stage uses embedding models (like `multilingual-e5-large`) to transform text into vectors for similarity search in a **vector database** (Pinecone is used in the example).\n*   **Rerankers:** Rerankers (cross-encoders) are significantly more accurate than embedding models because they analyze the query and document pair directly, avoiding the information loss inherent in compressing text into a single vector. They are used to maximize LLM recall by minimizing noise in the context window.\n*   **Chunking Strategies:** The example uses pre-chunked data from the `jamescalam/ai-arxiv-chunked` dataset, where each record is 1-2 paragraphs long.\n\nThe page does not explicitly detail **new efficient models** for embeddings, **alternatives** to RAG, or **hybrid search** strategies, although it implies that the first stage of retrieval could use sparse embedding models alongside vector search.",
    "published_date": null,
    "score": null
  },
  "https://www.singlegrain.com/blog-posts/link-building/llm-retrieval-optimization-for-reliable-rag-systems/": {
    "content_path": "data/contents/aa46792798c74558.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.830649",
    "title": "LLM Retrieval Optimization for Reliable RAG Systems",
    "summary": "The webpage provides a comprehensive guide on **LLM Retrieval Optimization for Reliable RAG Systems**, covering many of the concepts mentioned in your query.\n\nHere is a summary of how the page addresses the topics in your query:\n\n*   **Vector databases:** Mentioned as part of the retrieval backends and vector search decisions. The page discusses tuning vector search and its role in the RAG architecture.\n*   **Embeddings (new efficient models):** The page discusses \"Choosing embeddings\" as part of data preparation, noting the decision involves the model family, dimensionality, and cost trade-offs.\n*   **Rerankers:** Explicitly covered as a component in the retrieval stack and a key lever for optimization, used to reorder retrieved results with higher precision.\n*   **RAG architectures:** The page details the \"End-to-end RAG request flow\" and the \"Components in the retrieval stack,\" providing a clear model of a modern RAG system.\n*   **RAG alternatives:** The page contrasts RAG with \"standalone LLMs\" and discusses how RAG addresses the limitations of relying solely on pre-training data.\n*   **Hybrid search:** Explicitly compared against sparse (BM25) and dense (vector search) retrieval, noting it combines lexical precision with semantic recall.\n*   **Chunking strategies:** A dedicated section covers \"Chunking strategies that actually work,\" detailing fixed-size, structure-aware, and use-case-specific approaches.\n*   **Context window management:** While not using the exact term, the page addresses this through levers like \"Context window and answer length,\" noting that constraining the amount of evidence reduces token usage.\n\nThe page covers all the listed topics within the context of optimizing Retrieval-Augmented Generation (RAG) systems.",
    "published_date": "2025-12-05T00:00:00.000Z",
    "score": null
  },
  "https://www.applied-ai.com/briefings/enterprise-rag-architecture/": {
    "content_path": "../data/contents/e950bad7433a08d1.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.792512",
    "title": "Enterprise RAG Architecture: A Practitioner's Guide",
    "summary": "The webpage provides a practitioner's guide to **Enterprise RAG Architecture**, detailing the evolution from simple RAG to sophisticated production systems.\n\nKey topics covered relevant to your query include:\n\n*   **Vector Databases:** Discusses the decision matrix, noting that **pgvector** is often sufficient for datasets under 5 million vectors due to its **hybrid query capability** (SQL filtering alongside vector search). Purpose-built databases like **Qdrant**, **Weaviate**, and **Milvus** are recommended for larger scales or specific needs (e.g., multi-modal search, massive scale).\n*   **Embeddings:** The guide implies the use of dense embeddings but highlights that **embedding model mismatch** (general models failing on specialized domains) is a common failure mode, suggesting domain-specific models or fine-tuning.\n*   **Rerankers:** **Reranking is described as essential** for production systems. The guide details **Two-Stage Retrieval with Reranking** using cross-encoders (like Cohere Rerank) to significantly improve precision after initial retrieval.\n*   **RAG Architectures:** It outlines a maturity spectrum:\n    *   **Naive RAG** (prototyping baseline) is insufficient for production.\n    *   **Advanced RAG** (production standard) incorporates **Hybrid Search** (Vector + Keyword, often using RRF), **Late Interaction (ColBERT)**, and **Two-Stage Retrieval with Reranking**.\n    *   **GraphRAG** is covered for queries requiring relationship understanding.\n    *   **Agentic RAG** represents the frontier, involving AI agents for multi-step orchestration.\n*   **Hybrid Search:** Explicitly stated as **beating pure semantic search**, combining BM25/keyword methods with dense retrieval.\n*   **Chunking Strategies:** Discusses **Chunking Strategy Failures**, recommending **structure-aware chunking** (preserving document elements) and **Parent Document Retrieval** (embedding small chunks but retrieving larger context).\n*   **RAG Alternatives:** The guide lists scenarios where RAG is **NOT** recommended, such as when content is static and small enough for **fine-tuning** a model, or for exact match requirements best handled by traditional search.",
    "published_date": null,
    "score": null
  },
  "https://www.mongodb.com/resources/basics/artificial-intelligence/reranking-models": {
    "content_path": "../data/contents/ad159c2b96ba7129.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.795878",
    "title": "What are Rerankers?",
    "summary": "Rerankers are used in multi-stage retrieval systems, particularly within RAG (Retrieval-Augmented Generation) pipelines, to refine the initial set of retrieved documents.\n\nHere's a summary of key points about rerankers based on the text:\n\n*   **Function:** They take the initial set of documents retrieved (often via vector search) and refine them into a smaller set of highly relevant chunks before they are sent to the generative model. This improves the accuracy of the final response.\n*   **Mechanism:** Reranker models are **cross-encoders**. They encode each query-document pair individually and produce a relevance score.\n*   **Difference from Embedding Models:** Embedding models are **bi-encoders** that produce vectors (embeddings) for separate encoding and use efficient similarity computation. Rerankers do not produce embeddings; they require the input data to pass through the model for every pair, making the process computationally expensive but resulting in more precise relevance scoring and a better understanding of contextual nuances.\n*   **Model Selection:** The best reranker model depends on specific needs, balancing precision/accuracy against latency and computational resources. Larger models are more precise but slower; smaller models are faster but less precise.",
    "published_date": "2025-03-14T15:50:38.000Z",
    "score": null
  },
  "https://airev.us/retrieval-augmented-generation": {
    "content_path": "data/contents/42a82632d167c22d.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.797331",
    "title": "Retrieval-Augmented Generation",
    "summary": "The webpage provides a technical deep dive into Retrieval-Augmented Generation (RAG), covering several components mentioned in your query:\n\n*   **Vector databases:** These are specialized databases (like Faiss, Qdrant, Pinecone, Weaviate) used to store chunk embeddings and perform efficient Approximate Nearest Neighbor (ANN) similarity searches.\n*   **Embeddings (new efficient models):** The text discusses the centrality of embeddings for semantic similarity and mentions popular solutions like **OpenAI's text-embedding-3** (512 or 1536 dimensions), **Cohere's Embed 4** (state-of-the-art, multi-modal), **e5-large-v2** (open-weights), and **ImageBind** (cross-modal).\n*   **RAG architectures:** The document covers the basic \"retrieve, then generate\" workflow, **Iterative & Adaptive Retrieval** (multi-hop queries), and **Agentic RAG**, where the LLM acts as an agent using function calling to dynamically decide when and how to retrieve data.\n*   **Chunking strategies:** Large documents must be split into smaller chunks. Strategies mentioned include breaking books into $\\sim 10-20\\%$ of the context window, splitting Wikipedia articles by headings, and segmenting transcripts by time/speaker turns.\n*   **Context window management:** Chunking is essential for managing the context window, as the system retrieves multiple small, targeted chunks to feed the LLM, ensuring the total context fits within its processing limit.\n*   **Hybrid search:** This term is **not explicitly detailed** in the provided text, although the focus is heavily on embedding-based (vector) search.\n*   **Rerankers:** This term is **not explicitly detailed** in the provided text.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-information-retrieval": {
    "content_path": "../data/contents/30398159fa6f685d.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.802922",
    "title": "Information retrieval",
    "summary": "This page focuses on the **Information Retrieval Phase** of a Retrieval-Augmented Generation (RAG) solution, covering how to configure a search index, choose search approaches, and refine queries.\n\nHere is a summary of the topics relevant to your query:\n\n*   **Vector Databases & Embeddings:** The article discusses configuring the search index in a vector store, specifically mentioning vector search algorithms like **exhaustive k-nearest neighbors (KNN)** and **Hierarchical Navigable Small World (HNSW)** for approximate nearest neighbor (ANN) search. It also notes that the vector column's dimensions must match the output of the embedding model (e.g., 1,536 for `text-embedding-3-small`).\n*   **Rerankers:** Reranking strategies are covered, including using a **language model** or a **cross-encoder** (like Roberta) to re-evaluate and rank aggregated search results for better relevance. Azure AI Search also offers proprietary **semantic ranking**.\n*   **RAG Architectures & Hybrid Search:** The page details different search approaches: **vector search**, **full-text search**, and **hybrid search** (which combines vector and text searches, often using Reciprocal Rank Fusion for reranking in AI Search). It also describes a complex RAG pipeline that combines query augmentation, decomposition, rewriting, and execution steps.\n*   **Chunking Strategies:** While the page focuses on the retrieval phase, it references the **Chunking Phase** documentation for details on chunking strategies.\n*   **RAG Alternatives:** The **HyDE (Hypothetical Document Embeddings)** technique is presented as an alternative retrieval method where a generated answer (not the query) is embedded to find similar documents.\n\nThe page does not explicitly detail \"new efficient models\" for embeddings, nor does it provide a dedicated section on \"chunking strategies\" (referring to external documentation for that).",
    "published_date": "2025-10-01T00:00:00.000Z",
    "score": null
  },
  "https://milvus.io/docs/v2.0.x/overview.md": {
    "content_path": "../data/contents/5188bb1de16d0c7d.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.408178",
    "title": "Milvus vector database documentation",
    "summary": "The Milvus documentation covers several aspects related to your query, particularly concerning vector databases and search:\n\n*   **Vector Databases:** Milvus is a vector database, and the documentation provides information on how to install, use, and deploy it.\n*   **Hybrid Search:** \"Hybrid Search\" is listed under \"Recommended articles - Use.\"\n*   **Rerankers:** The \"What's new in docs\" section mentions guidance on how to use a **decay ranker**.\n*   **Embeddings:** The \"What's new in docs\" section mentions guidance on how to use an **embedding function**.\n\nThe documentation does not explicitly detail: **new efficient models**, **RAG architectures**, **RAG alternatives**, or **chunking strategies**.",
    "published_date": "2025-02-24T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/@vladris/embeddings-and-vector-databases-732f9927b377": {
    "content_path": "../data/contents/d078a3a756cdbe31.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.433398",
    "title": "Embeddings and Vector Databases",
    "summary": "The user query asks for a summary covering several topics related to Retrieval-Augmented Generation (RAG): **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, and chunking strategies.**\n\nThis webpage excerpt focuses primarily on **Embeddings** and **Vector Databases** as a method for implementing memory in Large Language Models (LLMs), which is a core component of RAG.\n\nHere is a summary of the relevant points covered in the text:\n\n*   **Embeddings:** These are numerical representations (vectors) of words or text sequences that capture semantic meaning and relationships. The text discusses using OpenAI's `text-embedding-ada-002` model. Similarity between embeddings is measured using **cosine similarity** (or cosine distance).\n*   **Memory based on Embeddings (Basic RAG Concept):** Embeddings are used to implement memory by computing the embedding of user input and comparing it (using cosine distance) to pre-computed embeddings of stored data chunks to retrieve the most relevant context for the LLM prompt.\n*   **Vector Databases:** For large datasets, iterating through all embeddings becomes slow. A **vector database** is introduced as a specialized database optimized for storing and efficiently querying high-dimensional vector data. The text demonstrates using **Chroma** as an example vector database to store and retrieve documents based on similarity search.\n*   **Other Vector Database Options:** The text lists several other vector database options, including Weaviate, Qdrant, Milvus, Pinecone, and extensions for existing databases like Redis and Postgres (pgvector).\n\n**Topics not covered in detail or mentioned:**\n\n*   **Rerankers:** Not discussed.\n*   **RAG Architectures:** The text describes a basic retrieval mechanism but does not detail formal RAG architectures.\n*   **RAG Alternatives:** Not discussed.\n*   **Hybrid Search:** Not discussed.\n*   **Chunking Strategies:** The text mentions that the unit of data (chunk size) is up to the user but does not detail specific strategies.\n*   **New Efficient Models (for embeddings):** While `text-embedding-ada-002` is mentioned as the model used, the text does not compare it against newer or more efficient models.",
    "published_date": "2023-08-18T15:36:07.000Z",
    "score": null
  },
  "https://huggingface.co/blog/qdrddr/what-are-embeddings-and-vector-databases": {
    "content_path": "../data/contents/6fd2fd8e9415de17.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.462423",
    "title": "What are Embeddings and Vector Databases?",
    "summary": "The webpage explains **Embeddings** as numerical representations of information that allow computers to determine similarity for tasks like search and classification. These embeddings act like \"digital fingerprints\" (vectors) for data, enabling fast, semantic search by finding data points whose vectors are mathematically closest to the query's vector.\n\nThe text also covers:\n*   **Vector Databases (Vector DB):** Where the encoded numerical representations (vectors) of a dataset are stored, allowing for quick retrieval of relevant information chunks, often as the first phase in a **RAG (Retrieval-Augmented Generation)** application.\n*   **How Embeddings Models Work:** They are trained on large datasets to find correlations (e.g., recognizing that \"Pride and Prejudice\" is semantically related to \"First Impressions\").\n*   **Why Use Embeddings:** It is faster and easier for computers to process and understand relationships numerically than by searching raw text directly.\n*   **Advantages & Disadvantages:** A key advantage is simplifying initial data retrieval without needing a schema. A major disadvantage is the **lack of transitivity** (if A is similar to B, and B is similar to C, A is not necessarily similar to C) and difficulty in synthesizing summarized concepts over large datasets, which can lead to less than 100% accurate RAG results.\n*   **Retrieval:** Vector database retrieval is presented as one method, and the text notes that vector search can be just the first step before using more complex techniques.\n\nThe query asks about **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies**.\n\nThe page directly discusses **Vector databases**, **embeddings**, **RAG architectures** (by describing the two phases of RAG), and **chunking strategies** (mentioning that information is split into overlapping chunks before being stored).\n\nThe page **does not** explicitly mention:\n*   New efficient embedding models (it only mentions that models are typically flavors of BERT).\n*   Rerankers.\n*   RAG alternatives.\n*   Hybrid search.\n\nSince the page does not cover all the requested topics, the summary focuses on what is present:\n\n**Summary:**\nThe page defines **Embeddings** as numerical vectors representing data for fast, semantic similarity search, and **Vector Databases** as the storage system for these vectors, often used in the first phase",
    "published_date": "2024-08-20T00:00:00.000Z",
    "score": null
  },
  "https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/": {
    "content_path": "../data/contents/076c89389bf2e566.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.496617",
    "title": "Vector Databases: from Embeddings to Applications",
    "summary": "The webpage describes a DeepLearning.AI short course titled \"Vector Databases: from Embeddings to Applications.\"\n\nThe course covers:\n*   **Vector Databases:** Understanding their role, especially with LLMs, and when to apply them.\n*   **Embeddings:** How to form vector representations of data.\n*   **Search Techniques:** Exploring algorithms for fast searches, including **sparse, dense, and hybrid search**.\n*   **Applications:** Building applications ranging from **RAG** to multilingual search.\n\nWhile the course focuses heavily on vector databases, embeddings, and RAG architectures, it **does not explicitly mention** new efficient models for embeddings, rerankers, RAG alternatives, or specific chunking strategies.",
    "published_date": "2023-11-08T23:11:33.000Z",
    "score": null
  },
  "https://aws.amazon.com/what-is/vector-databases/": {
    "content_path": "../data/contents/ae3a3a78e4d4b32f.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.529801",
    "title": "What is a Vector Database?",
    "summary": "The provided webpage focuses on **Vector Databases**, explaining what they are, why they are important, how they are used, who uses them, and their benefits.\n\nHere is a summary of the topics mentioned in the user query that are covered on the page:\n\n*   **Vector databases:** The entire page is dedicated to explaining vector databases, which store and retrieve vectors (embeddings) for efficient nearest-neighbor lookups.\n*   **Embeddings (new efficient models):** The text mentions that embedding models encode data into vectors that capture meaning and context, allowing for similarity searches.\n*   **RAG architectures (implied):** The page notes that vector databases can complement generative AI models by providing an **external knowledge base** for chatbots, which is a core component of Retrieval-Augmented Generation (RAG).\n*   **Hybrid search:** The text explicitly states that developers can index metadata alongside vectors to enable **hybrid search** on both keywords and vectors, and mentions using hybrid relevancy scoring models that blend traditional term frequency models (like BM25) with vector scores.\n\nThe following topics from the user query are **not explicitly detailed** on the page:\n\n*   Rerankers\n*   RAG alternatives\n*   Chunking strategies",
    "published_date": "2025-03-11T00:00:00.000Z",
    "score": null
  },
  "https://learn.microsoft.com/en-us/fabric/real-time-intelligence/vector-database": {
    "content_path": "../data/contents/f919c26aeb8cc31a.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.555716",
    "title": "Vector databases",
    "summary": "The webpage provides an overview of **Vector Databases**, explaining that they store and manage data as **vectors** (numerical arrays) to enable complex queries like vector similarity search, quantization, and clustering, which traditional databases struggle with due to high-dimensional data.\n\nKey concepts covered include:\n*   **Vector Similarity:** Measuring how similar or different two vectors are using distance metrics like Euclidean distance or cosine similarity.\n*   **Embeddings:** Mathematical representations of data (text, images, etc.) that capture semantic meaning. These are often generated by Large Language Models (LLMs) and are stored in the vector database.\n\nThe general workflow for using a vector database is:\n1.  **Embed data** using a model.\n2.  **Store vectors** in the database.\n3.  **Embed the query** using the same model.\n4.  **Query vectors** using similarity search to find matches.\n\nThe article then focuses on using **Eventhouse in Microsoft Fabric as a vector database**, highlighting features like the `dynamic` data type for storage, the `Vector16` encoding type for reduced storage and faster processing, and the `series_cosine_similarity` function for searching. It also details steps to **optimize for scale** by setting the encoding to `Vector16` and adjusting sharding and merging policies.",
    "published_date": "2024-11-19T00:00:00.000Z",
    "score": null
  },
  "https://vespa.ai/vector-database/": {
    "content_path": "../data/contents/e1080618e042edae.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:39.863341",
    "title": "The Fastest, Most Scalable Vector Database",
    "summary": "The webpage describes **Vespa** as a fast and scalable **Vector Database** that functions as a unified AI Search Platform.\n\nHere is a summary of how Vespa addresses the components mentioned in your query:\n\n*   **Vector Databases:** Vespa is presented as a high-performance vector database capable of handling billions of documents with sub-millisecond latency.\n*   **Embeddings (new efficient models):** Vespa supports storing and computing on tensors, which includes vectors (embeddings). It allows for multimodal and multi-vector support.\n*   **Rerankers:** Vespa supports **Multi-Phase and Model-Driven Ranking**, allowing users to deploy ranking models (using ONNX, XGBoost, or custom functions) directly within the serving layer to re-rank results after initial recall, maximizing accuracy.\n*   **RAG Architectures:** Vespa is explicitly positioned for **GenAI (RAG)**, unifying the necessary components (vector search, text search, ranking) into a single engine for scalable RAG pipelines.\n*   **RAG Alternatives:** While not detailing specific alternatives, the page contrasts Vespa's unified architecture against fragmented systems that require stitching together separate vector stores, keyword indexes, and re-ranking layers.\n*   **Hybrid Search:** Vespa natively supports **Unified Vector, Text, and Structured Retrieval**, allowing users to combine dense embeddings, keyword signals, and metadata filters in a single query. Its tensor-native architecture supports both dense and sparse features for hybrid search.\n*   **Chunking Strategies:** The page does not explicitly detail specific chunking strategies, though it mentions that new documents and embeddings become searchable immediately due to its real-time indexing capabilities.",
    "published_date": "2025-12-10T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2505.07233v2": {
    "content_path": "../data/contents/66226f67509f87c5.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.822332",
    "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation",
    "summary": "The user query asks for a summary of information related to: **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies.**\n\nThis webpage describes **DynamicRAG**, a novel Retrieval-Augmented Generation (RAG) framework.\n\nHere is a summary of the relevant topics covered in the text:\n\n*   **Rerankers:** The paper focuses heavily on the reranker component of RAG systems, proposing a **DynamicRAG** framework where the reranker dynamically adjusts both the order and the number ($k$) of retrieved documents based on the query. The reranker is modeled as an agent optimized through Reinforcement Learning (RL), using the Large Language Model's (LLM) output quality as a reward signal.\n*   **RAG Architectures:** The paper details the standard RAG phases (Retrieval, Encoding, Generation) and introduces DynamicRAG as an improved architecture that uses LLM output feedback to optimize the reranker, contrasting it with traditional RAG systems that use static ranking thresholds.\n*   **RAG Alternatives/Improvements:** DynamicRAG itself is an improvement over existing RAG methods, achieving state-of-the-art results compared to baselines like Reward-RAG, RankRAG, and others.\n\n**Topics NOT explicitly covered in detail:**\n\n*   **Vector databases:** Not mentioned.\n*   **Embeddings (new efficient models):** The text mentions that the retrieval phase can use \"dense retrieval with embeddings,\" but it does not discuss specific new or efficient embedding models.\n*   **Hybrid search:** Not mentioned.\n*   **Chunking strategies:** Not mentioned.\n\n**Summary:**\n\nThe webpage details the **DynamicRAG** framework, which significantly improves **RAG architectures** by introducing a **reranker** optimized via Reinforcement Learning using feedback from the generator's output quality. This allows the reranker to dynamically adjust the number and order of retrieved documents. The paper compares DynamicRAG against various **RAG alternatives** and baselines, showing superior performance. However, the text does not discuss **vector databases**, specific **embeddings (new efficient models)**, **hybrid search**, or **chunking strategies**.\n\nThe user query asks for a summary covering several topics related to Retrieval-Augmented Generation (RAG), including: **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, and chunking strategies.**\n\nThe provided web page text focuses on a specific RAG architecture called **DynamicRAG**, which leverages Large Language Model (LLM) outputs as feedback for **dynamic reranking**.\n\nHere is a summary of the relevant points from the text concerning the user's query:\n\n*   **Rerankers:** The paper heavily focuses on reranking. It compares different Reranker and Generator model sizes (e.g., 7B vs 13B) and finds that a larger Reranker can enhance performance. It also introduces a method where the Reranker and Generator share parameters, leading to improved performance. The core of DynamicRAG is using the LLM response quality as a reward signal to optimize the reranker via Reinforcement Learning (RL) and Direct Preference Optimization (DPO).\n*   **RAG Architectures:** The paper introduces **DynamicRAG**, a novel RL framework that optimizes reranking dynamically based on feedback. It is compared against **Vanilla-RAG** and shows significant performance improvements. Ablation studies confirm the critical importance of retrieval and the beneficial effect of reranking and RL in the architecture.\n*   **Embeddings/Retrieval:** The text mentions using different **retrievers** (DPR, Contriever, MonoT5) and shows that DynamicRAG's performance improves as the underlying retriever models get better, demonstrating robustness. The initial retrieval step uses the top 45 documents retrieved by **Contriever-MS MARCO**.\n*   **Efficiency:** The DynamicRAG model is shown to be highly efficient, requiring only two LLM calls in one evaluation scenario and demonstrating approximately **17x superior throughput** compared to a sequential scoring approach like RankRAG, as it processes documents contextually rather than sequentially.\n\n**Topics not explicitly covered in detail:**\n\n*   **Vector databases:** Not mentioned.\n*   **Embeddings (new efficient models):** While retrievers are used (which rely on embeddings), the text does not detail new efficient embedding models themselves, only the performance impact of using different existing retrievers.\n*   **RAG alternatives:** The text discusses comparisons to Vanilla-RAG",
    "published_date": "2024-01-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/pdf/2502.11116": {
    "content_path": "../data/contents/4cb4789179864774.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.824329",
    "title": "",
    "summary": "This page describes **Gumbel Reranking (G-Rerank)**, an end-to-end training framework designed to optimize **rerankers** within **Retrieval-Augmented Generation (RAG) architectures**.\n\nKey aspects covered in relation to your query:\n\n*   **Rerankers:** The core focus is on improving reranker optimization, especially when labeled query-document pairs are scarce. G-Rerank reframes reranking as learning an optimal document-wise **attention mask**.\n*   **RAG Architectures:** The method is specifically designed for RAG systems, aiming to minimize the final language modeling loss, thus aligning training and inference objectives.\n*   **RAG Alternatives/Hybrid Search/Chunking Strategies:** These specific topics are **not** directly discussed in detail. The paper focuses on the reranking component, assuming documents have already been retrieved.\n*   **Embeddings (new efficient models):** The paper mentions that retrieval models often leverage dense vectors and transformer architectures, but it does not introduce or detail new embedding models.\n*   **Differentiable Optimization:** It uses the **Gumbel Trick** and **Relaxed Top-k sampling** to create a Differentiable Masked Attention (DMA) mechanism, allowing the reranker to be optimized end-to-end using backpropagation based on the language model loss.\n*   **Performance:** Experiments show G-Rerank consistently improves performance over existing LLM-supervised fine-tuning methods, particularly excelling in identifying **indirectly relevant documents** in multi-hop QA tasks (e.g., 10.4% improvement in Recall@5 on HotpotQA).\n\nIn summary, the page details a novel training method for **rerankers** within **RAG architectures** that uses differentiable masked attention to optimize selection based on final generation loss.\n\nThe provided text discusses various aspects of **Retrieval-Augmented Generation (RAG)**, particularly focusing on **reranker training strategies** and **multi-hop Question Answering (QA)**.\n\nHere is a summary of the topics mentioned that align with your query:\n\n*   **RAG Architectures/Concepts:** The text extensively covers **Retrieval-Augmented Generation (RAG)**, mentioning specific architectures like **Fusion-in-Decoder (FiD)** and its variants (KG-FiD, FiDO, FiD-Light, RFiD, MG-FiD). It also discusses **multi-hop QA** which relies on RAG principles.\n*   **Rerankers:** There is a significant focus on **reranker training methods** supervised by Large Language Models (LLMs), including **Attention Distillation (ADist)**, **Perplexity Distillation (PDist)**, **Leave-one-out Perplexity Distillation (LOOP)**, and **EMDR2**. The text also introduces a novel approach using **Learnable Sampling Weights** which bypasses a dedicated reranker by optimizing document weights directly via the language modeling loss.\n*   **Embeddings/Retrieval:** While the text doesn't detail \"new efficient models\" for embeddings, it references several retrieval methods and models used in QA, such as **Dense Passage Retrieval (DPR)** (implied by context), **ColBERT**, and **Beam Dense Retrieval (BeamDR)**. It also mentions **hybrid search** implicitly through the discussion of different retrieval strategies.\n*   **Chunking Strategies:** The text does not explicitly detail \"chunking strategies,\" although the concept is fundamental to retrieval systems that process passages.\n*   **Vector Databases:** **Vector databases** are not explicitly mentioned, but the context of dense retrieval implies their use.\n\nIn essence, the page provides a detailed overview of **reranker training** within **RAG architectures** (especially for multi-hop QA) and proposes a novel method to learn document relevance weights directly, touching upon concepts related to retrieval and ranking.",
    "published_date": null,
    "score": null
  },
  "https://www.analyticsvidhya.com/blog/2025/06/top-rerankers-for-rag/": {
    "content_path": "../data/contents/01d6c3335fa07000.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.831714",
    "title": "Top 7 Rerankers for RAG",
    "summary": "The webpage focuses on **Rerankers for Retrieval-Augmented Generation (RAG)** systems.\n\nIt explains that RAG improves LLMs by retrieving external information, but initial retrieval (like vector search) can be noisy. **Rerankers** are introduced as a crucial second step to refine these initial results by reordering documents based on their specific relevance to the user's query, ensuring only the best context is passed to the LLM.\n\nThe page details the **enhanced RAG pipeline**: Retrieve $\\rightarrow$ **Rerank** $\\rightarrow$ Generate.\n\nIt then reviews the **Top 7 Reranking Models in 2025**, including:\n*   **Proprietary/API-based:** Cohere Rerank, Voyage Rerank.\n*   **Open-Source:** bge-reranker, ColBERT, FlashRank, MixedBread (mxbai-rerank-v2).\n*   **Mixed/Specialized:** Jina Reranker (which also offers Jina-ColBERT for long documents).\n\nThe summary also covers how to **evaluate** rerankers (using metrics like NDCG and MRR) and factors to consider when **choosing** one (accuracy, latency, cost, scalability).\n\n**Regarding your specific query components:**\n\n*   **Vector databases:** Mentioned as the source for initial retrieval in the RAG architecture diagram.\n*   **Embeddings (new efficient models):** Mentioned in the context of initial retrieval, and specific embedding models (like VoyageAIEmbeddings) are used in the examples.\n*   **Rerankers:** This is the primary topic of the entire page.\n*   **RAG architectures:** The standard and enhanced RAG pipelines are described.\n*   **RAG alternatives:** Not explicitly discussed.\n*   **Hybrid search:** Not explicitly discussed.\n*   **Chunking strategies:** Mentioned in the context of splitting documents before retrieval in the code examples (using `RecursiveCharacterTextSplitter`).",
    "published_date": "2025-06-27T01:01:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2502.18418": {
    "content_path": "../data/contents/d8eac95d47501f4d.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.835553",
    "title": "Computer Science > Information Retrieval",
    "summary": "The webpage describes a paper titled \"Rank1: Test-Time Compute for Reranking in Information Retrieval.\" This work introduces **Rank1**, a reranking model trained to utilize test-time compute, specifically leveraging reasoning language models (like OpenAI's o1 or Deepseek's R1) for distillation to enhance the performance of smaller models.\n\nThe paper focuses on **reranking** within information retrieval and mentions:\n*   Gathering and open-sourcing a dataset of R1 reasoning traces from MS MARCO queries and passages.\n*   Models trained on this data show state-of-the-art performance on reasoning tasks, work well out-of-distribution due to prompt responsiveness, and provide explainable reasoning chains useful for **RAG-based systems**.\n*   Demonstrating that quantized versions retain strong performance while using less compute/memory.\n\n**Regarding your specific query topics:**\n\n*   **Vector databases, embeddings (new efficient models), hybrid search, chunking strategies:** Not mentioned.\n*   **Rerankers:** Directly addressed by the introduction of the Rank1 model.\n*   **RAG architectures, RAG alternatives:** RAG is mentioned in the context of how the explainable reasoning chains can be provided to RAG-based systems, but the paper does not detail RAG architectures or alternatives themselves.\n\n**Summary relevant to the query:** The page discusses a new approach to **reranking** using test-time compute and mentions its applicability to **RAG-based systems** through explainable reasoning chains. It does not cover vector databases, embeddings, hybrid search, or RAG alternatives.",
    "published_date": "2025-02-25T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/pdf/2508.08742": {
    "content_path": "../data/contents/d17ad0717f899439.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.839697",
    "title": "",
    "summary": "The provided web page text focuses on **SciRerankBench**, a new benchmark designed to evaluate **rerankers** within **Retrieval-Augmented Generation (RAG) Large Language Models (LLMs)**, specifically in the context of **scientific literature question answering**.\n\nThe summary of topics covered in the page relevant to your query is:\n\n*   **Rerankers:** The core focus is on evaluating 13 widely used rerankers (including dense cross-encoders like BGE/BCE/MXBAI, sparse models like SPLADE, late interaction models like ColBert, and LLM-based models like LLM2Vec) across various scientific tasks.\n*   **RAG Architectures:** The paper discusses the standard two-stage RAG pipeline where the reranker acts as the crucial second stage for high-precision retrieval refinement.\n*   **RAG Alternatives/Context:** The evaluation compares performance with zero context, naive dense retrieval (without reranking), and RAG with reranked contexts.\n*   **Chunking Strategies:** While not explicitly detailed as a separate section, the construction of the dataset involves using scientific abstracts as the primary source for context, which implies a form of chunking/context selection.\n*   **Embeddings:** The initial retrieval stage uses dense retrieval (specifically mentioning BGE as the retriever), which relies on embeddings. The paper also mentions LLM-based embedding methods like LLM2Vec.\n*   **Hybrid Search:** Not explicitly discussed.\n*   **Vector Databases:** The text mentions that the collected scientific abstracts were imported into a **Qdrant vector database** to facilitate efficient dense retrieval.\n*   **New Efficient Models (for Reranking):** The paper evaluates various reranker architectures, categorizing them by efficiency (e.g., lightweight models like MiniLM vs. slower LLM-based models like RankGPT).\n\n**In summary, the page provides a detailed benchmark for evaluating RAG components, heavily emphasizing the role and performance of rerankers in scientific domains, and touches upon the use of vector databases and embedding-based retrieval.**\n\nThe provided text is an excerpt from a paper discussing biomedical literature search in the age of AI, heavily focused on **Retrieval-Augmented Generation (RAG)**, reranking methods, and evaluation benchmarks.\n\nRegarding your specific query on **'retrieval\\_and\\_embeddings: Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies'**:\n\n*   **Vector databases:** The text explicitly mentions the use of **Qdrant** as a vector similarity search engine (Reference [34] and Algorithm 1, Line 4: \"Insert(V into Qdrant index I)\").\n*   **Embeddings (new efficient models):** The text references **BCEmbedding** (Reference [31]) and mentions using embeddings for retrieval (Algorithm 1, Line 3: \"$V \\leftarrow E_{bge}(C')$\" implies using an embedding model, likely BGE).\n*   **Rerankers:** The document extensively discusses and evaluates various **rerankers** (Tables 4, 5, 6, 7), including models like **Jina-reranker-v2-base-multilingual** (Reference [21]), **MXBAI Rerank V1** (Reference [38]), **ColBertv2** (Reference [37]), **RankT5** (Reference [63]), and **ListT5** (Reference [55]).\n*   **RAG architectures:** The paper is centered around benchmarking rerankers for **Retrieval-Augmented Generated LLMs** (SciRerankBench title) and mentions **RankRAG** (Reference [56]) and a specific application of RAG in BeefBot (Reference [59]).\n*   **Hybrid search:** This specific term is **not explicitly mentioned**, although the evaluation includes sparse methods like SPLADE (Table 4), which is often a component of hybrid search.\n*   **Chunking strategies:** This specific term is **not explicitly mentioned**, although abstract preprocessing (Algorithm 1, Line 2: \"Filter abstracts: len $\\in$ [100, 500]\") implies a form of document segmentation or chunking.\n\nIn summary, the page provides direct evidence for **Vector databases (Qdrant)**, **Embeddings (BCEmbed",
    "published_date": "2025-08-13T00:00:00.000Z",
    "score": null
  },
  "https://www.infracloud.io/blogs/improving-rag-accuracy-with-rerankers/": {
    "content_path": "../data/contents/3ebc66478493cec5.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.841677",
    "title": "Improving RAG Accuracy with Rerankers",
    "summary": "The webpage focuses on **Rerankers** and how they improve the accuracy of **Retrieval-Augmented Generation (RAG)** frameworks.\n\nHere is a summary of the topics covered that relate to your query:\n\n*   **Rerankers:** They are models used to re-evaluate and prioritize information retrieved by a base retriever based on its relevance to the user's query, leading to more specific and pertinent answers.\n*   **Embeddings:** Rerankers are built on top of embeddings, which convert text into numerical vectors that LLMs use for processing and retrieval.\n*   **RAG Architectures:** The article details how rerankers are integrated into the existing RAG pipeline, specifically showing a workflow where a reranker acts as a compressor/filter after the initial retrieval step.\n*   **Types of Rerankers:** Three main types are discussed: BERT-based, Cross-Encoder (high precision, high latency), and Bi-Encoder (efficient, scalable).\n*   **Considerations:** Key factors for selecting a reranker include Relevance, Latency, Scalability, and Integration.\n*   **Open source vs. closed source reranker models:** The trade-offs between transparency/flexibility (open source) and high performance/support (closed source) are outlined.\n\nThe page **does not** explicitly discuss: Vector databases, new efficient embedding models (beyond mentioning embeddings are used), RAG alternatives, hybrid search, or chunking strategies.",
    "published_date": "2024-08-26T00:00:00.000Z",
    "score": null
  },
  "https://fin.ai/research/using-llms-as-a-reranker-for-rag-a-practical-guide/": {
    "content_path": "../data/contents/d57693ebed5e2da3.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.844256",
    "title": "Using LLMs as a Reranker for RAG: A Practical Guide",
    "summary": "The webpage provides a practical guide on **Using LLMs as a Reranker for Retrieval-Augmented Generation (RAG)** systems.\n\nKey points covered include:\n\n*   **Core Idea:** Reranking reorders passages retrieved by vector search to ensure the final answer is grounded on the most relevant context. The authors use an LLM reranker instead of traditional open-source cross-encoders for better quality.\n*   **Reranking Methods:** The guide focuses on **pointwise reranking**, where the LLM scores each passage individually (on a scale of 1-10), as it allows for easier optimization compared to listwise or pairwise methods.\n*   **Optimizations for Speed and Latency:** To address the latency issues caused by large inputs/outputs when reranking many passages ($K=40$):\n    *   **Reducing Output Tokens:** Switching to a `Dict` format and implementing thresholding (omitting scores below 5) significantly cut latency.\n    *   **Parallel Reranking:** Splitting the candidate passages into batches ($N$ workers) and scoring them in parallel. A **round-robin batching strategy** is used to ensure each batch receives a mix of high/medium/low similarity passages, mitigating positional bias from the initial vector search.\n*   **Impact:** The optimizations reduced added latency from $\\sim 5$ seconds to **$<1$s** and cut costs by $\\sim 8$x, while showing a statistically significant quality uplift over the open-source BGE reranker in A/B tests for both the Fin and Copilot agents.\n*   **RAG Architectures:** The article briefly mentions how Copilot maintains **source diversity** by retrieving and scoring content across different types (internal, public, conversation history) in parallel streams before merging.\n*   **Prompt:** The specific **pointwise prompt** used for the LLM reranker, including detailed grading criteria and strict JSON output formatting rules, is shared.\n\nThe page directly addresses **rerankers**, **RAG architectures**, and provides context on the role of retrieval in the overall RAG flow. It does not detail vector databases, new efficient embedding models, hybrid search, or chunking strategies, although it mentions vector search as the initial retrieval step.",
    "published_date": "2025-09-11T22:46:52.000Z",
    "score": null
  },
  "https://huggingface.co/blog/embeddinggemma": {
    "content_path": "../data/contents/08e81a39b6fe1237.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.851934",
    "title": "Welcome EmbeddingGemma, Google's new efficient embedding model",
    "summary": "The webpage introduces **EmbeddingGemma**, Google's new efficient, state-of-the-art multilingual embedding model, perfect for on-device use cases.\n\nHere is a summary of how it relates to your query topics:\n\n*   **Embeddings (new efficient models):** EmbeddingGemma is a new, efficient model with only **308M parameters** and a **2K context window**. It ranks as the highest text-only multilingual embedding model under 500M on the MTEB benchmark.\n*   **Vector Databases / Retrieval:** The model is designed for retrieval tasks, using a Gemma3 transformer backbone modified with bi-directional attention (making it an encoder). It supports **Matryoshka Representation Learning (MRL)**, allowing the 768-dimensional output vector to be truncated to 512, 256, or 128 dimensions for faster and cheaper downstream processing like similarity search in vector databases.\n*   **RAG Architectures / RAG Alternatives:** The model is explicitly mentioned as unlocking new possibilities for **mobile RAG pipelines** and agents.\n*   **Rerankers:** The model supports a specific prompt for **Reranking** tasks (`\"Reranking\": \"task: search result | query: \"`).\n*   **Hybrid Search:** Not explicitly mentioned, but the model's strong retrieval capabilities support the embedding component of hybrid search.\n*   **Chunking Strategies:** Not explicitly mentioned, though the model has a 2048-token context window, which is relevant to chunking decisions in RAG.\n\nThe page also details extensive **Usage** instructions across various frameworks relevant to RAG pipelines, including **LangChain**, **LlamaIndex**, **Haystack**, and **txtai**, and provides deployment options via **Text Embeddings Inference (TEI)**. Furthermore, it covers **Finetuning** the model for domain-specific tasks.",
    "published_date": "2025-07-01T00:00:00.000Z",
    "score": null
  },
  "https://qdrant.tech/documentation/overview": {
    "content_path": "../data/contents/7d1096a29fa241bd.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.816041",
    "title": "Introduction",
    "summary": "The webpage introduces **Vector Databases** as a new way to interact with abstract data representations (vectors or **embeddings**) derived from machine learning models. It highlights their use in applications like **semantic search** and **recommendation systems**.\n\nThe page focuses on **Qdrant**, describing it as a vector similarity search engine for storing, searching, and managing points (vectors with an optional payload).\n\nKey concepts covered include:\n*   **Vector Databases:** Optimized for storing and querying high-dimensional vectors efficiently, often using indexing techniques like HNSW.\n*   **Distance Metrics:** The page details the three most common metrics used for similarity search: **Cosine Similarity**, **Dot Product**, and **Euclidean Distance**.\n*   **Qdrant Architecture Components:** **Collections** (named sets of points), **Points** (vectors with an ID and payload), **Distance Metrics**, and **Storage** options (In-memory or Memmap).\n\nThe user query asks about: *Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies*.\n\n**Summary relative to the query:**\nThe page extensively covers **Vector databases** and **embeddings** (referring to them as abstract data representations derived from ML models). It also mentions **Distance Metrics** which are crucial for similarity search. However, the page **does not mention** **rerankers**, **RAG architectures**, **RAG alternatives**, **hybrid search**, or **chunking strategies**.\n\n**Conclusion:** The page provides information on vector databases and embeddings but does not answer the parts of the query related to RAG, rerankers, hybrid search, or chunking strategies.\n\nNo answer found (for the complete query).",
    "published_date": "2001-01-01T00:00:00.000Z",
    "score": null
  },
  "https://www.youtube.com/watch?v=dN0lsF2cvm4": {
    "content_path": "../data/contents/3d66362ddefef808.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.842497",
    "title": "Vector Databases simply explained! (Embeddings \\u0026 Indexes)",
    "summary": "This page provides a simple explanation of **Vector Databases**, covering what they are, how they work, and their use cases. It specifically mentions **vector embeddings** and **indexes**. The video also lists several **different vector database options**, including Pinecone, Weaviate, Chroma, Redis, Qdrant, Milvus, and Vespa.\n\nHowever, the page **does not** explicitly discuss:\n*   New efficient embedding models\n*   Rerankers\n*   RAG architectures or alternatives\n*   Hybrid search\n*   Chunking strategies\n\nTherefore, for the full scope of your query, the answer is **No answer found**.",
    "published_date": "2023-05-06T00:00:00.000Z",
    "score": null
  },
  "https://elastic.co/what-is/vector-database": {
    "content_path": "../data/contents/a57cc3b99bede103.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.866643",
    "title": "What is a vector database?",
    "summary": "The webpage provides a comprehensive overview of **vector databases** and related concepts crucial for modern information retrieval, particularly in the context of Large Language Models (LLMs) and Retrieval Augmented Generation (RAG).\n\nHere is a summary addressing the components mentioned in your query:\n\n*   **Vector Databases:** Specialized databases designed to store, manage, and search high-dimensional **vector embeddings**. They act as an external knowledge base to \"ground\" LLM responses, mitigating hallucination by enabling **semantic similarity searches**.\n*   **Embeddings (new efficient models):** Vector embeddings are numerical arrays representing data (text, images, etc.) generated by ML models. The page discusses **dense vectors** (high-dimensional, non-zero elements, common in transformer models) and **sparse vectors** (mostly zero elements, optimized for storage and compatible with traditional search like TF-IDF/BM25).\n*   **Rerankers:** **Semantic reranking** is a second-stage process in a \"retrieve-and-rerank\" pipeline. It uses a more accurate, computationally intensive model (like a **cross-encoder**) to reorder a smaller set of initial candidate documents retrieved by a faster method (like a **bi-encoder** or BM25) to improve final relevance.\n*   **RAG Architectures:** **Retrieval Augmented Generation (RAG)** is listed as a common use case for vector databases, where LLMs are provided with external, up-to-date knowledge to generate factual answers. The page details the two-stage **\"retrieve-and-rerank\"** process often used in these pipelines.\n*   **RAG Alternatives:** While the page focuses heavily on RAG, it implies alternatives by discussing the core components. The use of **hybrid search** (combining lexical and vector search) and the ability to build complex, modular search pipelines offer flexibility beyond a simple RAG setup.\n*   **Hybrid Search:** This is enabled by integrating vector storage with traditional database functionalities. It allows for a **single, unified query** that performs lexical search (keyword matching), vector search (semantic matching), and metadata filtering simultaneously, leading to more relevant results.\n*   **Chunking Strategies:** **No answer found** regarding specific chunking strategies. (The page mentions that embeddings are generated from data like \"words, phrases, or entire documents,\" but does not detail methods for splitting documents into chunks.)",
    "published_date": "2023-08-07T00:00:00.000Z",
    "score": null
  },
  "https://clickhouse.com/blog/how-to-build-ai-agents-mcp-12-frameworks": {
    "content_path": "data/contents/0d4c76e5626c3800.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.602970",
    "title": "How to build AI agents with MCP: 12 framework comparison (2025)",
    "summary": "The webpage provides a comparison of 12 major AI agent SDKs and frameworks that support the **Model Context Protocol (MCP)**, which standardizes how applications interact with LLMs and their tools.\n\nThe summary relevant to your query is:\n\nThe article discusses **Agent SDKs** as building blocks for agentic experiences, often providing boilerplate for integrating with **MCP servers**. It lists and compares 12 frameworks with MCP support, including:\n\n*   **LangChain** and **LlamaIndex** (noted for ecosystem breadth and data-aware applications/RAG, respectively).\n*   **OpenAI Agents SDK** (for delegation patterns).\n*   **Claude Agent SDK** (for security-first production, requiring explicit tool allowlists).\n*   **CrewAI** (for multi-agent workflows).\n*   **Google Agent Development Kit** (supports Java).\n*   **DSPy** (for automatic prompt optimization).\n*   **PydanticAI** (for type safety and data validation).\n*   **mcp-agent** (inspired by Anthropic patterns).\n*   **Agno** (for speed and minimal boilerplate).\n*   **Microsoft Agent Framework** (for Azure integration).\n*   **Upsonic** (targeted toward the financial sector).\n\nThe text also explains that **MCP servers** define sets of tools LLMs can interact with, similar to REST APIs but standardized for LLM tool usage. It touches upon concepts like **function calling** (noting MCP is model-agnostic while function calling is model-specific) and mentions that frameworks handle complexities like tool discovery and message passing.\n\n**Regarding other specific terms in your query:**\n\n*   **Agent memory, agentic memory, context compression, long context:** These advanced concepts are not explicitly detailed or compared across the frameworks in this summary, although frameworks like LangChain and LlamaIndex are generally associated with managing context and data retrieval (RAG).\n*   **Agent orchestration:** Mentioned in the context of CrewAI (multi-agent workflows) and Google ADK.\n*   **Agent evaluations, LLM reasoning trace evaluation:** Not explicitly covered as a feature comparison point for the listed frameworks.\n*   **Prompt engineering, context engineering:** DSPy is highlighted for automatic prompt optimization.\n*   **Supervised fine-tuning, reinforcement learning:** These advanced training methods are not discussed in relation to the",
    "published_date": "2025-10-13T00:00:00.000Z",
    "score": null
  },
  "https://apipie.ai/docs/blog/top-10-opensource-ai-agent-frameworks-may-2025": {
    "content_path": "../data/contents/1fd425c989155561.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:33.743763",
    "title": "Top 10 Open-Source AI Agent Frameworks of May 2025",
    "summary": "The webpage discusses the top open-source AI agent frameworks of May 2025 and the capabilities they provide, such as planning, tool use, memory maintenance, and multi-agent collaboration.\n\nHere is a summary of the information relevant to your query:\n\n*   **Agent Frameworks:** The article lists and details 10 top frameworks, including **LangChain/LangGraph**, **CrewAI**, **AG2**, **OpenAI Agents SDK**, **Google Agent Development Kit (ADK)**, **Microsoft Semantic Kernel (SK)**, **Hugging Face SmolAgents**, **LlamaIndex**, **Pydantic AI**, and **Agno**.\n*   **Tool Use/Function Calling:** Most frameworks support tool use.\n    *   **LangChain/LangGraph** has an extensive tool library.\n    *   **OpenAI Agents SDK** focuses on first-class function calling.\n    *   **Google ADK** supports OpenAPI specs as tools.\n    *   **Pydantic AI** enforces structured outputs via schema-validated function calling.\n*   **Agent Memory:** Memory support is a key feature:\n    *   **LangChain/LangGraph** supports multiple memory types and persistent context.\n    *   **LlamaIndex** specializes in memory via vector stores and indices (RAG focus).\n    *   **Semantic Kernel** offers rich abstractions for memory.\n*   **Agent Orchestration/Multi-Agent:** Several frameworks focus on orchestration:\n    *   **CrewAI** uses role-based collaboration and offers self-organizing or scripted flows.\n    *   **AG2** uses an event-driven, multi-agent conversation framework.\n    *   **Google ADK** provides explicit constructs for Sequential, Loop, and Parallel agents.\n*   **Agentic Memory/Agent Frameworks:** The entire article is about agent frameworks, which inherently deal with agentic memory and reasoning structures.\n*   **Structured Outputs:** **Pydantic AI** is highlighted for bringing type safety and schema enforcement to LLM outputs, ensuring structured results.\n*   **MCP Servers (Model Context Protocol):** The future trends section mentions the emerging **MCP (Model Context Protocol)**, and the **Google ADK** is noted for supporting it.\n*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** **LangChain",
    "published_date": "2025-05-01T00:00:00.000Z",
    "score": null
  },
  "https://github.com/hideya/mcp-langchain-tools-usage": {
    "content_path": "../data/contents/83d761f2b7c05d27.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.830281",
    "title": "GitHub - hideya/langchain-mcp-tools-ts-usage: MCP Tools Usage From LangChain ReAct Agent / Example in TypeScript",
    "summary": "The webpage describes a TypeScript project demonstrating the usage of **Model Context Protocol (MCP) servers** with a **LangChain ReAct Agent**.\n\nKey points related to your query:\n\n*   **Agent Frameworks/Libraries:** It specifically uses **LangChain** (in TypeScript) and leverages the `@h1deya/langchain-mcp-tools` library.\n*   **Tool Use/Function Calling:** It shows how to convert MCP server tools into **LangChain-compatible tools** (`StructuredTool[]`) using the `convertMcpToLangchainTools()` utility function.\n*   **LLMs/Providers:** It uses **Google GenAI's `gemini-2.5-flash`** as the LLM, with commented-out code for OpenAI and Anthropic LLMs.\n*   **Structured Outputs:** The conversion process handles LLM provider-specific schema transformations to prevent compatibility issues, implying support for structured tool definitions.\n\nThe page focuses on the integration between LangChain agents and external tools exposed via MCP servers, but it does not explicitly detail general concepts like **agent memory**, **agent orchestration**, or specific SDKs like **OpenAI Agents SDK**, **Anthropic Agents SDK**, or **Google SDK** beyond using Google GenAI for the LLM.",
    "published_date": null,
    "score": null
  },
  "https://playbooks.com/mcp/rectalogic-langchain": {
    "content_path": "../data/contents/6c8cdf68e98f6781.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.973443",
    "title": "LangChain Integration MCP server",
    "summary": "The webpage describes the **LangChain Integration MCP server**, which allows **LangChain-powered applications** to use **MCP tools** (Model Context Protocol) for controlled interaction with local resources like the filesystem.\n\nKey points relevant to your query:\n\n*   **Agent Infrastructure/Frameworks:** It specifically integrates with **LangChain** to provide **tool calling capabilities**.\n*   **Tool Use/Function Calling:** The MCP server acts as a tool that language models can invoke (e.g., to read files).\n*   **MCP Servers:** The core topic is setting up and using this specific MCP server (`langchain-mcp`).\n*   **Integration:** Instructions are provided for integrating this server with specific AI environments like **Claude Code**, **Cursor**, and **Claude Desktop**.\n\nThe page **does not** explicitly detail:\n*   MCP servers in general beyond this specific LangChain integration.\n*   Agent memory or agentic memory concepts.\n*   OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK.\n*   Agent orchestration.\n*   Structured outputs (though tool use implies structured interaction).",
    "published_date": "2024-11-26T00:00:00.000Z",
    "score": null
  },
  "https://blog.box.com/using-existing-mcp-server-langchain-mcp-adapters": {
    "content_path": "../data/contents/14992dbcd50e2c85.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.912826",
    "title": "Using an Existing MCP Server with LangChain MCP Adapters",
    "summary": "The webpage describes how the new `langchain-mcp-adapters` package allows developers to integrate existing **Model Context Protocol (MCP)** servers into **LangChain** agents.\n\n**Key points relevant to your query:**\n\n*   **MCP (Model Context Protocol):** A protocol developed by Anthropic for structured interactions between language models and tool servers.\n*   **LangChain Integration:** The adapter converts MCP tools into LangChain/LangGraph-compatible tools, allowing hundreds of existing MCP tools to be used seamlessly inside LangChain agents.\n*   **Agent Frameworks:** The article focuses on using this integration within **LangChain** and **LangGraph** workflows.\n*   **Tool Use:** It demonstrates reusing an existing Box MCP server (which wraps Box AI endpoints) to provide structured data extraction tools (like `box_ai_ask`) to a LangChain agent.\n*   **Benefits:** This avoids writing glue code, allows reusing existing MCP tools, and enables chaining multiple tool servers together in a single agent.\n\nThe page does not explicitly discuss **MCP servers** as a general infrastructure component outside of the context of LangChain integration, nor does it detail **agent memory**, **agentic memory**, **OpenAI Agents SDK**, **Anthropic Agents SDK**, **Google SDK**, **function calling**, **structured outputs**, or **agent orchestration** beyond the context of building LangGraph workflows that utilize MCP tools.",
    "published_date": "2025-09-29T00:00:00.000Z",
    "score": null
  },
  "https://docs.langchain.com/langgraph-platform/server-mcp": {
    "content_path": "../data/contents/9ec0c5a4e0fa8f7f.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.129788",
    "title": "MCP endpoint in Agent Server",
    "summary": "The webpage describes the **Model Context Protocol (MCP) endpoint in LangChain's Agent Server**.\n\n**Key Summary Points:**\n\n*   **What MCP is:** MCP is an open protocol for describing tools and data sources in a model-agnostic format, allowing LLMs to discover and use them via a structured API.\n*   **Implementation:** Agent Server implements MCP using the Streamable HTTP transport. This allows **LangGraph agents** to be exposed as **MCP tools**.\n*   **Endpoint Location:** The MCP endpoint is available at `/mcp` on the Agent Server.\n*   **Usage:** MCP-compliant clients (supporting Streamable HTTP) can connect to the Agent Server to use these exposed agents as tools. Examples are provided for connecting via Python (using `langchain-mcp-adapters`) and JavaScript/TypeScript.\n*   **Exposing Agents as Tools:** Agents are exposed with their name, description, and input schema. It is recommended to define custom agents with explicit input/output schemas rather than relying on the general `AnyMessage` state.\n*   **User-Scoped Tools:** Custom authentication middleware can be used to populate user context, allowing user-scoped MCP tools to be accessed within a LangSmith deployment.\n*   **Limitations:** The current LangGraph MCP implementation **does not support sessions**; each `/mcp` request is stateless.\n*   **Disabling MCP:** The endpoint can be disabled by setting `\"disable_mcp\": true` in the configuration file.\n\n**Regarding the User Query:**\n\nThe query lists several concepts related to agent infrastructure, frameworks, and SDKs: *agent\\_infrastructure: MCP servers, tool use, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration*.\n\nThe webpage directly addresses **MCP servers**, **tool use** (exposing agents as tools), **LangChain** (specifically LangGraph/Agent Server), and **structured outputs** (via defining explicit schemas).\n\nIt **does not** provide specific information on:\n*   Agent memory or agentic memory.\n*   LlamaIndex.\n*   OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK.\n*   Agent orchestration (beyond the context of LangGraph).\n\n**Summary tailored to the query:**\n\nThe page details the **",
    "published_date": "2025-05-12T00:00:00.000Z",
    "score": null
  },
  "https://www.ema.co/additional-blogs/addition-blogs/initialize-ai-agent-memory-openai-sdk": {
    "content_path": "../data/contents/3b0a4f2ae9a34bab.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.949761",
    "title": "How to Build an AI Agent with Memory Using the OpenAI SDK&lt;!-- --&gt;",
    "summary": "The webpage explains how to build an AI agent with memory using the **OpenAI SDK**.\n\n**Key takeaways related to your query:**\n\n*   **Agent Memory:** The article details three layers of AI memory: **Short-Term (Session) Memory** (for immediate context), **Episodic (Contextual) Memory** (continuity across recent sessions), and **Long-Term (Vector) Memory** (persistent knowledge stored in vector databases like Pinecone).\n*   **OpenAI SDK Capabilities:** The SDK allows developers to initialize agents with memory using stateful configurations, including built-in **Session** mechanisms (short-term) and integration with external **Vector stores** (long-term).\n*   **Agent Architecture:** It describes a conceptual flow involving a Gateway, **Agent Runtime** (built with the OpenAI SDK), a **Memory Layer**, a **Vector Store/Database**, and **External Tools**.\n*   **Agent Frameworks/SDKs Mentioned:** The article focuses specifically on the **OpenAI Agents SDK**. It mentions integrating external vector databases like Pinecone, Weaviate, or Zep for long-term memory.\n*   **Agent Orchestration/Tool Use:** The architecture section mentions the agent runtime orchestrating prompts and deciding when to call **external tools** (CRMs, ERPs). The enterprise solution **Ema** is introduced as an **orchestration** layer with a Generative Workflow Engine for managing multi-agent workflows.\n*   **Structured Outputs/Function Calling:** These specific terms are **not explicitly detailed** in the context of the OpenAI SDK memory implementation described here, although the concept of connecting to external tools implies tool use.\n*   **MCP Servers, LlamaIndex, Anthropic Agents SDK, Google SDK:** These specific terms are **not mentioned** in the provided text.\n\n**Summary:** The page focuses on enabling memory within agents using the OpenAI SDK, detailing short-term session memory and long-term vector memory, and positioning an enterprise platform called Ema for orchestration and scaling. It does not cover most of the other specific infrastructure components listed in your query.",
    "published_date": "2025-11-10T14:09:00.000Z",
    "score": null
  },
  "https://docs.retool.com/agents/guides/tools/connect-to-mcp-server": {
    "content_path": "../data/contents/fc2d033a20d6e356.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.194754",
    "title": "Connect an MCP server to an agent",
    "summary": "The webpage explains how to connect an **MCP (Module Context Protocol) server** to a **Retool Agent**.\n\n**Key points:**\n\n*   **MCP Server:** An open standard introduced by Anthropic that allows LLM-applications (clients) to call tools from a remote system (server).\n*   **Functionality:** Connecting an MCP server allows an agent to dynamically retrieve, select, and invoke tools from external data sources (like GitHub, Slack, etc.) provided by the server.\n*   **Connection Steps:** You add the MCP server as a new tool within the agent's configuration in Retool, providing the server URL and authentication details if necessary.\n*   **Authentication:** Retool supports OAuth 2.0 and Basic Auth for MCP resources.\n*   **Protocol Versions:** The page lists supported MCP protocol versions, with the latest being \"2025-06-18\".\n*   **Exposing Local Servers:** Instructions are provided on using tools like `supergateway` and `ngrok` to expose locally running MCP servers (which often run over `stdio`) via an HTTP gateway so Retool can connect to them.\n\nThe content directly relates to **agent infrastructure**, **tool use**, and **agent frameworks** (specifically within the Retool ecosystem).",
    "published_date": null,
    "score": null
  },
  "https://openai.github.io/openai-agents-python/tools/": {
    "content_path": "../data/contents/c36c22219c47e960.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.781389",
    "title": "Tools - OpenAI Agents SDK",
    "summary": "The provided web page details the **Tools** available in the **OpenAI Agents SDK**. Tools allow agents to take actions such as fetching data, running code, or calling external APIs.\n\nThe three classes of tools are:\n\n1.  **Hosted tools:** Built-in tools running on LLM servers, including `WebSearchTool`, `FileSearchTool` (for OpenAI Vector Stores), `ComputerTool`, `CodeInterpreterTool`, `HostedMCPTool` (exposing a remote MCP server's tools), `ImageGenerationTool`, and `LocalShellTool`.\n2.  **Function calling:** Allows using any Python function as a tool. The SDK automatically generates the tool schema from the function signature and uses docstrings for descriptions. It supports synchronous or asynchronous functions and can return text, images (`ToolOutputImage`), or files (`ToolOutputFileContent`). Custom `FunctionTool` objects can also be created manually.\n3.  **Agents as tools:** Enables one agent to call another agent, facilitating agent orchestration. This allows for complex workflows where a central agent delegates tasks to specialized agents (e.g., translation agents). Tools can be conditionally enabled/disabled at runtime using an `is_enabled` parameter (boolean or callable).\n\nThe page also covers advanced features like:\n*   Custom output extraction from sub-agent runs.\n*   Streaming nested agent runs via an `on_stream` callback.\n*   Handling errors in function tools using a `failure_error_function`.\n\n**Regarding your specific query terms:**\n\n*   **agent\\_infrastructure, agent frameworks, LangChain, LlamaIndex, Anthropic Agents SDK, Google SDK, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context, supervised fine-tuning, reinforcement learning:** These terms are **not** discussed in detail on this page, which focuses specifically on the *Tools* feature within the *OpenAI Agents SDK*.\n*   **tool use:** Directly addressed as the core topic of the page.\n*   **deep research:** Implied by the `WebSearchTool`.\n*   **agent memory, agentic memory:** Not explicitly discussed.\n*   **context compression:** Not explicitly discussed.\n*   **OpenAI Agents SDK:** The subject of the page.\n*   **function calling:** Explicitly detailed.\n*   **MCP servers:** Mentioned via the `Hosted",
    "published_date": null,
    "score": null
  },
  "https://github.com/langchain-ai/agent-protocol": {
    "content_path": "../data/contents/9b35372ec2607541.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.101806",
    "title": "Search code, repositories, users, issues, pull requests...",
    "summary": "The provided web page is the GitHub repository for the **Agent Protocol** by `langchain-ai`.\n\nThis protocol aims to codify the framework-agnostic APIs needed to serve LLM agents in production. It centers around three core concepts:\n\n1.  **Runs:** APIs for executing an agent (stateless one-shot interactions or background runs).\n2.  **Threads:** APIs to organize multi-turn executions, providing persistent state, history tracking, and concurrency controls.\n3.  **Store:** APIs for long-term memory, allowing for customizable memory scopes (user, thread, etc.) and structured data storage with CRUD and search capabilities.\n\nThe protocol defines specific endpoints for these concepts, including managing threads, initiating runs (with options for waiting or streaming output), introspecting agent capabilities, and interacting with the memory store. It also defines a **Message** spec that is a subset of formats supported by major LLM providers like OpenAI and Anthropic.\n\n**Regarding your specific query terms:**\n\n*   **agent\\_infrastructure, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** The page describes the **Agent Protocol** itself, which is designed to be framework-agnostic. It mentions that **LangGraph Platform** implements a superset of this protocol, but it does not detail the infrastructure, specific SDKs (like OpenAI, Anthropic, or Google), or other frameworks (like LlamaIndex) themselves, only the standardized API layer *for* serving agents.\n*   **tool use, function calling, structured outputs:** These concepts are implied by the need for agents to interact with external systems and define schemas (mentioned under Agent Introspection), but the document does not explicitly detail the mechanisms for tool use, function calling, or structured outputs beyond referencing JSON Schema for agent configuration.\n*   **agent memory, agentic memory:** The **Store** section directly addresses long-term memory capabilities within the protocol.\n*   **agent orchestration:** This is the overall goal of the protocol, managing runs, threads, and state for agent execution.\n*   **MCP servers:** This term is **not mentioned** in the text.\n\n**Summary in relation to the query:** The page describes the **Agent Protocol**, a standardized API specification for serving LLM agents, covering concepts like **runs**, **threads**, **agent orchestration**, and **agent memory (Store)**. It",
    "published_date": "2024-11-12T00:00:00.000Z",
    "score": null
  },
  "https://www.anthropic.com/engineering/advanced-tool-use": {
    "content_path": "../data/contents/eafa6e2f9f229d66.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.958454",
    "title": "Introducing advanced tool use on the Claude Developer Platform",
    "summary": "The webpage introduces three new beta features on the Claude Developer Platform designed to enhance **tool use** for building AI agents:\n\n1.  **Tool Search Tool:** This allows Claude to dynamically discover tools on-demand from a large library (like those connected to **MCP servers**) instead of loading all definitions upfront. This significantly reduces context window consumption (preserving up to 95% of context) and improves tool selection accuracy when dealing with many tools.\n2.  **Programmatic Tool Calling (PTC):** This allows Claude to orchestrate tool execution using code (like Python) within a code execution environment. This solves the problem of **context pollution** from intermediate results and reduces **inference overhead** by minimizing round-trips to the model. It is ideal for complex, multi-step workflows involving data processing or loops.\n3.  **Tool Use Examples:** This allows developers to provide concrete examples of how to use a tool alongside its JSON schema. This helps Claude learn usage patterns, parameter correlations, and domain-specific conventions, leading to more accurate **structured outputs** and fewer malformed calls than schema definitions alone provide.\n\nThese features are presented as foundational for building effective agents that handle scale and complexity, complementing existing agent concepts like **agent frameworks** and **function calling**. The article does not explicitly mention **agent memory**, **agentic memory**, **LangChain**, **LlamaIndex**, **OpenAI Agents SDK**, **Anthropic Agents SDK**, **Google SDK**, or **agent orchestration** beyond the general context of building agents.",
    "published_date": null,
    "score": null
  },
  "https://openai.github.io/openai-agents-python/agents/": {
    "content_path": "../data/contents/3a0ed7c956fa0893.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.255342",
    "title": "Agents - OpenAI Agents SDK",
    "summary": "The provided web page details the **OpenAI Agents SDK**, focusing on the core building block: the **Agent**.\n\nHere is a summary of the concepts mentioned in your query that are covered by the page:\n\n*   **Agent Frameworks/SDKs:** The page describes the **OpenAI Agents SDK** itself.\n*   **Agent Configuration:** Agents are configured with a `name`, `instructions` (system prompt), `model`, and **`tools`**.\n*   **Tool Use/Function Calling:** Agents can be equipped with tools (defined via `@function_tool`). The page discusses how to force tool use using the `tool_choice` setting (options include `auto`, `required`, or specifying a tool name).\n*   **Structured Outputs:** Agents can be configured to produce specific output types using the `output_type` parameter, which leverages Pydantic objects and forces the model to use **structured outputs** instead of plain text.\n*   **Agent Orchestration (Multi-agent systems):** Two design patterns are described:\n    1.  **Manager (agents as tools):** A central agent invokes specialized sub-agents exposed as tools.\n    2.  **Handoffs:** Peer agents delegate control to a specialized agent that takes over the conversation.\n*   **Agent Memory/Context:** The concept of **`context`** is introduced as a dependency-injection tool that serves as a \"grab bag of dependencies and state for the agent run,\" passed to every agent, tool, and handoff.\n\n**Concepts from your query *not* explicitly detailed or named in the text:**\n\n*   MCP servers\n*   Agent memory (though context serves as state)\n*   Agentic memory\n*   LangChain\n*   LlamaIndex\n*   Anthropic Agents SDK\n*   Google SDK\n\n**Summary:**\n\nThe OpenAI Agents SDK defines an Agent as an LLM configured with instructions and tools. It supports **tool use** (function calling) and **structured outputs** via Pydantic models. Multi-agent systems can be designed using **manager/orchestration** patterns or **handoffs**. Agents maintain state and dependencies through a configurable **context** object.",
    "published_date": "2011-06-09T00:00:00.000Z",
    "score": null
  },
  "https://research.aimultiple.com/agentic-frameworks/": {
    "content_path": "../data/contents/3f7abf615a9115d2.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.964871",
    "title": "Top 5 Open-Source Agentic Frameworks",
    "summary": "The webpage discusses the **Top 5 Open-Source Agentic Frameworks** (CrewAI, LangChain, OpenAI Swarm, and LangGraph are benchmarked, with AutoGen also mentioned).\n\nHere is a summary of how the mentioned concepts relate to the frameworks discussed:\n\n*   **Agent Frameworks:** The entire page is a comparison of agentic frameworks (LangGraph, AutoGen, CrewAI, OpenAI Swarm, LangChain).\n*   **Tool Use/Function Calling:** All frameworks support **tool use**, which enables agents to call external APIs or functions. CrewAI, LangGraph, and AutoGen use **structured functions** (often via annotations). OpenAI Swarm infers function behavior via **docstrings**. LangChain uses explicit interfaces.\n*   **Agent Memory:**\n    *   **LangGraph** supports in-thread (short-term) and cross-thread (long-term) memory.\n    *   **CrewAI** provides layered memory out of the box, using ChromaDB and SQLite for short-term and long-term storage, and supports entity memory.\n    *   **LangChain** supports both short-term (in-memory buffers) and long-term memory (integrating with external vector stores).\n    *   **AutoGen** uses a contextual memory model for short-term context but lacks built-in persistent memory.\n    *   **OpenAI Swarm** is stateless and does not manage memory natively.\n*   **Agent Orchestration:**\n    *   **LangGraph** excels at complex workflows using a **graph-based (DAG)** approach for fine-grained orchestration.\n    *   **CrewAI** uses a role-based, declarative architecture, but its orchestration is limited to linear or loop-based flows.\n    *   **LangChain** operates primarily through single-agent patterns, requiring manual orchestration for multi-agent setups.\n    *   **AutoGen** uses free-form, asynchronous message passing for collaboration.\n    *   **OpenAI Swarm** currently operates via a single-agent control loop using routines.\n*   **Structured Outputs:** The page notes that most frameworks include built-in handling for **structured outputs** (e.g., JSON).\n*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** **LangChain** is benchmarked and discussed extensively. **Llama",
    "published_date": "2025-11-11T00:00:00.000Z",
    "score": null
  },
  "https://docs.aws.amazon.com/pdfs/bedrock-agentcore/latest/devguide/bedrock-agentcore-dg.pdf": {
    "content_path": "../data/contents/e255abab717cacb0.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.967592",
    "title": "",
    "summary": "The Amazon Bedrock AgentCore Developer Guide covers several components related to building and managing agents. Regarding your query:\n\n*   **agent\\_infrastructure:** The document details the **Amazon Bedrock AgentCore Runtime** which hosts agents and tools, and mentions support for deploying **MCP servers**. It also discusses using various **agent frameworks** like **LangGraph**, **OpenAI Agents SDK**, and **Google ADK** (though not explicitly listing all mentioned SDKs like LlamaIndex or Anthropic Agents SDK, it mentions integration capabilities).\n*   **tool use:** The guide covers **AgentCore Built-in Tools** (Code Interpreter, Browser) and how agents interact with tools via **AgentCore Gateway** (supporting Lambda, API Gateway, OpenAPI schemas, Smithy models, and **MCP servers** targets).\n*   **agent memory/agentic memory:** There is a dedicated section on **AgentCore Memory**, detailing **short-term memory** (ephemeral context) and **long-term memory** (using retrieval and custom strategies).\n*   **agent frameworks:** The guide explicitly mentions support for using frameworks like **Strands Agents**, **LangGraph**, **Google ADK**, **OpenAI Agents SDK**, **Microsoft AutoGen**, and **CrewAI**.\n*   **LangChain/LlamaIndex:** **LangChain** (and **LangGraph**) is mentioned under \"Use any agent framework.\" **LlamaIndex** is not explicitly mentioned.\n*   **OpenAI Agents SDK/Anthropic Agents SDK/Google SDK:** **OpenAI Agents SDK** and **Google ADK** are mentioned under supported frameworks. **Anthropic Agents SDK** is not explicitly mentioned.\n*   **function calling/structured outputs:** These concepts are not explicitly detailed in the provided text snippets, although they are common features in agent development.\n*   **agent orchestration:** The document discusses agent lifecycle, session management, and using frameworks like LangGraph, which implies orchestration capabilities within the AgentCore ecosystem.\n\nIn summary, the document provides significant information on **MCP servers**, **tool use**, **agent memory**, **agent frameworks** (including LangChain/LangGraph and OpenAI/Google SDKs), **agent orchestration** (via frameworks and session management), and the **AgentCore Runtime** infrastructure.\n\nThe user query is a list of keywords related to agent infrastructure, frameworks, and capabilities: 'agent\\_infrastructure: MCP servers, tool use, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration'.\n\nThe provided webpage text discusses **Amazon Bedrock AgentCore**, which is an agentic platform. It mentions several components and concepts relevant to the query:\n\n*   **Agent Frameworks:** AgentCore Runtime works with custom and open-source frameworks including **CrewAI, LangGraph, LlamaIndex, Google ADK, OpenAI Agents SDK, and Strands Agents**.\n*   **Agent Memory:** AgentCore includes a **Memory** service for building context-aware agents, supporting short-term and long-term memory, and integrating with frameworks like **LangGraph, LangChain, and LlamaIndex**.\n*   **MCP Servers:** The **Gateway** service converts APIs into Model Context Protocol (**MCP**)-compatible tools and connects to pre-existing **MCP servers**. The text also details the **AgentCore MCP server** setup.\n*   **Tool Use/Function Calling:** The **Gateway** service enables agents to use tools (APIs, Lambda functions). The **Policy** capability controls agent-to-tool interactions.\n*   **SDKs:** The text mentions the **AgentCore Python SDK** and integration with **OpenAI Agents SDK** and **Google ADK** (which relates to Google SDK). It does not explicitly mention the **Anthropic Agents SDK**.\n*   **Structured Outputs:** Not explicitly detailed, but implied through agent capabilities and evaluation.\n*   **Agent Orchestration:** AgentCore Runtime supports multi-agent workloads and extended execution time, suggesting orchestration capabilities.\n\n**Summary:**\n\nAmazon Bedrock AgentCore is an agentic platform that supports building, deploying, and operating AI agents. It integrates with various **agent frameworks** such as **LangGraph, LlamaIndex, CrewAI, and the OpenAI Agents SDK** and **Google ADK**. It provides dedicated services for **agent memory** (supporting short-term and long-term context) and a **Gateway** service to convert APIs into tools compatible with the **Model Context Protocol (MCP)**, allowing connection to existing **MCP servers**. The platform also offers **AgentCore Policy** to govern **tool use** and agent interactions",
    "published_date": "2025-07-16T00:00:00.000Z",
    "score": null
  },
  "https://reference.langchain.com/python/langchain_mcp_adapters/": {
    "content_path": "../data/contents/ffe8b03e4a148c39.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.106197",
    "title": "langchain-mcp-adapters ¶",
    "summary": "The provided web page details the `langchain-mcp-adapters` package, which facilitates connecting LangChain applications to **MCP servers**.\n\nHere is a summary of how the page relates to your query:\n\n*   **agent\\_infrastructure: MCP servers**: The core of the documentation is about connecting to and interacting with **MCP servers** using the `MultiServerMCPClient`.\n*   **tool use**: The package includes functionality to load **LangChain-compatible tools** from MCP servers (`get_tools`, `load_mcp_tools`) and manage tool execution via **interceptors** (`ToolCallInterceptor`).\n*   **agent frameworks, LangChain**: The package explicitly adapts MCP components (tools, prompts, resources) to be **LangChain-compatible**.\n*   **function calling, structured outputs**: The `MCPToolArtifact` indicates that structured content is returned from MCP tool calls, which relates to structured outputs often used in function calling patterns.\n*   **agent memory, agentic memory, agent orchestration, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK**: **No direct information** is provided in this document regarding agent memory, agentic memory, orchestration, or specific SDKs like LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK. The focus is purely on the adapter layer between LangChain and MCP servers.\n\n**In summary:** This page describes the LangChain adapter for interacting with **MCP servers** to load **tools** and **prompts**, which are components of an agent infrastructure. It does not cover agent memory, agentic memory, or the other specific agent frameworks/concepts listed in your query.",
    "published_date": null,
    "score": null
  },
  "https://apidog.com/blog/langchain-mcp-adapters/": {
    "content_path": "../data/contents/4059bd06f6c7cb0a.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.156972",
    "title": "How to Integrate LangChain with MCP Servers Using langchain-mcp-adapters",
    "summary": "The webpage details how to integrate tools exposed via the **Model Context Protocol (MCP)** servers into **LangChain** applications using the `langchain-mcp-adapters` library.\n\nThis library acts as a bridge, converting MCP tools into LangChain-compatible `BaseTool` objects. Key components include:\n\n*   **MCP Server:** Exposes functions (tools) using decorators (`@()`) and can run via `stdio` (subprocess) or `sse` (web service).\n*   **`langchain-mcp-adapters`:** Provides the `MultiServerMCPClient` to connect to one or more MCP servers simultaneously, handling transport details (`stdio` or `sse`).\n*   **Tool Conversion:** Functions like `load_mcp_tools` fetch tool definitions and wrap them into LangChain `StructuredTool` objects.\n*   **Agent Integration:** These converted tools can then be passed directly to **LangGraph** agents (e.g., via `create_react_agent`).\n\nThe article provides step-by-step examples for setting up a single math server (`stdio`) and connecting to multiple servers (math via `stdio` and a weather server via `sse`) using the `MultiServerMCPClient`. It also shows how to integrate this setup into a persistent **LangGraph API Server** using an async context manager for client lifecycle management.",
    "published_date": "2025-04-14T05:07:49.000Z",
    "score": null
  },
  "https://mcp.so/server/langchain-mcp-client/datalayer": {
    "content_path": "../data/contents/57b1bc41c477b3a4.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.974053",
    "title": "MCP Servers",
    "summary": "The webpage describes the **LangChain MCP Client**, which allows users to connect to **Model Context Protocol (MCP) servers** and utilize **LangChain-compatible language models**.\n\nWhile the user query lists many related concepts like \"tool use,\" \"agent memory,\" \"agent frameworks,\" \"function calling,\" and various SDKs, the provided text specifically focuses on:\n\n*   **MCP servers** (connecting to them).\n*   **LangChain** (the client is LangChain-compatible).\n\nThe text **does not** explicitly detail: tool use, agent memory, agentic memory, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, structured outputs, or agent orchestration.\n\n**Summary based on the query and page content:**\n\nThe page details the **LangChain MCP Client**, a tool for connecting to **MCP servers** and using **LangChain-compatible LLMs**.",
    "published_date": "2025-02-20T00:00:00.000Z",
    "score": null
  },
  "https://mcpmarket.com/server/langchain-client": {
    "content_path": "../data/contents/ffc467ba057f9327.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.230632",
    "title": "LangChain Client: Connect ReAct Agents to MCP Servers",
    "summary": "The LangChain MCP Client facilitates the integration of **LangChain agents** with **Model Context Protocol (MCP) servers**. It converts MCP server tools into **LangChain-compatible tools** using the `langchain_mcp_tools` library, allowing for dynamic interaction via a CLI. Configuration involves `.env` and `llm_mcp_config.json5` files for API keys, LLM parameters, and MCP server details. Key features include using any **LangChain-compatible LLM**, connecting to any MCP server, and parallel initialization of multiple servers. Use cases involve automating tasks by chaining MCP server tools and building conversational AI applications that leverage these external services.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://docs.langchain.com/use-these-docs": {
    "content_path": "../data/contents/3831ba1a2b57527d.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.980007",
    "title": "Use docs programmatically",
    "summary": "The webpage describes how to use the LangChain documentation programmatically, primarily through the **Model Context Protocol (MCP) server**.\n\nKey points related to your query:\n\n*   **agent_infrastructure:** The page focuses on making the documentation accessible to AI tools and workflows.\n*   **agent memory/agentic memory:** The MCP server allows AI applications to query the latest docs in real-time, effectively providing a source of up-to-date knowledge for agents.\n*   **agent frameworks:** The documentation is for **LangChain** and **LangGraph**.\n*   **tool use/function calling/structured outputs:** While the page doesn't detail these concepts directly, it shows how to connect the documentation as a data source (via MCP) to AI assistants/tools (like Claude Code, Codex CLI, Cursor, VS Code) that would utilize these capabilities.\n*   **agent orchestration:** The ability to connect documentation to various tools suggests integration into broader agent workflows.\n*   **agent frameworks (LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK):** The page is specifically about **LangChain** documentation. It shows integration examples with **Claude** (Anthropic) and mentions **OpenAI Codex CLI**.\n*   **agent_infrastructure: MCP servers:** The core feature discussed is the built-in **Model Context Protocol (MCP) server** provided by the LangChain docs.\n\n**Summary:**\n\nThis page explains how to programmatically access the LangChain documentation using its built-in **Model Context Protocol (MCP) server**. This server allows AI applications (like those built with **LangChain** or using tools like **Claude** or **Codex CLI**) to query the documentation in real-time, serving as a source of current information for agents. It provides specific instructions on how to connect this MCP server to various tools, including **Claude Code**, **Claude Desktop**, **Codex CLI**, **Cursor/VS Code**, and **Antigravity**.",
    "published_date": "2025-04-01T00:00:00.000Z",
    "score": null
  },
  "https://docs.oap.langchain.com/setup/mcp-server": {
    "content_path": "../data/contents/6de3fbdb38ac4254.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.200165",
    "title": "MCP Server - Docs by LangChain",
    "summary": "The provided web page is documentation from **LangChain** specifically about setting up and configuring an **MCP Server** within the **Open Agent Platform**.\n\nIt details how to connect agents to MCP servers that support **Streamable HTTP requests**, covering two main configuration methods:\n\n1.  **Unauthenticated Servers:** Set the `NEXT_PUBLIC_MCP_SERVER_URL` environment variable to the server's URL.\n2.  **Authenticated Servers:** Requires setting `NEXT_PUBLIC_MCP_AUTH_REQUIRED=true`. This involves a proxy route (`/api/oap_mcp`) that uses a JWT (like Supabase's) to exchange for an MCP access token, which is then used for subsequent requests.\n\nThe page also explains how to **change the MCP Server URL** using a script located in `apps/web/scripts/`, which updates agent configurations via LangSmith authentication.\n\n**Regarding your query:** The page focuses specifically on **MCP Servers** within the LangChain ecosystem. It does **not** provide information on: tool use, agent memory, agentic memory, other agent frameworks (besides LangChain itself), LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, or general agent orchestration.",
    "published_date": "2025-05-12T00:00:00.000Z",
    "score": null
  },
  "https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-memory": {
    "content_path": "../data/contents/0a6e95bba417da68.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.366193",
    "title": "Using memory with Agents",
    "summary": "The provided web page focuses on **Using memory with Semantic Kernel Agents**, specifically detailing two memory components: **Mem0** for long-term, personalized memory across conversations, and **Whiteboard Memory** for short-term context retention within ongoing conversations, even if the chat history is truncated.\n\nThe summary relevant to your query is:\n\n*   **Agent Memory:** The page discusses adding memory functionality to Semantic Kernel Agents.\n*   **Agent Frameworks:** The context is specifically within the **Semantic Kernel** framework.\n*   **Agentic Memory:** It details two mechanisms for agent memory:\n    *   **Mem0:** A self-improving memory layer integrated via `Microsoft.SemanticKernel.Memory.Mem0Provider` to remember user preferences and context across multiple threads (long-term memory). It supports scoping by User ID.\n    *   **Whiteboard Memory:** Implemented via `Microsoft.SemanticKernel.Memory.WhiteboardProvider`, this captures and retains the most relevant information (requirements, decisions, actions) from the current conversation for short-term context.\n*   **Orchestration/Combining:** It shows how to combine both Mem0 and Whiteboard memory providers within the same `AgentThread` for a balance of long-term and short-term context.\n\n**The page does not mention or discuss:** MCP servers, tool use, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, or agent orchestration outside of combining the two memory types within Semantic Kernel.\n\n**Conclusion:** The page provides information on **agent memory** and **agent frameworks (Semantic Kernel)**, but does not cover most of the other specific technologies listed in your query.",
    "published_date": "2025-06-09T00:00:00.000Z",
    "score": null
  },
  "https://docs.oracle.com/en-us/iaas/Content/generative-ai-agents/function-calling-java-sdk.htm": {
    "content_path": "../data/contents/9b3e04aebc1eb022.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.987043",
    "title": "Function Calling with SDKs in Generative AI Agents",
    "summary": "The provided web page is a guide on **Function Calling with SDKs in Generative AI Agents**, specifically using the **OCI Java SDK** to integrate function calling into an application within Oracle Cloud Infrastructure (OCI) Generative AI Agents.\n\nThe summary of the page relevant to your query is:\n\nThe page details the process of using the **OCI Java SDK** to implement **function calling** for OCI Generative AI Agents. This involves:\n1.  **Prerequisites**: Setting up an agent, an agent endpoint, and the necessary OCI SDK.\n2.  **Creating a Function Calling Tool**: Demonstrating how to configure the client, define a function (e.g., creating an OCI Object Storage bucket) with its parameters, embed it in a `ToolConfig`, and then create the tool using the SDK.\n3.  **Chatting with an Agent**: Showing how to create a chat session, send a user message that triggers the function call, extract the required action (including the function name and arguments) from the agent's response, and then perform the action by sending the function output back to the agent to receive the final result.\n\n**Regarding your specific query terms:**\n\n*   **agent\\_infrastructure**: The context is OCI Generative AI Agents infrastructure.\n*   **tool use / function calling**: This is the primary topic of the page.\n*   **agent frameworks (LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK)**: These specific external frameworks are **not mentioned** on this page; the guide focuses exclusively on the **OCI Java SDK**.\n*   **agent memory / agentic memory**: These concepts are **not mentioned**.\n*   **structured outputs**: The process involves defining structured parameters for the function call, which relates to structured input/output, but the term \"structured outputs\" itself is **not explicitly discussed** outside the function definition schema.\n*   **agent orchestration**: This is **not mentioned**.\n*   **MCP servers**: This term is **not mentioned**.\n\n**Summary:** The page explains how to implement **function calling** using the **OCI Java SDK** within **OCI Generative AI Agents**. It does not cover MCP servers, agent memory, or the specific external agent frameworks listed in your query.",
    "published_date": "2025-10-17T16:48:10.000Z",
    "score": null
  },
  "https://docs.mem0.ai/cookbooks/integrations/agents-sdk-tool": {
    "content_path": "../data/contents/f26d36281c0c553a.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:09.999749",
    "title": "Memory-Powered Agent SDK",
    "summary": "The provided web page details the **Memory-Powered Agent SDK (Mem0)**, which allows developers to integrate persistent memory capabilities into AI agents, specifically highlighting its integration with **OpenAI Agents SDK**.\n\nThe summary relevant to your query is:\n\n*   **Agent Frameworks/SDKs:** The page focuses on using the Mem0 SDK with the **OpenAI Agents SDK** to create agents with persistent memory. It mentions the concept of building a \"Companion\" and using \"Playbooks.\"\n*   **Agent Memory:** The core feature is providing persistent memory by exposing Mem0 memories as **callable tools** within agent workflows. This allows agents to **Add to Memory**, **Search Memory**, and **Get All Memories**.\n*   **Tool Use/Function Calling:** The implementation explicitly defines memory operations (`add_to_memory`, `search_memory`, `get_all_memory`) as `@function_tool`s that the agent can call.\n*   **Structured Outputs:** While not explicitly detailed as a primary focus, the use of `pydantic` for defining context (`Mem0Context`) and the nature of tool definitions imply structured interaction.\n*   **Agent Orchestration:** The page shows a basic runtime loop demonstrating how the agent uses the memory tools to process user input and generate responses, which is a form of basic agent orchestration around memory access.\n\n**Missing/Unmentioned Topics:**\n\nThe page **does not** explicitly mention or detail: **MCP servers**, **agentic memory** (though it provides memory functionality), **Anthropic Agents SDK**, **Google SDK**, or **LangChain/LlamaIndex** (though it mentions \"ReAct Agents with Memory\" and \"Gemini 3 with Mem0\" in the navigation, the main body focuses only on the OpenAI SDK integration).\n\n**In summary, the page describes how to use the Mem0 SDK to add persistent memory (tool use) to agents built with the OpenAI Agents SDK.**",
    "published_date": null,
    "score": null
  },
  "https://blog.cloudflare.com/building-agents-with-openai-and-cloudflares-agents-sdk": {
    "content_path": "../data/contents/a5338625fc9f10cc.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:10.005505",
    "title": "Building agents with OpenAI and Cloudflare’s Agents SDK",
    "summary": "The webpage discusses building AI agents by combining the **OpenAI Agents SDK** (for cognition, planning, and tool orchestration) with **Cloudflare's Agents SDK** (for the execution layer, providing persistence, identity, and state management via Durable Objects).\n\nRegarding your specific query terms:\n\n*   **agent_infrastructure:** The article focuses on the execution layer provided by Cloudflare Durable Objects as the infrastructure where agents run, abstracting away traditional infrastructure concerns.\n*   **tool use, function calling, structured outputs:** These capabilities are primarily associated with the **OpenAI Agents SDK**, which defines the agent's \"brain\" and how it uses external tools.\n*   **agent memory, agentic memory:** The article highlights that OpenAI's SDK provides a *memory abstraction*, but Cloudflare's Durable Objects provide the *persistent storage* for this memory across invocations.\n*   **agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** The article explicitly mentions using the **OpenAI Agents SDK** as the example for defining the agent's cognition. It notes that the Cloudflare execution layer is designed to integrate with *any* agent runtime, implying compatibility with other frameworks, though it only names OpenAI's SDK specifically in the implementation example.\n*   **agent orchestration:** The OpenAI SDK handles planning and tool orchestration within the agent's logic, while Cloudflare enables multi-agent systems and workflow coordination through composable agents.\n\n**Summary:** The page details a pattern where the OpenAI SDK handles the agent's intelligence and tool use, while Cloudflare's SDK (using Durable Objects) provides the necessary persistent infrastructure, identity, and execution environment for these agents to live, remember, and scale.",
    "published_date": "2025-06-25T14:00:00.000Z",
    "score": null
  },
  "https://docs.mem0.ai/integrations/openai-agents-sdk": {
    "content_path": "../data/contents/d95915eb7e761c5e.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:10.013125",
    "title": "OpenAI Agents SDK",
    "summary": "The webpage describes the **OpenAI Agents SDK** integration with **Mem0** for providing persistent memory to agents.\n\nKey aspects covered that relate to your query include:\n\n*   **Agent Frameworks:** The page is listed under Agent Frameworks and specifically details the **OpenAI Agents SDK**.\n*   **Agent Memory/Agentic Memory:** The core feature is enabling agents to access **persistent memory** across conversations using Mem0 for enhanced context retention and personalization through automatic memory integration, multi-agent memory sharing, and flexible memory operations (saving and retrieving).\n*   **Tool Use/Function Calling:** The integration uses **function tools** (`search_memory` and `save_memory`) defined for the agent to interact with the memory system.\n*   **Agent Orchestration:** It demonstrates **Multi-Agent Workflow with Handoffs**, where specialized agents (like a Travel Planner and Health Advisor) are routed by a triage agent, all sharing the same memory store.\n\nThe page **does not explicitly mention or detail**:\n\n*   **MCP servers**\n*   **Structured outputs** (though function calling implies structured input/output for tools)\n*   **LangChain** or **LlamaIndex** (though they are listed in the navigation sidebar as other frameworks)\n*   **Anthropic Agents SDK** or **Google SDK** (though Google ADK is listed in the navigation)\n\n**Summary relevant to the query:** The page details how to integrate **Mem0** for **agent memory** and **tool use** within the **OpenAI Agents SDK** to support **agent orchestration** via multi-agent handoffs. It does not cover MCP servers, structured outputs, LangChain, LlamaIndex, Anthropic Agents SDK, or the Google SDK.",
    "published_date": "2023-07-15T00:00:00.000Z",
    "score": null
  },
  "https://blog.langchain.com/langmem-sdk-launch": {
    "content_path": "../data/contents/860f49ccf25988d1.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:10.015898",
    "title": "LangMem SDK for agent long-term memory",
    "summary": "The LangMem SDK is a library designed to provide **long-term memory** for AI agents, enabling them to learn and improve over time through memory extraction, prompt updates, and tracking of behaviors, facts, and events.\n\nThe SDK supports three main types of memory:\n1.  **Semantic Memory:** Stores key facts and knowledge (like user preferences) that ground an agent's responses, often overlapping with traditional RAG but focused on knowledge gained through interaction.\n2.  **Procedural Memory:** Focuses on evolving agent behavior by saving learned procedures as updated instructions in the agent's prompt, using prompt optimizers based on successful/unsuccessful interactions.\n3.  **Episodic Memory:** Stores memories of past interactions (specific experiences), often distilled into few-shot examples.\n\nLangMem integrates with any storage system and any **Agent framework**, and it natively integrates with **LangGraph's** long-term memory layer. It also mentions concepts like memory **namespaces** for privacy and scoping.\n\nThe user query lists many general agent infrastructure components, including **agent frameworks (LangChain, LlamaIndex)**, **agent memory**, **tool use**, **function calling**, and **agent orchestration**. The page directly addresses **agent memory** (specifically long-term memory) and its integration with **LangChain** (via LangGraph). It does not explicitly detail MCP servers, tool use, function calling, structured outputs, or the other specific SDKs mentioned (OpenAI Agents SDK, Anthropic Agents SDK, Google SDK).\n\n**Summary relevant to the query:**\nThe LangMem SDK provides a solution for **agent memory** (long-term memory) that can be used within **agent frameworks** like **LangChain** (via LangGraph integration). It focuses on enabling agents to learn through semantic, procedural, and episodic memory types.",
    "published_date": "2025-02-18T15:50:31.000Z",
    "score": null
  },
  "https://docs.cloud.google.com/agent-builder/agent-engine/memory-bank/quickstart-adk": {
    "content_path": "../data/contents/1c347aa37cead995.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:10.018375",
    "title": "Quickstart with Agent Development Kit \n\n    \n    \n       \n    \n\n     \n      \n      Stay organized with collections\n     \n     \n      \n      Save and categorize content based on your preferences.",
    "summary": "The provided web page is a quickstart guide for developing agents using the **Agent Development Kit (ADK)**, focusing on how to integrate and use **Memory Bank** for managing long-term memories.\n\nThe summary of the page relevant to your query is:\n\n*   **Agent Orchestration:** The ADK agent orchestrates calls to Memory Bank via an **ADK Runner** or **Agent Engine ADK template** to manage long-term memories.\n*   **Agent Memory/Agentic Memory:** The core focus is on using **Memory Bank** for long-term memory management. This is achieved by including a `Memory` tool (like `PreloadMemoryTool`) in the ADK agent definition, which controls when memories are retrieved and included in the prompt.\n*   **Agent Frameworks:** The guide specifically uses the **Vertex AI Agent Builder**'s **Agent Development Kit (ADK)**. It mentions using an **ADK Runner** (often in local environments like Colab) or the **Agent Engine ADK template** (for deployment to Agent Engine Runtime).\n*   **Tool Use:** The example agent includes a `Memory` tool (`PreloadMemoryTool`) to interact with memory services.\n*   **Function Calling/Structured Outputs:** These specific concepts are **not explicitly detailed** in the context of the ADK setup described here, although tool use implies some form of function calling capability.\n*   **MCP servers, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** These specific technologies are **not mentioned** in the provided text.\n\n**In summary, the page details setting up an agent using the Google Cloud ADK to leverage Memory Bank for agent memory and orchestration, but it does not cover MCP servers, LangChain, LlamaIndex, or the specific SDKs/frameworks you listed (OpenAI, Anthropic, Google SDK in a general sense beyond ADK).**",
    "published_date": "2025-10-29T00:00:00.000Z",
    "score": null
  },
  "https://openai.github.io/openai-agents-python/mcp/": {
    "content_path": "data/contents/72264555003dddc1.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.894308",
    "title": "Model context protocol (MCP)",
    "summary": "The provided web page details the **Model Context Protocol (MCP)**, an open standard that dictates how applications expose tools and context to Language Models (LLMs), likening it to a \"USB-C port for AI applications.\"\n\nThe summary relevant to your query focuses on the **tool use** and **agent infrastructure** aspects described:\n\n*   **Agent Infrastructure & Tool Use:** MCP standardizes how applications provide context and tools to LLMs. The OpenAI Agents Python SDK supports multiple MCP transports for connecting agents to tools:\n    *   **Hosted MCP server tools:** Tools are managed within OpenAI's infrastructure via `HostedMCPTool`, allowing the model to invoke remote tools without extra callbacks to the Python process. This supports connectors (like Google Calendar) and optional approval flows.\n    *   **Streamable HTTP MCP servers:** Using `MCPServerStreamableHttp` for connecting to HTTP servers where the application manages the network connection.\n    *   **HTTP with SSE MCP servers:** Using `MCPServerSse` for servers implementing HTTP with Server-Sent Events.\n    *   **stdio MCP servers:** Using `MCPServerStdio` for servers run as local subprocesses communicating over stdin/stdout (useful for quick proofs of concept).\n*   **Tool Filtering:** MCP servers support filtering tools either statically (allow/block lists) or dynamically using a callable function based on the context.\n*   **Prompts:** MCP servers can dynamically provide agent instructions via prompt templates (`list_prompts()` and `get_prompt()`).\n*   **Caching and Tracing:** The SDK supports caching tool lists to reduce latency and automatically captures tracing information for MCP server interactions.\n\n**Regarding the other terms in your query:**\n\nThe document **does not** mention or discuss: `deep research`, `agent memory`, `agentic memory`, `context compression`, `agent frameworks`, `LangChain`, `LlamaIndex`, `Anthropic Agents SDK`, `Google SDK`, `structured outputs` (though it mentions `structured_content` preference in one transport), `agent orchestration`, `agent evaluations`, `LLM reasoning trace evaluation`, `prompt engineering`, `context engineering`, `long context`, `supervised fine-tuning`, or `reinforcement learning`.\n\n**Summary:** The page focuses specifically on the **Model Context Protocol (MCP)** for standardizing **tool use** within the **OpenAI Agents SDK** infrastructure, detailing various",
    "published_date": "2011-06-09T00:00:00.000Z",
    "score": null
  },
  "https://www.letta.com/blog/agent-memory?amp%3Butm_medium=newsletter": {
    "content_path": "../data/contents/ad620857bd28eea8.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:10.023545",
    "title": "Agent Memory: How to Build Agents that Learn and Remember",
    "summary": "The provided webpage focuses on **Agent Memory**, detailing how to build agents that learn and remember over time by managing their context window.\n\nKey concepts discussed include:\n*   **Agent Memory as Context Management:** Designing memory is essentially context engineering—determining what information enters the agent's context window.\n*   **Types of Agent Memory:**\n    *   **Message Buffer:** Stores recent messages for immediate conversational context.\n    *   **Core Memory:** In-context, editable memory blocks focused on specific topics (user, persona, task).\n    *   **Recall Memory:** Stores and allows searching of the complete conversational history.\n    *   **Archival Memory:** Explicitly formulated, indexed knowledge stored in external databases (like vector or graph DBs).\n*   **Techniques for Memory Management:** Message eviction, recursive summarization, managing structured memory blocks, and using external storage/retrieval (RAG).\n*   **Engineering Systems:** Mentions **MemGPT** (treating context as a constrained resource with memory tiers) and **Sleep-Time Compute** (using asynchronous agents for proactive memory refinement).\n\n**Regarding your specific query:**\n\nThe query lists several technical components related to the agent stack: 'agent\\_infrastructure: MCP servers, tool use, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration'.\n\nThe webpage directly addresses **agent memory** and **agentic memory**, **tool use** (via tool calling for retrieval), and mentions **agent frameworks** implicitly through concepts like MemGPT and the discussion of context engineering. It also touches upon **structured outputs** via memory blocks.\n\nHowever, the page **does not mention or discuss**:\n*   MCP servers\n*   LangChain\n*   LlamaIndex\n*   OpenAI Agents SDK\n*   Anthropic Agents SDK\n*   Google SDK\n*   Agent orchestration\n\nSince the page does not cover the majority of the specific infrastructure and SDK components listed in your query, a complete summary addressing all points is not possible.\n\n**No answer found** for the full scope of the query.",
    "published_date": "2025-07-07T00:00:00.000Z",
    "score": null
  },
  "https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html": {
    "content_path": "../data/contents/e7167fc2395d254b.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:10.026021",
    "title": "Add memory to your Amazon Bedrock AgentCore agent",
    "summary": "The provided webpage focuses on **AgentCore Memory** within **Amazon Bedrock AgentCore**, which is a fully managed service that enables AI agents to remember past interactions for more intelligent and context-aware conversations.\n\nIt details two types of memory:\n1.  **Short-term memory:** Captures turn-by-turn interactions within a single session.\n2.  **Long-term memory:** Automatically extracts and stores key insights across multiple sessions for persistent knowledge retention (e.g., user preferences).\n\nThe key benefits include creating more natural conversations, delivering personalized experiences, and reducing development complexity by managing conversational state. Common use cases mentioned are conversational agents, task-oriented/workflow agents, multi-agent systems, and autonomous/planning agents. The page also notes that AgentCore Memory supports a variety of SDKs and agent frameworks.\n\n**Regarding your specific query terms:**\n\nThe page **does not** mention or discuss: **agent\\_infrastructure: MCP servers, tool use, agent frameworks (LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK), function calling, structured outputs, or agent orchestration** (though it mentions \"task-oriented / workflow agents\" and \"multi-agent systems\" as use cases). It focuses specifically on the memory component of Amazon Bedrock AgentCore.\n\n**Summary relevant to the page content:** The page describes how Amazon Bedrock AgentCore Memory provides short-term and long-term memory capabilities to AI agents to maintain context and personalize interactions, addressing the challenge of statelessness in agentic AI.\n\n**No answer found** for the majority of the specific infrastructure and framework terms listed in your query.",
    "published_date": "2024-10-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2503.20757": {
    "content_path": "../data/contents/17f68dccc8c8be9c.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.702661",
    "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search",
    "summary": "The webpage describes **MCTS-RAG**, a novel approach that enhances the reasoning capabilities of small language models (LMs) on knowledge-intensive tasks by combining **Retrieval-Augmented Generation (RAG)** with **Monte Carlo Tree Search (MCTS)**.\n\nKey points related to your query:\n\n*   **Reasoning LLMs & Planning with LLMs:** MCTS-RAG enhances reasoning by using MCTS to refine reasoning paths through an iterative decision-making process, integrating structured reasoning with adaptive retrieval.\n*   **MCTS (Monte Carlo Tree Search) for language models:** The core of the method is leveraging MCTS to guide the integration of retrieval and reasoning.\n*   **Inference-time compute:** The method achieves performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute.\n*   **Hallucination reduction and detection & Grounding, factuality:** MCTS-RAG is shown to reduce hallucinations and ensure improved factual accuracy and response consistency by dynamically integrating retrieval and reasoning, unlike standard RAG or MCTS methods that rely solely on internal knowledge.\n\nThe paper focuses on improving reasoning and factuality in LMs using MCTS and RAG.",
    "published_date": "2025-03-26T00:00:00.000Z",
    "score": null
  },
  "https://huggingface.co/papers/2501.14304": {
    "content_path": "../data/contents/019a8a723759388c.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.470562",
    "title": "MASTER: A Multi-Agent System with LLM Specialized MCTS",
    "summary": "The MASTER framework is a novel approach that improves problem-solving in Large Language Models (LLMs) by integrating a **Multi-Agent System** with **Tactical Execution** and a specialized **MCTS (Monte Carlo Tree Search)** algorithm.\n\nThe paper addresses limitations of using standard MCTS with LLMs, specifically:\n1.  The difficulty in obtaining **objective rewards** for tasks like question answering, unlike games like Go.\n2.  The **excessive token usage and time consumption** required for statistically significant reward estimations (typically needing over 30 simulations).\n\nMASTER uses LLM-specialized MCTS to coordinate agent recruitment and communication, autonomously adjusting the number of agents based on task complexity and ensuring focused communication. Experiments show it achieves state-of-the-art performance on HotpotQA (76% accuracy) and WebShop (80% accuracy).\n\nWhile the query covers broad topics like **reasoning and planning, chain-of-thought, self-reflection, grounding, and hallucination reduction**, the paper specifically focuses on enhancing **planning capability** using a specialized **MCTS** within a **Multi-Agent System** framework.",
    "published_date": "2025-02-20T00:00:00.000Z",
    "score": null
  },
  "https://www.researchgate.net/publication/389130353_Inference-Time_Computations_for_LLM_Reasoning_and_Planning_A_Benchmark_and_Insights": {
    "content_path": "data/contents/eb5d67602689acb5.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.190434",
    "title": "Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights",
    "summary": "The webpage summarizes a study titled \"Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights,\" which examines the reasoning and planning capabilities of Large Language Models (LLMs) using inference-time techniques.\n\nThe study focuses on:\n*   **Reasoning and Planning in LLMs:** Analyzing their ability to solve complex tasks across five categories: arithmetic, logical, common sense, algorithmic reasoning, and planning.\n*   **Inference-Time Techniques:** Evaluating methods like **Chain-of-Thought (CoT)**, **Self-Consistency (SC)**, **Tree-of-Thought (ToT)**, and **Reasoning as Planning with World Models (RAP)** (which uses **MCTS**).\n*   **Benchmark Creation:** Introducing **Sys2Bench**, a comprehensive benchmark with eleven diverse tasks to evaluate these techniques.\n*   **Key Findings:** The study concludes that **simply scaling inference-time computation has limitations**, as no single technique consistently performs well across all reasoning and planning tasks. Tree search methods (ToT, RAP) often struggle with increasing complexity or tasks requiring self-verification, while CoT and SC perform well on arithmetic tasks. Large Reasoning Models (LRMs) like OpenAI's o1 show strong performance, especially in arithmetic and planning, but still struggle with tasks requiring advanced spatial reasoning (like Rubik's Cube). The research suggests a need for more diverse approaches beyond just scaling computation.\n\nThe user query asks about several specific concepts: **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, factuality.**\n\nThe page directly addresses:\n*   **Reasoning LLMs:** The entire paper is about their reasoning and planning capabilities.\n*   **Chain-of-Thought (CoT):** Explicitly mentioned and evaluated as a baseline inference-time technique.\n*   **Inference-Time Compute/Scaling:** The core focus is on examining and finding limitations in scaling inference-time techniques.\n*   **Planning with LLMs:** A major category of tasks evaluated in Sys2Bench.\n*   **MCTS for Language Models:** Mentioned as part of the RAP technique, which uses Monte Carlo Tree Search.\n*   **Factuality/Hallucination:** The discussion section notes that LLMs",
    "published_date": "2025-02-18T00:00:00.000Z",
    "score": null
  },
  "https://aclanthology.org/2023.emnlp-main.507/": {
    "content_path": "data/contents/6b79a6536e8f55d8.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.243030",
    "title": "Reasoning with Language Model is Planning with World Model - ACL Anthology",
    "summary": "The webpage describes a paper titled \"Reasoning with Language Model is Planning with World Model,\" which introduces a framework called **Reasoning via Planning (RAP)**.\n\nThis framework addresses the limitations of Large Language Models (LLMs) in complex reasoning tasks (like plan generation, math, and logic) that stem from their lack of an internal \"world model\" for predicting future states and simulating outcomes.\n\nRAP overcomes this by repurposing the LLM to act as **both a world model and a reasoning agent**. It incorporates a planning algorithm based on **Monte Carlo Tree Search (MCTS)** to strategically explore the reasoning space, balancing exploration and exploitation to find high-reward reasoning paths.\n\nThe paper notes that LLMs show remarkable reasoning capabilities with **Chain-of-Thought (CoT)** prompting, but RAP is shown to be superior to CoT and other baselines in challenging reasoning problems.",
    "published_date": "2025-12-10T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/pdf/2310.04406": {
    "content_path": "data/contents/48a8edcf19754740.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.323148",
    "title": "",
    "summary": "The user query asks for a summary related to several concepts in the field of **Reasoning and Planning with Large Language Models (LLMs)**, including: Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, Monte Carlo Tree Search (MCTS) for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.\n\nThe provided webpage introduces **Language Agent Tree Search (LATS)**, a general framework that synergizes LLM capabilities in **reasoning, acting, and planning**.\n\nHere is a summary of how LATS addresses the concepts in the query:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** LATS builds upon and extends reasoning techniques like CoT by framing the problem as a search over possible reasoning and acting steps. It uses the base LLM ($p_\\theta$) for reasoning.\n*   **Planning with LLMs & MCTS (Monte Carlo Tree Search):** LATS's core contribution is integrating a variant of **MCTS** to enable deliberate planning, moving beyond the simple, reflexive acting processes of prior methods. This allows LATS to search over a combinatorial space of reasoning and acting steps.\n*   **Self-Reflection:** LATS incorporates a **Reflection** operation. Upon reaching an unsuccessful terminal node, the LLM generates a verbal self-reflection summarizing errors, which is stored and used as additional context in future trials, enabling the agent to learn from trial and error without explicit training.\n*   **Grounding & Factuality (and Hallucination Reduction):** LATS enhances sensibility and addresses limitations of purely internal reasoning (which risks hallucination) by incorporating **external feedback** (observations from the environment). The value function also incorporates a **self-consistency score** to improve value assignment.\n*   **Inference-Time Compute & Test-Time Scaling:** LATS involves a higher computational cost than simpler prompting methods (like ReAct) because of the tree search. However, the paper notes that LATS achieves better performance and, in some comparisons, requires fewer overall nodes/tokens upon success than other tree-based search methods (like ToT and RAP), suggesting a more efficient search mechanism. The authors acknowledge the higher compute cost but expect it to decrease over time.\n\nIn summary, LATS unifies reasoning, acting, and planning by using **\n\nThe provided webpage text is a list of references and appendices detailing the methodology, ablation studies, and prompts used in a research paper, likely about \"Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models\" (LATS).\n\nThe user query asks for a summary related to: **'reasoning\\_and\\_planning: Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, factuality'**\n\nThe text mentions several relevant concepts:\n\n*   **Reasoning LLMs and Chain-of-Thought (CoT):** References to \"Chain of thought prompting elicits reasoning in large language models\" (Wei et al., 2022) and \"Self-consistency improves chain of thought reasoning\" (Wang et al., 2022) are present. The LATS algorithm itself is a planning/reasoning method.\n*   **Planning with LLMs and MCTS:** The core subject is **LATS (Language Agent Tree Search)**, which unifies reasoning, acting, and planning. The text explicitly mentions MCTS (Monte Carlo Tree Search) in the context of LATS (referencing Kocsis and Szepesvari, 2006, and Swiechowski et al., 2021) and its application to language models (e.g., \"Mastering the game of Go with deep neural networks and tree search\" by Silver et al., 2016, is cited as inspiration).\n*   **Self-reflection:** The text discusses **Self-Refine** (Madaan et al., 2023) and includes a \"Reflection Prompt\" section for LATS, indicating the use of self-reflection/self-feedback.\n*   **Inference-time compute/Test-time scaling:** Section B discusses the **Computational cost** of LATS compared to simpler methods like ReAct, noting that LATS has a higher cost but can be mitigated by adjusting the number of sampled nodes ($n$).\n*   **Grounding/Factuality:** The environment details mention **HotPotQA**, a dataset for question-answering requiring reasoning over supporting documents, which relates to grounding and factuality. The **WebShop** environment also involves grounded language understanding",
    "published_date": "2024-06-07T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/pdf/2305.14992": {
    "content_path": "../data/contents/228b5578101a678d.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.619877",
    "title": "",
    "summary": "The user query asks for a summary related to several topics in LLM reasoning: **reasoning and planning, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**\n\nThe provided webpage text introduces a framework called **Reasoning via Planning (RAP)**, which addresses limitations in current LLM reasoning, particularly the lack of an internal world model needed for deliberate planning.\n\nHere is a summary of how the text relates to the query topics:\n\n*   **Reasoning and Planning / Planning with LLMs:** The core of the paper is the RAP framework, which repurposes the LLM as both a **world model** (to predict future states) and a **reasoning agent** to perform strategic planning. This directly addresses the need for better planning capabilities in LLMs, contrasting with the purely autoregressive nature of methods like CoT.\n*   **Chain-of-Thought (CoT):** CoT is mentioned as a baseline method that generates reasoning steps sequentially but struggles with complex, multi-step problems because it lacks simulation capabilities. RAP is shown to significantly outperform CoT in plan generation and math reasoning tasks.\n*   **MCTS (Monte Carlo Tree Search) for Language Models:** RAP explicitly incorporates **Monte Carlo Tree Search (MCTS)** as the principled planning algorithm to efficiently explore the vast reasoning space, balancing **exploration vs. exploitation** (a key aspect of inference-time compute/strategy).\n*   **Self-Reflection:** The reward design in RAP includes **\"Self-evaluation by the LLM,\"** where the model criticizes its own reasoning step (\"Is this reasoning step correct?\"), using the resulting probability as a reward signal.\n*   **Grounding and Factuality:** The introduction of the **world model** (which predicts the next state based on actions) helps LLMs produce a more **grounded and coherent inference** by simulating the environment's state, which is crucial for maintaining factuality in multi-step reasoning. In logical reasoning tasks, RAP achieves high proof accuracy, suggesting improved grounding in the provided facts and rules.\n*   **Inference-time Compute / Test-time Scaling:** The use of MCTS inherently involves managing **inference-time compute** by iterating a set number of times (e.g., RAP(1\n\nThe webpage discusses **Reasoning via Planning (RAP)**, a novel framework that equips Large Language Models (LLMs) with strategic planning abilities, allowing them to act as both a world model and a reasoning agent.\n\nKey aspects related to your query:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** RAP is presented as an improvement over the **chain-of-thought (CoT)** prompting baseline, showing superior performance, especially when many steps are required.\n*   **Planning with LLMs & MCTS:** RAP achieves effective balance between exploration and exploitation by using **Monte Carlo Tree Search (MCTS)** for language models. The document details modifications made to classic MCTS to handle the large reasoning space and high computational cost of LLMs, including sampling potential actions and using lightweight local rewards during selection.\n*   **Inference-time compute & Test-time scaling:** The framework operates during the inference stage. The performance comparison against CoT shows that RAP maintains a high success rate where CoT drops significantly when more steps are needed, suggesting better scaling with reasoning depth.\n*   **Self-reflection (Self-evaluation reward):** The framework utilizes a **self-evaluation reward** as part of its reward structure. Experiments show that the self-evaluation reward is crucial for guiding reasoning, especially in mathematical reasoning where detecting errors post-generation is feasible.\n*   **Grounding & Hallucination Reduction:** While not explicitly using the terms \"grounding\" or \"hallucination reduction,\" the framework's use of an LLM as a **world model** to simulate states and anticipate outcomes, combined with planning, serves to structure the reasoning process, which is a method to improve factuality and reduce errors compared to simple CoT. The text also mentions the importance of grounding in future work (combining external tools).\n*   **Reward Choice:** The effectiveness of different rewards (action likelihood, task-specific, self-evaluation) is explored, showing that combinations, particularly those including self-evaluation, boost performance across different tasks (Blocksworld and math reasoning).\n\nThe document focuses heavily on the RAP framework, its comparison to CoT, and its implementation using MCTS and various reward mechanisms to enhance reasoning.",
    "published_date": "2023-10-23T00:00:00.000Z",
    "score": null
  },
  "https://galileo.ai/blog/self-reflection-in-language-models": {
    "content_path": "../data/contents/eb52b57d23840b3e.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.616660",
    "title": "The Need for Self Reflection in Language Models",
    "summary": "The user query asks for a summary related to several advanced LLM concepts: 'reasoning\\_and\\_planning: Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, factuality'.\n\nThe provided webpage focuses heavily on **Self-Reflection in Language Models** and directly addresses several components of the query:\n\n*   **Self-reflection:** This is the main topic, defined as the ability of LLMs to generate, review, and revise outputs through an internal audit process to improve quality, reduce hallucinations, and mitigate bias.\n*   **Chain-of-thought:** The text mentions **Chain-of-thought self-evaluation** as a core mechanism of self-reflection, where the model audits each step of its reasoning process for logic and consistency.\n*   **Hallucination reduction and detection:** Self-reflection is explicitly stated to improve quality and **reduce hallucinations**. It helps prevent **overconfident wrong answers** through uncertainty estimation.\n*   **Factuality/Grounding:** Self-reflection helps catch contradictions and ensures factual alignment, especially when combined with external retrieval (mentioned in the context of Self-RAG).\n*   **Reasoning LLMs:** The mechanisms described (CoT evaluation, iterative refinement) are techniques used to enhance reasoning capabilities.\n*   **Planning with LLMs:** Iterative response refinement mentions approaches like **Tree-of-Thoughts** (a planning technique) which formalizes evaluating alternative reasoning trajectories.\n\nThe page does **not** explicitly discuss:\n*   Inference-time compute (beyond mentioning latency trade-offs in integration methods).\n*   MCTS (Monte Carlo Tree Search) for language models (though Tree-of-Thoughts is mentioned as a related formalization).\n*   Test-time scaling.\n*   Grounding (though factuality/RAG integration is mentioned).\n\n**Summary:**\n\nThe webpage details the concept and implementation of **Self-Reflection in Language Models** as a method to improve output quality, consistency, and reliability. Key mechanisms driving self-reflection include **Chain-of-Thought self-evaluation** (auditing reasoning steps), **Uncertainty Estimation** (to prevent overconfident errors), and **Iterative Response Refinement** (which incorporates concepts similar to planning, such",
    "published_date": "2025-07-18T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2305.14078": {
    "content_path": "../data/contents/9a453e14f0a482a9.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.488526",
    "title": "Computer Science > Robotics",
    "summary": "The webpage discusses using Large Language Models (LLMs) as a **commonsense knowledge** source for **Large-Scale Task Planning**.\n\nSpecifically, it highlights:\n*   LLMs can provide a **world model** (commonsense knowledge) in addition to acting as a policy.\n*   This world model and policy can be combined within a search algorithm like **Monte Carlo Tree Search (MCTS)** (MCTS for language models is mentioned).\n*   The LLM-induced world model provides a **commonsense prior belief** for MCTS, leading to effective **reasoning**.\n*   The LLM-induced policy acts as a heuristic to guide the search, improving efficiency.\n*   The proposed **LLM-MCTS** algorithm outperforms MCTS alone and policies induced solely by LLMs (GPT2 and GPT3.5) on complex tasks.\n*   The findings suggest that using the LLM as a world model for model-based planning is beneficial when the description length of the world model is substantially smaller than that of the policy (related to **minimum description length (MDL)**).\n\nWhile the query covers many topics like \"chain-of-thought,\" \"self-reflection,\" \"test-time scaling,\" and \"hallucination reduction,\" the paper directly addresses **planning with LLMs** and **MCTS for language models**, leveraging LLMs for **reasoning** through a world model.",
    "published_date": "2023-05-23T00:00:00.000Z",
    "score": null
  },
  "https://www.alignmentforum.org/posts/byNYzsfFmb2TpYFPW/o1-a-technical-primer": {
    "content_path": "../data/contents/21f7f5bbdcfa8d88.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.491155",
    "title": "o1: A Technical Primer",
    "summary": "The user query asks for information related to **reasoning and planning in LLMs**, specifically mentioning: Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.\n\nThe webpage, \"o1: A Technical Primer,\" discusses OpenAI's \"reasoning model\" o1, which exhibits **test-time scaling laws**.\n\nHere is a summary of how the page addresses the query topics:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** The page centers on o1, described as OpenAI's first \"reasoning model.\" It explicitly states that o1 performs **implicit search via chain of thought (CoT)**, which was trained using Reinforcement Learning (RL) to improve productive thinking. Standard CoT techniques are mentioned as predecessors.\n*   **Inference-time compute & Test-time scaling:** This is a core theme. The text introduces **test-time scaling laws**, which show how to exchange **inference-time compute** for better decisions, contrasting with traditional training-time scaling laws.\n*   **Planning with LLMs & MCTS (Monte Carlo Tree Search) for language models:** The hypotheses for how o1 works include **Guidance** (Hypothesis 3), which discusses using intermediate feedback to guide sampling, mentioning **beam search** and the more complex **Monte-Carlo Tree Search (MCTS)** as potential inspiration or mechanisms for distillation into the model.\n*   **Self-reflection (Error Correction/Backtracking):** The capabilities exhibited by o1 include **Error Correction** (\"learns to recognize and correct its mistakes\") and **Backtracking** (\"learns to try a different approach\"). The text notes that OpenAI claims these are *emergent* capabilities, though one hypothesis (Combination) involves explicitly training for self-correction.\n*   **Hallucination reduction and detection / Factuality / Grounding:** While the terms \"hallucination reduction,\" \"detection,\" \"grounding,\" and \"factuality\" are not explicitly used as section headers, the concept is addressed through the necessity of a **verifier** (a function returning the probability of correctness) used during training or inference. One comment suggests that performance improved significantly in domains with **ground truth** (like code and math",
    "published_date": "2024-12-09T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2408.03314v1": {
    "content_path": "../data/contents/9cca2f735e306688.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.494994",
    "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
    "summary": "The user query asks for a summary related to several topics in **reasoning and planning with LLMs**, including: Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.\n\nThe provided webpage focuses heavily on **scaling inference-time compute optimally** for LLMs, which directly addresses the \"inference-time compute\" and \"test-time scaling\" aspects of the query.\n\nHere is a summary of the page content relevant to the query:\n\nThe paper investigates how to optimally scale **test-time computation** in Large Language Models (LLMs) to improve performance on challenging prompts, suggesting this can be more effective than scaling model parameters alone.\n\n**Key Findings and Concepts Related to Reasoning and Test-Time Compute:**\n\n*   **Test-Time Compute Scaling:** The core finding is that the effectiveness of different test-time compute methods (like iterative self-refinement/revisions or search against a verifier) critically depends on the **difficulty of the prompt**. This motivates a **\"compute-optimal\" scaling strategy** that adaptively allocates test-time compute based on prompt difficulty.\n*   **Comparison to Model Scaling:** Using this compute-optimal strategy, the authors found that for certain problems, test-time compute can outperform a model that is $14\\times$ larger in parameters, suggesting a trade-off where smaller models with more inference compute might be preferable to very large pretrained models in some settings.\n*   **Mechanisms for Scaling Compute:** The study analyzes two primary mechanisms:\n    1.  **Updating the model's distribution adaptively (Proposal Distribution Refinement):** This includes **sequential revisions** (self-reflection/self-critique), where the model iteratively refines its answer. Sequential revisions were found to be generally better than parallel sampling for refinement tasks, especially on easier problems.\n    2.  **Searching against a Verifier (Planning):** This involves using a **Process Reward Model (PRM)** to score intermediate steps. Search methods like **Beam Search** were found to be more effective than simple **Best-of-N** sampling on harder questions, as they help guide the model toward correct high-level approaches. The paper notes that Lookahead Search is a specialized, deterministic version of **MCTS** applied at\n\nThe user query asks for a summary related to several advanced LLM reasoning and planning concepts, including **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**\n\nThe provided webpage text focuses heavily on **test-time compute scaling**, specifically comparing sequential revisions (self-reflection/refinement) and parallel sampling (search with verifiers) based on question difficulty. It also discusses the **exchange between pretraining compute and test-time compute**.\n\nHere is a summary of the relevant points from the text concerning the user's query:\n\n*   **Test-Time Compute Scaling & Optimality:** The text demonstrates that there is an ideal ratio between sequential revisions (refining the proposal distribution) and parallel sampling (search with verifiers) for maximizing accuracy at a given generation budget. This optimal ratio depends critically on the **question's difficulty**: easier questions benefit more from sequential revisions, while harder questions require a balance.\n*   **Efficiency of Compute Scaling:** By optimally scaling test-time compute according to question difficulty, the model can outperform the standard best-of-N baseline using up to **4x less test-time compute**.\n*   **Exchanging Pretraining and Test-Time Compute:** The study investigates whether increasing test-time compute can compensate for less pretraining compute (FLOPs-matched setting).\n    *   On **easy and medium questions** or in settings with **small inference requirements** ($R \\ll 1$), test-time compute is often more effective than scaling model parameters during pretraining.\n    *   On **challenging questions** or under **higher inference requirements** ($R \\gg 1$), increasing pretraining compute (scaling model parameters) is more effective.\n*   **Related Work (Context for Reasoning/Planning):** The related work section mentions techniques that fall under the user's query, such as:\n    *   **Chain-of-Thought** (implied by step-by-step reasoning).\n    *   **Self-reflection/Refinement** (critique and revise).\n    *   **Test-time compute scaling** using **verifiers** and **search** (which relates to planning/MCTS).\n\n**Conclusion regarding the query:** The page directly addresses **test-",
    "published_date": "2024-07-03T00:00:00.000Z",
    "score": null
  },
  "https://llm-mcts.github.io/": {
    "content_path": "data/contents/fb5dc370a1307613.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.635456",
    "title": "SOCIAL MEDIA TITLE TAG",
    "summary": "The webpage describes a paper titled \"Large Language Models as Commonsense Knowledge for Large-Scale Task Planning.\" It focuses on using Large Language Models (LLMs) within the **Monte Carlo Tree Search (MCTS)** framework for large-scale task planning.\n\nSpecifically, the paper proposes an **LLM-MCTS algorithm** where:\n1.  The LLM acts as a **commonsense world model**, providing a prior belief for MCTS to enable effective **reasoning**.\n2.  The LLM also acts as a **heuristic policy** to guide the search, improving efficiency.\n\nThe results show that this LLM-MCTS approach outperforms MCTS alone and policies induced solely by LLMs for complex tasks. The authors suggest that using the LLM as a world model for model-based planning is likely better than using it only as a policy when the description length of the world model is substantially smaller than that of the policy (related to the **minimum description length (MDL)** principle).\n\nWhile the user query covers a broad range of topics related to **reasoning and planning with LLMs** (including chain-of-thought, self-reflection, inference-time compute, hallucination reduction, grounding, etc.), this specific page directly addresses **planning with LLMs** using **MCTS** and leveraging the LLM as a **world model** for **reasoning**.",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/abs/2305.02556": {
    "content_path": "../data/contents/f3926d4efff86d4a.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.315786",
    "title": "Faithful Question Answering with Monte-Carlo Planning",
    "summary": "The webpage describes a paper titled \"Faithful Question Answering with Monte-Carlo Planning\" which proposes **FAME** (FAithful question answering with MontE-carlo planning). This method aims to answer questions based on **faithful reasoning steps** organized as a structured entailment tree. It formulates the task as a discrete decision-making problem solved by a controller interacting with a reasoning environment. A **Monte-Carlo planning algorithm** is introduced to perform look-ahead search and select actions that lead to high-quality steps.\n\nWhile the user query covers broad topics like **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality**, the paper specifically focuses on:\n\n*   **Planning with LLMs** (using Monte-Carlo planning for action selection).\n*   **MCTS (Monte Carlo Tree Search) for language models** (explicitly using a Monte-Carlo planning algorithm).\n*   **Faithfulness** and producing valid reasoning steps (related to grounding and factuality in the context of QA).\n\nThe paper does not explicitly detail reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, test-time scaling, or hallucination reduction/detection beyond the scope of producing faithful reasoning steps for question answering.",
    "published_date": "2023-05-04T00:00:00.000Z",
    "score": null
  },
  "https://www.linkedin.com/pulse/reasoning-llms-mcts-over-tokens-motivation-three-papers-bou-ammar-rl6uc": {
    "content_path": "data/contents/6da2f85f67311eaa.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.144706",
    "title": "Reasoning in LLMs as MCTS over Tokens (Motivation and three papers)",
    "summary": "The webpage discusses **Reasoning in LLMs as MCTS over Tokens**, focusing on how **Monte Carlo Tree Search (MCTS)** can be integrated with Large Language Models (LLMs) to improve their capabilities in **planning** and **reasoning**.\n\nKey points related to your query:\n\n*   **Reasoning LLMs & Planning with LLMs:** LLMs often struggle with long-term planning and goal-conditioned reasoning because they are trained primarily for next-token prediction. Integrating MCTS allows LLMs to explore multiple potential continuations, simulate outcomes, and guide generation towards long-term objectives.\n*   **MCTS (Monte Carlo Tree Search) for language models:** MCTS is presented as a method to transform the LLM reasoning process into a strategic exploration of solutions. It involves defining **states** (the current status of reasoning), **actions** (meaningful reasoning moves or next tokens), and a **reward mechanism** to guide the search toward successful outcomes. This helps LLMs navigate the vast search space of language generation.\n*   **Inference-time compute:** The MCTS process inherently involves additional computation at inference time as it iteratively builds and explores a search tree to find optimal solutions.\n\nThe page does not explicitly detail concepts like *chain-of-thought*, *self-reflection*, *test-time scaling*, *hallucination reduction and detection*, *grounding*, or *factuality* in depth, although improving coherence and optimizing reasoning through MCTS might indirectly address some of these issues.",
    "published_date": "2024-12-09T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2502.12521": {
    "content_path": "data/contents/fb3d5e45ea0aa01d.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.374238",
    "title": "Computer Science > Artificial Intelligence",
    "summary": "The webpage is an abstract for the arXiv paper \"[2502.12521] Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights.\"\n\nThe paper examines the **reasoning and planning capabilities of Large Language Models (LLMs)** using **inference-time techniques** (techniques applied during inference without additional training) to enhance performance. It specifically mentions the novel multi-step reasoning and verification used by OpenAI's o1 model.\n\nThe authors constructed a comprehensive benchmark called **Sys2Bench** and evaluated existing inference-time techniques across eleven diverse tasks covering arithmetic, logical, common sense, algorithmic reasoning, and **planning**.\n\nThe key finding is that **scaling inference-time computation has limitations**, as no single technique consistently performs well across all reasoning and planning tasks.\n\nThis directly addresses several parts of your query: **reasoning and planning with LLMs**, **inference-time compute**, and implicitly touches upon the need for better reasoning techniques (which relates to hallucination reduction and grounding, though these specific terms are not detailed in the abstract).",
    "published_date": "2025-02-18T00:00:00.000Z",
    "score": null
  },
  "https://dl.acm.org/doi/10.5555/3600270.3602070": {
    "content_path": "data/contents/eb13b3fd1a509a6d.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.796187",
    "title": "Chain-of-thought prompting elicits reasoning in large language models",
    "summary": "The webpage is an abstract and citation information for a research article titled **\"Chain-of-thought prompting elicits reasoning in large language models\"**.\n\nThe abstract states that generating a **chain of thought** (a series of intermediate reasoning steps) significantly improves the ability of large language models (LLMs) to perform complex reasoning. This is achieved through **chain-of-thought prompting**, where a few chain-of-thought demonstrations are provided as examples in the prompt. Experiments showed this method improves performance on arithmetic, commonsense, and symbolic reasoning tasks, with striking empirical gains, such as achieving state-of-the-art accuracy on the GSM8K benchmark for math word problems using a PaLM 540B model with just eight chain-of-thought exemplars.\n\nThe user query covers several topics related to LLM reasoning and planning: *reasoning & planning, Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, factuality*.\n\n**Summary relevant to the query:**\n\nThe page directly addresses **Reasoning LLMs** and **chain-of-thought** prompting, showing that providing intermediate reasoning steps significantly improves complex reasoning abilities in LLMs across arithmetic, commonsense, and symbolic tasks.\n\n**Topics not explicitly covered in the abstract:** inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.",
    "published_date": "2022-11-28T00:00:00.000Z",
    "score": null
  },
  "https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/": {
    "content_path": "data/contents/145d1c6f81853c0a.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.003449",
    "title": "Language Models Perform Reasoning via Chain of Thought",
    "summary": "The provided webpage discusses **Chain of Thought (CoT) Prompting** as a method to enable large language models (LLMs) to perform multi-step reasoning tasks, such as arithmetic word problems and commonsense reasoning.\n\nKey points related to your query:\n\n*   **Reasoning LLMs and Chain-of-Thought:** CoT prompting involves prompting the model to produce intermediate reasoning steps before giving the final answer, mimicking an intuitive thought process. This method is shown to improve reasoning abilities in LLMs of sufficient scale ($\\sim$100B parameters).\n*   **Inference-time Compute/Scaling:** The benefits of CoT prompting are an **emergent property of model scale**; performance improvements are only seen with larger models (around 100B parameters or more).\n*   **Planning with LLMs:** While the text focuses on reasoning decomposition, the concept of breaking down a problem into intermediate steps is foundational to planning.\n*   **Performance Improvements:** CoT prompting led to state-of-the-art performance on the GSM8K arithmetic reasoning benchmark when combined with the 540B parameter PaLM model. Performance also improved on commonsense reasoning tasks.\n\nThe page **does not** explicitly discuss:\n*   MCTS (Monte Carlo Tree Search) for language models.\n*   Test-time scaling (beyond the observation that CoT is scale-dependent).\n*   Hallucination reduction and detection.\n*   Grounding or factuality (though improved reasoning might indirectly help).\n*   Self-reflection.",
    "published_date": "2024-05-30T00:00:00.000Z",
    "score": null
  },
  "https://aclanthology.org/2025.emnlp-main.329.pdf": {
    "content_path": "../data/contents/24c3bac5fc11857d.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.516400",
    "title": "",
    "summary": "The provided web page text details a framework called **LCoT2Tree** designed for the **structural analysis of Long Chain-of-Thought (LCoT) reasoning in Large Language Models (LLMs)**.\n\nHere is a summary addressing the key concepts in your query:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** The paper focuses on LCoT, a strategy where LLMs engage in deliberate, step-by-step reasoning (System 2 thinking) before answering, which has enabled expert-level performance in complex tasks.\n*   **Inference-time Compute & Test-time Scaling:** The paper addresses the \"overthinking\" phenomenon, where overly long reasoning chains can degrade performance, suggesting that simply increasing inference-time compute (length) is an unreliable predictor of correctness.\n*   **Self-reflection & Planning with LLMs:** The LCoT2Tree framework analyzes structural patterns like **exploration, backtracking, and verification** within the reasoning chain, which are key components of structured reasoning and self-correction.\n*   **MCTS (Monte Carlo Tree Search) for language models:** While MCTS is not explicitly mentioned, the tree structure derived by LCoT2Tree models hierarchical reasoning paths, which shares conceptual similarities with tree-based search strategies.\n*   **Hallucination Reduction and Detection & Grounding/Factuality:** The paper focuses on *structural* success rather than semantic correctness. It notes that structural analysis alone cannot capture semantic errors (like calculation mistakes or misinterpretation), indicating that combining structural insights with semantic verification is necessary for comprehensive **factuality** assessment.\n*   **Structural Analysis:** The core contribution is LCoT2Tree, which converts sequential LCoTs into hierarchical trees. Graph Neural Networks (GNNs) are used on these trees to predict answer correctness with significantly higher accuracy than length-based methods.\n*   **Practical Application:** The structural analysis is applied to improve **Best-of-N decoding** by selecting candidates based on the quality of their reasoning structure rather than just the final outcome.\n\nIn summary, the paper introduces a method to diagnose and improve LLM reasoning by analyzing the *structure* of the thought process, finding that structural patterns are stronger predictors of success than response length.\n\nThe webpage text primarily details the **LCoT2Tree framework**, a method for converting Long Chain-of-Thought (LCoT) reasoning traces from Large Language Models (LLMs) into structured tree representations for analysis and classification.\n\nKey aspects related to your query include:\n\n*   **Reasoning LLMs and Chain-of-Thought (CoT):** The work analyzes reasoning traces from various LLMs (DeepSeek-32B, QwQ-32B, DeepSeek-R1, Seed-1.5-Thinking-pro, Grok-3-mini-beta) across benchmarks like MATH, GPQA, LCB, and MMLU-Pro. It references foundational work on CoT prompting (e.g., Kojima et al., 2022; Wang et al., 2023b).\n*   **Inference-time Compute/Scaling:** The references section includes papers discussing scaling inference compute (Snell et al., 2024; Wu et al., 2025) and test-time scaling (Muennighoff et al., 2025).\n*   **Self-Reflection/Self-Correction:** The references mention work on self-correction (Gou et al., 2024, CRITIC) and self-refinement (Madaan et al., 2023, Self-refine).\n*   **Planning with LLMs/MCTS:** The references include \"Graph of thoughts\" (Besta et al., 2024) and \"Tree of thoughts\" (Yao et al., 2023), which are related to structured planning methods.\n*   **Hallucination Reduction and Detection/Factuality:** The analysis of error behaviors (Section D.1) and the use of reasoning structure for correctness classification implicitly address issues related to factual errors or flawed reasoning paths. The framework aims to classify correct vs. incorrect reasoning structures.\n*   **Grounding:** The references include work on grounded mathematical proof generation (Welleck et al., 2022).\n\nThe core contribution described is the **LCoT2Tree process**, which uses an LLM in five stages (Extract Sketch, Split Thought, Assign Step, Identify Function, Build Tree) to structure the CoT, allowing for the analysis of reasoning structure (e.g",
    "published_date": "2025-10-22T00:00:00.000Z",
    "score": null
  },
  "https://www.reddit.com/r/LocalLLaMA/comments/1mkza1b/new_paper_reveals_chainofthought_reasoning_of/": {
    "content_path": "data/contents/46c2fe5e746f9e88.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.200464",
    "title": "New paper reveals Chain-of-Thought reasoning of LLMs a mirage",
    "summary": "The webpage discusses a new paper suggesting that the Chain-of-Thought (CoT) reasoning observed in Large Language Models (LLMs), where they decompose complex problems into intermediate steps resembling human reasoning, might be a **mirage**.",
    "published_date": "2025-08-08T00:00:00.000Z",
    "score": null
  },
  "https://aclanthology.org/2025.coling-main.719.pdf": {
    "content_path": "../data/contents/9e5e97cd9bab3288.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.788239",
    "title": "",
    "summary": "This webpage provides a comprehensive survey of **Chain-of-X (CoX) paradigms for Large Language Models (LLMs)**, which generalize the widely used Chain-of-Thought (CoT) prompting method.\n\nThe summary covers:\n\n*   **Chain-of-X (CoX) Definition:** CoX extends CoT by constructing a sequence of problem-related components (the 'X' or 'node') that either compose a solution or iteratively refine outputs.\n*   **Taxonomy by Nodes (Components):** CoX methods are categorized based on the type of node used in the chain:\n    *   **Chain-of-Intermediates:** Involves breaking down problems (Problem Decomposition, e.g., classic CoT, Least-to-Most) or accumulating information (Knowledge Composition, e.g., Chain-of-Knowledge).\n    *   **Chain-of-Augmentation:** Augments the chain with external data, including **Instructions**, **Histories**, **Retrievals** (e.g., ReAct, Self-Ask), and **Tools** (e.g., MultiToolCoT).\n    *   **Chain-of-Feedback:** Uses feedback interlaced during generation, categorized as **Self Feedback** (e.g., Self-Refine, Chain-of-Verification) or **External Feedback**.\n    *   **Chain-of-Models:** Leverages the distinct strengths of multiple LLMs working sequentially (e.g., Chain-of-Experts, Chain-of-Discussion).\n*   **Taxonomy by Tasks (Applications):** CoX methods are applied across diverse areas:\n    *   **Multi-Modal Interaction:** Handling text alongside images, tables, code, or speech.\n    *   **Factuality & Safety:** Reducing **hallucination** (via verification or knowledge grounding) and improving **alignment** with human preferences.\n    *   **Multi-Step Reasoning:** Solving complex problems requiring logical progression.\n    *   **Instruction Following:** Enhancing the ability to follow complex, sequential instructions.\n    *   **LLMs as Agents:** Boosting the **planning** abilities of LLM-based agents.\n    *   **Evaluation Tools:** Using CoX structures to probe and evaluate LLM vulnerabilities and performance.\n*   **Future Directions:** The paper suggests future research should focus on causal analysis of intermediate steps, reducing the",
    "published_date": "2024-12-08T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/@myschang/chain-of-thought-cot-in-large-language-models-introduction-and-applications-910363d82431": {
    "content_path": "../data/contents/e1068b0618b106a4.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.524701",
    "title": "Chain-of-Thought (CoT) in Large Language Models: Introduction and Applications",
    "summary": "The provided webpage focuses on **Chain-of-Thought (CoT)** in Large Language Models (LLMs) as a technique to enhance their **reasoning ability**.\n\nKey points related to your query include:\n\n*   **Reasoning LLMs:** The text highlights that sufficiently large LLMs exhibit \"emerging abilities\" like human-like reasoning, allowing them to break down complex tasks.\n*   **Chain-of-Thought (CoT):** This is the central topic, described as a method to \"excite the reasoning ability of LLMs\" by providing reasoning examples (Few-shot CoT) or simple prompts like \"Let’s think step by step\" (Zero-shot CoT), enabling models to generate explicit reasoning steps.\n*   **Planning with LLMs:** The reasoning ability of LLMs is noted as being applicable as a \"planer or decomposer in many downstream tasks.\" An example is given where an LLM is prompted to suggest possible goals for an RL agent (ELLM method).\n\nThe page **does not explicitly discuss** the following terms from your query: *inference-time compute, self-reflection, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, or factuality*.",
    "published_date": "2023-04-26T05:31:09.000Z",
    "score": null
  },
  "https://par.nsf.gov/biblio/10463284-faithful-chain-thought-reasoning": {
    "content_path": "../data/contents/89637e9e214776dc.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.527291",
    "title": "Faithful Chain-of-Thought Reasoning | NSF Public Access Repository",
    "summary": "The webpage describes a research paper titled \"Faithful Chain-of-Thought Reasoning.\" This work proposes a framework called **Faithful CoT** to address the issue that standard Chain-of-Thought (CoT) reasoning chains generated by Language Models (LMs) may not faithfully reflect how the model arrived at the answer.\n\nFaithful CoT involves two stages:\n1.  **Translation:** Converting the Natural Language query into a **symbolic reasoning chain** using an LM.\n2.  **Problem Solving:** Using a **deterministic solver** on the reasoning chain to derive the final answer.\n\nThis approach guarantees that the reasoning chain provides a faithful explanation. Empirically, Faithful CoT outperforms standard CoT on 9 out of 10 benchmarks across diverse domains, showing significant accuracy gains in areas like **Math Word Problems (MWP)**, **Planning**, **Multi-hop Question Answering (QA)**, and **Relational Inference**. It also achieved new state-of-the-art few-shot performance on several datasets when tested with GPT-4 and Codex.\n\nWhile the user query covers a broad range of topics related to Reasoning LLMs (including MCTS, self-reflection, grounding, and hallucination reduction), the provided text specifically focuses on **Chain-of-Thought (CoT) reasoning** and improving its **faithfulness** and **empirical performance**.",
    "published_date": "2023-11-01T00:00:00.000Z",
    "score": null
  },
  "https://www.arxiv.org/pdf/2407.11511v2": {
    "content_path": "../data/contents/d5a825b2d60ffba3.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.530568",
    "title": "",
    "summary": "The provided web page is a survey on **Multi-Step Reasoning with Large Language Models (LLMs)**. It focuses on how LLMs, particularly through **Chain-of-Thought (CoT)** prompting, can perform complex reasoning tasks beyond simple next-token prediction.\n\nHere is a summary of the page content relevant to your query:\n\n*   **Reasoning LLMs and Chain-of-Thought (CoT):** The paper reviews the field of LLM reasoning, which gained traction with the CoT approach. CoT involves prompting the LLM to generate intermediate reasoning steps (\"Let’s think step by step\") to solve difficult problems, especially math word problems (like those in the GSM8K benchmark).\n*   **Inference-Time Compute:** In-context learning, which includes CoT, occurs at **inference-time** where model parameters are not adapted. The performance gains rely on the model's existing knowledge and the prompt structure.\n*   **Planning with LLMs:** The survey discusses a general three-stage pipeline for multi-step reasoning: **Generate, Evaluate, and Control** steps. Planning falls under the **Control** stage, which can involve complex strategies like ensemble methods or using **Reinforcement Learning (RL)** algorithms (including in-context RL) to explore different reasoning paths dynamically.\n*   **MCTS (Monte Carlo Tree Search) for Language Models:** While MCTS is not explicitly named, the concept of tree search is mentioned under the **Control** stage (3.3.3), where algorithms like **Tree-of-Thoughts (ToT)** use search methods (like BFS/DFS) to scaffold the reasoning process, allowing the LLM to roll back and try different steps.\n*   **Self-Reflection:** **Self-reflection** is explicitly mentioned as being used in many multi-step methods, often implemented via **Self-verification** (where the LLM evaluates its own steps) or as part of reinforcement learning fine-tuning loops.\n*   **Hallucination Reduction and Detection / Grounding:** The text touches upon grounding in the context of robotics, where approaches like **Say-can** use external physics models to ground reasoning steps in reality, which helps reduce errors (a form of hallucination reduction). Tool-based validation (using external interpreters for generated code) also serves to ground the reasoning in verifiable logic.\n*   **Factuality:** While the term \"\n\nThe user query asks for a summary related to several advanced topics in Reasoning and Planning with Large Language Models (LLMs), including: **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**\n\nThe provided webpage text discusses many of these concepts in detail:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** The text extensively covers various reasoning approaches, including **Chain-of-Thought (CoT)**, **Least-to-most prompting**, **Program-of-thought**, and **Program-aided-language**, which use LLMs to generate step-by-step reasoning.\n*   **Planning with LLMs:** Planning is discussed in the context of **Robotic Behavior** (e.g., Say-can, Inner-monologue, Chain-of-tools) and in the control stage using search algorithms like **Tree-of-Thoughts (ToT)**, which explores a search tree with backtracking.\n*   **Inference-time Compute & Control:** The text details three main approaches for controlling reasoning steps at inference time: **greedy selection** (like standard CoT), **ensemble strategy** (like Self-consistency), and **Reinforcement Learning (RL)** search, which involves external algorithms traversing a search tree.\n*   **Self-Reflection & Self-Improvement:** Several methods involve the model evaluating and correcting its own output, such as **Self-debugging**, **Refiner**, **Self-correction**, **Self-improvement**, **Self-refine**, and **Reflexion** (which uses an actor, evaluator, and reflector model).\n*   **Hallucination Reduction & Grounding:** The text notes that reasoning suffers from **hallucination** when not properly grounded. **Grounding** is achieved by combining LLMs with external tools or knowledge sources, such as using external models for **robotic affordances** (Say-can) or retrieving information from **Wikipedia** (ReAct).\n*   **Factuality & Faithfulness:** The text addresses the issue of **faithfulness**—whether the model followed the stated reasoning steps or arrived at the answer through an unfaithful path. Methods like **Faithful-chain-of-thought** are proposed to address this by translating",
    "published_date": "2025-08-14T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/@tahirbalarabe2/what-is-llm-chain-of-thought-prompting-1d4b57a4dd22": {
    "content_path": "data/contents/1fb530e90c553ade.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.257115",
    "title": "🧠What is LLM Chain of Thought Prompting?",
    "summary": "Chain of Thought (CoT) prompting is a technique used with Large Language Models (LLMs) to improve their ability to reason through complex problems. It works by guiding the LLM to break down a problem into smaller, sequential reasoning steps, showing its work before providing the final answer.\n\n**Key aspects covered in the text related to your query:**\n\n*   **Reasoning LLMs & Chain-of-Thought:** CoT prompting is explicitly designed to help LLMs reason through problems requiring logical thinking, common sense, or cause-and-effect relationships, leading to improved accuracy.\n*   **Inference-time compute:** While not explicitly detailed as a metric, the step-by-step process inherently involves more computation during inference compared to a direct answer.\n*   **Self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction, grounding, factuality:** The provided text focuses solely on Chain of Thought prompting and does not discuss these other advanced topics (self-reflection, MCTS, grounding, etc.).\n\n**Summary:**\n\nChain of Thought prompting is a prompt engineering technique that enhances LLM performance on complex tasks by instructing the model to articulate its reasoning process step-by-step. This method significantly improves accuracy for problems requiring logical deduction (like math or multi-step planning) and increases the transparency of the model's solution path. It is useful for both direct user interaction and backend API systems.\n\n**Regarding the other terms in your query (reasoning\\_and\\_planning: Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, factuality):** The webpage only details **Chain of Thought prompting** and its benefits for **Reasoning LLMs**. It does not provide information on self-reflection, MCTS, test-time scaling, grounding, or hallucination reduction.",
    "published_date": "2025-03-23T00:00:00.000Z",
    "score": null
  },
  "https://www.emergentmind.com/articles/2201.11903": {
    "content_path": "../data/contents/3c459409840f6a98.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.536002",
    "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models\n     \n      (2201.11903v1)",
    "summary": "The webpage summarizes the paper \"Chain of Thought Prompting Elicits Reasoning in Large Language Models\" (2201.11903).\n\n**Summary relevant to the user query:**\n\nThe paper introduces **Chain-of-Thought (CoT) prompting**, a technique that elicits **reasoning** in Large Language Models (LLMs) by including intermediate reasoning steps in few-shot examples.\n\n*   **Reasoning LLMs & Chain-of-Thought:** CoT prompting guides LLMs to generate a \"chain of thought\"—a series of short sentences mimicking a human reasoning process—before providing the final answer. This significantly improves performance on multi-step reasoning tasks like arithmetic, commonsense, and symbolic manipulation, which large models often struggle with using standard prompting.\n*   **Emergent Ability & Inference-Time Compute:** CoT reasoning was found to be an **emergent ability** tied to model scale; it only consistently improved performance for models larger than approximately 100 billion parameters. The increased output length implies increased **inference-time compute**.\n*   **Hallucination Reduction and Detection (Factuality):** While CoT improves performance, the paper notes a limitation: there is **no guarantee that the generated chains of thought are factually correct or logically sound**, even if they lead to a correct answer. Manual analysis showed that errors in reasoning (major errors) and calculation/logic (minor errors) still occur, highlighting an open challenge in ensuring the **factuality** of the generated reasoning.\n*   **Planning with LLMs:** The paper tested CoT on commonsense reasoning tasks like StrategyQA and SayCan robot **planning**, showing applicability beyond pure arithmetic.\n*   **Grounding:** The paper does not explicitly discuss grounding in the context of external knowledge bases, but the manual analysis suggests that major errors stem from semantic understanding issues, which relates to how well the model grounds its reasoning in the problem's context.\n\n**Topics not explicitly covered or detailed:**\n\n*   MCTS (Monte Carlo Tree Search) for language models.\n*   Self-reflection (though CoT is a form of self-correction/step-by-step verification).\n*   Test-time scaling (beyond the observation that CoT emerges at scale).",
    "published_date": null,
    "score": null
  },
  "https://inovex.de/de/blog/mcts-meets-llms-enabling-complex-reasoning-and-strategic-planning": {
    "content_path": "../data/contents/9f0245d716bbbcbd.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.538852",
    "title": "MCTS meets LLMs: Enabling Complex Reasoning and Strategic Planning",
    "summary": "The webpage discusses a novel framework that integrates **Monte Carlo Tree Search (MCTS)** with **Large Language Models (LLMs)** to enhance their capabilities in complex **reasoning** and **strategic planning**, particularly demonstrated through a case study in Visual Question Answering (VQA).\n\nKey points related to your query:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** The text acknowledges the need for LLMs to reason and mentions **Chain-of-Thought (CoT)** prompting as a technique to elicit more thoughtful responses, contrasting it with a simple, incorrect answer.\n*   **MCTS for Language Models (Planning with LLMs):** The core of the framework uses **MCTS** as the environment dynamics manager. The LLM acts as the agent, making decisions (actions like \"Retrieve\" or \"Answer\") at each node of the MCTS tree. This iterative exploration allows the agent to foresee consequences and refine its decision-making, similar to trial-and-error learning.\n*   **Inference-time Compute & Test-time Scaling:** The MCTS process inherently involves iterative computation during inference (tree growth and simulation/rollouts) to determine the best sequence of actions, which relates to inference-time compute. The framework's flexibility allows for adjusting parameters governing the tree search, balancing speed and exploration.\n*   **Hallucination Reduction and Detection/Grounding/Factuality:** While not explicitly using the terms \"hallucination reduction\" or \"detection,\" the framework addresses related issues:\n    *   The MCTS approach is described as more **failure-tolerant** than CoT because it allows the LLM to explore multiple paths and **correct itself** if a path is derailed, unlike CoT where recovery from mistakes is difficult.\n    *   The \"Retrieve\" action, which uses a vector database and a vision-language model to gather external evidence, serves to **ground** the LLM's response in external data, mitigating reliance solely on internal knowledge.\n    *   However, the text explicitly states that **hallucinations and overconfidence persist as challenges** despite the advancements.\n*   **Self-reflection:** The LLM evaluates the utility of actions and assesses the quality of states, which is a form of iterative self-assessment within the planning loop.\n\n**In summary:** The page details how MCTS provides a structured planning mechanism around an LLM",
    "published_date": "2024-06-19T11:57:32.000Z",
    "score": null
  },
  "https://github.com/1989Ryan/llm-mcts": {
    "content_path": "data/contents/b840ff4b8b76371a.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.516699",
    "title": "GitHub - 1989Ryan/llm-mcts: [NeurIPS 2023] We use large language models as commonsense world model and heuristic policy within Monte-Carlo Tree Search, enabling better-reasoned decision-making for daily task planning problems.",
    "summary": "The webpage describes the **llm-mcts** GitHub repository, which implements a method for **task planning** using **Large Language Models (LLMs)** within **Monte Carlo Tree Search (MCTS)**.\n\nSpecifically, the LLMs are used as both the **commonsense world model** and the **heuristic policy** within MCTS. This approach aims to enable **better-reasoned decision-making for daily task planning problems** by providing MCTS with a commonsense prior belief of states and guiding the search to reduce complexity. The work is associated with a **NeurIPS 2023** paper.\n\nThe query covers several topics related to advanced LLM capabilities, including: *reasoning, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction, grounding, and factuality*.\n\nThe page directly addresses **planning with LLMs** and **MCTS (Monte Carlo Tree Search) for language models** in the context of task planning. It does not explicitly detail reasoning techniques like chain-of-thought, inference-time compute specifics, self-reflection, test-time scaling, or methods for hallucination reduction/detection, grounding, or factuality beyond the general goal of \"better-reasoned decision-making.\"",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/html/2505.00610v1": {
    "content_path": "data/contents/b444763903eb51fb.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.374199",
    "title": "Combining LLMs with Logic-Based Framework to Explain MCTS",
    "summary": "The webpage describes a framework that combines **Large Language Models (LLMs)** with a **Logic-Based Framework** to provide explanations for the **Monte Carlo Tree Search (MCTS)** algorithm, particularly in sequential planning problems like paratransit routing.\n\nKey aspects related to your query:\n\n*   **Reasoning and Planning:** The core focus is on explaining decisions made by MCTS, a planning algorithm used in complex sequential planning problems.\n*   **MCTS (Monte Carlo Tree Search) for language models:** The framework is specifically designed to explain MCTS decisions.\n*   **Grounding and Factuality:** The framework ensures that explanations are factually consistent with the underlying environmental dynamics and constraints by transforming user queries into logic statements that are evaluated against the MCTS search tree. The evaluation section explicitly measures **factual consistency** (using FactCC) and relevance (using BERTScore), showing significant improvement over using basic LLMs alone.\n*   **LLMs:** LLMs are used in multiple components: interpreting user queries, generating logic statements, and generating the final natural language explanations.\n\nThe page does not explicitly detail concepts like \"chain-of-thought,\" \"inference-time compute,\" \"self-reflection,\" \"test-time scaling,\" or \"hallucination reduction and detection\" in the context of general LLM reasoning, but it addresses the need for **factuality** and **grounding** in explanations derived from a planning process.",
    "published_date": "2019-06-30T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2410.01707": {
    "content_path": "data/contents/8e198f19bd5ede3e.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.431385",
    "title": "Computer Science > Computation and Language",
    "summary": "The webpage describes a paper titled \"Interpretable Contrastive Monte Carlo Tree Search Reasoning,\" which proposes **SC-MCTS***, a novel **Monte Carlo Tree Search (MCTS)** reasoning algorithm for **Large Language Models (LLMs)**.\n\nThe work addresses:\n1.  The slower speed of previous MCTS LLM reasoning methods compared to Chain-of-Thought (CoT).\n2.  The limited quantitative analysis of MCTS components from an interpretability perspective.\n3.  The crucial role of the reward model in MCTS.\n\nThe authors conducted ablation studies and introduced:\n*   A highly **interpretable reward model** based on contrastive decoding.\n*   An average **51.9% speed improvement** per node using speculative decoding.\n*   Improvements to the UCT node selection strategy and backpropagation.\n\nThis new method, SC-MCTS*, outperformed a baseline (o1-mini) by an average of **17.4%** on the Blocksworld multi-step reasoning dataset using Llama-3.1-70B.\n\nIn summary, the page focuses on **MCTS for language models** and improvements in **reasoning** speed and interpretability.",
    "published_date": "2024-10-02T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2501.14304": {
    "content_path": "../data/contents/e11b94bcbfbc1030.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:34.067626",
    "title": "Computer Science > Artificial Intelligence",
    "summary": "The webpage describes a paper titled \"MASTER: A Multi-Agent System with LLM Specialized MCTS.\" This work addresses the challenges of using **Monte Carlo Tree Search (MCTS)** to augment the **strategic planning capability** of **Large Language Models (LLMs)**.\n\nSpecifically, it notes that MCTS struggles with tasks like question answering where objective rewards are unavailable, and that achieving statistically significant results requires excessive token usage and time.\n\nThe proposed solution, MASTER, is a novel framework that coordinates agent recruitment and communication using **LLM specialized MCTS**. This system autonomously adjusts the number of agents based on task complexity.\n\nWhile the query covers broad topics like **reasoning and planning**, **chain-of-thought**, **inference-time compute**, **self-reflection**, **planning with LLMs**, **MCTS for language models**, **test-time scaling**, **hallucination reduction and detection**, **grounding**, and **factuality**, the provided text focuses primarily on **planning with LLMs** and **MCTS for language models**, and demonstrates improved performance on question-answering tasks (HotpotQA and WebShop).",
    "published_date": "2025-01-24T00:00:00.000Z",
    "score": null
  },
  "https://openreview.net/pdf/a10ef341ba027467f2859f6f5e8c463ee11150d1.pdf": {
    "content_path": "../data/contents/7c8b200c3d3a854d.md",
    "source": "exa",
    "saved_at": "2025-12-25T19:36:11.554026",
    "title": "",
    "summary": "The webpage discusses the effectiveness of **test-time scaling** (increasing inference-time computation via long reasoning chains, like Chain-of-Thought) for **knowledge-intensive tasks**.\n\nKey findings related to your query:\n\n*   **Reasoning and Planning (Test-Time Compute):** Increasing test-time computation does **not consistently improve accuracy** on knowledge-intensive tasks for most models evaluated (14 reasoning models). In many cases, it leads to **more hallucinations**.\n*   **Hallucination Reduction and Detection:** Reduced hallucinations with longer reasoning are often due to the model choosing to **abstain** from answering rather than improved factual recall. Conversely, increased hallucinations occur because longer reasoning encourages models to **attempt previously unanswered questions**, many of which result in fabrication.\n*   **Self-Reflection/Confirmation Bias:** Extended reasoning can induce **confirmation bias**, leading models to fabricate details to support an initial belief, resulting in overconfident hallucinations (demonstrated with `gpt-oss-20b`).\n*   **Grounding/Factuality:** The study emphasizes that for knowledge-intensive tasks requiring high factual accuracy, current test-time scaling is limited because models struggle to verify facts without external sources, leading to reasoning chains filled with fabricated details.\n*   **Thinking vs. Non-Thinking:** Despite the limitations of scaling *within* the thinking mode, enabling thinking (compared to a non-thinking mode) is still generally **beneficial**, improving accuracy (especially for multi-hop reasoning) and reducing hallucinations for most models.\n\nIn summary, while reasoning techniques are employed, simply increasing the inference-time compute via test-time scaling is currently ineffective and potentially detrimental to factuality in knowledge-intensive domains.\n\nThe provided text discusses experimental settings, prompt details, and results related to **test-time reasoning/scaling** for Language Models (LLMs), including comparisons between models with and without \"thinking\" (reasoning effort/budget). It also explores **parallel sampling** (self-consistency) for knowledge-intensive tasks, noting that it primarily reduces hallucinations through abstention rather than improving accuracy. Finally, it provides case studies illustrating how increased reasoning computation can lead to either abstention or **hallucination** (confirmation bias).\n\nWhile the text heavily focuses on **test-time compute**, **hallucination reduction and detection**, and **grounding** (implied by the focus on factual accuracy and abstention), it does not explicitly detail:\n*   **Reasoning LLMs** (beyond the context of test-time reasoning methods).\n*   **Chain-of-thought** (though the \"thinking\" mechanism is related).\n*   **Self-reflection** (though the process of increasing computation might imply some form of internal checking).\n*   **Planning with LLMs** or **MCTS (Monte Carlo Tree Search) for language models**.\n\nSince the query is a broad list of advanced LLM topics, and the text only covers a subset of these (specifically test-time scaling, hallucination, and grounding/factuality via abstention), a complete summary covering all listed points is not possible.\n\nNo answer found.",
    "published_date": "2025-11-23T00:00:00.000Z",
    "score": null
  },
  "https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/": {
    "content_path": "../data/contents/d0ac66f24c1ddfb5.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:33.722125",
    "title": "Workflows and agents",
    "summary": "The webpage, titled \"Workflows and agents\" from LangChain Docs, discusses common patterns for building systems using LangChain and LangGraph, specifically focusing on **Workflows** and **Agents**.\n\nHere is a summary of the content relevant to your query:\n\n*   **Agent Frameworks:** The page is part of the **LangChain** documentation and heavily features **LangGraph** for building these systems.\n*   **Tool Use/Function Calling:** Agents are described as LLMs performing actions using **tools**. The \"Using tools\" section provides an example of defining tools (like `multiply`, `add`, `divide`) and augmenting an LLM to use them, which is equivalent to **function calling**.\n*   **Structured Outputs:** The setup section mentions that workflows and agents can use any chat model that supports **structured outputs** and tool calling. Examples throughout the document demonstrate using Pydantic models for structured output (e.g., `SearchQuery`, `Route`, `Feedback`).\n*   **Agent Orchestration:** The **Orchestrator-worker** pattern explicitly describes an orchestrator breaking down tasks, delegating to workers, and synthesizing results.\n*   **Agent Memory:** The navigation sidebar lists **Memory** as a capability of LangGraph, though the main body of the text focuses more on workflows and agents' immediate actions rather than deep dives into agentic memory concepts.\n*   **MCP Servers, Agent Memory, Agentic Memory, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** These specific terms are **not explicitly detailed** in the main body of the text provided. The text mentions using Anthropic models in the setup example, but does not discuss the specific SDKs for OpenAI, Anthropic, or Google, nor does it detail MCP servers or the distinction between agent memory and agentic memory.\n\n**In summary:** The page covers **agent frameworks (LangGraph/LangChain)**, **tool use/function calling**, **structured outputs**, and **agent orchestration** patterns. It does not provide specific information on **MCP servers, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, agent memory, or agentic memory**.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://blog.mozilla.ai/introducing-any-agent-an-abstraction-layer-between-your-code-and-the-many-agentic-frameworks/": {
    "content_path": "../data/contents/514f26c1518e1e9c.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.858207",
    "title": "Introducing Any-Agent: An abstraction layer between your code and the many agentic frameworks",
    "summary": "The webpage introduces **Any-Agent**, an abstraction layer designed to simplify building and experimenting with different agentic frameworks.\n\nIt addresses the complexity arising from the variety of agent frameworks available (like LangChain, Smolagents, etc.), which often have opinionated implementations for agentic logic, routing, and default prompts.\n\n**Key features and concepts mentioned in relation to the user query:**\n\n*   **Agent Frameworks:** The page explicitly mentions and compares frameworks like **LangChain** and **Smolagents**, and lists others such as OpenAI Agents Python SDK, LangGraph Python SDK, AWS Bedrock Agents SDK, CrewAI Python SDK, AutoGen Python SDK, and Agno Python SDK.\n*   **Tool Use & Structured Outputs:** These are mentioned as techniques used to improve the capability of LLM-powered applications.\n*   **Agent Orchestration/Workflows vs. Agents:** Anthropic's definition is used to distinguish between prescriptive **workflows** (predefined code paths orchestrating LLMs and tools) and true **agents** (systems where LLMs dynamically direct their own processes and tool usage).\n*   **MCP Servers:** The library is designed to load and configure **Model Context Protocol (MCP) Servers** for various agent frameworks, abstracting away framework-specific loading semantics.\n*   **Agent Memory/Agentic Memory:** While the concept of agents is central, specific details about implementing or abstracting \"agent memory\" or \"agentic memory\" are not elaborated upon beyond the general discussion of agent capabilities.\n*   **SDKs:** The page lists several SDKs related to agents, including the **OpenAI Agents SDK** (implied by mentioning LangChain's use of OpenAI models) and the **Anthropic Agents SDK** (implied by referencing Anthropic's definition). The **Google SDK** is not explicitly mentioned in the context of agent frameworks, though other providers are listed.\n*   **Function Calling:** This is not explicitly mentioned by name, but it is related to the concept of **tool usage**.\n\n**In summary, Any-Agent allows developers to write agent code once and easily switch between different underlying agent frameworks (like LangChain) while handling configuration details like MCP Server loading.**",
    "published_date": "2025-04-10T00:00:00.000Z",
    "score": null
  },
  "https://docs.langchain.com/langsmith/server-mcp": {
    "content_path": "../data/contents/e38b9ce0a921d221.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.001701",
    "title": "MCP endpoint in Agent Server",
    "summary": "The webpage describes the **Model Context Protocol (MCP) endpoint in LangChain's Agent Server**.\n\n**Key Summary Points:**\n\n*   **What MCP is:** MCP is an open protocol for describing tools and data sources in a model-agnostic format, allowing LLMs to discover and use them via a structured API.\n*   **Implementation:** Agent Server implements MCP using the Streamable HTTP transport. This allows **LangGraph agents** to be exposed as **MCP tools**.\n*   **Endpoint Location:** The MCP endpoint is available at `/mcp` on the Agent Server.\n*   **Usage:** MCP-compliant clients (supporting Streamable HTTP) can connect to the Agent Server to use these exposed agents as tools. Examples are provided for connecting via Python (using `langchain-mcp-adapters`) and JavaScript/TypeScript.\n*   **Exposing Agents as Tools:** Agents are exposed with their name, description, and input schema. It is recommended to define custom agents with explicit input/output schemas rather than relying on the general `AnyMessage` state.\n*   **User-Scoped Tools:** Custom authentication middleware can be used to populate user context, allowing user-scoped MCP tools to be accessed within a LangSmith deployment.\n*   **Limitations:** The current LangGraph MCP implementation **does not support sessions**; each `/mcp` request is stateless.\n*   **Disabling MCP:** The endpoint can be disabled by setting `\"disable_mcp\": true` in the configuration file.\n\n**Regarding the User Query:**\n\nThe query lists several concepts related to agent infrastructure, frameworks, and SDKs: *agent\\_infrastructure: MCP servers, tool use, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration*.\n\nThe webpage directly addresses **MCP servers**, **tool use** (exposing agents as tools), **LangChain** (specifically LangGraph/Agent Server), and **structured outputs** (via defining explicit schemas).\n\nIt **does not** provide specific information on:\n*   Agent memory or agentic memory.\n*   LlamaIndex.\n*   OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK.\n*   Agent orchestration (beyond the context of LangGraph).\n\n**Summary tailored to the query:**\n\nThe page details the **",
    "published_date": "2025-03-26T00:00:00.000Z",
    "score": null
  },
  "https://ai.plainenglish.io/build-a-tiny-ai-agent-that-uses-mcp-tools-with-langgraph-58a1bd3e6616": {
    "content_path": "../data/contents/5025f8173b903082.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.054969",
    "title": "Build a Tiny AI Agent that Uses MCP Tools with LangGraph",
    "summary": "The webpage describes how to build a tiny AI agent using **Model Context Protocol (MCP)** servers and **LangGraph**.\n\nHere is a summary of the concepts mentioned in relation to the user query:\n\n*   **agent_infrastructure:** The core infrastructure discussed is the combination of an **MCP server** (to expose tools via a standard protocol) and a **LangGraph agent** (to control the agent loop and call those tools).\n*   **tool use:** The example demonstrates an agent calling a simple `add(a, b)` tool exposed via an MCP server.\n*   **agent frameworks:** **LangGraph** is the primary framework used for building the stateful, controllable agent loop.\n*   **LangChain:** The agent is built using LangChain components, specifically `create_react_agent` and OpenAI models (`langchain-openai`).\n*   **LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** These are **not** directly discussed or used in the implementation, although the text notes that if you prefer Anthropic, you would swap out the OpenAI provider for the Anthropic equivalent within the LangChain setup.\n*   **function calling, structured outputs:** The MCP tool definition uses Python type hints (`a: int, b: int -> int`), and the agent interacts with the tool call, which returns structured data (though the example normalizes it to a string).\n*   **agent orchestration:** The \"Production notes\" section suggests that as the agent grows, one should look into splitting capabilities into specialists and orchestrating them (router, supervisor/worker patterns), which is a form of orchestration.\n*   **agent memory, agentic memory:** These concepts are **not** discussed in the provided text.\n\n**In summary:** The page focuses on using **MCP** for standardized tool exposure and **LangGraph** for agent control, built on top of **LangChain**. It does not cover agent memory or specific SDKs like LlamaIndex or the dedicated Anthropic/Google Agents SDKs.",
    "published_date": "2025-09-09T00:00:00.000Z",
    "score": null
  },
  "https://developers.openai.com/apps-sdk/concepts/mcp-server/": {
    "content_path": "data/contents/9edebd79b4a2e0f6.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.857368",
    "title": "MCP",
    "summary": "The provided web page describes the **Model Context Protocol (MCP)**, which is an open specification for connecting large language model clients to external tools and resources.\n\n**Key aspects of MCP mentioned on the page:**\n\n*   **Purpose:** An MCP server exposes **tools** that a model can call during a conversation, returning results with specified parameters. It also allows returning metadata, such as inline HTML for rendering interfaces via the Apps SDK.\n*   **Role with Apps SDK:** MCP is the backbone that keeps the server, model, and UI in sync, allowing ChatGPT to reason about your app similarly to built-in tools by standardizing wire format, authentication, and metadata.\n*   **Protocol Building Blocks:** A minimal MCP server implements:\n    1.  **List tools:** Advertises supported tools, including their JSON Schema input/output contracts.\n    2.  **Call tools:** Executes actions requested by the model and returns structured content.\n    3.  **Return components:** Optionally points to an embedded resource representing the interface to render in the client.\n*   **Benefits of using MCP:** Discovery integration, conversation awareness (state flows through the conversation), multiclient support (works across web and mobile), and extensible authentication.\n\n**Regarding your specific query terms:**\n\nThe page explicitly mentions **MCP Server** and **tool use** (via tool calling). It also mentions **function calling** implicitly through the description of tool calling with JSON Schema contracts.\n\nHowever, the page **does not** contain information about: `agent_infrastructure`, `deep research`, `agent memory`, `agentic memory`, `context compression`, `agent frameworks`, `LangChain`, `LlamaIndex`, `OpenAI Agents SDK`, `Anthropic Agents SDK`, `Google SDK`, `structured outputs` (beyond the tool contract), `agent orchestration`, `agent evaluations`, `LLM reasoning trace evaluation`, `prompt engineering`, `context engineering`, `long context`, `supervised fine-tuning`, or `reinforcement learning`.\n\n**Summary based on the query:**\n\nThe page explains that **MCP (Model Context Protocol)** is a specification for connecting LLM clients to external tools, enabling **tool use** via a server that advertises tools and handles `call_tool` requests. It is the foundation for the Apps SDK.\n\n**No answer found** for the majority of the specific agent infrastructure and advanced LLM training/engineering terms listed in your",
    "published_date": "2025-06-18T00:00:00.000Z",
    "score": null
  },
  "https://mcpservers.org/servers/mcp-use/mcp-use": {
    "content_path": "data/contents/816ed12274d2bd1c.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.991034",
    "title": "Unified MCP Client Library",
    "summary": "The webpage describes the **Unified MCP Client Library (mcp-use)**, an open-source Python library designed to connect **any LLM to any MCP server**, enabling the creation of custom agents with tool access.\n\nHere is a summary of how the page relates to your query terms:\n\n*   **agent\\_infrastructure: MCP servers, tool use, agent frameworks:** The core purpose of the library is to facilitate agent infrastructure by connecting LLMs to **MCP servers** to enable **tool use**. It integrates with **agent frameworks** like **LangChain** (via providers).\n*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** The library explicitly supports integration with **LangChain** for connecting to various LLMs (like **OpenAI** and **Anthropic**). It does not explicitly mention LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK, though it supports LLMs from those ecosystems via LangChain.\n*   **function calling, structured outputs:** The documentation notes that only models with **tool calling capabilities** (which implies function calling) can be used with `mcp_use`.\n*   **agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context, supervised fine-tuning, reinforcement learning:** None of these advanced agent development, training, or evaluation concepts are directly discussed in the context of the `mcp-use` library features.\n*   **agent memory, agentic memory, context compression:** These memory and context management techniques are **not mentioned** in the provided text.\n\n**In summary:** The page details a library for building tool-using agents by connecting LLMs (via LangChain) to MCP servers, supporting features like streaming, multi-server orchestration, and sandboxed execution. It **does not cover** agent memory, context compression, or advanced training/evaluation methods listed in your query.",
    "published_date": "2024-06-20T00:00:00.000Z",
    "score": null
  },
  "https://code.visualstudio.com/docs/copilot/customization/mcp-servers": {
    "content_path": "data/contents/c717c11e66ba9f98.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.646141",
    "title": "Use MCP servers in VS Code",
    "summary": "The provided web page details how to use **Model Context Protocol (MCP) servers** within **VS Code** to extend chat capabilities with external tools and services.\n\n**Key takeaways related to your query:**\n\n*   **MCP Servers and Tool Use:** MCP servers provide tools for AI models to interact with external services (like file operations, databases, or APIs) through a unified interface. VS Code supports using these tools in chat, either automatically invoked by agents or explicitly referenced.\n*   **Agent Infrastructure Context:** While the page focuses on the VS Code implementation of MCP, it directly relates to the concept of providing **tool use** capabilities to agents within an IDE environment.\n*   **No mention of specific terms:** The page **does not** mention or discuss the following terms from your query: *deep research, agent memory, agentic memory, context compression, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context, supervised fine-tuning, or reinforcement learning.*\n\n**Summary:** The page explains that MCP servers in VS Code enable **tool use** for AI models via a standardized protocol, but it does not cover the broader concepts of agent frameworks, memory, advanced training methods, or specific SDKs listed in your query.\n\nNo answer found for the majority of the specific terms in your query.",
    "published_date": "2025-07-08T00:00:00.000Z",
    "score": null
  },
  "https://glama.ai/mcp/servers/@surya-madhav/MCP/blob/main/docs/03-building-mcp-servers-python.md": {
    "content_path": "data/contents/f89d3e57e7dd2c67.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.025024",
    "title": "MCP Web Tools Server",
    "summary": "The provided webpage is a guide titled \"MCP Web Tools Server\" that details how to build Model Context Protocol (MCP) servers using Python.\n\nThe summary relevant to your query is:\n\nThe page focuses on **MCP servers** and the **Python SDK** for building them, covering:\n*   **Setup and Structure:** Installation, project structure, and using the `FastMCP` high-level API.\n*   **Core Components:** Implementing **tools** (basic, async, parameterized, with error handling) and **resources** (file, dynamic, metadata, binary) that LLMs can use.\n*   **Prompts:** Creating templates for LLM interaction.\n*   **Transport:** Using STDIO (default) or SSE transport for communication.\n*   **Context and Lifespan:** Utilizing the `Context` object and managing server lifecycle.\n*   **Testing and Debugging:** Using the MCP Inspector and logging.\n*   **Deployment and Security:** Options for local and web deployment, and security considerations (input validation, resource access).\n*   **Example:** A complete web scraping server example.\n\n**Regarding the specific terms in your query:**\n\n*   **agent\\_infrastructure:** The entire document describes building infrastructure (MCP servers) for agents.\n*   **MCP servers:** This is the central topic.\n*   **tool use:** Covered extensively under \"Implementing Tools.\"\n*   **deep research, agent memory, agentic memory, context compression, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context, supervised fine-tuning, reinforcement learning:** These advanced agent concepts are **not explicitly discussed** in detail. While MCP servers facilitate tool use (a form of function calling/structured output), the document focuses on the server implementation protocol itself, not the broader ecosystem, advanced memory techniques, or specific training/evaluation methods listed.\n\n**Conclusion:** The page covers **MCP servers** and **tool use** within that framework, but does not provide information on the other specific agent infrastructure, memory, or training concepts listed in your query.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://mcpserver.cc/server/mcp-server-toolhouse": {
    "content_path": "data/contents/4caffd6c6d8981e5.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.690889",
    "title": "Mcp Server Toolhouse",
    "summary": "The provided webpage describes the **Toolhouse MCP Server**, which uses the Model Context Protocol (MCP) to connect MCP clients (like the Claude Desktop App) with tools available on the Toolhouse platform, leveraging **Toolhouse** and **Groq's API** for fast inference.\n\nKey aspects mentioned include:\n\n*   **Purpose:** Allows MCP Clients to access a library of tools to enhance their capabilities.\n*   **Protocol:** Built on the Model Context Protocol (MCP), an open protocol for integrating LLM applications with external data sources and tools.\n*   **Configuration:** Requires a **Toolhouse API Key** and a **Toolhouse Bundle** (a group of selected tools). Instructions are provided for setting environment variables and configuring client applications (like Claude) to use this server via `uvx` or `uv`.\n*   **Future Improvements (TODO):** Adding test coverage and extending API support for enhanced tool configurations.\n*   **Debugging:** Recommends using the **MCP Inspector** for debugging, as MCP servers run over stdio.\n\n**Regarding your specific query terms:**\n\nThe page focuses on the **MCP server infrastructure** and **tool use** via the Toolhouse platform. It mentions the MCP protocol itself. However, it **does not** discuss: *deep research, agent memory, agentic memory, context compression, agent frameworks (LangChain, LlamaIndex, SDKs), function calling, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context, supervised fine-tuning, or reinforcement learning.*\n\n**Summary relevant to the query:**\n\nThe Toolhouse MCP Server provides an **MCP server** infrastructure that enables **tool use** by connecting LLM clients to tools defined in a **Toolhouse Bundle**.\n\n**No answer found** for the majority of the specific agent infrastructure and advanced LLM technique terms listed in your query.",
    "published_date": "2025-09-10T00:00:00.000Z",
    "score": null
  },
  "https://python.langchain.com/v0.1/docs/modules/memory/agent_with_memory/": {
    "content_path": "../data/contents/386be16d7c394155.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:33.775755",
    "title": "LangChain overview",
    "summary": "LangChain is a framework for building applications powered by Large Language Models (LLMs), particularly focusing on **agents**.\n\nThe page mentions several components and concepts relevant to your query:\n\n*   **Agent Frameworks:** LangChain itself is an agent framework. It also mentions **LangGraph** as a low-level agent orchestration framework and runtime built upon which LangChain agents operate.\n*   **Tool Use/Function Calling:** LangChain includes **Tools** as a core component, which enables agents to interact with external functions (similar to function calling).\n*   **Agent Memory:** It lists **Short-term memory** and **Long-term memory** under advanced usage.\n*   **Structured Outputs:** **Structured output** is listed as a core component.\n*   **Agent Orchestration:** This is handled by LangChain and, for advanced needs, by **LangGraph**.\n*   **Model Context Protocol (MCP):** This is listed under advanced usage, allowing programmatic connection to documentation via services like Claude and VSCode for real-time answers.\n*   **SDKs/Integrations:** LangChain provides integrations to connect to **OpenAI**, **Anthropic**, and **Google** models.\n\n**Missing/Not Explicitly Detailed:** The page does not explicitly detail \"agentic memory\" (though it covers memory generally), \"OpenAI Agents SDK,\" \"Anthropic Agents SDK,\" or \"Google SDK\" as separate entities, only that LangChain integrates with the underlying providers.",
    "published_date": "2024-01-01T00:00:00.000Z",
    "score": null
  },
  "https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/": {
    "content_path": "../data/contents/ec8136328e637e44.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.412640",
    "title": "Memory | LlamaIndex Python Documentation",
    "summary": "The provided web page is documentation for **Memory** within the **LlamaIndex Python Documentation**.\n\nHere is a summary of the page relevant to your query:\n\n*   **Agent Memory/Agentic Memory:** Memory is described as a core component of agentic systems, allowing for the storage and retrieval of past information.\n*   **LlamaIndex Framework:** The documentation details how memory is implemented and customized within LlamaIndex agents.\n*   **Memory Classes:** It discusses the current standard, the `Memory` class (which replaces the deprecated `ChatMemoryBuffer`), offering both short-term (FIFO queue of messages) and optional long-term memory.\n*   **Short-Term Memory Configuration:** This is managed by setting `token_limit`, `chat_history_token_ratio`, and `token_flush_size`.\n*   **Long-Term Memory (Memory Blocks):** Long-term memory is handled via `MemoryBlock` objects that process flushed short-term messages. Predefined blocks include:\n    *   `StaticMemoryBlock`\n    *   `FactExtractionMemoryBlock` (uses an LLM to extract facts)\n    *   `VectorMemoryBlock` (stores/retrieves messages from a vector database)\n*   **Customization:** Users can create custom memory blocks (e.g., `MentionCounter`).\n*   **Remote Memory:** The default in-memory database can be replaced with remote databases (e.g., PostgreSQL) by changing the `async_database_uri`.\n*   **Integration with Agents:** Memory is configured when running an agent (e.g., `FunctionAgent`) by passing the memory object into the `run()` method.\n*   **Memory vs. Workflow Context:** It distinguishes the simpler `Memory` object (holding chat history/blocks) from the more complex, serializable `Context` object used in workflows.\n\n**Regarding other terms in your query:**\n\nThe document focuses exclusively on **Memory** within the **LlamaIndex** framework. It does **not** mention:\n\n*   MCP servers\n*   Tool use (though agents using memory often use tools, this page doesn't detail tool use itself)\n*   Agent frameworks (other than LlamaIndex itself)\n*   OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK\n*   Function calling\n*   Structured outputs\n*   Agent orchestration",
    "published_date": "2018-06-09T00:00:00.000Z",
    "score": null
  },
  "https://developers.llamaindex.ai/python/framework/module_guides/deploying/agents/memory/": {
    "content_path": "../data/contents/c8d0c759f7b6b32d.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.440476",
    "title": "Memory | LlamaIndex Python Documentation",
    "summary": "The provided web page is documentation for **Memory** within the **LlamaIndex Python Documentation**.\n\nHere is a summary of the page relevant to your query:\n\n*   **Agent Memory/Agentic Memory:** Memory is described as a core component of agentic systems, allowing for the storage and retrieval of past information.\n*   **LlamaIndex Framework:** The documentation details how memory is implemented and customized within LlamaIndex agents.\n*   **Memory Classes:** It discusses the current standard, the `Memory` class (which replaces the deprecated `ChatMemoryBuffer`), offering both short-term (FIFO queue of messages) and optional long-term memory.\n*   **Short-Term Memory Configuration:** This is managed by setting `token_limit`, `chat_history_token_ratio`, and `token_flush_size`.\n*   **Long-Term Memory (Memory Blocks):** Long-term memory is handled via `MemoryBlock` objects that process flushed short-term messages. Predefined blocks include:\n    *   `StaticMemoryBlock`\n    *   `FactExtractionMemoryBlock` (uses an LLM to extract facts)\n    *   `VectorMemoryBlock` (stores/retrieves messages from a vector database)\n*   **Customization:** Users can create custom memory blocks (e.g., `MentionCounter`).\n*   **Remote Memory:** The default in-memory database can be replaced with remote databases (e.g., PostgreSQL) by changing the `async_database_uri`.\n*   **Integration with Agents:** Memory is configured when running an agent (e.g., `FunctionAgent`) by passing the memory object into the `run()` method.\n*   **Memory vs. Workflow Context:** It distinguishes the simpler `Memory` object (holding chat history/blocks) from the more complex, serializable `Context` object used in workflows.\n\n**Regarding other terms in your query:**\n\nThe document focuses exclusively on **Memory** within the **LlamaIndex** framework. It does **not** mention:\n\n*   MCP servers\n*   Tool use (though agents using memory often use tools, this page doesn't detail tool use itself)\n*   Agent frameworks (other than LlamaIndex itself)\n*   OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK\n*   Function calling\n*   Structured outputs\n*   Agent orchestration",
    "published_date": null,
    "score": null
  },
  "https://www.llamaindex.ai/blog/improved-long-and-short-term-memory-for-llamaindex-agents": {
    "content_path": "../data/contents/2ba3d5373c99e948.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.465019",
    "title": "Improved Long & Short-Term Memory for LlamaIndex Agents",
    "summary": "The provided web page focuses on **Improved Long & Short-Term Memory for LlamaIndex Agents**.\n\nIt details the introduction of a new `Memory` component in LlamaIndex, which is crucial for agentic applications that need to retain information about past conversations or users.\n\nKey aspects covered include:\n\n*   **When to Use Memory:** Necessary for applications with chat interfaces or those relying on persistent user information.\n*   **Basic Usage (Short-Term Memory):** Storing chat message history up to a token limit, often defaulting to an in-memory SQLite database.\n*   **Long-Term Memory Blocks:** Advanced implementations that handle data overflow from short-term memory:\n    *   **Static Memory Block:** Stores static, non-changing information (e.g., user's name).\n    *   **Fact Extraction Memory Block:** Uses an LLM to extract and store facts from the conversation.\n    *   **Vector Memory Block:** Uses a vector store (like Weaviate or Qdrant) to store chat messages for retrieval via vector search.\n*   **Customizing Memory:** The ability to create custom memory blocks by extending the `BaseMemoryBlock` class.\n*   **Future Improvements:** Plans include allowing memory to be used as a tool for tool-calling agents, adding support for NoSQL databases (like MongoDB, Redis), and enabling structured outputs for the Fact Extraction Memory Block.\n\nThe page **does not** specifically discuss **MCP servers**, **tool use** (beyond mentioning future plans for memory as a tool), **agent frameworks** other than LlamaIndex, **OpenAI Agents SDK**, **Anthropic Agents SDK**, **Google SDK**, **function calling**, or **agent orchestration**.",
    "published_date": "2025-05-13T00:00:00.000Z",
    "score": null
  },
  "https://docs.llamaindex.ai/en/latest/module_guides/deploying/agents/": {
    "content_path": "../data/contents/47f6c5831cd16550.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.513212",
    "title": "Agents | LlamaIndex Python Documentation",
    "summary": "The LlamaIndex documentation defines an **agent** as a system that uses an LLM, memory, and tools to handle user inputs. It contrasts this with **agentic**, which refers to any system involving LLM decision-making.\n\nKey components and concepts discussed in the context of LlamaIndex agents include:\n\n*   **Agent Types:** Examples include `FunctionAgent` (which uses function/tool calling capabilities), `ReActAgent`, and `CodeActAgent`.\n*   **Tools:** These can be defined as Python functions or customized using classes like `FunctionTool` and `QueryEngineTool`. LlamaIndex also offers pre-defined tools via **Tool Specs**.\n*   **Memory:** A core component; by default, LlamaIndex agents use a `ChatMemoryBuffer`.\n*   **Agent Workflow:** The process involves the agent receiving input, sending tool schemas and history to the LLM, responding with either a direct answer or tool calls, executing the tools, updating the history, and invoking the agent again.\n*   **Multi-Modal Agents:** Agents can handle inputs involving multiple modalities, such as images and text, using content blocks.\n*   **Multi-Agent Systems:** Agents can be combined into systems (e.g., using `AgentWorkflow`) to coordinate task completion.\n*   **Manual Agents:** Provides a lower-level approach for building custom agent loops using `LLM` objects directly for full control over tool calling and error handling.\n\nThe document focuses specifically on the **LlamaIndex framework** for building agents and does not explicitly detail general concepts like **MCP servers**, **agent orchestration** (beyond the mention of `AgentWorkflow`), **agent frameworks** other than LlamaIndex, or specific SDKs like **OpenAI Agents SDK**, **Anthropic Agents SDK**, or **Google SDK**. It mentions **function calling** and **structured outputs** implicitly through the `FunctionAgent`. **Agent memory** is covered under the \"Memory\" section.",
    "published_date": "2013-05-09T00:00:00.000Z",
    "score": null
  },
  "https://www.zenml.io/blog/llamaindex-vs-langchain": {
    "content_path": "../data/contents/88258c56316c7959.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.489248",
    "title": "LlamaIndex vs LangChain: Which Framework Is Best for Agentic AI Workflows?",
    "summary": "The webpage compares two popular agent frameworks, **LlamaIndex** and **LangChain**, focusing on their suitability for agentic AI workflows.\n\nHere is a summary of how the page addresses the components mentioned in your query:\n\n*   **Agent Frameworks (LangChain, LlamaIndex):** The entire article is a comparison between these two frameworks, detailing their maturity, orchestration models, agent primitives, and feature sets.\n*   **Agent Orchestration:** Both frameworks offer distinct orchestration models:\n    *   **LlamaIndex** uses **Workflows**, an event-driven, async-first model.\n    *   **LangChain** uses **LangGraph**, a stateful graph structure that supports cycles (loops) for iterative agents.\n*   **Tool Use & Function Calling:**\n    *   **LangChain** offers a wide library of pre-built **Tools** and supports function calling via model wrappers.\n    *   **LlamaIndex** supports function-based agents, including an `OpenAI` FunctionCallingAgent, and turns its data structures (Indexes) into tools.\n*   **Structured Outputs:** Both support enforcing structured outputs using Pydantic models:\n    *   **LlamaIndex** uses Pydantic Programs with `output_cls`.\n    *   **LangChain** uses **Output Parsers** combined with Pydantic support and native function calling.\n*   **Agent Memory:** The page notes that both frameworks cover memory management:\n    *   **LlamaIndex** can store chat history in SQLite or vector memory.\n    *   **LangChain** offers various short and long-term memory utilities (e.g., `ConversationBufferMemory`).\n*   **Agent Infrastructure (MCP servers, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK):** The page does not specifically mention \"MCP servers.\" It discusses integrations with LLM providers:\n    *   Both support **OpenAI** and **Anthropic** models.\n    *   **LlamaIndex** also mentions PaLM (Google).\n    *   The page focuses on the frameworks themselves rather than specific SDKs for agent building (like OpenAI Agents SDK or Anthropic Agents SDK), though they utilize the underlying model SDKs.\n*   **Agentic Memory:** This is covered under the general \"Agent Memory\" point above.\n\nThe article also introduces **ZenML** as an MLOps",
    "published_date": "2025-09-09T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/mitb-for-all/beyond-vibe-coding-of-agentic-ai-understanding-langchain-langgraph-and-llamaindex-5d470fb51e29": {
    "content_path": "../data/contents/2d764b72cedde809.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.538568",
    "title": "Beyond vibe-coding of Agentic AI — understanding LangChain, LangGraph and LlamaIndex",
    "summary": "The webpage provides a detailed guide on understanding and implementing agentic AI concepts using the **LangChain**, **LangGraph**, and **LlamaIndex** frameworks.\n\nThe summary relevant to your query is as follows:\n\n*   **Agent Frameworks (LangChain, LangGraph, LlamaIndex):** The article extensively covers the foundational building blocks of these three major frameworks, showing how to create sequential chains (LangChain), stateful graphs (LangGraph), and RAG-focused pipelines (LlamaIndex).\n*   **Tool Use:** The concept of tool use is demonstrated in both LangChain and LangGraph.\n    *   In **LangChain**, it shows how to explicitly use a tool (Tavily search) in a chain and how to implement **Tool Calling** where the LLM decides which of the defined tools (`tavily_search` or `internal_lookup`) to invoke based on the query.\n    *   In **LangGraph**, it shows how to build a graph structure that incorporates tool selection and execution using conditional edges, analogous to the ReAct pattern.\n*   **Agent Orchestration:** LangChain's `initialize_agent` and LangGraph's `create_react_agent` are shown as methods for orchestrating tool use and reasoning steps. LlamaIndex's `ReActAgent` is also covered for similar autonomous decision-making.\n*   **Agent Memory/Agentic Memory:** Memory is demonstrated in LangChain using `RunnableWithMessageHistory` and in LangGraph using `MemorySaver` within the state definition, allowing the agent to retain context across conversational turns.\n*   **Function Calling & Structured Outputs:** While the article focuses more on general tool use, the concept of defining functions (`@tool` decorator in LangChain) that the agent can call is central to the tool-use sections. Structured outputs are implicitly handled by parsers in LangChain chains and by defined `TypedDict` states in LangGraph.\n*   **MCP Servers, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** These specific infrastructure components or SDKs are **not mentioned** in the provided text. The examples primarily use OpenAI's models (`ChatOpenAI`, `OpenAI`) and the Tavily search API.\n\n**In conclusion, the page covers agent frameworks, tool use, agent orchestration, and agent memory, but does not discuss MCP servers or the specific SDKs for Anthropic",
    "published_date": "2025-09-16T00:00:00.000Z",
    "score": null
  },
  "https://llama-index.readthedocs.io/zh/stable/how_to/integrations/using_with_langchain.html": {
    "content_path": "../data/contents/e0817359d7610851.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.561946",
    "title": "Using with Langchain 🦜🔗 ",
    "summary": "The webpage details how to use **LlamaIndex** with **LangChain**. Specifically, it covers:\n\n*   **Llama Tool abstractions:** LlamaIndex provides Tool abstractions that allow you to use LlamaIndex components (like a `QueryEngine`) directly with a LangChain agent. This is demonstrated by creating an `IndexToolConfig` or using a `LlamaToolkit`.\n*   **Agent Creation:** You can create a LangChain-based chat agent using commands like `create_llama_agent` or `create_llama_chat_agent`, incorporating the LlamaIndex toolkit, an LLM, and a memory module.\n*   **Memory Module:** LlamaIndex can be used as a memory module within a LangChain chatbot to insert arbitrary amounts of conversation history.\n\nThe page mentions **LangChain** and **memory** modules, but it does not discuss **MCP servers**, **tool use** (beyond the LlamaIndex tool abstraction itself), **agentic memory**, **agent frameworks** (other than LangChain), **LlamaIndex** (only in the context of LangChain integration), **OpenAI Agents SDK**, **Anthropic Agents SDK**, **Google SDK**, **function calling**, **structured outputs**, or **agent orchestration**.",
    "published_date": null,
    "score": null
  },
  "https://www.statsig.com/perspectives/llamaindex-vs-langchain-rag": {
    "content_path": "../data/contents/387952ec7fbbeec9.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.586032",
    "title": "LlamaIndex vs LangChain: RAG framework differences",
    "summary": "The user query lists several technical terms related to agent infrastructure, including **agent frameworks (LangChain, LlamaIndex)**, **agent memory**, **tool use**, **function calling**, and **agent orchestration**.\n\nThe webpage primarily compares **LlamaIndex** and **LangChain** as Retrieval-Augmented Generation (RAG) frameworks.\n\n*   **LlamaIndex** is described as **retrieval-first**, focusing on document ingestion, indexing, and fast query-time routing for precise answers from document-heavy tasks.\n*   **LangChain** is described as **orchestration-first**, leaning into **chains, agents, memory, and tool calling**, making it better suited for complex, multi-step workflows.\n\nThe page mentions **tool use** and **memory** in the context of LangChain's strengths for complex workflows, and it touches upon **orchestration** when discussing hybrid approaches (using LlamaIndex for retrieval and LangChain for orchestration). It does not explicitly detail **MCP servers**, **agentic memory**, **OpenAI Agents SDK**, **Anthropic Agents SDK**, **Google SDK**, or **structured outputs**.\n\n**Summary based on the query:**\n\nThe page compares the RAG frameworks **LlamaIndex** and **LangChain**. **LangChain** is highlighted as the better choice for complex workflows that require **tool use**, **memory**, and **agent orchestration**. **LlamaIndex** is favored for document-heavy tasks requiring tight retrieval. The text suggests a hybrid approach where LlamaIndex handles retrieval and LangChain handles orchestration, tools, and memory. Specific details on **MCP servers**, **agentic memory**, **OpenAI Agents SDK**, **Anthropic Agents SDK**, **Google SDK**, or **structured outputs** are not provided.",
    "published_date": "2025-10-31T00:00:00.000Z",
    "score": null
  },
  "https://www.techaheadcorp.com/blog/top-agent-frameworks/": {
    "content_path": "../data/contents/b3c5c29608a46617.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.612722",
    "title": "Comparing the Top Agent Frameworks: LangChain vs LlamaIndex vs AutoGen vs CrewAI .",
    "summary": "The webpage compares four top agent frameworks: **LangChain, LlamaIndex, CrewAI, and AutoGen**, focusing on their architecture, integrations, features, scalability, and ideal use cases for enterprise AI development.\n\nHere is a summary of how the mentioned concepts relate to the frameworks discussed:\n\n*   **Agent Frameworks:** The entire page compares LangChain, LlamaIndex, AutoGen, and CrewAI as solutions to simplify building AI applications, handling tasks like memory, tool integration, and orchestration.\n*   **LangChain:** Features a modular, component-based architecture (Models, Prompts, **Memory**, Chains, **Agents**, **Tools**). It has extensive integrations with major **LLM providers** (OpenAI, Anthropic, **Google SDK** mentioned implicitly via LLM integration), vector databases, and enterprise tools. It supports **function calling** (via Output Parsers/Tool use) and has a robust **callback system** for observability.\n*   **LlamaIndex:** Primarily focuses on connecting LLMs with enterprise data (**RAG**). It supports **OpenAI Function Calling** and offers flexible component customization, including swapping embedding models.\n*   **CrewAI:** A lightweight framework emphasizing **agent orchestration** through \"Crews\" and \"Flows.\" It supports role-based agents and complex decision logic, offering high performance.\n*   **AutoGen:** Developed by Microsoft, it emphasizes a multi-agent conversational approach with an event-driven architecture. It supports **tool usage** and integration with external services.\n\n**Concepts Not Explicitly Detailed or Covered:**\n\n*   **agent\\_infrastructure: MCP servers:** Not mentioned.\n*   **agent memory / agentic memory:** Discussed generally under LangChain's features (Multiple memory implementations) and in the FAQ regarding context management, but not specifically tied to \"agentic memory\" as a distinct concept or infrastructure component.\n*   **OpenAI Agents SDK / Anthropic Agents SDK:** While LangChain and LlamaIndex integrate with OpenAI and Anthropic models, the specific SDKs themselves are not the focus of the comparison.\n*   **structured outputs:** Mentioned under LangChain's \"Output Parsers\" feature, which handles structuring LLM responses into JSON schemas or typed objects.\n\n**Conclusion regarding the query:**\n\nThe page extensively covers **agent frameworks (LangChain, LlamaIndex, AutoGen, CrewAI)**, **tool use",
    "published_date": "2025-10-22T00:00:00.000Z",
    "score": null
  },
  "https://docs.anthropic.com/en/api/openai-sdk": {
    "content_path": "../data/contents/0da0f874760926a5.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:33.802904",
    "title": "OpenAI SDK compatibility",
    "summary": "The webpage describes **OpenAI SDK compatibility** with the Anthropic Claude API, which allows users to test Claude capabilities using the OpenAI SDK with minor code adjustments (changing the base URL, API key, and model name).\n\nKey points relevant to your query:\n\n*   **Tool Use/Function Calling:** The `strict` parameter for function calling is **ignored**, meaning tool use JSON is not guaranteed to follow the supplied schema. For guaranteed schema conformance, the native Claude API with **Structured Outputs** is recommended.\n*   **Agent Memory/Agent Frameworks/Orchestration:** The page does not discuss general agent infrastructure components like MCP servers, agent memory, agentic memory, agent frameworks (LangChain, LlamaIndex), or orchestration tools beyond the specific compatibility layer for SDKs.\n*   **SDKs Mentioned:** It focuses on using the **OpenAI SDK** to interface with Claude. It recommends the native **Claude API** for full feature access (including PDF processing, citations, extended thinking, and prompt caching).\n*   **Structured Outputs:** Mentioned as the solution for guaranteed JSON output conformance, contrasting with the ignored `response_format` field when using the OpenAI SDK compatibility layer.\n\n**Summary in relation to your query:**\n\nThe page details how to use the **OpenAI SDK** to interface with the Claude API via a compatibility layer. It specifically notes that **tool use** (function calling) is supported but the `strict` parameter is ignored, recommending **Structured Outputs** via the native Claude API for guaranteed schema conformance. It does **not** provide information on **MCP servers, agent memory, agentic memory, agent frameworks (LangChain, LlamaIndex), OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, or agent orchestration** beyond the basic SDK compatibility setup.",
    "published_date": "2023-12-21T00:00:00.000Z",
    "score": null
  },
  "https://mcp.so/server/mcptools/f": {
    "content_path": "data/contents/dc475af5606e9ef0.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.957098",
    "title": "Swiss Army Knife for MCP Servers",
    "summary": "The webpage describes **MCP Tools**, a command-line interface for interacting with **MCP (Model Context Protocol) servers** using both stdio and HTTP transport.\n\nKey features include:\n*   Supporting both stdio (default) and HTTP transport.\n*   Allowing users to discover and call tools, list resources, and interact with MCP-compatible services.\n*   Providing output formats like table, JSON, and pretty JSON.\n\nThe user query lists many advanced agent-related concepts (e.g., agent frameworks like LangChain, memory techniques, context engineering, various SDKs, function calling, agent orchestration, and training methods like SFT/RL). **The webpage does not mention or discuss these specific agent infrastructure components.**\n\n**Summary relative to the query:** The page focuses solely on a CLI tool for interacting with MCP servers. It does not cover the extensive list of agent infrastructure topics mentioned in your query.",
    "published_date": "2025-05-14T00:00:00.000Z",
    "score": null
  },
  "https://www.mcp-zone.com/": {
    "content_path": "data/contents/563e14056d5308bd.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.745069",
    "title": "Discover Model Context Protocol Servers",
    "summary": "The webpage is a directory for **Model Context Protocol (MCP) Servers**, which are tools designed to enhance the capabilities of Claude by providing access to various external services and functionalities.\n\nThe user query lists many advanced concepts related to **AI agents, frameworks, memory, context management, and training techniques** (e.g., `agent_infrastructure`, `LangChain`, `LlamaIndex`, `function calling`, `agent orchestration`, `supervised fine-tuning`).\n\nWhile the directory lists several tools that could be used by AI agents (like **AgentQL** for structured data, **AgentRPC** for connecting functions, and servers for **GitHub**, **Docker**, **Notion**, and **Brave Search**), the page **does not directly discuss or provide information** on the specific agent infrastructure concepts, frameworks, memory management techniques, or training methods listed in the user's query.\n\n**No answer found** for the specific technical details requested in the query.",
    "published_date": "2025-07-21T00:00:00.000Z",
    "score": null
  },
  "https://github.com/metorial/mcp-inspector": {
    "content_path": "../data/contents/a37ef49c98aa5ac7.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:33.815372",
    "title": "GitHub - metorial/mcp-inspector: Visual testing tool for MCP servers",
    "summary": "No answer found",
    "published_date": "2025-04-21T00:00:00.000Z",
    "score": null
  },
  "https://mcp-framework.com/docs/debugging": {
    "content_path": "../data/contents/f4c1995b3102af0c.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:33.817963",
    "title": "Debugging MCP Servers",
    "summary": "The provided web page focuses on **Debugging MCP Servers** using the **MCP Inspector**, an external developer tool for the **Model Context Protocol (MCP) Framework**.\n\nThe summary of the page content is:\n\nThe MCP Inspector allows users to test and debug MCP servers through a user interface. It can be run via `npx` and allows customization of client UI and proxy server ports. The Inspector features tabs for:\n*   **Server Connection:** Viewing status, capabilities, and metadata.\n*   **Testing Tools:** Viewing schemas, testing tools with custom inputs, and reviewing results.\n*   **Inspecting Resources:** Browsing resources, viewing metadata, and testing retrieval/subscriptions.\n*   **Testing Prompts:** Viewing templates and testing prompts with different arguments.\n\nThe page also details how to use **Framework Logging** within the MCP Framework, including different log levels, and outlines a **Development Workflow** involving iterative testing and monitoring logs. Finally, it lists **Common Issues** related to tools, resources, and prompts, along with **Best Practices** for logging, error handling, and performance monitoring.\n\n**No answer found** for the specific terms in your query related to general agent infrastructure concepts like \"tool use,\" \"agent memory,\" \"agent frameworks,\" \"LangChain,\" \"LlamaIndex,\" \"OpenAI Agents SDK,\" \"Anthropic Agents SDK,\" \"Google SDK,\" \"function calling,\" \"structured outputs,\" or \"agent orchestration,\" as the page is exclusively about debugging the MCP Framework.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2501.08603": {
    "content_path": "data/contents/cea1ccff5949a954.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.464745",
    "title": "Monte Carlo Tree Search for Comprehensive Exploration in LLM-Based Automatic Heuristic Design",
    "summary": "The provided webpage discusses the use of **Monte Carlo Tree Search (MCTS)** for **LLM-based automatic heuristic design (AHD)**. This method aims to generate high-quality heuristics for complex optimization tasks (like route planning) by organizing LLM-generated heuristics in a tree structure to explore the solution space more comprehensively than existing population-based evolutionary computation methods.\n\nWhile the user query covers a broad range of topics related to **Reasoning LLMs, chain-of-thought, planning with LLMs, MCTS for language models, and hallucination reduction**, the webpage specifically focuses on:\n\n*   **Planning with LLMs** (in the context of generating heuristics for optimization tasks).\n*   **MCTS (Monte Carlo Tree Search) for language models** (specifically applied to LLM-based heuristic evolution).\n\nIt does not explicitly detail reasoning, chain-of-thought, inference-time compute, self-reflection, test-time scaling, grounding, or factuality reduction techniques.",
    "published_date": "2025-01-15T00:00:00.000Z",
    "score": null
  },
  "https://llm-mcts.github.io/static/pdfs/paper.pdf": {
    "content_path": "data/contents/181f994677267ac9.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.594449",
    "title": "",
    "summary": "The webpage describes **LLM-MCTS**, a new algorithm for large-scale task planning that combines the strengths of using Large Language Models (LLMs) as a **world model** (L-Model) and as a **heuristic policy** (L-Policy) within a **Monte Carlo Tree Search (MCTS)** framework.\n\nHere is a summary addressing the concepts in your query:\n\n*   **Reasoning and Planning with LLMs:** The core idea is to leverage the LLM's commonsense knowledge for complex task planning, specifically in object rearrangement tasks in household environments.\n*   **Monte Carlo Tree Search (MCTS) for Language Models:** LLM-MCTS uses MCTS. The LLM-induced world model provides a **commonsense prior belief** for MCTS to achieve effective reasoning (guiding the search based on likely states). The LLM-induced policy acts as a **heuristic** to guide the search, vastly improving efficiency.\n*   **Inference-time Compute / Test-time Scaling:** The LLM is queried during the MCTS simulation phase to guide action selection ($\\pi_{\\text{rollout}}$ and the heuristic policy $\\hat{\\pi}$ in PUCT), which involves significant inference computation at test time.\n*   **Grounding and Factuality (Related to World Model):** The LLM is used to generate an initial belief state ($b(s_0)$) about object locations based on commonsense, which is then refined through interaction with the environment. This grounds the planning in the LLM's world knowledge.\n*   **Self-reflection/Hallucination Reduction (Implied):** While not explicitly named \"self-reflection,\" the combination of the world model (prior belief) and the search mechanism (MCTS) helps mitigate the limitations of using the LLM purely as a policy (L-Policy), which is prone to generalization errors on complex tasks. The ablation study shows that removing the heuristic policy component (relying only on the world model prior belief) results in zero success, highlighting the need for the guiding heuristic.\n*   **Model vs. Policy Choice:** The paper suggests using the **Minimum Description Length (MDL) principle** to decide when L-Model (model-based planning) is better than L-Policy (pure policy execution). For tasks like multiplication or travel planning, where a compact algorithmic description (L-Model) exists, the model\n\nThe user query asks for a summary related to: 'reasoning\\_and\\_planning: Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, factuality'.\n\nThe provided webpage text discusses using **Large Language Models (LLMs)** in the context of **task planning** and **reasoning**, specifically by integrating them with **Monte Carlo Tree Search (MCTS)**.\n\nHere is a summary of the relevant points from the text:\n\n*   **Planning with LLMs and MCTS:** The paper proposes using LLMs as a **commonsense world model** and a **heuristic policy** within an **MCTS** framework for task planning (specifically object rearrangement tasks in VirtualHome).\n*   **Reasoning and Planning:** MCTS allows the LLM to leverage its world modeling knowledge for **informed reasoning** and explore new action combinations for novel tasks. The LLM guides the search algorithm, reducing the search space and producing \"reasoned results.\"\n*   **LLM as Policy vs. Model:** The text compares an **L-Model** approach (LLM as a world model) against an **L-Policy** approach (LLM directly generating actions/plans). An MDL (Minimum Description Length) analysis suggests the L-Model approach is preferable when the description length of the world is substantially shorter than the policy.\n*   **Limitations (Hallucination/Factuality):** The text acknowledges that LLMs used as policies can suffer from **hallucinations**. Model errors (incorrect predicted object positions) can also mislead the MCTS exploration. The belief update mechanism in LLM-MCTS attempts to mitigate this by incorporating new observations.\n*   **Inference-Time Compute:** The runtime performance section notes that the LLM-MCTS approach is currently **hindered by multiple LLM calls**, indicating high **inference-time compute**.\n\nThe text directly addresses **Reasoning LLMs**, **planning with LLMs**, **MCTS for language models**, and touches upon issues related to **hallucination** and **factuality** (via model error and grounding in observations). It does not explicitly detail \"chain-of-thought,\" \"self-reflection,\" or \"test-time scaling\" as distinct concepts, though the MCTS process involves iterative",
    "published_date": "2023-11-03T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/pdf/2410.21333": {
    "content_path": "data/contents/497e8934803373c4.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.306332",
    "title": "",
    "summary": "The user query asks for a summary related to several topics in reasoning and planning with LLMs, including Chain-of-Thought (CoT), inference-time compute, self-reflection, planning, MCTS, test-time scaling, hallucination reduction, grounding, and factuality.\n\nThe provided webpage text is an arXiv paper titled \"Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse.\"\n\nHere is a summary of the page content relevant to the query:\n\n**Chain-of-Thought (CoT) and Inference-Time Reasoning:**\nThe paper investigates settings where Chain-of-Thought (CoT) prompting, a widely used strategy for inference-time reasoning, systematically *reduces* model performance. While CoT generally improves performance, especially in symbolic reasoning, the authors explore when it harms results by drawing parallels with cognitive psychology tasks where human deliberation (verbal thinking) hurts performance.\n\n**Key Findings on CoT Performance Reduction:**\nThe study found significant performance drops from CoT in three archetypes of tasks where human verbal thinking is detrimental:\n1.  **Implicit Statistical Learning (e.g., Artificial Grammar Learning):** CoT drastically decreased performance across multiple LLMs (e.g., a 36.3% absolute accuracy drop for o1-preview compared to GPT-4o zero-shot).\n2.  **Tasks where Language is Ill-suited to Stimuli (e.g., Face Recognition):** All six tested LMMs showed a drop in performance with CoT.\n3.  **Classifying Data with Rules that Contain Exceptions:** CoT significantly increased the number of passes required for models to learn correct labels (e.g., GPT-4o needed 331% more rounds).\n\n**Tasks Where CoT Did Not Consistently Reduce Performance (Mismatch with Humans):**\nFor tasks where human performance is hurt by thinking due to factors like lack of formal logic training, reliance on motor simulation, or working memory limitations, the effects of CoT on models were mixed or positive. This is attributed to differences in capabilities (e.g., LLMs having access to vast logical priors or long context windows).\n\n**Relevance to Query Topics:**\n*   **Reasoning LLMs, Chain-of-Thought, Inference-time compute:** This is the central focus of the paper, detailing when CoT reasoning fails.\n\nThe provided webpage text is a bibliography and a detailed description of several experiments comparing zero-shot prompting versus Chain-of-Thought (CoT) prompting in Large Language Models (LLMs) across various cognitive science tasks (Artificial Grammar Learning, Facial Recognition, Data with Exceptions, Logical Inconsistency, Spatial Intuition, and Working Memory Preference).\n\nThe user query asks for a summary related to: **'reasoning\\_and\\_planning: Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, factuality'**.\n\nThe text directly discusses **Chain-of-Thought (CoT)** prompting extensively, which is a key component of the user's query related to reasoning. It also mentions **Tree-of-Thought (ToT)**, which is a planning/reasoning technique, and touches upon **reasoning** in the context of LLMs failing or succeeding with CoT across different tasks.\n\nHowever, the text **does not explicitly discuss or define** the following specific terms from the query:\n*   Inference-time compute (beyond the context of prompting methods)\n*   Self-reflection\n*   Planning with LLMs (though ToT is related to planning)\n*   MCTS (Monte Carlo Tree Search) for language models (ToT is mentioned, but not MCTS specifically)\n*   Test-time scaling\n*   Hallucination reduction and detection\n*   Grounding\n*   Factuality\n\nSince the document is primarily an experimental setup investigating the *effects* of CoT on reasoning-like tasks, and does not cover the broader theoretical or technical aspects of the other listed terms, a comprehensive summary covering all points is not possible.\n\n**Summary based on available information:**\n\nThe webpage details several experiments comparing standard (zero-shot) prompting against Chain-of-Thought (CoT) prompting across six tasks inspired by cognitive psychology studies (Artificial Grammar Learning, Facial Recognition, Data with Exceptions, Logical Inconsistency, Spatial Intuition, and Working Memory Preference). The central theme is how CoT, a method intended to elicit reasoning, sometimes **reduces performance** compared to zero-shot prompting, particularly in tasks requiring visual matching or learning exceptions to a general rule. The text also briefly explores **Tree-of-Thought (",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/abs/2404.05966": {
    "content_path": "data/contents/725871f70c965562.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.693367",
    "title": "Computer Science > Computation and Language",
    "summary": "The provided webpage describes **THOUGHTSCULPT**, a general reasoning and search method for tasks where outputs can be broken down into components.\n\nKey aspects relevant to your query include:\n\n*   **Reasoning and Search:** THOUGHTSCULPT explores a search tree of potential solutions using **Monte Carlo Tree Search (MCTS)**.\n*   **Intermediate Revision:** A critical feature is the inclusion of **revision actions** in the action space, allowing the model to revise previous output components rather than just continuing to build the solution.\n\nWhile the paper focuses on a specific reasoning method (THOUGHTSCULPT using MCTS and revision), it directly addresses concepts related to **Reasoning LLMs**, **MCTS for language models**, and **planning with LLMs** (as MCTS is a planning technique). It does not explicitly mention chain-of-thought, inference-time compute, self-reflection, test-time scaling, hallucination reduction/detection, grounding, or factuality.",
    "published_date": "2024-04-09T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2506.18183v1": {
    "content_path": "data/contents/9de41582ca521d3c.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.744119",
    "title": "Reasoning about Uncertainty: \n Do Reasoning Models Know When They Don’t Know?",
    "summary": "The user query asks for a summary related to several topics concerning Reasoning LLMs, including chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction/detection, grounding, and factuality.\n\nThe provided webpage focuses heavily on **Reasoning LLMs**, **chain-of-thought (CoT)**, **inference-time compute** (referred to as \"deeper reasoning\" or \"inference-time scaling\"), **self-reflection** (introduced as \"introspective uncertainty quantification\"), **hallucination reduction and detection** (addressed via uncertainty quantification and calibration), and **factuality** (assessed using benchmarks like SimpleQA).\n\nHere is a summary of the key findings from the page relevant to the query:\n\n**Reasoning LLMs and Chain-of-Thought (CoT):**\n*   Reasoning language models (like Claude 3.7 Sonnet, DeepSeek R1, Gemini 2 Flash Thinking, and o3-Mini) achieve SOTA performance by using multi-step reasoning, often induced via reinforcement learning or CoT prompting.\n*   The study investigates the calibration (trustworthiness) of these models when they self-verbalize their confidence.\n\n**Inference-Time Compute / Deeper Reasoning (Test-Time Scaling):**\n*   Deeper reasoning (scaling reasoning effort/tokens) generally leads to **higher accuracy**.\n*   However, as accuracy saturates, deeper reasoning causes models to become **even more overconfident** (calibration errors increase, especially ECE), particularly on wrongly-answered questions.\n*   Increased reasoning depth can also lead to a **decrease in the completion rate** as models struggle with format instructions.\n\n**Self-Reflection (Introspective Uncertainty Quantification - IUQ):**\n*   The paper introduces **Introspective Uncertainty Quantification (IUQ)**, where a model reasons about its own chain-of-thought trace to update its confidence, mimicking human self-checking.\n*   More critical introspection (IUQ-Medium and IUQ-High, which explicitly ask the model to find flaws) generally **improves calibration** (reduces miscalibration/overconfidence) for models like o3-Mini and DeepSeek R1, especially on challenging datasets.\n*   However, this introspection **degrades the calibration of Claude 3.7 Sonnet**.\n\n**Hall\n\nThe user query asks for a summary related to several topics in **reasoning and planning with LLMs**, including: Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction/detection, grounding, and factuality.\n\nThe provided webpage focuses on the **calibration and uncertainty quantification (UQ)** of **reasoning models**.\n\nHere is a summary of the relevant points from the text concerning the user's query topics:\n\n*   **Reasoning LLMs & Calibration:** The paper examines the calibration of reasoning models, finding that they are generally **overconfident** and become even more so with deeper reasoning, especially when accuracy doesn't improve.\n*   **Chain-of-Thought (CoT) Prompting:** The study explored CoT prompting alongside basic, multi-step, and top-K prompts. It found that **CoT prompting and multi-step prompting do not significantly outperform the basic prompt strategy** for reasoning models. This is because reasoning models are already trained to generate reasoning traces by default, unlike non-reasoning models where CoT is effective for eliciting reasoning.\n*   **Self-Reflection (Introspection):** Critical introspection (a form of self-reflection) **enables models to identify flaws and hallucinations** in their reasoning traces, leading to improved calibration by reducing overconfidence (though Claude became more overconfident).\n*   **Hallucination Reduction/Detection:** Introspection is shown to help models identify flaws and **hallucinations** in their reasoning traces, which aids in confidence reassessment.\n*   **Planning with LLMs / MCTS:** While the paper discusses reasoning models and their traces, it does not explicitly detail planning with LLMs or the use of MCTS (Monte Carlo Tree Search) for language models.\n*   **Factuality/Grounding:** The paper focuses on *confidence* calibration rather than explicitly measuring grounding or factuality, although improving calibration is related to reducing incorrect outputs (hallucinations).\n*   **Inference-time Compute / Test-time Scaling:** These topics are not directly addressed, although introspection and multi-step prompting relate to increased inference-time computation.\n\n**In summary:** The page primarily discusses the calibration of reasoning models, finding that specialized prompt strategies like Chain-of-Thought do not significantly improve performance over basic prompting for these",
    "published_date": "2025-12-17T00:00:00.000Z",
    "score": null
  },
  "https://paperswithcode.com/paper/chain-of-thought-prompting-elicits-reasoning": {
    "content_path": "data/contents/1c547c94d84f7360.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.878966",
    "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
    "summary": "The webpage discusses **Chain-of-Thought Prompting**, a method that enhances the **reasoning capabilities** of **Large Language Models (LLMs)** by having them generate a series of intermediate reasoning steps. This technique shows significant improvement on **arithmetic, commonsense, and symbolic reasoning tasks**. The text specifically mentions that providing a few chain-of-thought demonstrations as exemplars in the prompt elicits these reasoning abilities in sufficiently large LLMs. It also notes that this method, when applied to a 540B-parameter model, achieved state-of-the-art accuracy on the GSM8K benchmark for math word problems.\n\nWhile the user query covers a broad range of topics related to reasoning and planning in LLMs (including MCTS, self-reflection, hallucination reduction, etc.), this specific page focuses primarily on **Chain-of-Thought Prompting** as a method for eliciting **reasoning** in LLMs.",
    "published_date": "2022-01-28T00:00:00.000Z",
    "score": null
  },
  "https://www.k2view.com/blog/chain-of-thought-reasoning/": {
    "content_path": "data/contents/e38271f2441c6012.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.086007",
    "title": "Chain-of-thought reasoning supercharges enterprise LLMs",
    "summary": "Chain-of-thought (CoT) reasoning is a technique used to break down complex queries into simpler, manageable steps for Large Language Models (LLMs), leading to more accurate and reliable responses.\n\n**Key aspects of CoT reasoning mentioned in the text:**\n\n*   **Enhancement:** CoT improves LLM prompting by guiding the model through intermediate reasoning steps, mimicking human problem-solving, and helping to reduce hallucinations.\n*   **Implementation Methods:**\n    *   **Explicit Instruction:** Clearly outlining the required problem-solving steps in the prompt (also called prompt chaining).\n    *   **Implicit Instruction:** Suggesting a step-by-step approach using phrases like \"Let's think it out step by step.\"\n    *   **Demonstrative Example:** Providing one-shot or few-shot examples of how to solve similar problems.\n*   **Benefits:** Enhanced accuracy, improved explainability and trust, increased versatility, superior handling of complex problems, and facilitated debugging.\n*   **Best Practices:** Designing prompts carefully, defining clear instructions, validating intermediate steps, and conducting rigorous testing.\n*   **Application:** K2view's GenAI Data Fusion uses CoT prompting combined with Retrieval-Augmented Generation (RAG) to ground GenAI applications with enterprise data, enabling chatbots to handle complex, data-driven customer service inquiries by dynamically gathering and summarizing relevant customer information.\n\nThe user query asks about several advanced LLM topics, including **reasoning and planning, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction and detection, grounding, and factuality.**\n\nThe webpage focuses heavily on **Chain-of-Thought reasoning**, its implementation, benefits, and its use in conjunction with **RAG** for **grounding** and **hallucination reduction**. It does not explicitly detail **inference-time compute, self-reflection, planning with LLMs (beyond the step-by-step nature of CoT), MCTS, or test-time scaling.**\n\n**Summary based on the query:**\n\nThe page primarily discusses **Chain-of-Thought (CoT) reasoning** as a technique to enhance LLM prompting by breaking down complex tasks into sequential steps. This process improves **accuracy and reliability** and aids in **hallucination reduction**. CoT is shown to work in conjunction with **Retrieval",
    "published_date": null,
    "score": null
  },
  "https://ar5iv.labs.arxiv.org/html/2309.15402": {
    "content_path": "data/contents/48e6b627757280b2.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.143760",
    "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future",
    "summary": "The user query asks for a summary covering several topics related to reasoning and planning with Large Language Models (LLMs), including: **Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**\n\nThis webpage is a survey titled \"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future,\" which focuses heavily on **Chain-of-Thought (CoT)** reasoning (referred to broadly as X-of-Thought or XoT).\n\nHere is a summary of how the page addresses the concepts in the query:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** The survey is fundamentally about CoT reasoning, which enhances LLMs' capabilities for complex reasoning tasks (like mathematical and commonsense reasoning) by generating step-by-step rationales. It covers the introduction of CoT, its mathematical formulation, and taxonomies based on construction (Manual, Automatic, Semi-automatic), structure (Chain, Tree, Graph), and enhancement methods.\n*   **Planning with LLMs:** Planning is explicitly mentioned as a **Frontier Application** of XoT (§5.2). Examples provided include ToT (Tree-of-Thought) and ReAct.\n*   **Self-Reflection & Hallucination Reduction/Detection & Factuality:** These concepts are strongly covered under **XoT Enhancement Methods**, particularly in the **Verify and Refine** section (§4.3.1). This section discusses strategies like Self-Refine, Reflexion (which uses reinforcement learning for reflection), and methods that use verification (like RCoT or FOBAR) to reduce cascading errors and factual mistakes (hallucinations). External knowledge integration (§4.3.3) also aims to reduce factual errors.\n*   **MCTS (Monte Carlo Tree Search) for Language Models:** This is addressed under **XoT Structural Variants** (§4.2) in the **Tree Structure** subsection, where Tree-of-Thought (ToT) methods are discussed, which often incorporate tree search algorithms to explore reasoning paths.\n*   **Inference-time compute & Test-time scaling:** While the survey discusses methods that increase complexity (like Tree and Graph structures) and methods that improve efficiency (like SoT\n\nThe webpage provides information related to several aspects of your query concerning **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, hallucination reduction and detection, and grounding/factuality**.\n\nHere is a summary of the relevant sections:\n\n**Reasoning LLMs & Chain-of-Thought (CoT):**\n*   The document surveys **Chain of Thought (CoT) Reasoning** and its generalized form, **X-of-Thought (XoT)**.\n*   **CoT Distillation (§5.3)** discusses methods where reasoning chains generated by larger LLMs are distilled into smaller models to enhance their reasoning capabilities.\n*   **Faithfulness (§6.2)** addresses the issue of **hallucination** (factual mistakes and contextual inconsistencies) in CoT reasoning, mentioning techniques that use external knowledge for evaluation, reflection mechanisms for correction, and question decomposition.\n*   **CoT Theory (§6.3)** explores the empirical and theoretical underpinnings of why CoT works, noting its reliance on semantic knowledge and its potential to reduce the complexity of in-context learning.\n\n**Inference-Time Compute & Efficiency:**\n*   Section **4.3.5 Efficiency** discusses the expensive overheads of LLM reasoning. Methods mentioned to reduce inference costs include dynamically adjusting the number of samples (self-consistency), parallel question decomposition, and selectively skipping intermediate layers during reasoning.\n\n**Self-Reflection:**\n*   In **§5.2 Planning**, methods like **Self-Refine** (where the model evaluates and feeds back on its own output) and **Reflexion** (which reflects on and rectifies previous errors) are discussed as techniques to improve error correction.\n\n**Planning with LLMs & MCTS for Language Models:**\n*   Section **§5.2 Planning** details advanced planning techniques beyond basic CoT:\n    *   **Tree-of-Thought (ToT)** allows LLMs to explore multiple reasoning paths in a tree structure and self-evaluate.\n    *   **Reasoning via Planning (RAP)** uses the **Monte Carlo Tree Search (MCTS)** algorithm, employing LLMs as both the world model and the reasoning agent.\n    *   **Graph of Thought (GoT)** uses graph nodes for thoughts and external Graph Neural Networks for organization.\n    *",
    "published_date": "2022-01-01T00:00:00.000Z",
    "score": null
  },
  "https://huggingface.co/papers/2201.11903": {
    "content_path": "../data/contents/99766ecb29619a7b.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.867716",
    "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
    "summary": "The webpage discusses **Chain-of-Thought Prompting**, a method that enhances the **reasoning capabilities** of large language models (LLMs) by having them generate a series of intermediate reasoning steps. This technique shows significant improvement on **arithmetic, commonsense, and symbolic reasoning tasks**. The text specifically mentions that providing a few chain-of-thought demonstrations as exemplars in the prompt elicits these reasoning abilities in sufficiently large LLMs. It also notes that this method, when applied to a 540B-parameter model, achieved state-of-the-art accuracy on the GSM8K benchmark for math word problems.\n\nWhile the user query covers a broad range of topics related to reasoning and planning in LLMs (including MCTS, self-reflection, hallucination reduction, etc.), this specific page focuses primarily on **Chain-of-Thought Prompting** as a method for eliciting **reasoning** in LLMs.",
    "published_date": "2024-04-19T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2412.12119": {
    "content_path": "data/contents/a9ef0bae0eeb5676.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.550785",
    "title": "Computer Science > Artificial Intelligence",
    "summary": "The webpage describes a paper titled \"Mastering Board Games by External and Internal Planning with Language Models.\" This work focuses on **advancing the planning and reasoning capabilities of Large Language Models (LLMs)** to enable reliable performance in complex domains, specifically demonstrated through board games (Chess, Fischer Random/Chess960, Connect Four, and Hex).\n\nThe paper introduces and compares two search-based planning approaches:\n1.  **External Search:** The LLM guides **Monte Carlo Tree Search (MCTS)** rollouts and evaluations without relying on an external game engine.\n2.  **Internal Search:** The LLM is trained to generate a linearized tree of search and the final choice **in-context**.\n\nBoth methods build upon an LLM pre-trained on domain knowledge to capture transition and value functions with minimal **hallucinations**. The results show substantial improvements in strength over the base model, achieving **Grandmaster-level performance in chess** while operating within a human-like search budget. This approach suggests general future applications beyond board games.\n\nThe summary directly addresses several concepts in your query:\n*   **Reasoning LLMs** and **planning with LLMs** are central themes.\n*   **MCTS (Monte Carlo Tree Search) for language models** is explicitly mentioned as part of the external search approach.\n*   **Hallucination reduction** is noted as a goal achieved by minimizing hallucinations.",
    "published_date": "2024-12-02T00:00:00.000Z",
    "score": null
  },
  "https://openreview.net/forum?id=9SpWvX9ykp&noteId=kH2xomkeeu": {
    "content_path": "data/contents/ef020498cb6689d0.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.583965",
    "title": "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search | OpenReview",
    "summary": "The webpage describes a paper titled \"Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search.\"\n\nThe user query is a broad list of topics related to advanced LLM capabilities, including: 'reasoning\\_and\\_planning: Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, factuality'.\n\nThe webpage directly addresses several of these concepts:\n*   **Planning with LLMs / MCTS (Monte Carlo Tree Search) for language models:** The core of the work involves using Monte Carlo Tree Search (MCTS) to guide the generation of code world models by an LLM, and then using these code models for planning.\n*   **Reasoning LLMs / Reasoning and Planning:** The goal is to use code generated by an LLM as a world model for model-based Reinforcement Learning (RL) planning, aiming for precision and reliability over direct LLM calls for planning.\n\nHowever, the page does not explicitly discuss or detail: chain-of-thought, inference-time compute, self-reflection (beyond the \"Fix\" step in GIF-MCTS), test-time scaling, hallucination reduction and detection, grounding, or factuality in a general sense.\n\n**Summary relevant to the query:**\n\nThis paper proposes using Large Language Models (LLMs) to generate \"Code World Models\" (Python code representing RL environments) for model-based Reinforcement Learning (RL) planning. To improve the code generation necessary for this task, they introduce **Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS)**, a strategy that leverages **MCTS** to guide the LLM. The resulting code world models are shown to enable **planning** that is more precise, reliable, and efficient than direct LLM planning.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2408.10490": {
    "content_path": "../data/contents/0d4ffb36138e02f6.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:34.093470",
    "title": "Computer Science > Computation and Language",
    "summary": "The provided webpage discusses an analysis of **plan-based retrieval for grounded text generation**. The core idea is to leverage the **planning capabilities of instruction-tuned LLMs** to guide retrieval mechanisms, which in turn helps to **reduce the frequency of hallucinations** in generated text. The paper evaluates how improving the coverage of relevant facts through plan-guided retrieval leads to more informative responses with higher attribution to source documents.\n\nWhile the query covers a broad range of topics related to reasoning and planning in LLMs (including chain-of-thought, MCTS, self-reflection, etc.), this specific paper focuses on using **planning to guide retrieval** as a method for **grounding** and **hallucination reduction/detection**.\n\n**Summary relevant to the query:** The paper addresses **planning with LLMs** and **grounding/hallucination reduction** by using LLM planning capabilities to guide information retrieval for text generation.\n\n**No answer found** for specific details on: Reasoning LLMs (general), chain-of-thought, inference-time compute, self-reflection, MCTS for language models, test-time scaling, or factuality (beyond grounding).",
    "published_date": "2024-08-20T00:00:00.000Z",
    "score": null
  },
  "https://www.researchgate.net/publication/358232899_Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models": {
    "content_path": "data/contents/e0d6861785f86c4e.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.689078",
    "title": "(PDF) Chain of Thought Prompting Elicits Reasoning in Large Language Models",
    "summary": "The webpage describes a paper titled **\"Chain of Thought Prompting Elicits Reasoning in Large Language Models.\"**\n\nThe core idea is that while scaling up language models improves performance on many tasks, they still struggle with complex **reasoning tasks** (like math word problems, symbolic manipulation, and commonsense reasoning).\n\nThe paper introduces **Chain of Thought (CoT) prompting** as a method to enable these large models to perform better on these reasoning tasks. CoT involves prompting the model to generate a series of short sentences that mimic a human's step-by-step reasoning process before giving the final answer.\n\nKey findings related to your query:\n\n*   **Reasoning LLMs & Chain-of-Thought:** CoT prompting allows sufficiently large language models (around 100B parameters or more) to better perform reasoning tasks where standard prompting results in flat scaling curves.\n*   **Inference-time Compute:** The paper suggests CoT allows models to allocate more computation (intermediate tokens) to problems requiring more reasoning steps.\n*   **Hallucination Reduction/Factuality (Indirectly):** By examining incorrect answers, the study found that many errors in CoT were minor mistakes or logical errors, suggesting the generated chain provides a window into the reasoning path, which can be debugged (though full characterization is an open question). Using an external calculator to verify equations within the chain further improved performance.\n*   **Planning with LLMs (Related Concept):** While the paper focuses on eliciting reasoning steps rather than explicit planning algorithms like MCTS, the CoT approach decomposes a multi-step problem into intermediate steps, which is foundational to planning.\n\nThe paper demonstrates the effectiveness of CoT across **arithmetic reasoning, symbolic reasoning, and commonsense reasoning** tasks, showing significant performance improvements over standard prompting, especially for challenging problems where model scale alone was insufficient.\n\nThe provided webpage text is primarily a list of references and appendices from a research paper titled \"(PDF) Chain of Thought Prompting Elicits Reasoning in Large Language Models.\"\n\nWhile the title and the content of the appendices discuss **Chain of Thought Prompting** and its application to **reasoning** (especially in math word problems), the text does not directly contain detailed information or definitions for all the specific concepts listed in your query, such as:\n\n*   inference-time compute\n*   self-reflection\n*   planning with LLMs\n*   MCTS (Monte Carlo Tree Search) for language models\n*   test-time scaling\n*   hallucination reduction and detection\n*   grounding\n*   factuality\n\nThe text focuses heavily on demonstrating how Chain of Thought (CoT) prompting elicits reasoning, analyzing errors in CoT, and comparing CoT to finetuning, particularly on arithmetic reasoning tasks (like GSM8K).\n\n**Summary based on the query and the text:**\n\nThe document is about **Chain of Thought Prompting** and how it **elicits reasoning** in Large Language Models, particularly for solving math word problems. It analyzes the correctness and types of errors in the reasoning chains generated by the model. While it strongly relates to the \"Reasoning LLMs\" and \"chain-of-thought\" aspects of your query, it does not cover the other advanced topics like MCTS, self-reflection, or hallucination reduction mentioned.\n\n**Conclusion:**\n\nNo answer found for the full scope of your query, as the text is focused specifically on Chain of Thought prompting and reasoning analysis, not the broader set of advanced techniques you listed.",
    "published_date": "2022-01-27T00:00:00.000Z",
    "score": null
  },
  "https://jamwithai.substack.com/p/chunking-strategies-and-hybrid-rag": {
    "content_path": "../data/contents/6798d3d3eb02d243.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.135644",
    "title": "Chunking strategies and Hybrid RAG system",
    "summary": "This page details the implementation of advanced search techniques for a Retrieval-Augmented Generation (RAG) system, focusing on **chunking strategies** and **hybrid search**.\n\nHere is a summary of the topics mentioned in your query that are covered in the text:\n\n*   **Chunking strategies:** The page describes a **hybrid section-based strategy** that respects document structure (combining small sections, using perfect sections, splitting large sections, and falling back to paragraph-based chunking).\n*   **Embeddings (new efficient models):** The system implements **production-grade embeddings** using **Jina AI v3** (1024-dimensional vectors) for semantic understanding.\n*   **Rerankers:** While not explicitly named as a separate reranker component, the **Reciprocal Rank Fusion (RRF)** used in hybrid search effectively combines and re-ranks results from BM25 and vector search.\n*   **RAG architectures:** The page focuses on enhancing the **Search** component of the RAG chain (infra → data → **search** → generation) by implementing hybrid search on top of a proven BM25 foundation.\n*   **Hybrid search:** A core focus is the implementation of **Hybrid RRF search**, which fuses the precision of BM25 keyword search with the semantic recall of vector similarity search.\n*   **Chunking strategies:** This is covered extensively, as detailed above.\n\n**Topics from your query that are NOT explicitly detailed or covered:**\n\n*   **RAG alternatives:** The page focuses on *improving* RAG search, not detailing alternatives to the overall RAG architecture.\n*   **Vector databases:** While the system uses OpenSearch to support vector search, the page does not detail the general concept or comparison of vector databases themselves.",
    "published_date": "2025-08-28T00:00:00.000Z",
    "score": null
  },
  "https://dev.to/ashokan/smart-chunking-embeddings-for-rag-4ok": {
    "content_path": "../data/contents/b0cf0344eae7f4e7.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:34.351047",
    "title": "Smart Chunking & Embeddings for RAG",
    "summary": "This page provides a deep dive into **Chunking Strategies** and **Embedding Techniques** for building Retrieval-Augmented Generation (RAG) systems, using **Qdrant** as the vector database.\n\n**Key topics covered:**\n\n*   **Chunking Strategies:** Discusses various methods including Fixed-size + overlap, Recursive, Document-structure–aware, Sentence-window, Semantic chunking, and Hierarchical chunking, emphasizing that good chunking is crucial for RAG success.\n*   **Embedding Techniques:** Reviews modern embedding models (like BGE-M3, Nomic Embed, Voyage) and discusses task-aware prompting (e.g., Nomic's prefixes) and hybrid-ready models.\n*   **Vector Databases (Qdrant):** Details Qdrant's capabilities, including support for dense vectors, **sparse vectors**, and **hybrid search** (combining dense and sparse retrieval). It provides quickstarts using FastEmbed integration and manual setup.\n*   **RAG Architectures/Components:** The visual overview and subsequent sections cover the flow: Chunking $\\rightarrow$ Embedding $\\rightarrow$ Indexing (Qdrant) $\\rightarrow$ **Hybrid Retrieval** $\\rightarrow$ **Reranking** (optional, e.g., with ColBERT) $\\rightarrow$ LLM.\n*   **RAG Alternatives/Enhancements:** Hybrid search (dense + sparse) is presented as a robust pattern, and reranking is highlighted as an impactful step after initial retrieval.\n\nThe page directly addresses **Vector databases**, **embeddings (new efficient models)**, **rerankers**, **RAG architectures**, **hybrid search**, and **chunking strategies**. It does not explicitly detail \"RAG alternatives\" beyond the hybrid approach, nor does it focus on new \"efficient models\" beyond listing current strong performers.",
    "published_date": "2025-11-07T14:42:43.000Z",
    "score": null
  },
  "https://rohan-paul.com/p/vector-databases-for-rag-literature": {
    "content_path": "../data/contents/e51cbe7c7ad901a8.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.107798",
    "title": "Rohan's Bytes",
    "summary": "The webpage provides a literature review focused on **Vector Databases for Retrieval-Augmented Generation (RAG)**, covering several aspects mentioned in your query:\n\n*   **Vector Databases:** The core of the article reviews five leading Vector Database Management Systems (VDBMS): Pinecone, Weaviate, Milvus, Qdrant, and Chroma, evaluating them based on retrieval speed, storage efficiency, scalability, and query accuracy.\n*   **RAG Architectures:** The introduction explains that RAG pipelines rely on vector databases to store and search document embeddings, grounding LLM answers in relevant data.\n*   **Chunking Strategies:** The introduction mentions that documents are \"digitized and **chunked** into passages, each encoded as a high-dimensional vector.\"\n*   **Retrieval and Embeddings:** The entire review centers on the retrieval process using vector similarity search based on embeddings.\n\n**Regarding the other specific terms in your query:**\n\n*   **Embeddings (new efficient models):** The text discusses the *output* of embedding models (the vectors) and how they are stored and searched, but it does not detail or compare specific \"new efficient models\" used to *create* the embeddings.\n*   **Rerankers:** This term is **not mentioned** in the text.\n*   **RAG Alternatives:** The text focuses on vector databases *within* the RAG architecture; it does not discuss alternatives to RAG itself.\n*   **Hybrid Search:** This term is **not explicitly detailed** in the comparison sections, although Qdrant's integration with ecosystem tools and Weaviate's hybrid architecture (vector search with symbolic filters) hint at related capabilities.\n\n**Summary:**\n\nThe page thoroughly reviews **Vector Databases** used in **RAG architectures**, detailing the importance of **chunking** and the retrieval process based on **embeddings**. It compares leading databases on performance metrics. However, it does not discuss **rerankers**, **RAG alternatives**, or specific details on **new efficient embedding models**.",
    "published_date": "2025-06-16T00:00:00.000Z",
    "score": null
  },
  "https://www.baseten.co/resources/guide/high-performance-embedding-model-inference/": {
    "content_path": "data/contents/acdeca98c2f43bb2.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.302101",
    "title": "High-performance embedding model inference",
    "summary": "The webpage provides a guide on **high-performance embedding model inference**, covering how to make embeddings fast, reliable, and cost-efficient at scale.\n\nIt explains:\n*   **What embeddings are:** Vector representations of text (or other input) used for semantic similarity, powering applications like semantic search and **RAG (Retrieval-Augmented Generation)**.\n*   **Vector Databases:** Specialized systems needed to store and query embeddings efficiently (mentioning Chroma, Pinecone, Qdrant, Weaviate, and vector support in PostgreSQL, MongoDB, etc.).\n*   **State-of-the-art models:** Discusses the proliferation of open-source models (like Qwen, Gemma, BGE) and the need for optimized serving infrastructure for production use.\n*   **Challenges:** Embedding inference workloads require optimizing for both **high-throughput backfills** (offline indexing) and **low-latency lookups** (online user queries).\n*   **Baseten's Solution:** Details the **Baseten Inference Stack**, which includes the high-performance runtime **BEI** (Baseten Embedding Inference), optimized infrastructure (Multi-cloud Capacity Management, autoscaling), the **Baseten Performance Client** (to overcome client-side bottlenecks), and **Baseten Chains** (for multi-step pipelines like RAG).\n\nWhile the page extensively covers **Vector databases**, **embeddings (new efficient models)**, and **RAG architectures**, it **does not** specifically detail:\n*   RAG alternatives\n*   Hybrid search\n*   Chunking strategies\n*   Context window management\n\nNo answer found for the specific topics of 'RAG alternatives, hybrid search, chunking strategies, context window management'.",
    "published_date": "2025-10-16T00:00:00.000Z",
    "score": null
  },
  "https://openreview.net/pdf?id=bwGaZOVo0c": {
    "content_path": "data/contents/fdc0d8123e45767e.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.731625",
    "title": "",
    "summary": "The webpage describes a proposed **Entropy-Based Dynamic Hybrid Retrieval** method designed to improve Retrieval-Augmented Generation (RAG) pipelines.\n\nHere is a summary of the key concepts mentioned in the query that are addressed by the paper:\n\n*   **RAG Architectures:** The paper focuses on enhancing the retrieval component within RAG pipelines, contrasting its dynamic hybrid approach against pure dense retrieval (FAISS) and fixed-weight hybrid baselines.\n*   **Hybrid Search:** The core of the work is a hybrid retrieval system that combines sparse retrieval (BM25) and dense retrieval (FAISS).\n*   **Retrieval and Embeddings:** The system uses **MiniLM L6-v2 embeddings** for dense retrieval and **BM25** for sparse retrieval.\n*   **Rerankers:** The proposed method involves a multi-round, entropy-based **reweighting** process that iteratively optimizes the combination of sparse and dense scores, effectively acting as an adaptive reranking mechanism based on retrieval confidence.\n*   **Context Window Management / Chunking Strategies:** While the paper does not explicitly detail chunking strategies, it focuses on managing the *relevance* of the retrieved top-k documents (k=5 or 7 in experiments) before they are passed to the LLM, which is a crucial aspect of context management.\n\n**Concepts not explicitly detailed or central to this paper:**\n\n*   **Vector Databases:** The paper uses FAISS for dense retrieval but does not discuss the architecture or features of dedicated vector databases.\n*   **Embeddings (new efficient models):** It uses the established MiniLM L6-v2 model; it does not introduce a new efficient embedding model.\n*   **RAG Alternatives:** The paper focuses on improving hybrid RAG, not exploring alternatives like pure generation or other architectural paradigms outside the retrieval step.",
    "published_date": "2025-07-01T00:00:00.000Z",
    "score": null
  },
  "https://qdrant.tech/qdrant-vector-database": {
    "content_path": "../data/contents/b0a317e4ac051b84.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:39.739815",
    "title": "Qdrant Vector Database, High-Performance Vector Search Engine - Qdrant",
    "summary": "The webpage is about **Qdrant**, a high-performance vector database.\n\nIt mentions the following relevant topics from your query:\n\n*   **Vector databases:** Qdrant is explicitly described as a vector database.\n*   **RAG (Retrieval-Augmented Generation):** RAG is listed as a key use case.\n*   **Embeddings:** Qdrant supports integrations for \"all leading embeddings and frameworks.\"\n\nThe page **does not explicitly mention or detail**:\n\n*   New efficient models for embeddings.\n*   Rerankers.\n*   RAG alternatives.\n*   Hybrid search (though it mentions sparse vector support, which can be related to hybrid retrieval).\n*   Chunking strategies.\n\n**Summary based on the query:**\n\nQdrant is a high-performance vector database designed for efficient and scalable vector search. It supports **RAG** as a primary use case and integrates with leading **embeddings**. The database offers advanced features like quantization for memory reduction, sparse vector support, and advanced filtering.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://www.linkedin.com/pulse/alternatives-rag-systems-kag-cag-soroush-mozooni-6b5ge": {
    "content_path": "../data/contents/8dafe6953e3a7831.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.604662",
    "title": "Alternatives to RAG Systems: KAG and CAG",
    "summary": "The webpage discusses alternatives to Retrieval-Augmented Generation (RAG) systems, specifically **Cache-Augmented Generation (CAG)** and **Knowledge-Augmented Generation (KAG)**.\n\n*   **CAG** simplifies the process by preloading all necessary knowledge into the LLM's context using a precomputed \"cache,\" eliminating the need for a separate retrieval pipeline. It is best for tasks with small or manageable knowledge bases (e.g., FAQs, simple chatbots) due to its speed and simplicity.\n*   **KAG** integrates **knowledge graphs (KGs)** with LLMs to enable logical reasoning and provide more professional, accurate answers grounded in structured, verified information. It is best for complex, knowledge-intensive tasks requiring domain expertise (e.g., legal analysis, medical diagnostics).\n\nThe article mentions that these alternatives aim to overcome challenges associated with RAG, such as retrieval delays and errors.\n\n**Regarding your specific query components:**\n\n*   **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, hybrid search, chunking strategies:** These topics are related to the standard RAG architecture, which the article discusses in the context of its limitations and alternatives. The article itself focuses on CAG and KAG as alternatives, not detailing these specific RAG components.\n*   **RAG alternatives:** This is the main focus of the article (CAG and KAG).\n\n**Summary:** The page details two alternatives to RAG: **CAG** (for speed and simplicity via preloaded context/cache) and **KAG** (for advanced reasoning via knowledge graphs). It does not provide specific details on vector databases, embeddings, rerankers, hybrid search, or chunking strategies, though these are implied components of the RAG system being contrasted.",
    "published_date": "2025-01-07T00:00:00.000Z",
    "score": null
  },
  "https://www.fingoweb.com/blog/what-are-the-best-rag-alternatives/": {
    "content_path": "../data/contents/25b527247339b0aa.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.687369",
    "title": "What are the best RAG Alternatives?",
    "summary": "The webpage discusses several promising alternatives to Retrieval-Augmented Generation (RAG) for improving the accuracy and relevance of AI-generated content. These alternatives are:\n\n1.  **Prompt engineering with context windows:** Injecting domain knowledge directly into prompts using structured formats, avoiding external retrieval systems. This is beneficial for lower latency and system complexity in stable knowledge domains.\n2.  **Toolformer and API-calling models:** Allowing language models to autonomously decide when to invoke external tools (like APIs or calculators) based on uncertainty or knowledge gaps, enabling dynamic and context-aware execution.\n3.  **LangChain agents and function-calling architectures:** Using LLMs as orchestrators to chain reasoning, information retrieval, and external tool execution in iterative loops, suitable for multi-step reasoning and complex workflows.\n4.  **Fine-tuning with domain-specific data:** Directly training the model on specialized data when the domain is narrow, responses need to be highly consistent, or latency is critical. Open-source tools like LoRA, PEFT, and Axolotl facilitate efficient fine-tuning.\n\nThe article also notes that the most lightweight alternative involves combining simple retrieval mechanisms with prompt engineering. For production-level AI, LangChain agents and fine-tuning are often recommended.",
    "published_date": "2025-07-28T00:00:00.000Z",
    "score": null
  },
  "https://dev.to/naresh_007/beyond-vanilla-rag-the-7-modern-rag-architectures-every-ai-engineer-must-know-4l0c": {
    "content_path": "../data/contents/db843bad84cc6b80.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.700844",
    "title": "Beyond Vanilla RAG: The 7 Modern RAG Architectures Every AI Engineer Must Know",
    "summary": "The webpage, titled \"Beyond Vanilla RAG: The 7 Modern RAG Architectures Every AI Engineer Must Know,\" is likely to discuss various Retrieval-Augmented Generation (RAG) architectures.\n\nBased on your query, the page is **highly relevant** as it is expected to cover:\n\n*   **RAG architectures** (specifically modern ones beyond \"vanilla\").\n*   It is very likely to touch upon **vector databases**, **embeddings** (especially with new efficient models), **rerankers**, **hybrid search**, and **chunking strategies**, as these are core components of modern RAG systems.\n\nHowever, the provided text snippet is only the title and navigation elements, not the main content. Therefore, while the topic strongly suggests the content you are looking for, I cannot confirm the exact details without the body of the article.\n\n**Summary based on the title and query:** The page discusses **7 Modern RAG Architectures** and is expected to cover key components like **vector databases, embeddings, rerankers, hybrid search, and chunking strategies** relevant to advanced RAG implementation.",
    "published_date": "2025-12-10T18:21:58.000Z",
    "score": null
  },
  "https://raghunaathan.medium.com/exploring-retrieval-augmented-generation-rag-and-its-alternatives-bf9e2f337f88": {
    "content_path": "../data/contents/7d011c4b9ac4e6fe.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:40.013066",
    "title": "Exploring Retrieval-Augmented Generation (RAG) and Its Alternatives",
    "summary": "The webpage provides a comprehensive overview of Retrieval-Augmented Generation (RAG), its architecture, types, benefits, and comparisons with fine-tuning.\n\nHere is a summary of the components mentioned in your query:\n\n*   **Vector databases (Vector Store):** These are part of the Knowledge Base, where document embeddings are indexed and stored, forming the foundation for the retrieval stage.\n*   **Embeddings:** The process involves transforming chunked text content into embeddings using an embedding model before storing them in the vector store. The text also mentions the influence of the embedding model on chunking strategies.\n*   **Rerankers:** Reranking the retriever results is a key component of the **Advanced RAG** implementation and the **Reader** function. Rerankers evaluate candidate documents based on relevance using mechanisms like cross-attention to refine the initial retrieval.\n*   **RAG Architectures:** The article details three main types: **Naive**, **Advanced**, and **Modular** RAG, highlighting that Advanced RAG includes pre- and post-retrieval steps like query rewriting and candidate reranking to improve quality.\n*   **RAG Alternatives:** The article primarily contrasts RAG with **Fine-tuning** LLMs, and also discusses **Hybrid Approaches** (RAG + fine-tuning) as alternatives or enhancements.\n*   **Hybrid Search:** This is covered under the **Hybrid Approach** section, specifically mentioning methods like **RAG-guided Fine-tuning** and **RAG with Attention-based Fusion**.\n*   **Chunking Strategies:** The text mentions that content is chunked based on the **embedding model and the LLM context window** during the Knowledge Base creation process.\n\nThe article also discusses retrieval mechanisms (Sparse, Dense, Contextual retrieval), prompt compression, prompt generation, guardrails, and evaluation metrics.",
    "published_date": "2024-12-28T18:39:33.013Z",
    "score": null
  },
  "https://dev.to/superorange0707/the-complete-developers-guide-to-graphrag-lightrag-and-agenticrag-14go": {
    "content_path": "../data/contents/926e6b9474e66cd0.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.632957",
    "title": "The Complete Developer’s Guide to GraphRAG, LightRAG, and AgenticRAG",
    "summary": "The provided web page discusses advanced Retrieval-Augmented Generation (RAG) architectures that go beyond \"plain RAG\" to handle complex reasoning and real-world use cases.\n\nThe key topics covered are:\n\n*   **GraphRAG:** Integrates a **knowledge graph** into the retrieval process to preserve and reason over relationships between entities, excelling at multi-hop reasoning tasks (e.g., legal or biomedical analysis).\n*   **LightRAG:** A leaner, faster, and cheaper alternative that uses **lightweight graph indexing** and **two-level retrieval** (local and global search) to achieve high-quality retrieval on smaller hardware, reducing cost and complexity compared to GraphRAG.\n*   **AgenticRAG:** The most ambitious approach, using **autonomous agents** that plan, iteratively refine queries, choose appropriate retrieval tools (vector search, graph search, etc.), and self-evaluate to handle complex, multi-step tasks.\n\nThe page also includes a comparison table summarizing the core idea, strengths, cost, and best use cases for each architecture.",
    "published_date": "2025-11-24T00:00:00.000Z",
    "score": null
  },
  "https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6?gi=19ae08278b2a": {
    "content_path": "data/contents/6f5fd342ab12db15.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.866204",
    "title": "Advanced RAG Techniques: an Illustrated Overview",
    "summary": "The webpage provides an overview of advanced Retrieval Augmented Generation (RAG) techniques. It covers:\n\n*   **Vector Databases and Embeddings:** Discusses the role of vector stores (like Chroma, Pinecone, Weaviate) and the importance of choosing search-optimized embedding models (like bge-large or E5 embeddings).\n*   **RAG Architectures and Alternatives:** Details the \"Vanilla RAG\" starting point and explores advanced techniques such as Hierarchical indices, Hypothetical Questions and HyDE, Context enrichment (Sentence Window Retrieval, Auto-merging Retriever/Parent Document Retriever), and Fusion retrieval/Hybrid search (combining keyword search like BM25 with vector search, often using Reciprocal Rank Fusion for reranking).\n*   **Rerankers and Filtering:** Mentions using Postprocessors for filtering results based on scores or metadata, and reranking retrieved results using models like cross-encoders or LLMs.\n*   **Query Transformations:** Describes using LLMs to modify the user input, including decomposing complex queries into sub-queries, step-back prompting (generating a more general query), and query re-writing.\n*   **Context Window Management:** Techniques like Sentence Window Retrieval and Auto-merging Retriever are discussed as ways to retrieve small, precise chunks but provide the LLM with a larger surrounding context.\n*   **Agentic Behavior:** Covers Query Routing (using an LLM to decide which index or route to use) and Agents, which equip an LLM with tools to complete tasks, including multi-document agent setups.\n*   **Encoder and LLM Fine-tuning:** Discusses fine-tuning the Encoder model for better embeddings, fine-tuning cross-encoders for reranking, and fine-tuning the LLM itself on synthetic Q&A pairs generated by a stronger model.\n\nThe page extensively covers the components mentioned in your query, including vector databases, embeddings, rerankers, RAG architectures, hybrid search, chunking strategies, and context window management.",
    "published_date": "2024-01-03T00:00:00.000Z",
    "score": null
  },
  "https://www.exxactcorp.com/blog/deep-learning/alternative-rag-models": {
    "content_path": "data/contents/25097d1cd876b14c.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.334349",
    "title": "Three Alternative RAG Models - SQL, Knowledge Bases, & APIs",
    "summary": "The webpage discusses Retrieval-Augmented Generation (RAG) and explores three alternatives to the most common vector-based RAG: **Structured Retrieval RAG (SQL/Tabular RAG)**, **API-Augmented RAG**, and **Knowledge-Based RAG (Symbolic RAG)**.\n\nWhile the page mentions that common RAG systems use **vector databases** and **embeddings**, and discusses limitations of vector-based RAG (like semantic drift and lack of structure-awareness), it does not detail **rerankers**, **hybrid search**, **chunking strategies**, or **context window management**.\n\nThe three alternative RAG models focus on different data types and retrieval needs:\n1.  **Structured RAG:** Best for enterprise data and regulated fields, using precise queries against structured sources (like SQL) for deterministic, schema-aware results.\n2.  **API-Augmented RAG:** Best for real-time and volatile information by calling external APIs live.\n3.  **Knowledge-Based RAG:** Best for logic-heavy and compliance-focused tasks, using knowledge graphs for precise, explainable, rule-based retrieval.\n\nThe choice of architecture depends on data structure, timeliness, and required precision.\n\n**No answer found** for detailed information on: retrieval\\_and\\_embeddings: **rerankers, hybrid search, chunking strategies, context window management**.",
    "published_date": "2025-04-17T00:00:00.000Z",
    "score": null
  },
  "https://2www.sentic.net/retrieval-augmented-generation.pdf": {
    "content_path": "../data/contents/af081a460da3367c.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.893401",
    "title": "",
    "summary": "The webpage provides a systematic analysis of **Retrieval-Augmented Generation (RAG)** systems from a fundamental algorithmic perspective.\n\nHere is a summary addressing the components mentioned in your query:\n\n*   **RAG Architectures:** The page defines the general RAG framework as integrating a **Retriever** ($\\rho$) and a **Generator** ($\\theta$) to produce an output based on an input query and external knowledge ($D^*$). It provides a comprehensive taxonomy classifying retrievers (Fundamental vs. Advanced) and generators (Fundamental vs. Advanced).\n*   **Vector Databases / Embeddings (new efficient models):** This falls under **Dense Retrieval**, which uses Dual Encoder architectures (like DPR) to map queries and documents into a shared vector space. Relevance is determined by vector similarity (e.g., dot product). The text mentions that **Approximate Nearest Neighbor (ANN)** methods, such as those implemented in Faiss using techniques like **IVFPQ** (a form of vector quantization), are used to enable efficient top-K retrieval over large corpora in this vector space.\n*   **Rerankers:** Reranking is explicitly covered as a **Post-retrieval Optimization** technique under Advanced Retrieval Methods. The formal RAG equation shows that reranking refines the initial document set ($D_{top-K}$) by assigning new scores via a scoring function $f_\\rho(\\text{input}, d)$, which is then normalized into the final retrieval distribution $p_\\rho(d|\\text{input}, D^*)$.\n*   **RAG Alternatives:** The text focuses on RAG itself, but it implicitly contrasts it with standalone Large Language Models (LLMs) whose reliance on static parameters limits accuracy. Generative Retrieval methods like DSI, which \"memorize\" the corpus and directly generate document identifiers, represent a paradigm shift that bypasses the traditional separate retrieval/generation pairing.\n*   **Hybrid Search:** **Hybrid Retrieval** is listed as an **Advanced Retrieval Method**, with **RRF (Reciprocal Rank Fusion)** cited as a representative example.\n*   **Chunking Strategies:** While the term \"chunking strategies\" is not explicitly used, the concept is inherent in how documents are processed. The retrieval methods discussed (Sparse, Dense) operate on documents or passages retrieved from a corpus. The paper focuses on the *retrieval* and *generation* algorithms rather than the preprocessing steps like chunking, though\n\nThe provided text discusses various aspects of Retrieval-Augmented Generation (RAG) systems, including different **retrieval methods** (Sparse Retrieval, Dense Retrieval, Graph Retrieval, Multimodal Retrieval, Hybrid Retrieval, Adaptive Retrieval, Iterative Retrieval) and **generation modules** (Encoder-Decoder, Decoder-Only, Diffusion Models, Reasoning Generation, RL-Augmented Generation).\n\nWhile the text details retrieval methods and generation techniques, it **does not explicitly mention or summarize** the following specific topics from your query:\n\n*   **Vector databases**\n*   **Embeddings (new efficient models)**\n*   **Rerankers** (though \"Re2G: Retrieve, rerank, generate\" is mentioned, a general summary of rerankers is not provided)\n*   **RAG architectures** (it discusses combinations of retrieval/generation, but not a taxonomy of RAG architectures)\n*   **RAG alternatives**\n*   **Chunking strategies**\n\nThe text does mention **Hybrid Retrieval** and **Adaptive Retrieval**, which are related to the query.\n\n**Conclusion:** The page provides a broad overview of retrieval and generation techniques within RAG but does not specifically summarize **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, or chunking strategies.**\n\nNo answer found.",
    "published_date": "2025-05-11T00:00:00.000Z",
    "score": null
  },
  "https://www.alpha-sense.com/industries/venture-capital/": {
    "content_path": "data/contents/9eb0ea4d42a32936.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.643776",
    "title": "Turn insight into action . Deal sourcing to Investment Committee in minutes.",
    "summary": "AlphaSense provides AI agents and comprehensive content to automate discovery, analysis, and synthesis for Venture Capital (VC) workflows, aiming to speed up the process from market signals to investment decisions.\n\n**Key features and capabilities relevant to the query:**\n\n* **AI Agents for Workflows:** AI agents automate multi-step research tasks, synthesize answers with full citations, detect investment signals, and transform research findings into Investment Committee (IC) memos and competitive analyses.\n* **Content Access:** Access to over 500 million premium documents, including expert insights, earnings calls, SEC filings, and sell-side research, curated for investment decision-making.\n* **Workflow Automation:** Automates manual research tasks, including generating IC memos, competitive landscape analysis, and continuous portfolio company monitoring.\n* **Use Cases:** Supports deal sourcing, due diligence (compressing diligence time significantly, as noted in testimonials), portfolio monitoring, and generating LP updates/board materials.\n* **Security and Scale:** Enterprise-grade security (SOC 2 Type II compliance) suitable for demanding investment teams.\n\nWhile the page heavily focuses on **Venture Capital** and mentions **AI agents** for analysis, **due diligence automation**, **investment opportunity analysis**, **deal sourcing**, and **portfolio monitoring**, it **does not explicitly mention** private equity, general finance agents, financial report generation, or direct integrations with PitchBook, Preqin, or CapIQ (though it implies comprehensive content access). It does mention **AlphaSense** integration.",
    "published_date": null,
    "score": null
  },
  "https://www.youtube.com/watch?v=mLZoRvWIN-o": {
    "content_path": "data/contents/f7f2d2f4a49711ec.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.615892",
    "title": "How AI Agents Automate PE Due Diligence | V7 Go, Eurazeo \\u0026 Moonfare",
    "summary": "The webpage discusses how **AI agents are being deployed in Private Equity (PE) to automate workflows and address the operational drag caused by manual document processing** in areas like data rooms and investment memos.\n\nSpecifically, the content covers:\n*   **Automation across the investment lifecycle**, including using AI for **deal sourcing** (e.g., spotting signals on LinkedIn) and **portfolio monitoring** (creating intelligent, self-updating knowledge bases).\n*   Generating a **first-draft investment memo** from a Virtual Data Room (VDR) quickly.\n*   The importance of **verifiable and traceable AI** for high-stakes financial decisions.\n*   The continuing **role of human judgment** alongside AI augmentation.\n\nWhile the page focuses heavily on **AI agents automating PE due diligence, deal sourcing, and portfolio monitoring**, it **does not explicitly mention** specific integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**, nor does it detail agents specifically for **venture capital** or **financial report generation** beyond the investment memo draft.",
    "published_date": "2025-07-30T00:00:00.000Z",
    "score": null
  },
  "https://www.v7labs.com/agents/ai-due-diligence-agent": {
    "content_path": "../data/contents/b95e0e4eb3dd7b0a.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.007481",
    "title": "AI Financial Due Diligence Agent",
    "summary": "The webpage describes an **AI Financial Due Diligence Agent** designed to automate the first-pass due diligence process for **Private Equity, Venture Capital, and Corporate Development**.\n\nKey capabilities relevant to your query include:\n*   **Due Diligence Automation:** It navigates Virtual Data Rooms (VDRs), extracts and normalizes financials, analyzes commercial contracts, and flags risks.\n*   **Financial Analysis:** It performs **Financial Statement Spreading** (extracting data from income statements, balance sheets, and cash flow statements) and populates standardized financial model templates.\n*   **Risk Identification:** It proactively searches for diligence risks like customer concentration, margin erosion, and problematic contract clauses.\n*   **Cap Table Analysis:** It parses capitalization tables to validate ownership and check for liquidation preference issues.\n*   **Data Integration:** It mentions the ability to **Import your files** from sources including **PitchBook** and **S&P Capital IQ** (CapIQ).\n\nThe page also mentions other related agents, such as an **AI Investment Analysis Agent** and a **Financial Valuation Agent**, which cover aspects like investment opportunity analysis and deal sourcing/analysis.",
    "published_date": null,
    "score": null
  },
  "https://www.getdynamiq.ai/post/ai-agents-for-private-equity-use-cases-kpis-and-deployment-strategy": {
    "content_path": "../data/contents/4691a68c2f6d50cc.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.339790",
    "title": "AI Agents for Private Equity: Use Cases, KPIs, and Deployment Strategy",
    "summary": "AI agents are being adopted in private equity for various functions, including automating back-end processes, conducting due diligence, and evaluating portfolios.\n\nSpecific use cases for generative AI agents in private equity include:\n*   **Customer Support:** Handling routine customer queries for portfolio companies.\n*   **Lead Scoring:** Analyzing startup profiles, news, and market signals to evaluate investment opportunities and continuously re-rank scores based on dynamic signals.\n*   **FP&A Copilots:** Integrating with financial systems (like NetSuite, SAP, QuickBooks) to generate real-time forecasts, run simulations, and detect anomalies, presenting results in annotated reports.\n*   **KPI Reporting:** Standardizing Key Performance Indicators across different portfolio companies by understanding context in column names and inferring missing data.\n*   **RFP Copilot:** Processing Request for Proposal documents based on company requirements and automatically assembling or drafting responses.\n*   **Document Processing and Review:** Synthesizing information from complex documents like Confidential Information Memorandums (CIMs) and financial audits, and searching for inconsistencies.\n\nAgents are also deployed across the deal lifecycle:\n*   **Deal Sourcing:** Automatically ranking deals and generating summary sheets.\n*   **Pipeline Monitoring:** Detecting inconsistencies in CRM entries and pitch decks.\n*   **Due Diligence:** Identifying discrepancies and generating Q&A templates.\n*   **Meeting Optimization:** Transcribing and categorizing conference notes.\n*   **Exit Strategy Planning:** Archiving deal insights and lessons learned.\n\nThe text mentions that AI agents can integrate with financial systems and that generative AI can be used to generate financial reports, but it **does not specifically mention** integrations with external data providers like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.",
    "published_date": "2025-06-12T00:00:00.000Z",
    "score": null
  },
  "https://www.alpha-sense.com/": {
    "content_path": "data/contents/9ea6824575b0b4a0.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.428256",
    "title": "Accelerate your workflow with  AI insights you can trust",
    "summary": "The webpage describes **AlphaSense**, an enterprise-grade platform for financial and business professionals that uses AI to accelerate workflows and provide trusted insights.\n\nRegarding your query about **multi-agent systems for finance and related activities**:\n\n*   **AI Workflow Agents:** AlphaSense features **AI Workflow Agents** that automate time-consuming workflows like ramp-ups, earnings prep, competitive benchmarking, and **diligence** (which relates to due diligence). With a single click, these agents search, synthesize, and deliver reports or insights.\n*   **Investment Focus:** The platform is used by professionals in **Investment Banking**, **Hedge Funds**, and **Private Equity**.\n*   **Data Sources:** It integrates various data sets, including standardized financials (from Canalyst), expert transcripts (Tegus), broker research, company filings, and public/private financial data.\n*   **Due Diligence:** Royalty Pharma is mentioned as using AlphaSense and Expert Insights for **due diligence**.\n*   **Integrations/Specific Tools:** While the page heavily features **AlphaSense** itself and mentions **Tegus** and **Canalyst**, it **does not explicitly mention** integrations with **PitchBook, Preqin, CapIQ, or Bloomberg**, nor does it detail agents specifically for deal sourcing, investment opportunity analysis, portfolio monitoring, or financial report generation beyond the general capabilities of the Workflow Agents.\n\nIn summary, AlphaSense offers AI agents to automate research and diligence workflows relevant to finance and private equity, but it does not confirm support for all the specific external platforms listed in your query.",
    "published_date": null,
    "score": null
  },
  "https://the-drawdown.com/articles/category/ai?__hstc=67505038.7b0a84774b1d1a0e0ec627e769dcbdb8.1749772800153.1749772800154.1749772800155.1&__hssc=67505038.1.1749772800156&__hsfp=3087218293": {
    "content_path": "../data/contents/8608779c6463d04f.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.119304",
    "title": "AI in private equity",
    "summary": "The webpage discusses the growing use of AI in private equity operations, mentioning various technology developments, partnerships, and product launches.\n\nRegarding your specific query about **multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, and Bloomberg**:\n\n*   **Multi-agent systems/Agents:** The page mentions MSCI launching an **\"agentic AI platform\"** and Allvue launching an **\"AI agent\"** and expanding its **\"AI product suite\"** which includes a portfolio optimiser and document extractor. Juniper Square launched an **\"agentic AI platform\"** and a CRM powered by its AI, JunieAI. Foresight launched a **\"conversational AI\"** tool. GFT launched an **\"AI credit risk assistant\"**. Auquan unveiled an **\"AI credit agent\"**. There is also mention of an **\"Agent Office Space\"** article.\n*   **Due diligence automation/Investment opportunity analysis/Deal sourcing:** Several tools mentioned relate to these areas, such as Meridian launching an **\"AI document builder\"** that integrates deal scoring, Arcanis launching an **\"AI platform\"** to support VC investors with the gap between research and decision-making, and in-house sourcing tools being developed by GPs.\n*   **Portfolio monitoring:** Holland Mountain and PortF teaming up to support clients with **portfolio monitoring**, and S&P launching a **performance analytics tool** through a collaboration are mentioned. PortOptix launched an **\"AI tool for portfolio management\"** with an AI assistant.\n*   **Data analysis/Financial report generation:** Tools for data integration (e.g., Passthrough and Ark teaming up for automatic data flow) and analysis are frequently mentioned.\n*   **Integrations:**\n    *   **Preqin:** Hebbia integrates **Preqin data**.\n    *   **PitchBook:** PitchBook integrates with **OpenAI** and teams up with four **LLM platforms** (Anthropic, Perplexity, Rogo, and Hebbia).\n    *   **CapIQ (Capital IQ):** Acuity integrates **Capital IQ data**.\n    *   **AlphaSense and Bloomberg:** These specific integrations are **not explicitly mentioned** in the provided text snippets.\n\nIn summary",
    "published_date": "2025-11-13T00:00:00.000Z",
    "score": null
  },
  "https://us.fitgap.com/search/ai-agents/private-equity": {
    "content_path": "data/contents/ac568ad42d3959cf.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.766608",
    "title": "Best AI agents 2025 12 - FitGap",
    "summary": "The webpage mentions **Salesforce Agentforce**, an autonomous AI agent platform built on the Einstein AI foundation, designed to help **private equity firms automate complex workflows**.\n\nThis partially addresses the user query regarding **agents and finance**, specifically mentioning **private equity**. However, it does not detail the other specific areas mentioned in the query such as venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, data analysis, financial report generation, or integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": null,
    "score": null
  },
  "https://pathway.com/blog/ai-agents-finance-due-diligence": {
    "content_path": "../data/contents/197413759250c467.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.147133",
    "title": "How AI Agents in Finance Are Transforming Financial Due Diligence: FA3STER",
    "summary": "FA3STER is an autonomous, multi-agent Real-Time RAG (Retrieval-Augmented Generation) system designed to transform the resource-intensive process of **financial due diligence (FDD)**.\n\n**Key features and capabilities relevant to your query:**\n\n* **Multi-Agent System:** It utilizes specialized agents for finance, document grading, SQL querying, and data visualization, which collaborate and iterate to refine financial insights.\n* **Financial Due Diligence Automation:** It automates document analysis, generates concise FDD reports (in 10–12 minutes), and creates quick-look dashboards.\n* **Data Analysis and Interpretation:** It includes a Reasoning Agent for converting financial data into visualizations and a Finance Agent for analyzing market trends and key financial metrics.\n* **Architecture:** It employs an Agentic RAG framework with intelligent retrieval (context-aware chunking), autonomous post-retrieval processing, and a \"Vertical Autonomous Layer\" for self-improving analysis.\n* **User Modes:** It supports an **Interactive Chat Mode** for real-time query resolution and a **Report Generation Mode** for automated FDD reports.\n\n**Regarding specific integrations mentioned in your query:**\n\nThe text mentions using **Tavily Search** for real-time external insights and specialized agents for data interpretation, but it **does not explicitly mention integrations** with specific proprietary databases like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.**",
    "published_date": "2025-03-26T04:12:54.000Z",
    "score": null
  },
  "https://www.llamaindex.ai/use-cases/financial-due-diligence": {
    "content_path": "../data/contents/9dc0bae1ca9bfd79.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.176118",
    "title": "Financial due diligence",
    "summary": "The webpage describes how **LlamaIndex** helps investment and audit teams automate **financial due diligence** using **AI agents** to parse complex documents like contracts, financial statements, and compliance documents.\n\nKey capabilities mentioned include:\n*   **Accelerating diligence** and **reducing risk** by automating the review of documents that contain challenging tables and charts.\n*   **Putting diligence on autopilot** by extracting key terms, summarizing risks/obligations, comparing filings, and generating structured diligence reports.\n*   Specific agent use cases: **Investment Research** (synthesizing filings/reports), **Contract Analyzer**, **Risk Summarizer**, **Entity Comparator**, and **Audit Assistant**.\n\nWhile the page focuses heavily on **document intelligence** and **agent workflows** for due diligence, it **does not explicitly mention** the specific platforms listed in your query: **private equity, venture capital, deal sourcing, portfolio monitoring, financial report generation, PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg integrations.**",
    "published_date": null,
    "score": null
  },
  "https://www.deloitte.com/cz-sk/en/services/consulting/blogs/where-is-the-value-of-AI-in-MA-why-multi-agent-systems-needs-modern-data-architecture.html": {
    "content_path": "../data/contents/2b0debc24c7333d6.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.261381",
    "title": "Where is the value of AI in M&A",
    "summary": "The webpage discusses the value of Artificial Intelligence (AI), specifically **multi-agent systems (MAS)**, in Mergers & Acquisitions (M&A) and finance, emphasizing the necessity of a **modern data architecture** to support them.\n\n**Key points relevant to your query:**\n\n*   **Multi-Agent Systems in Finance/M&A:** MAS are described as collections of specialized, cooperating AI agents that can tackle complex workflows. The article highlights their application across the M&A lifecycle:\n    *   **Deal Sourcing and Strategic Scanning:** Agents can continuously scan data from various sources (including databases like **PitchBook** or **CapIQ**) to identify potential acquisition targets.\n    *   **Due Diligence and Document Analysis:** Agents automate the review of thousands of documents, summarizing contracts, flagging anomalies, and extracting key data points.\n    *   **Valuation and Financial Modeling:** Agents automate the updating of financial models, search for comparable transactions, and test synergy assumptions, leveraging integrated financial databases.\n    *   **Portfolio Monitoring:** Private equity firms are experimenting with AI agents that aggregate KPIs across portfolio companies, even if they use disparate internal systems, providing an integrated overview.\n*   **Data Integration:** The success of these agents relies on a modern data architecture that provides a unified, trusted, and scalable data foundation. The text explicitly mentions agents plugging into external databases like **CapIQ** and **PitchBook** as part of this modern data environment.\n*   **General Finance Applications:** The article mentions JPMorgan Chase’s DeepX system, which uses multiple agents to analyze market indicators for investment recommendations, demonstrating the use of MAS in investment analysis.\n\nWhile the text covers many aspects of agents in finance, due diligence, deal sourcing, and portfolio monitoring, it **does not explicitly detail** agents specifically for:\n\n*   Venture Capital (VC) (though Private Equity is mentioned).\n*   Automated financial report generation (it mentions agents summarizing documents and updating models, but not generating formal reports).\n*   Integrations with **AlphaSense** or **Preqin** (though PitchBook and CapIQ are mentioned).\n*   Integrations with **Bloomberg** (though general financial databases are implied).\n\n**Summary:** The page confirms that multi-agent systems are highly valuable for M&A and finance, covering deal sourcing, due diligence, and portfolio monitoring, and explicitly mentions integration with data sources like PitchBook and Cap",
    "published_date": "2025-07-22T00:00:00.000Z",
    "score": null
  },
  "https://www.v7labs.com/automations/ddq-documents": {
    "content_path": "data/contents/48950f66f87691f4.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.085315",
    "title": "AI DDQ Automation",
    "summary": "The webpage describes **V7 Go**, an AI-powered solution focused on **Automating Due Diligence Questionnaires (DDQ)** for fund management teams.\n\n**Key features and capabilities relevant to your query:**\n\n*   **DDQ Automation:** V7 Go automates DDQ completion by extracting data from fund documents, validating responses, and ensuring regulatory compliance, reducing completion time by up to 85% (from weeks to days).\n*   **AI Agents:** The system utilizes \"Multiple agents available to do the work\" which can be selected and tailored to specific DDQ documents.\n*   **Data Processing & Analysis:** It processes various fund documents (LPA, reports, compliance certifications, etc.) to extract information for generating accurate responses.\n*   **Accuracy and Validation:** It aims for a 99% accuracy rate through GenAI reasoning, multi-step validation against source documents, and providing precise citations for audit trails.\n*   **Security:** It offers enterprise-grade security, including SOC 2 Type II compliance, end-to-end encryption, and data privacy controls.\n*   **Related Agents/Tools:** The \"More Documents\" section lists other finance-related AI agents, including:\n    *   **M&A Data Insights:** For analyzing deal patterns, market trends, and valuation metrics.\n    *   **Financial Health Dashboard:** For consolidating data and tracking KPIs.\n    *   **Deal Flow Reports:** For consolidating pipeline data and creating summaries.\n    *   **Deal Data** and **Structured Deal Data:** For processing and structuring deal information.\n    *   **Risk Analysis Models:** For extracting metrics and validating assumptions.\n\n**What is missing based on your query:**\n\nThe page focuses heavily on **DDQ automation** and general finance agents. It **does not explicitly mention** specific integrations with **PitchBook, AlphaSense, Preqin, or CapIQ**. It only generally states that V7 Go integrates with \"popular fund administration platforms, document management systems, and CRM tools through APIs.\"",
    "published_date": "2025-10-20T00:00:00.000Z",
    "score": null
  },
  "https://beam.ai/use-cases/private-equity": {
    "content_path": "../data/contents/5942a2e9e46b7b64.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.368154",
    "title": "Automate Private Equity with AI Agents: How Beam AI Transforms Fund Management | Beam AI",
    "summary": "Beam AI utilizes specialized AI agents to automate and transform private equity fund management. These agents address challenges in areas like **budget management** (automating financial tracking, reporting, and budget comparison across portfolio companies) and, for real estate focused firms, **property management** (automating maintenance, tenant communication, and financial tracking).\n\nThe platform also supports **automated fund valuations** for enhanced decision-making, real-time monitoring, and risk management. Furthermore, Beam AI's **Budget Proposal tool** streamlines the creation of funding requests for firms raising capital or assisting portfolio companies.\n\nBeam AI emphasizes **multi-agent capabilities** for comprehensive automation and **seamless integration** with existing tools. Future applications mentioned include **advanced due diligence**, **portfolio optimization**, and **investor relations** automation. The text focuses on private equity but also mentions applications for venture capital processes (streamlining budget proposals).\n\nThe page **does not explicitly mention** agents for deal sourcing, investment opportunity analysis, portfolio monitoring (beyond general valuation/optimization), or integrations with specific data providers like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": null,
    "score": null
  },
  "https://digiqt.com/blog/ai-agents-for-private-equity/": {
    "content_path": "../data/contents/6885a12f39465fca.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.395576",
    "title": "AI Agents in Private Equity: Proven Wins and Pitfalls | Digiqt Blog",
    "summary": "AI agents in Private Equity (PE) are goal-driven software entities that use large language models (LLMs), rules, and tools to autonomously perform tasks across the PE lifecycle, such as deal origination, due diligence, portfolio monitoring, and fund operations.\n\n**Key features and capabilities mentioned that relate to your query:**\n\n*   **Data Analysis and Report Generation:** Agents support tasks like financial report generation (implied through portfolio monitoring, KPI checks, and cash flow forecasting QA) and data analysis (e.g., analyzing transaction data, mining spend data).\n*   **Integrations with Financial Databases:** The \"Tools\" section explicitly lists connectors to external data APIs, including **PitchBook**, **Capital IQ (CapIQ)**, and **AlphaSense**.\n*   **Due Diligence Automation:** Agents are used for teaser and CIM triage, key metrics extraction, market mapping, and automating DDQ responses.\n*   **Finance/Portfolio Monitoring:** Agents monitor portco KPIs, flag risks, recommend actions (pricing, churn), and assist with cash flow forecasting and fee calculations QA.\n*   **Multi-Agent Collaboration:** The system supports specialist agents for sourcing, diligence, ops, and IR that hand off work with context.\n\nThe text confirms the use of agents for data analysis, financial report generation, and integration with PitchBook, AlphaSense, and CapIQ within the context of Private Equity operations, which encompasses aspects of finance, due diligence, and investment analysis.",
    "published_date": "2025-09-21T15:00:28.000Z",
    "score": null
  },
  "https://www.meridian-ai.com/product/private-equity-ai": {
    "content_path": "../data/contents/94085224a1cd119b.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.313362",
    "title": "Scout, the AI agent built for private equity",
    "summary": "Scout, the AI agent built for private equity by Meridian, is designed to power the entire deal workflow. It offers features like instant market mapping, data enrichment with zero manual input, contextual updates for meeting prep, and automated data extraction (including CIM summarization) for faster deal reviews.\n\nMeridian's AI is deeply embedded across the deal process, providing AI-powered data extraction, instant deal benchmarking against private and public comps, and single-click investment insights. Scout is pre-trained for private equity workflows and learns from your firm's historical data to deliver targeted insights without manual prompting. It can help identify new deals by analyzing historical deals and market activity.\n\nThe platform also features seamless integrations, automation, flexible reporting, and a fundraising/investor relations module. It is built on enterprise-grade infrastructure with SOC 2 compliance, full data encryption, and SSO support.",
    "published_date": null,
    "score": null
  },
  "https://aiagents4privateequity.com/": {
    "content_path": "../data/contents/795443ca0a542454.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.422951",
    "title": "AI Agents for Private Equity",
    "summary": "The webpage discusses how **AI is transforming Private Equity** by automating **deal sourcing**, accelerating **due diligence**, and optimizing **portfolio company operations** for value creation.",
    "published_date": null,
    "score": null
  },
  "https://www.forbes.com/sites/kolawolesamueladebayo/2025/09/09/ai-is-becoming-private-equitys-new-playbook/": {
    "content_path": "../data/contents/5a3fb4022ac3f73a.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.452009",
    "title": "AI Is Becoming Private Equity’s New Playbook",
    "summary": "The webpage discusses how Artificial Intelligence (AI) is becoming central to the private equity (PE) playbook, moving beyond experimentation into core workflows like sourcing targets and assessing risks.\n\nKey points relevant to your query include:\n\n*   **Due Diligence Automation:** AI tools are significantly speeding up diligence processes, allowing analysts to assess companies in hours instead of weeks (e.g., at Carlyle Group). Firms are training AI agents on standard workflows to generate diligence reports.\n*   **Deal Sourcing:** AI is being used to sharpen the search for investment opportunities by capturing digital signals to identify readiness to transact, potentially boosting inbound deal flow.\n*   **Data Analysis and Insight Generation:** AI processes massive volumes of data (financial statements, contracts, etc.) at speed, allowing investors to focus on generating insight and making decisions.\n*   **Implementation:** Firms are grappling with whether to build, buy, or partner for AI solutions, emphasizing the need for systems that can evolve quickly alongside large language models (LLMs). Data security and quality are noted as critical constraints.\n\nThe article mentions the general trend of AI adoption in PE but **does not specifically name or detail integrations with external platforms like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.**",
    "published_date": "2025-09-09T00:00:00.000Z",
    "score": null
  },
  "https://sourcealpha.ai/": {
    "content_path": "../data/contents/09e8885a632f4ea1.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.480239",
    "title": "The future of sourcing is agentic",
    "summary": "The webpage describes an AI-powered platform focused on **Agentic Sourcing for Private Capital**. It details how autonomous agents can be used for **deal sourcing**, including identifying and qualifying investment opportunities across multiple data sources. The platform offers features like a **Custom Rules Engine** to apply proprietary investment criteria, **Intelligent Alerts** for significant company events, and **Relationship Automation** for outreach. It emphasizes a **Human-in-the-loop** approach to augment decision-making while maintaining user control, transparency, and audit trails.\n\nWhile the user query asks about **multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg**, the webpage specifically focuses on **Agentic Lead Sourcing** within the private capital/investment space. It confirms capabilities related to **investment opportunity analysis** and **deal sourcing** using agents, but it **does not explicitly mention** due diligence automation, portfolio monitoring, financial report generation, or specific integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": null,
    "score": null
  },
  "https://www.solulab.com/ai-agents-for-due-diligence/": {
    "content_path": "data/contents/1e11e1675d9c2ba6.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.312727",
    "title": "Transform Due Diligence with Automation AI Agents",
    "summary": "The webpage discusses the use of **AI Agents for Due Diligence Automation**, focusing on how they transform processes in areas like investment analysis.\n\n**Key points relevant to your query:**\n\n*   **AI Agents for Data Analysis:** AI agents are used to automate the examination of massive datasets, improving accuracy and providing deeper insights into potential risks and opportunities.\n*   **Due Diligence Automation:** AI can cut document review time by up to 70%, analyze financial data for trends and anomalies, and automate processes like regulatory monitoring, document management, risk assessment, contract review, and data extraction.\n*   **Financial Context:** While the page focuses heavily on due diligence (which is critical in finance, private equity, and venture capital), it **does not explicitly mention** the specific platforms you listed: **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg integrations.**\n\n**Summary:** The page details how AI agents automate and enhance the due diligence process through advanced data analysis, document review, and risk assessment, which are core activities in finance, private equity, and venture capital. However, it does not confirm the use of specific third-party financial data platforms like PitchBook or Bloomberg.\n\n**No answer found** for the specific integrations mentioned in your query.",
    "published_date": "2025-01-24T00:00:00.000Z",
    "score": null
  },
  "https://www.akira.ai/blog/multi-agent-system-for-financial-compliance": {
    "content_path": "../data/contents/8be88ed40847a9e5.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.534202",
    "title": "Multi-Agent System for Flawless Financial Compliance",
    "summary": "The provided webpage focuses on **Akira AI's Multi-Agent System for Flawless Financial Compliance**.\n\nIt details how multi-agent systems (MAS) automate and enhance compliance processes in finance by:\n*   **Automating tasks** like KYC, due diligence, transaction monitoring, sanctions screening, and risk assessment.\n*   **Ensuring adherence** to regulations such as GDPR, SOX, and KYC.\n*   **Providing operational benefits** like increased productivity, improved efficiency, higher accuracy, and cost savings compared to traditional methods.\n*   **Utilizing a technological stack** involving frameworks like LangChain, LangGraph, AutoGen, and various ML/RAG components.\n\n**The user query asks about:** Multi-agent systems for **private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, financial report generation, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg.**\n\n**Summary relative to the query:**\n\nThe webpage discusses multi-agent systems in finance, specifically focusing on **compliance** (KYC, transaction monitoring, regulatory reporting). It mentions **Due Diligence** in the context of **Customer Onboarding** (KYC and Due Diligence Agent). However, it **does not** cover the specific areas of **private equity, venture capital, investment opportunity analysis, deal sourcing, portfolio monitoring, or integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.**\n\n**No answer found** for the majority of the specific investment and data platform integration aspects mentioned in the user query.",
    "published_date": "2024-11-13T00:00:00.000Z",
    "score": null
  },
  "https://www.leewayhertz.com/ai-agents-for-due-diligence/": {
    "content_path": "../data/contents/e6e423323e73ce15.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.560908",
    "title": "AI agents for due diligence: Role, use cases and applications, benefits, and implementation",
    "summary": "The provided webpage discusses the role, use cases, applications, benefits, and implementation of **AI agents in due diligence**.\n\nWhile the user query is very specific about **\"agents\\_and\\_finance: Multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations,\"** the webpage focuses primarily on **AI agents for due diligence** in general, covering:\n\n*   **Due Diligence Automation:** AI agents streamline the process by analyzing vast amounts of data (contracts, financial statements) quickly and accurately, reducing review time by up to 70%.\n*   **Use Cases:** Includes automated screening, deal sourcing, data automation/analysis, risk identification, financial analysis (ratio analysis, peer comparison), and compliance checks.\n*   **Financial Analysis:** AI agents can analyze financial statements to identify risks and opportunities, and forecast financial performance.\n*   **Key Components:** Details the structure of these agents (Core, Planning Module, Memory/RAG, Tools, Databases).\n\n**However, the webpage does not explicitly mention or detail the integration or use of AI agents specifically with the following platforms listed in the query:**\n\n*   Private equity or venture capital specific workflows (beyond general deal sourcing/due diligence).\n*   Portfolio monitoring.\n*   Financial report generation (though data analysis is mentioned).\n*   Specific data providers like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg integrations.**\n\n**Summary relative to the query:**\n\nThe page confirms that AI agents are transforming **due diligence automation** and are used for **deal sourcing** and **data analysis** within the financial context (e.g., analyzing financial statements). However, it does not provide information on multi-agent systems specifically tailored for private equity/VC, portfolio monitoring, or integrations with specific financial data platforms like PitchBook or Bloomberg.\n\n**No answer found** for the specific integrations and specialized finance applications mentioned in the query.\n\nThe webpage focuses on building and implementing **AI agents for due diligence**, detailing the steps involved, challenges, and the services offered by LeewayHertz.\n\nRegarding your specific query about **agents and finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg**:\n\n*   **Due Diligence Automation & Financial Report Generation:** The page extensively covers how LLM-powered AI agents can automate due diligence tasks, including **document review** (analyzing financial statements) and **financial statement analysis** as a custom development task.\n*   **Data Analysis & Investment Opportunity Analysis:** The text mentions developing **reasoning and analysis capabilities** for data analysis, pattern recognition, and trend identification, which supports investment analysis.\n*   **Finance/PE/VC Context:** While the page discusses due diligence generally, it explicitly mentions tailoring agents for domains like **mergers and acquisitions** and **financial analysis**. LeewayHertz's custom development includes tasks like **Financial statement analysis** and **Market and competitor intelligence gathering**, which are highly relevant to Private Equity and Venture Capital.\n*   **Integrations (PitchBook, AlphaSense, etc.):** The page discusses the need for **Knowledge integration systems** and **Seamless integration** with existing systems and data repositories. However, it **does not specifically mention** integrations with external financial data providers like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.\n\n**Summary:** The page provides a comprehensive guide on building AI agents for due diligence, covering automation, financial analysis, and data analysis relevant to finance/investment. However, it **does not explicitly confirm** the integration capabilities with the specific financial data platforms you listed (PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg).",
    "published_date": null,
    "score": null
  },
  "https://builder.aws.com/content/2uDxjoo105xRO6Q7mfkogmOYTVp/multi-agent-system-patterns-in-financial-services-architectures-for-next-generation-ai-solutions": {
    "content_path": "../data/contents/1f9c12ca58feb2b7.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.508768",
    "title": "AWS Builder Center",
    "summary": "No answer found",
    "published_date": "2025-03-12T00:00:00.000Z",
    "score": null
  },
  "https://www.sourcescrub.com/": {
    "content_path": "../data/contents/9d3f61697de6abdc.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.586767",
    "title": "Never Miss a Deal ™",
    "summary": "The webpage describes **Sourcescrub**, an AI-driven deal sourcing platform for investment research and origination. It focuses on continuously crawling the digital universe to find private companies and connecting them with numerous sources to help users discover, track, and engage the best targets.\n\nWhile the platform is clearly aimed at **Private Equity** and related fields (deal sourcing, investment research, portfolio tracking, etc.), the provided text **does not mention** the specific terms from your query: **multi-agent systems**, **venture capital**, **due diligence automation**, **investment opportunity analysis**, **deal sourcing** (though it is a deal sourcing platform, it doesn't detail the *agent-based* approach), **portfolio monitoring**, **agents for data analysis**, **financial report generation**, or integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.\n\nNo answer found.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://devblogs.microsoft.com/ise/multimodal-rag-with-vision/": {
    "content_path": "data/contents/a1537b13ea477664.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.540433",
    "title": "Multimodal RAG with Vision: From Experimentation to Implementation - ISE Developer Blog",
    "summary": "This page summarizes an experimentation process for optimizing a **multimodal RAG** pipeline that handles documents containing both text and images, utilizing **multimodal LLMs** like **GPT-4V** and **GPT-4o**.\n\nKey aspects covered include:\n\n*   **Multimodal RAG Pattern:** Transforming image content into text descriptions using an MLLM during ingestion so that both text and image descriptions can be stored and retrieved from a vector database.\n*   **Ingestion Flow:** Detailed steps involving custom loading, image enrichment (using MLLMs to generate descriptions, sometimes aided by surrounding text context), and ingestion into Azure AI Search.\n*   **Experimentation Learnings:**\n    *   **Prompt Engineering:** Tailoring prompts for image enrichment and inference to improve description quality and structured output (JSON format).\n    *   **Metadata Impact:** Including document-level metadata significantly improved source recall performance.\n    *   **Chunking Strategy:** Using **separate chunks** for image annotations (instead of inline) resulted in better retrieval metrics for vision-related queries.\n    *   **Classifier Use:** Using an image classifier (like Azure Computer Vision) to filter out irrelevant images (e.g., logos) improved ingestion efficiency without sacrificing recall.\n    *   **Inference Model Choice:** **GPT-4o** was recommended for inference due to better quality, speed, and cost compared to GPT-4.\n    *   **Enrichment Model Choice:** **GPT-4V** was found to perform better than GPT-4o for generating image summaries during the ingestion/enrichment phase.\n*   **Evaluation:** The process relied on a curated Q&A dataset and specific retrieval and generative metrics to assess performance systematically.",
    "published_date": "2024-10-11T00:00:00.000Z",
    "score": null
  },
  "https://github.com/zhengxuJosh/Awesome-RAG-Vision": {
    "content_path": "../data/contents/296dba2c30fcf89d.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.291649",
    "title": "GitHub - zhengxuJosh/Awesome-RAG-Vision: Awesome-RAG-Vision: a curated list of advanced retrieval augmented generation (RAG) for Computer Vision",
    "summary": "The webpage is a curated list of state-of-the-art papers on **Retrieval-Augmented Generation (RAG) in Computer Vision**.\n\nIt covers various applications of RAG in vision tasks, including:\n*   **Visual Understanding** (Image Understanding, Long Video Understanding, Visual Spatial Understanding)\n*   **Multi-modal** RAG applications.\n*   **Visual Generation** (Image/Video Generation, 3D Generation).\n*   **Embodied AI**.\n\nThe page also lists resources, including tutorials on **Multimodal RAG** for images and video, and a section dedicated to **RAG for Document** processing, which mentions:\n*   Multimodal RAG for PDFs with Text, Images, and Charts.\n*   Multimodal RAG with Document Retrieval and Vision Language Models (VLMs).\n*   Multi-Vector Retriever for RAG on tables, text, and images.\n\nWhile the page extensively covers **Multimodal RAG**, **Vision-Language Models (VLMs)**, and **Document Understanding** (including PDF/chart/table aspects via the \"RAG for Document\" resources), it **does not explicitly list or detail** specific models like **GPT-4V, Claude vision, or Gemini**, nor does it focus on **report generation with LLMs** or **structured document output** as primary topics, although these concepts are implied within the broader RAG and document understanding context.\n\n**Summary relative to the query:**\n\nThe page is highly relevant to **multimodal RAG**, **vision-language models**, **document understanding**, **PDF parsing**, and **chart/table extraction** as it curates resources and papers in these areas, particularly under the \"Multi-modal\" and \"RAG for Document\" sections. However, it does not provide specific information on **report generation with LLMs**, **GPT-4V, Claude vision, Gemini**, or **structured document output** beyond the general scope of multimodal document RAG.",
    "published_date": "2024-11-21T07:35:07.000Z",
    "score": null
  },
  "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/llm-parser": {
    "content_path": "data/contents/9f638a5b274e60b6.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.640115",
    "title": "Use the LLM parser",
    "summary": "The webpage describes how to use the **Vertex AI RAG Engine LLM parser** for document processing.\n\nThe LLM parser leverages Large Language Models (LLMs) to:\n*   Understand and interpret semantic content across various formats.\n*   Retrieve relevant document chunks and extract meaningful information.\n*   Understand and interact with visuals, including extracting data from charts and diagrams, and describing images.\n*   Provide contextually rich and accurate responses.\n\n**Supported Models** include various versions of Gemini (e.g., Gemini 2.5 Pro, Gemini 2.0 Flash).\n**Supported File Types** include PDFs and common image formats (PNG, JPEG, etc.).\n\nThe page also details **pricing**, provides **code examples** (REST and Python) for importing files with the LLM parser enabled, and offers guidance on **prompt engineering** for document parsing, including a template table and general best practices (be specific, detailed, constraining, clear, and structured).\n\nFinally, it shows **parsing quality analysis** examples, demonstrating improvements in linking sections across slides, understanding tables and flowcharts, extracting data from graphs, and capturing heading-text relationships.\n\nWhile the page discusses advanced document understanding capabilities like chart/table extraction and leveraging LLMs for parsing, it **does not explicitly mention** \"multimodal RAG,\" \"document understanding\" as a general term (though it implies it), \"PDF parsing\" (though it supports PDF files), \"report generation with LLMs,\" \"GPT-4V,\" \"Claude vision,\" or \"structured document output\" in the context of the user query's specific terms, other than the general capabilities of the LLM parser.",
    "published_date": "2025-11-18T00:00:00.000Z",
    "score": null
  },
  "https://www.asterlabs.ai/blog/pdf-understanding": {
    "content_path": "data/contents/b9cf29ec12ec7a0a.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.770476",
    "title": "How to add PDF understanding to your AI Agent",
    "summary": "The webpage explains how to add PDF understanding to an AI Agent by leveraging **multimodal** capabilities of modern LLMs like GPT-4V, Claude vision, and Gemini.\n\nThe core solution involves:\n1.  **Text Extraction:** Converting the readable text from the PDF into a string using tools like `PyMuPDF` (Python) or `pdf-parse` (JavaScript).\n2.  **Image Conversion:** Converting each page of the PDF into an image format (PNG) so that vision models can process the visual layout.\n\nThe article highlights that relying solely on traditional OCR loses crucial context like table structure and image information. By sending **both the extracted text and the page images** to a multimodal LLM, the agent achieves a much better understanding of the document's content. The ordering of text and images in the prompt matters, with Anthropic preferring images before text, and OpenAI preferring text before images.\n\nFor **Retrieval Augmented Generation (RAG)** applications, the process extends to:\n*   Chunking both the text and LLM-generated descriptions of the content.\n*   Generating embeddings for retrieval.\n*   Storing the corresponding image file path with the text chunk so that relevant images can be retrieved and passed to the LLM alongside the text during a query.",
    "published_date": "2024-12-15T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2411.04952v1": {
    "content_path": "../data/contents/b3a72f5cbbff03e8.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:37.287228",
    "title": "M3DocRAG : Multi-modal Retrieval is What You Need \n for Multi-page Multi-document Understanding",
    "summary": "The webpage describes **M3DocRAG**, a novel **multi-modal Retrieval-Augmented Generation (RAG) framework** designed for understanding multi-page and multi-document contexts.\n\nKey aspects relevant to your query include:\n\n*   **Vision-Language Models (MLMs) and Multimodal RAG:** M3DocRAG uses a **multi-modal retriever (ColPali)** to find relevant document pages (as images) and then employs a **Multi-modal Language Model (MLM)** (like Qwen2-VL) for question answering based on the retrieved visual and textual information. This contrasts with text-based RAG, which ignores visual elements.\n*   **Document Understanding and PDF Parsing:** The framework is designed to handle complex document contexts, including information spread across multiple pages or documents. It processes document pages as **RGB images**, allowing it to inherently handle visual elements.\n*   **Chart/Table Extraction:** The framework accommodates various evidence modalities, including **text, chart, and figure**, by operating on the visual representation of the document pages.\n*   **Report Generation with LLMs:** The final stage involves using an MLM to **generate the final answer** based on the retrieved multi-modal evidence.\n*   **Specific Models Mentioned:** The paper specifically mentions using **ColPali** as the retrieval model and **Qwen2-VL 7B** as the default MLM, achieving state-of-the-art results on several benchmarks. While **GPT-4V, Claude vision, and Gemini** are not explicitly used or detailed as components in the M3DocRAG pipeline described, the framework is designed to be flexible and could potentially integrate them.\n*   **Structured Document Output:** The goal is to answer questions accurately, which implies generating structured textual output (the answer), though the paper focuses more on the retrieval and VQA generation process rather than specific structured output formats.\n\nIn summary, M3DocRAG is a multi-modal RAG system that leverages vision-language models to overcome the limitations of traditional text-only RAG and single-page VQA systems when dealing with complex, multi-modal, multi-page documents.",
    "published_date": "2019-01-02T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/centric-tech-views/ready-to-move-your-ai-from-text-to-vision-heres-your-guide-to-multimodal-rag-4679f7b58e23": {
    "content_path": "../data/contents/62c9906044d59ec9.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:37.262507",
    "title": "Ready to Move Your AI From Text to Vision? Here’s Your Guide to Multimodal RAG",
    "summary": "Multimodal RAG (Retrieval-Augmented Generation) is an AI approach that allows systems to understand both text and visual components (like diagrams, charts, and graphs) within technical documents simultaneously, making information retrieval more intuitive.\n\n**Key aspects covered in the page:**\n\n*   **What it is:** It combines traditional text-based RAG with the ability to \"see\" visual data, using techniques like **Contrastive Language-Image Pretraining (CLIP)** to create connections between words and images.\n*   **How it works:** It breaks down PDFs into text and images, creates numerical \"fingerprints\" for both, stores them in a smart database, and uses these fingerprints to find relevant information when a user asks a question.\n*   **Use Cases:** It is highly beneficial for technical document analysis, such as analyzing system architecture documents, performance reports (by reading charts), incident reports, and legacy documentation containing diagrams.\n*   **Challenges:** It faces issues with poor input quality (blurry images), requires high computing resources, and has a learning curve for setup.\n*   **Getting Started:** Steps include starting small, getting the right tools (like Python and CLIP), practicing with examples, and being patient.\n*   **Future:** The technology is expected to handle more complex visuals, potentially including video content.\n\nThe page focuses heavily on **Multimodal RAG** for **technical document analysis**, explaining its mechanism and real-life benefits for architects and engineers dealing with documents containing diagrams and charts.",
    "published_date": "2025-10-28T18:39:32.829Z",
    "score": null
  },
  "https://aclanthology.org/2025.emnlp-main.708.pdf": {
    "content_path": "../data/contents/e5ed4412a35b896b.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.349201",
    "title": "",
    "summary": "The webpage describes **MoLoRAG**, a novel **logic-aware retrieval framework** designed for **multi-modal, multi-page Document Understanding (DocQA)**.\n\nHere is a summary tailored to your query:\n\n*   **Vision-Language Models (LVLMs) & Document Understanding:** The paper addresses the limitation of LVLMs in handling multi-page documents due to constrained input sizes. MoLoRAG uses LVLMs for the final question answering step but focuses on improving the preceding retrieval stage.\n*   **Multimodal RAG & PDF Parsing/Chart/Table Extraction:** MoLoRAG is a Retrieval Augmented Generation (RAG) method that works directly with document page images (multi-modal input), bypassing the need for perfect text extraction (like PDF parsing) while still leveraging visual information. It is tested on datasets containing charts and tables (PaperTab, FetaTab).\n*   **Logic-aware Retrieval:** The core innovation is incorporating **logical relevance** alongside semantic relevance. MoLoRAG constructs a **page graph** representing contextual relationships between pages. A lightweight VLM then performs **graph traversal** to identify logically connected pages, which are often missed by semantic-only retrieval methods (like M3DocRAG or MDocAgent).\n*   **GPT-4V, Claude vision, Gemini (General LVLMs):** MoLoRAG is designed to be compatible with **arbitrary LVLMs** (like GPT-4V, Gemini equivalents) for the final generation step, as it only passes the top-K relevant pages to the LVLM.\n*   **Report Generation with LLMs & Structured Document Output:** The final stage uses an LVLM to generate an answer based on the retrieved context. The paper also introduces **MoLoRAG+**, a fine-tuned variant that uses GPT-4o to generate training triplets for better logical relevance scoring, suggesting an approach for improving structured output generation based on retrieved evidence.\n\nIn essence, MoLoRAG enhances existing LVLM-based RAG systems for complex documents by using graph reasoning to find pages that are logically connected to the query, not just semantically similar.\n\nThe webpage text discusses various aspects of **multimodal Retrieval-Augmented Generation (RAG)**, particularly for visually rich documents, and related techniques.\n\nKey topics covered include:\n\n*   **Multimodal RAG Frameworks:** Mentions research papers on \"Visdom: Multi-document qa with visually rich elements using multimodal retrieval-augmented generation\" and hierarchical multimodal transformers for document VQA.\n*   **Document Understanding and Extraction:** References tools and research for precise document content extraction (e.g., Mineru) and general OCR theory (GOT-OCR-2.0).\n*   **Vision-Language Models (VLMs):** Mentions specific models like GPT-4V, Claude vision, Gemini, and Qwen2-vl for enhancing perception.\n*   **MoLoRAG Framework:** A significant portion details the MoLoRAG framework, which uses a combination of **semantic relevance** and **logical relevance** scores (derived from a VLM) within a graph traversal algorithm for document retrieval.\n*   **Report Generation/Question Answering:** Describes how LLMs and LVLMs are queried for question answering based on retrieved context/images.\n*   **Data Curation:** Details the process of using GPT-4o (or Qwen2.5-VL-32B) to generate high-quality training data for fine-tuning retrieval components based on relevance scores.\n*   **Performance Analysis:** Compares MoLoRAG against baselines (LLM w. Text RAG, Direct LVLM Inference, M3DocRAG, MDocAgent) across metrics like Recall, Precision, and efficiency (inference time). It shows MoLoRAG excels in cross-page reasoning and maintains efficiency.\n*   **Ablation Studies:** Evaluates the necessity of combining semantic and logical relevance, concluding that the combination is superior to relying on logical relevance alone.\n\nIn summary, the page focuses heavily on an advanced multimodal RAG system called **MoLoRAG** that leverages graph structures and VLM-derived logical relevance alongside traditional semantic relevance for improved document retrieval and question answering.",
    "published_date": "2025-11-04T00:00:00.000Z",
    "score": null
  },
  "https://browse.arxiv.org/html/2401.02384v1": {
    "content_path": "../data/contents/f26f01541c3799fb.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.366224",
    "title": "A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning",
    "summary": "The webpage describes **ChartAssistant**, a novel vision-language model specifically designed for **universal chart comprehension and reasoning**.\n\nKey aspects relevant to your query include:\n\n*   **Vision-Language Models (VLM) for Charts:** ChartAssistant is a VLM built on two variants: ChartAst-D (based on Donut) and ChartAst-S (based on SPHINX, leveraging LLMs).\n*   **Training Methodology:** It uses a two-stage training process on a comprehensive chart-specific benchmark called **ChartSFT**:\n    1.  **Pre-training on Chart-to-Table Translation:** This step aligns the chart visual information with its structured text representation (like a table). This directly relates to **document understanding** and **PDF parsing/chart extraction** aspects of your query, as charts are a form of structured document.\n    2.  **Multitask Instruction Tuning:** Fine-tuning on various chart-related tasks (QA, summarization, etc.) without requiring task-specific fine-tuning for downstream performance.\n*   **Performance:** ChartAssistant reportedly outperforms previous state-of-the-art methods like UniChart and even advanced general-purpose models like **GPT-4V(ision)** on real-world chart data.\n*   **Chart Extraction/Understanding:** The model excels at tasks like **chart-to-table translation** (extracting structured data from charts), **chart numerical QA** (requiring mathematical reasoning), and **chart summarization**.\n\nWhile the page focuses specifically on charts rather than general multimodal RAG or report generation from arbitrary documents, it details advanced techniques for **vision-language models** applied to **structured document understanding (charts)**, including **extraction** (chart-to-table) and **reasoning**, which are core components of your query.",
    "published_date": "2024-01-04T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2403.12596": {
    "content_path": "../data/contents/9f44f1218ff1a522.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.377301",
    "title": "Computer Science > Computation and Language",
    "summary": "The webpage describes a paper titled \"Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs.\" The research proposes a technique to transfer reasoning capabilities from Large Language Models (LLMs) to Vision-Language Models (VLMs), specifically addressing the limited reasoning abilities of smaller VLMs.\n\nThe method involves:\n1. Improving chart representation by continuing pre-training using an improved chart-to-table translation task.\n2. Constructing a much larger dataset.\n3. Synthesizing reasoning traces using the table representation of charts to improve general reasoning and numerical operations.\n4. Fine-tuning the model using a multitask loss.\n\nThe resulting model, **ChartPaLI-5B**, achieves state-of-the-art performance on the ChartQA dataset and shows significant improvements on PlotQA and FigureQA. Furthermore, when rationales are refined, the model outperforms **Gemini Ultra** and **GPT-4V**.\n\nWhile the page discusses **Vision-language models (VLMs)**, **chart/table extraction** (implied by chart-to-table translation), and **report generation** (implied by reasoning and output capabilities), it does not explicitly detail **multimodal RAG**, **document understanding**, **PDF parsing**, or the specific use of **Claude vision** or **structured document output** beyond chart reasoning.",
    "published_date": "2024-03-19T00:00:00.000Z",
    "score": null
  },
  "https://github.com/BastinFlorian/multimodal-analysis-for-table-and-charts-gpt4-vision/blob/main/tutorials/1_extract_pdf_table.ipynb": {
    "content_path": "../data/contents/f368a80866f325fe.md",
    "source": "exa",
    "saved_at": "2025-12-26T23:59:35.383724",
    "title": "multimodal-analysis-for-table-and-charts-gpt4-vision/tutorials/1_extract_pdf_table.ipynb at main · BastinFlorian/multimodal-analysis-for-table-and-charts-gpt4-vision",
    "summary": "No answer found",
    "published_date": null,
    "score": null
  },
  "https://blog.google/products/gemini/gemini-3/": {
    "content_path": "../data/contents/39d91b15c6d3876b.md",
    "source": "jina",
    "saved_at": "2025-12-28T12:40:23.920433",
    "title": "A new era of intelligence with Gemini 3"
  },
  "https://openai.com/index/introducing-gpt-5-2/": {
    "content_path": "../data/contents/98e47f285e915a34.md",
    "source": "jina",
    "saved_at": "2025-12-28T12:40:24.609277",
    "title": "Introducing GPT-5.2"
  },
  "https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills": {
    "content_path": "../data/contents/2e50ab69ebea3b2c.md",
    "source": "jina",
    "saved_at": "2025-12-28T12:40:24.774502",
    "title": "Equipping agents for the real world with Agent Skills"
  },
  "https://openai.com/index/gpt-5-2-codex": {
    "content_path": "../data/contents/bcec01148b652923.md",
    "source": "jina",
    "saved_at": "2025-12-28T12:40:25.209071",
    "title": "Introducing GPT-5.2-Codex"
  },
  "https://arxiv.org/abs/2512.10573v1": {
    "content_path": "../data/contents/2512.10573v1.md",
    "pdf_path": "../data/contents/2512.10573v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:42:39.178329",
    "title": "Is the Information Bottleneck Robust Enough? Towards Label-Noise Resistant Information Bottleneck Learning",
    "abstract": "Summary: The Information Bottleneck (IB) principle facilitates effective representation learning by preserving label-relevant information while compressing irrelevant information. However, its strong reliance on accurate labels makes it inherently vulnerable to label noise, prevalent in real-world scenarios, resulting in significant performance degradation and overfitting. To address this issue, we propose LaT-IB, a novel Label-Noise ResistanT Information Bottleneck method which introduces a \"Minimal-Sufficient-Clean\" (MSC) criterion. Instantiated as a mutual information regularizer to retain task-relevant information while discarding noise, MSC addresses standard IB's vulnerability to noisy label supervision. To achieve this, LaT-IB employs a noise-aware latent disentanglement that decomposes the latent representation into components aligned with to the clean label space and the noise space. Theoretically, we first derive mutual information bounds for each component of our objective including prediction, compression, and disentanglement, and moreover prove that optimizing it encourages representations invariant to input noise and separates clean and noisy label information. Furthermore, we design a three-phase training framework: Warmup, Knowledge Injection and Robust Training, to progressively guide the model toward noise-resistant representations. Extensive experiments demonstrate that LaT-IB achieves superior robustness and efficiency under label noise, significantly enhancing robustness and applicability in real-world scenarios with label noise.",
    "paper_id": "2512.10573v1"
  },
  "https://arxiv.org/abs/2512.10547v1": {
    "content_path": "../data/contents/2512.10547v1.md",
    "pdf_path": "../data/contents/2512.10547v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:42:40.950431",
    "title": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders",
    "abstract": "Summary: The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.",
    "paper_id": "2512.10547v1"
  },
  "https://arxiv.org/abs/2512.10561v1": {
    "content_path": "../data/contents/2512.10561v1.md",
    "pdf_path": "../data/contents/2512.10561v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:42:45.577765",
    "title": "Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models",
    "abstract": "Summary: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.",
    "paper_id": "2512.10561v1"
  },
  "https://arxiv.org/abs/2512.10563v1": {
    "content_path": "../data/contents/2512.10563v1.md",
    "pdf_path": "../data/contents/2512.10563v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:42:45.912206",
    "title": "NormCode: A Semi-Formal Language for Context-Isolated AI Planning",
    "abstract": "Summary: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.",
    "paper_id": "2512.10563v1"
  },
  "https://arxiv.org/html/2509.00100v1": {
    "content_path": "../data/contents/f87d24d014700da5.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.381085",
    "title": "MODE: Mixture of Document Experts for RAG",
    "summary": "The webpage describes **MODE (Mixture of Document Experts)**, a lightweight and efficient alternative framework for Retrieval-Augmented Generation (RAG), particularly suited for small to medium-sized datasets.\n\nHere is a summary addressing the components mentioned in your query:\n\n*   **Vector databases:** MODE **eliminates the need for dedicated vector databases** by replacing fine-grained vector search with a cluster-and-route mechanism.\n*   **Embeddings (new efficient models):** The framework uses embeddings for initial chunking and for calculating cluster centroids, but the core efficiency gain comes from avoiding large-scale vector search over these embeddings during inference.\n*   **Rerankers:** MODE **eliminates the need for re-rankers** entirely, as the cluster-and-route mechanism aims to provide highly topical context directly, reducing latency.\n*   **RAG architectures:** MODE is presented as an **architectural alternative** to the standard \"index-retrieve-rerank\" pipeline. It partitions the corpus into semantically coherent clusters (\"document experts\") and routes queries to the most relevant cluster using fast centroid-based matching.\n*   **RAG alternatives:** MODE itself is the proposed alternative, focusing on simplicity and efficiency for non-web-scale corpora.\n*   **Hybrid search:** The paper does not explicitly discuss hybrid search (combining sparse and dense retrieval). It focuses on replacing dense retrieval with a clustering approach.\n*   **Chunking strategies:** The ingestion phase includes a step for **Chunking and Embedding**. The paper mentions using 300-token windows with 15% overlap as a default, and defines corpus sizes based on the number of chunks (small=100, medium=200, large=500).\n\nIn essence, MODE achieves efficiency by performing **cluster-based routing** instead of nearest-neighbor search across a massive vector index, thereby simplifying the RAG architecture significantly.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2512.19986v1": {
    "content_path": "../data/contents/2512.19986v1.md",
    "pdf_path": "../data/contents/2512.19986v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:48:58.471854",
    "title": "Covariance-Aware Simplex Projection for Cardinality-Constrained Portfolio Optimization",
    "abstract": "Summary: Metaheuristic algorithms for cardinality-constrained portfolio optimization require repair operators to map infeasible candidates onto the feasible region. Standard Euclidean projection treats assets as independent and can ignore the covariance structure that governs portfolio risk, potentially producing less diversified portfolios. This paper introduces Covariance-Aware Simplex Projection (CASP), a two-stage repair operator that (i) selects a target number of assets using volatility-normalized scores and (ii) projects the candidate weights using a covariance-aware geometry aligned with tracking-error risk. This provides a portfolio-theoretic foundation for using a covariance-induced distance in repair operators. On S&P 500 data (2020-2024), CASP-Basic delivers materially lower portfolio variance than standard Euclidean repair without relying on return estimates, with improvements that are robust across assets and statistically significant. Ablation results indicate that volatility-normalized selection drives most of the variance reduction, while the covariance-aware projection provides an additional, consistent improvement. We further show that optional return-aware extensions can improve Sharpe ratios, and out-of-sample tests confirm that gains transfer to realized performance. CASP integrates as a drop-in replacement for Euclidean projection in metaheuristic portfolio optimizers.",
    "paper_id": "2512.19986v1"
  },
  "https://arxiv.org/abs/2512.19958v1": {
    "content_path": "../data/contents/2512.19958v1.md",
    "pdf_path": "../data/contents/2512.19958v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:48:58.661072",
    "title": "Towards Analysing Invoices and Receipts with Amazon Textract",
    "abstract": "Summary: This paper presents an evaluation of the AWS Textract in the context of extracting data from receipts. We analyse Textract functionalities using a dataset that includes receipts of varied formats and conditions. Our analysis provided a qualitative view of Textract strengths and limitations. While the receipts totals were consistently detected, we also observed typical issues and irregularities that were often influenced by image quality and layout. Based on the analysis of the observations, we propose mitigation strategies.",
    "paper_id": "2512.19958v1"
  },
  "https://arxiv.org/abs/2512.19950v1": {
    "content_path": "../data/contents/2512.19950v1.md",
    "pdf_path": "../data/contents/2512.19950v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:48:58.893311",
    "title": "Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems",
    "abstract": "Summary: Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.",
    "paper_id": "2512.19950v1"
  },
  "https://arxiv.org/abs/2512.19949v1": {
    "content_path": "../data/contents/2512.19949v1.md",
    "pdf_path": "../data/contents/2512.19949v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:02.969058",
    "title": "How Much 3D Do Video Foundation Models Encode?",
    "abstract": "Summary: Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.",
    "paper_id": "2512.19949v1"
  },
  "https://arxiv.org/abs/2512.19941v1": {
    "content_path": "../data/contents/2512.19941v1.md",
    "pdf_path": "../data/contents/2512.19941v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:04.199723",
    "title": "Block-Recurrent Dynamics in Vision Transformers",
    "abstract": "Summary: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \\ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.",
    "paper_id": "2512.19941v1"
  },
  "https://arxiv.org/abs/2512.19937v1": {
    "content_path": "../data/contents/2512.19937v1.md",
    "pdf_path": "../data/contents/2512.19937v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:04.681414",
    "title": "Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs",
    "abstract": "Summary: Recent research has explored using very large language models (LLMs) as proxies for humans in tasks such as simulation, surveys, and studies. While LLMs do not possess a human psychology, they often can emulate human behaviors with sufficiently high fidelity to drive simulations to test human behavioral hypotheses, exhibiting more nuance and range than the rule-based agents often employed in behavioral economics. One key area of interest is the effect of personality on decision making, but the requirement that a prompt must be created for every tested personality profile introduces experimental overhead and degrades replicability. To address this issue, we leverage interpolative decoding, representing each dimension of personality as a pair of opposed prompts and employing an interpolation parameter to simulate behavior along the dimension. We show that interpolative decoding reliably modulates scores along each of the Big Five dimensions. We then show how interpolative decoding causes LLMs to mimic human decision-making behavior in economic games, replicating results from human psychological research. Finally, we present preliminary results of our efforts to ``twin'' individual human players in a collaborative game through systematic search for points in interpolation space that cause the system to replicate actions taken by the human subject.",
    "paper_id": "2512.19937v1"
  },
  "https://arxiv.org/abs/2512.19908v1": {
    "content_path": "../data/contents/2512.19908v1.md",
    "pdf_path": "../data/contents/2512.19908v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:06.074189",
    "title": "Counterfactual LLM-based Framework for Measuring Rhetorical Style",
    "abstract": "Summary: The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.",
    "paper_id": "2512.19908v1"
  },
  "https://arxiv.org/abs/2512.20136": {
    "content_path": "../data/contents/2512.20136.md",
    "pdf_path": "../data/contents/2512.20136.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:07.872026",
    "title": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation",
    "abstract": "Recent multimodal RAG approach that retrieves and prunes multi-hop audio-visual knowledge from multimodal knowledge graphs for more grounded MLLM reasoning.",
    "paper_id": "2512.20136"
  },
  "https://arxiv.org/abs/2512.20688v1": {
    "content_path": "../data/contents/2512.20688v1.md",
    "pdf_path": "../data/contents/2512.20688v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:08.671652",
    "title": "Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems",
    "abstract": "Summary: Autonomous multi-agent systems are fundamentally fragile: they struggle to solve the Hayekian Information problem (eliciting dispersed private knowledge) and the Hurwiczian Incentive problem (aligning local actions with global objectives), making coordination computationally intractable. I introduce Mechanism-Based Intelligence (MBI), a paradigm that reconceptualizes intelligence as emergent from the coordination of multiple \"brains\", rather than a single one. At its core, the Differentiable Price Mechanism (DPM) computes the exact loss gradient $$ \\mathbf{G}_i = - \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_i} $$ as a dynamic, VCG-equivalent incentive signal, guaranteeing Dominant Strategy Incentive Compatibility (DSIC) and convergence to the global optimum. A Bayesian extension ensures incentive compatibility under asymmetric information (BIC). The framework scales linearly ($\\mathcal{O}(N)$) with the number of agents, bypassing the combinatorial complexity of Dec-POMDPs and is empirically 50x faster than Model-Free Reinforcement Learning. By structurally aligning agent self-interest with collective objectives, it provides a provably efficient, auditable and generalizable approach to coordinated, trustworthy and scalable multi-agent intelligence grounded in economic principles.",
    "paper_id": "2512.20688v1"
  },
  "https://arxiv.org/abs/2512.10888": {
    "content_path": "../data/contents/2512.10888.md",
    "pdf_path": "../data/contents/2512.10888.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:12.285864",
    "title": "PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction",
    "abstract": "Introduces a large benchmark for full-page and multi-page table extraction plus the POTATR model, directly targeting table structure extraction from PDFs.",
    "paper_id": "2512.10888"
  },
  "https://arxiv.org/abs/2512.19905v1": {
    "content_path": "../data/contents/2512.19905v1.md",
    "pdf_path": "../data/contents/2512.19905v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:14.696726",
    "title": "Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling",
    "abstract": "Summary: Recent developments in large language models have shown advantages in reallocating a notable share of computational resource from training time to inference time. However, the principles behind inference time scaling are not well understood. In this paper, we introduce an analytically tractable model of inference-time scaling: Bayesian linear regression with a reward-weighted sampler, where the reward is determined from a linear model, modeling LLM-as-a-judge scenario. We study this problem in the high-dimensional regime, where the deterministic equivalents dictate a closed-form expression for the posterior predictive mean and variance. We analyze the generalization error when training data are sampled from a teacher model. We draw $k$ inference-time samples and select via softmax at a temperature applied to a quadratic reward. When the reward is not too different from the teacher, the generalization error decreases monotonically with increasing inference time samples $k$. However, the specific reward that optimizes inference-time selection generally differs from the teacher. In contrast, substantial reward misspecification induces a finite optimal $k$ beyond which more sampling can increase the generalization error. For fixed $k$, there exists an optimal sampling temperature. We experimentally verify these facts in large language model inference with an additional large language model as a judge. In the \"best-of-$k$\" limit with the teacher as reward, we theoretically show that the generalization error decays as $Θ(1/k^2)$ and determine the leading coefficient via extreme value theory. These formulas delineate domains where scaling inference-time computation is provably preferable to collecting more data. Finally, we demonstrate that when task difficulty increases, the previously mentioned advantage of inference-time compute degrades.",
    "paper_id": "2512.19905v1"
  },
  "https://arxiv.org/abs/2512.02498": {
    "content_path": "../data/contents/2512.02498.md",
    "pdf_path": "../data/contents/2512.02498.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:23.630717",
    "title": "dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model",
    "abstract": "End-to-end VLM for layout parsing + text recognition + reading order across languages; introduces multilingual benchmark components (XDocParse).",
    "paper_id": "2512.02498"
  },
  "https://arxiv.org/abs/2512.19879v1": {
    "content_path": "../data/contents/2512.19879v1.md",
    "pdf_path": "../data/contents/2512.19879v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:27.633489",
    "title": "Fine-Tuned In-Context Learners for Efficient Adaptation",
    "abstract": "Summary: When adapting large language models (LLMs) to a specific downstream task, two primary approaches are commonly employed: (1) prompt engineering, often with in-context few-shot learning, leveraging the model's inherent generalization abilities, and (2) fine-tuning on task-specific data, directly optimizing the model's parameters. While prompt-based methods excel in few-shot scenarios, their effectiveness often plateaus as more data becomes available. Conversely, fine-tuning scales well with data but may underperform when training examples are scarce. We investigate a unified approach that bridges these two paradigms by incorporating in-context learning directly into the fine-tuning process. Specifically, we fine-tune the model on task-specific data augmented with in-context examples, mimicking the structure of k-shot prompts. This approach, while requiring per-task fine-tuning, combines the sample efficiency of in-context learning with the performance gains of fine-tuning, leading to a method that consistently matches and often significantly exceeds both these baselines. To perform hyperparameter selection in the low-data regime, we propose to use prequential evaluation, which eliminates the need for expensive cross-validation and leverages all available data for training while simultaneously providing a robust validation signal. We conduct an extensive empirical study to determine which adaptation paradigm - fine-tuning, in-context learning, or our proposed unified approach offers the best predictive performance on a concrete data downstream-tasks.",
    "paper_id": "2512.19879v1"
  },
  "https://arxiv.org/abs/2512.19849v1": {
    "content_path": "../data/contents/2512.19849v1.md",
    "pdf_path": "../data/contents/2512.19849v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:28.097170",
    "title": "UCCL-EP: Portable Expert-Parallel Communication",
    "abstract": "Summary: Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.\n  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.",
    "paper_id": "2512.19849v1"
  },
  "https://arxiv.org/abs/2511.18434": {
    "content_path": "../data/contents/2511.18434.md",
    "pdf_path": "../data/contents/2511.18434.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:50.933775",
    "title": "DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation",
    "abstract": "Benchmark focusing on real photographed documents (distortion/lighting) for robust parsing + translation—more realistic than scanned-PDF-only evals.",
    "paper_id": "2511.18434"
  },
  "https://arxiv.org/abs/2512.19832v1": {
    "content_path": "../data/contents/2512.19832v1.md",
    "pdf_path": "../data/contents/2512.19832v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:51.892769",
    "title": "How Tech Workers Contend with Hazards of Humanlikeness in Generative AI",
    "abstract": "Summary: Generative AI's humanlike qualities are driving its rapid adoption in professional domains. However, this anthropomorphic appeal raises concerns from HCI and responsible AI scholars about potential hazards and harms, such as overtrust in system outputs. To investigate how technology workers navigate these humanlike qualities and anticipate emergent harms, we conducted focus groups with 30 professionals across six job functions (ML engineering, product policy, UX research and design, product management, technology writing, and communications). Our findings reveal an unsettled knowledge environment surrounding humanlike generative AI, where workers' varying perspectives illuminate a range of potential risks for individuals, knowledge work fields, and society. We argue that workers require comprehensive support, including clearer conceptions of ``humanlikeness'' to effectively mitigate these risks. To aid in mitigation strategies, we provide a conceptual map articulating the identified hazards and their connection to conflated notions of ``humanlikeness.''",
    "paper_id": "2512.19832v1"
  },
  "https://arxiv.org/abs/2512.20687v1": {
    "content_path": "../data/contents/2512.20687v1.md",
    "pdf_path": "../data/contents/2512.20687v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:49:52.895500",
    "title": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation",
    "abstract": "Summary: Transformers operate as horizontal token-by-token scanners; at each generation step, the model attends to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding increasingly memory-bound, as KV-cache reads and writes dominate inference throughput rather than arithmetic computation. We propose Parallel Hierarchical Operation for Top-down Networks (PHOTON), a hierarchical autoregressive model that replaces flat scanning with vertical, multi-resolution context access. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder progressively compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, offering significant advantages in long-context and multi-query tasks. This reduces decode-time KV-cache traffic, yielding up to $10^{3}\\times$ higher throughput per unit memory.",
    "paper_id": "2512.20687v1"
  },
  "https://arxiv.org/abs/2511.10390": {
    "content_path": "../data/contents/2511.10390.md",
    "pdf_path": "../data/contents/2511.10390.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:50:07.269723",
    "title": "MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns",
    "abstract": "Document parsing system targeting complex layouts (multi-level tables, formulas, cross-page structures) with RL-style verification for table parsing.",
    "paper_id": "2511.10390"
  },
  "https://arxiv.org/abs/2512.19805v1": {
    "content_path": "../data/contents/2512.19805v1.md",
    "pdf_path": "../data/contents/2512.19805v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:50:07.460362",
    "title": "Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy",
    "abstract": "Summary: This paper introduces a marketing decision framework that converts heterogeneous-treatment uplift into constrained targeting strategies to maximize revenue and retention while honoring business guardrails. The approach estimates Conditional Average Treatment Effects (CATE) with uplift learners and then solves a constrained allocation to decide who to target and which offer to deploy under limits such as budget or acceptable sales deterioration. Applied to retention messaging, event rewards, and spend-threshold assignment, the framework consistently outperforms propensity and static baselines in offline evaluations using uplift AUC, Inverse Propensity Scoring (IPS), and Self-Normalized IPS (SNIPS). A production-scale online A/B test further validates strategic lift on revenue and completion while preserving customer-experience constraints. The result is a reusable playbook for marketers to operationalize causal targeting at scale, set guardrails, and align campaigns with strategic KPIs.",
    "paper_id": "2512.19805v1"
  },
  "https://arxiv.org/abs/2512.19691v1": {
    "content_path": "../data/contents/2512.19691v1.md",
    "pdf_path": "../data/contents/2512.19691v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:50:10.637113",
    "title": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight",
    "abstract": "Summary: Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.",
    "paper_id": "2512.19691v1"
  },
  "https://arxiv.org/abs/2510.18234": {
    "content_path": "../data/contents/2510.18234.md",
    "pdf_path": "../data/contents/2510.18234.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:50:12.251280",
    "title": "DeepSeek-OCR: Contexts Optical Compression",
    "abstract": "Explores “vision-text compression” by mapping long text/docs into 2D images to reduce tokens for long-context document understanding/OCR.",
    "paper_id": "2510.18234"
  },
  "https://arxiv.org/abs/2512.19682v2": {
    "content_path": "../data/contents/2512.19682v2.md",
    "pdf_path": "../data/contents/2512.19682v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:50:12.524245",
    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
    "abstract": "Summary: Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
    "paper_id": "2512.19682v2"
  },
  "https://arxiv.org/abs/2512.19644v1": {
    "content_path": "../data/contents/2512.19644v1.md",
    "pdf_path": "../data/contents/2512.19644v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:50:14.286268",
    "title": "More code, less validation: Risk factors for over-reliance on AI coding tools among scientists",
    "abstract": "Summary: Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.",
    "paper_id": "2512.19644v1"
  },
  "https://arxiv.org/abs/2510.14528": {
    "content_path": "../data/contents/2510.14528.md",
    "pdf_path": "../data/contents/2510.14528.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:50:35.064316",
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
    "abstract": "Ultra-compact multilingual document parsing VLM (tables/formulas/charts) with strong efficiency—relevant for production OCR+layout extraction.",
    "paper_id": "2510.14528"
  },
  "https://arxiv.org/abs/2512.19620v1": {
    "content_path": "../data/contents/2512.19620v1.md",
    "pdf_path": "../data/contents/2512.19620v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:50:35.280519",
    "title": "Exploring the features used for summary evaluation by Human and GPT",
    "abstract": "Summary: Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
    "paper_id": "2512.19620v1"
  },
  "https://arxiv.org/abs/2512.19606v1": {
    "content_path": "../data/contents/2512.19606v1.md",
    "pdf_path": "../data/contents/2512.19606v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:50:36.054454",
    "title": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
    "abstract": "Summary: RAPID-LLM is a unified performance modeling framework for large language model (LLM) training and inference on GPU clusters. It couples a DeepFlow-based frontend that generates hardware-aware, operator-level Chakra execution traces from an abstract LLM specification (model shape, batch/sequence settings, training vs. inference, and hybrid parallelism choices) with an extended Astra-Sim backend that executes those traces on explicit multi-dimensional network topologies with congestion-aware routing and support for degraded and faulty links. The frontend assigns per-operator latency using a tile-based model that accounts for SM under-utilization and multi-level memory traffic (SRAM/ L2/ HBM), and prunes memory-infeasible configurations using an activation-liveness traversal under recomputation, parallelism and ZeRO/FDSP sharding policies.\n  Across A100-based validation cases, RAPID-LLM predicts Llama inference step latency and GPT-scale training time per batch within 10.4\\% relative to published measurements, and matches ns-3 packet-level results within 8\\% on representative communication workloads. Case studies demonstrate how RAPID-LLM enables fast, exhaustive sweeps over hybrid-parallel configurations, quantifies sensitivity to soft link faults under realistic routing and congestion, and evaluates hypothetical GPU design variants including HBM bandwidth throttling effects.",
    "paper_id": "2512.19606v1"
  },
  "https://arxiv.org/abs/2509.22186": {
    "content_path": "../data/contents/2509.22186.md",
    "pdf_path": "../data/contents/2509.22186.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:50:58.471145",
    "title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing",
    "abstract": "Two-stage high-resolution document parsing VLM (layout then crop-based recognition) designed for efficient, accurate PDF-to-structured extraction.",
    "paper_id": "2509.22186"
  },
  "https://arxiv.org/abs/2512.19557v1": {
    "content_path": "../data/contents/2512.19557v1.md",
    "pdf_path": "../data/contents/2512.19557v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:02.611766",
    "title": "Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations",
    "abstract": "Summary: Current approaches to Explainable AI (XAI) face a \"Scalability-Stability Dilemma.\" Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel \"Asymmetry of Discovery.\" When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad \"Safety Nets\" (retention patterns) but struggle to capture specific \"Risk Traps\" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of \"Rule Writers\" to \"Exception Handlers.\"",
    "paper_id": "2512.19557v1"
  },
  "https://arxiv.org/abs/2510.03663": {
    "content_path": "../data/contents/2510.03663.md",
    "pdf_path": "../data/contents/2510.03663.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:05.167694",
    "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG",
    "abstract": "Large-scale benchmark for document-centric multimodal RAG built from real PDFs; compares text-only vs image-only vs fusion paradigms.",
    "paper_id": "2510.03663"
  },
  "https://arxiv.org/abs/2512.19554v1": {
    "content_path": "../data/contents/2512.19554v1.md",
    "pdf_path": "../data/contents/2512.19554v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:06.065592",
    "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
    "abstract": "Summary: Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",
    "paper_id": "2512.19554v1"
  },
  "https://arxiv.org/abs/2512.19537v1": {
    "content_path": "../data/contents/2512.19537v1.md",
    "pdf_path": "../data/contents/2512.19537v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:09.653863",
    "title": "Event Extraction in Large Language Model",
    "abstract": "Summary: Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.",
    "paper_id": "2512.19537v1"
  },
  "https://arxiv.org/abs/2412.07626": {
    "content_path": "../data/contents/2412.07626.md",
    "pdf_path": "../data/contents/2412.07626.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:16.570163",
    "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
    "abstract": "Foundational benchmark for diverse PDF parsing with detailed annotations/metrics across text, tables, formulas, and reading order.",
    "paper_id": "2412.07626"
  },
  "https://arxiv.org/abs/2512.19535v1": {
    "content_path": "../data/contents/2512.19535v1.md",
    "pdf_path": "../data/contents/2512.19535v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:17.850207",
    "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
    "abstract": "Summary: Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
    "paper_id": "2512.19535v1"
  },
  "https://arxiv.org/abs/2512.19527v1": {
    "content_path": "../data/contents/2512.19527v1.md",
    "pdf_path": "../data/contents/2512.19527v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:18.092737",
    "title": "Deep Learning for Unrelated-Machines Scheduling: Handling Variable Dimensions",
    "abstract": "Summary: Deep learning has been effectively applied to many discrete optimization problems. However, learning-based scheduling on unrelated parallel machines remains particularly difficult to design. Not only do the numbers of jobs and machines vary, but each job-machine pair has a unique processing time, dynamically altering feature dimensions. We propose a novel approach with a neural network tailored for offline deterministic scheduling of arbitrary sizes on unrelated machines. The goal is to minimize a complex objective function that includes the makespan and the weighted tardiness of jobs and machines. Unlike existing online approaches, which process jobs sequentially, our method generates a complete schedule considering the entire input at once. The key contribution of this work lies in the sophisticated architecture of our model. By leveraging various NLP-inspired architectures, it effectively processes any number of jobs and machines with varying feature dimensions imposed by unrelated processing times. Our approach enables supervised training on small problem instances while demonstrating strong generalization to much larger scheduling environments. Trained and tested on instances with 8 jobs and 4 machines, costs were only 2.51% above optimal. Across all tested configurations of up to 100 jobs and 10 machines, our network consistently outperformed an advanced dispatching rule, which incurred 22.22% higher costs on average. As our method allows fast retraining with simulated data and adaptation to various scheduling conditions, we believe it has the potential to become a standard approach for learning-based scheduling on unrelated machines and similar problem environments.",
    "paper_id": "2512.19527v1"
  },
  "https://arxiv.org/abs/2509.11937": {
    "content_path": "../data/contents/2509.11937.md",
    "pdf_path": "../data/contents/2509.11937.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:18.825633",
    "title": "MMORE: Massive Multimodal Open RAG & Extraction",
    "abstract": "Open pipeline for large-scale multimodal ingestion/extraction across many file types to support multimodal RAG systems in practice.",
    "paper_id": "2509.11937"
  },
  "https://arxiv.org/abs/2512.19526v1": {
    "content_path": "../data/contents/2512.19526v1.md",
    "pdf_path": "../data/contents/2512.19526v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:23.791050",
    "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
    "abstract": "Summary: Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.",
    "paper_id": "2512.19526v1"
  },
  "https://arxiv.org/abs/2510.15253": {
    "content_path": "../data/contents/2510.15253.md",
    "pdf_path": "../data/contents/2510.15253.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:26.275716",
    "title": "Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding",
    "abstract": "Survey/taxonomy of multimodal RAG for document understanding, summarizing retrieval modalities, benchmarks, and open challenges.",
    "paper_id": "2510.15253"
  },
  "https://arxiv.org/abs/2512.19524v1": {
    "content_path": "../data/contents/2512.19524v1.md",
    "pdf_path": "../data/contents/2512.19524v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:26.558279",
    "title": "Initialization of a Polyharmonic Cascade, Launch and Testing",
    "abstract": "Summary: This paper concludes a series of studies on the polyharmonic cascade, a deep machine learning architecture theoretically derived from indifference principles and the theory of random functions. A universal initialization procedure is proposed, based on symmetric constellations in the form of hyperoctahedra with a central point. This initialization not only ensures stable training of cascades with tens and hundreds of layers (up to 500 layers without skip connections), but also radically simplifies the computations. Scalability and robustness are demonstrated on MNIST (98.3% without convolutions or augmentations), HIGGS (AUC approximately 0.885 on 11M examples), and Epsilon (AUC approximately 0.963 with 2000 features). All linear algebra is reduced to 2D operations and is efficiently executed on GPUs. A public repository and an archived snapshot are provided for full reproducibility.",
    "paper_id": "2512.19524v1"
  },
  "https://arxiv.org/abs/2512.19512v2": {
    "content_path": "../data/contents/2512.19512v2.md",
    "pdf_path": "../data/contents/2512.19512v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:26.806485",
    "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
    "abstract": "Summary: Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
    "paper_id": "2512.19512v2"
  },
  "https://arxiv.org/abs/2512.19510v1": {
    "content_path": "../data/contents/2512.19510v1.md",
    "pdf_path": "../data/contents/2512.19510v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:27.109336",
    "title": "Toward Scalable and Valid Conditional Independence Testing with Spectral Representations",
    "abstract": "Summary: Conditional independence (CI) is central to causal inference, feature selection, and graphical modeling, yet it is untestable in many settings without additional assumptions. Existing CI tests often rely on restrictive structural conditions, limiting their validity on real-world data. Kernel methods using the partial covariance operator offer a more principled approach but suffer from limited adaptivity, slow convergence, and poor scalability. In this work, we explore whether representation learning can help address these limitations. Specifically, we focus on representations derived from the singular value decomposition of the partial covariance operator and use them to construct a simple test statistic, reminiscent of the Hilbert-Schmidt Independence Criterion (HSIC). We also introduce a practical bi-level contrastive algorithm to learn these representations. Our theory links representation learning error to test performance and establishes asymptotic validity and power guarantees. Preliminary experiments suggest that this approach offers a practical and statistically grounded path toward scalable CI testing, bridging kernel-based theory with modern representation learning.",
    "paper_id": "2512.19510v1"
  },
  "https://arxiv.org/abs/2512.19491v1": {
    "content_path": "../data/contents/2512.19491v1.md",
    "pdf_path": "../data/contents/2512.19491v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:32.968998",
    "title": "Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico",
    "abstract": "Summary: Detecting fraud and corruption in public procurement remains a major challenge for governments worldwide. Most research to-date builds on domain-knowledge-based corruption risk indicators of individual contract-level features and some also analyzes contracting network patterns. A critical barrier for supervised machine learning is the absence of confirmed non-corrupt, negative, examples, which makes conventional machine learning inappropriate for this task. Using publicly available data on federally funded procurement in Mexico and company sanction records, this study implements positive-unlabeled (PU) learning algorithms that integrate domain-knowledge-based red flags with network-derived features to identify likely corrupt and fraudulent contracts. The best-performing PU model on average captures 32 percent more known positives and performs on average 2.3 times better than random guessing, substantially outperforming approaches based solely on traditional red flags. The analysis of the Shapley Additive Explanations reveals that network-derived features, particularly those associated with contracts in the network core or suppliers with high eigenvector centrality, are the most important. Traditional red flags further enhance model performance in line with expectations, albeit mainly for contracts awarded through competitive tenders. This methodology can support law enforcement in Mexico, and it can be adapted to other national contexts too.",
    "paper_id": "2512.19491v1"
  },
  "https://arxiv.org/abs/2512.19484v1": {
    "content_path": "../data/contents/2512.19484v1.md",
    "pdf_path": "../data/contents/2512.19484v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:40.790295",
    "title": "Structured Event Representation and Stock Return Predictability",
    "abstract": "Summary: We find that event features extracted by large language models (LLMs) are effective for text-based stock return prediction. Using a pre-trained LLM to extract event features from news articles, we propose a novel deep learning model based on structured event representation (SER) and attention mechanisms to predict stock returns in the cross-section. Our SER-based model provides superior performance compared with other existing text-driven models to forecast stock returns out of sample and offers highly interpretable feature structures to examine the mechanisms underlying the stock return predictability. We further provide various implications based on SER and highlight the crucial benefit of structured model inputs in stock return predictability.",
    "paper_id": "2512.19484v1"
  },
  "https://arxiv.org/abs/2512.19481v1": {
    "content_path": "../data/contents/2512.19481v1.md",
    "pdf_path": "../data/contents/2512.19481v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:41.005967",
    "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
    "abstract": "Summary: Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
    "paper_id": "2512.19481v1"
  },
  "https://arxiv.org/abs/2510.10973": {
    "content_path": "../data/contents/2510.10973.md",
    "pdf_path": "../data/contents/2510.10973.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:42.103929",
    "title": "Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning",
    "abstract": "Targets chart understanding with verifiable rewards to improve robustness and explanation fidelity—useful for chart QA and chart-to-text generation.",
    "paper_id": "2510.10973"
  },
  "https://arxiv.org/abs/2512.19475v1": {
    "content_path": "../data/contents/2512.19475v1.md",
    "pdf_path": "../data/contents/2512.19475v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:42.440902",
    "title": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting",
    "abstract": "Summary: Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.",
    "paper_id": "2512.19475v1"
  },
  "https://arxiv.org/abs/2512.19472v1": {
    "content_path": "../data/contents/2512.19472v1.md",
    "pdf_path": "../data/contents/2512.19472v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:43.099354",
    "title": "Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications",
    "abstract": "Summary: The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.",
    "paper_id": "2512.19472v1"
  },
  "https://arxiv.org/abs/2501.17811": {
    "content_path": "../data/contents/2501.17811.md",
    "pdf_path": "../data/contents/2501.17811.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:44.119952",
    "title": "Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling",
    "abstract": "Technical paper on Janus-Pro 7B model achieving significant advances in both understanding and text-to-image generation.",
    "paper_id": "2501.17811"
  },
  "https://arxiv.org/abs/2512.19458v1": {
    "content_path": "../data/contents/2512.19458v1.md",
    "pdf_path": "../data/contents/2512.19458v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:46.959991",
    "title": "An Agentic Framework for Autonomous Materials Computation",
    "abstract": "Summary: Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
    "paper_id": "2512.19458v1"
  },
  "https://arxiv.org/abs/2512.19456v1": {
    "content_path": "../data/contents/2512.19456v1.md",
    "pdf_path": "../data/contents/2512.19456v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:47.199719",
    "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
    "abstract": "Summary: Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.",
    "paper_id": "2512.19456v1"
  },
  "https://arxiv.org/abs/2512.19455v2": {
    "content_path": "../data/contents/2512.19455v2.md",
    "pdf_path": "../data/contents/2512.19455v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:48.016209",
    "title": "SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation",
    "abstract": "Summary: Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines translated high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.",
    "paper_id": "2512.19455v2"
  },
  "https://arxiv.org/abs/2512.19432v1": {
    "content_path": "../data/contents/2512.19432v1.md",
    "pdf_path": "../data/contents/2512.19432v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:50.391735",
    "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
    "abstract": "Summary: Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
    "paper_id": "2512.19432v1"
  },
  "https://arxiv.org/abs/2512.19428v1": {
    "content_path": "../data/contents/2512.19428v1.md",
    "pdf_path": "../data/contents/2512.19428v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:50.613550",
    "title": "Attention Is Not What You Need",
    "abstract": "Summary: We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.\n  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.\n  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.",
    "paper_id": "2512.19428v1"
  },
  "https://arxiv.org/abs/2512.19426v1": {
    "content_path": "../data/contents/2512.19426v1.md",
    "pdf_path": "../data/contents/2512.19426v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:52.969850",
    "title": "A Computationally Efficient Framework for Overlapping Community Detection in Large Bipartite Graphs",
    "abstract": "Summary: Community detection, which uncovers closely connected vertex groups in networks, is vital for applications in social networks, recommendation systems, and beyond. Real-world networks often have bipartite structures (vertices in two disjoint sets with inter-set connections), creating unique challenges on specialized community detection methods. Biclique percolation community (BCPC) is widely used to detect cohesive structures in bipartite graphs. A biclique is a complete bipartite subgraph, and a BCPC forms when maximal bicliques connect via adjacency (sharing an (alpha, beta)-biclique). Yet, existing methods for BCPC detection suffer from high time complexity due to the potentially massive maximal biclique adjacency graph (MBAG). To tackle this, we propose a novel partial-BCPC based solution, whose key idea is to use partial-BCPC to reduce the size of the MBAG. A partial-BCPC is a subset of BCPC. Maximal bicliques belonging to the same partial-BCPC must also belong to the same BCPC. Therefore, these maximal bicliques can be grouped as a single vertex in the MBAG, significantly reducing the size of the MBAG. Furthermore, we move beyond the limitations of MBAG and propose a novel BCPC detection approach based on (alpha, beta)-biclique enumeration. This approach detects BCPC by enumerating all (alpha, beta)-bicliques and connecting maximal bicliques sharing the same (alpha, beta)-biclique, which is the condition for maximal bicliques to be adjacent. It also leverages partial-BCPC to significantly prune the enumeration space of (alpha, beta)-biclique. Experiments show that our methods outperform existing methods by nearly three orders of magnitude.",
    "paper_id": "2512.19426v1"
  },
  "https://arxiv.org/abs/2512.19424v1": {
    "content_path": "../data/contents/2512.19424v1.md",
    "pdf_path": "../data/contents/2512.19424v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:53.928675",
    "title": "CodeSimpleQA: Scaling Factuality in Code Large Language Models",
    "abstract": "Summary: Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.",
    "paper_id": "2512.19424v1"
  },
  "https://arxiv.org/abs/2512.19414v1": {
    "content_path": "../data/contents/2512.19414v1.md",
    "pdf_path": "../data/contents/2512.19414v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:54.423500",
    "title": "From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions",
    "abstract": "Summary: The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.",
    "paper_id": "2512.19414v1"
  },
  "https://arxiv.org/abs/2512.19410v1": {
    "content_path": "../data/contents/2512.19410v1.md",
    "pdf_path": "../data/contents/2512.19410v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:54.674050",
    "title": "Research Program: Theory of Learning in Dynamical Systems",
    "abstract": "Summary: Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.",
    "paper_id": "2512.19410v1"
  },
  "https://arxiv.org/abs/2512.19399v1": {
    "content_path": "../data/contents/2512.19399v1.md",
    "pdf_path": "../data/contents/2512.19399v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:54.868604",
    "title": "Brain-Grounded Axes for Reading and Steering LLM States",
    "abstract": "Summary: Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
    "paper_id": "2512.19399v1"
  },
  "https://arxiv.org/abs/2512.19396v1": {
    "content_path": "../data/contents/2512.19396v1.md",
    "pdf_path": "../data/contents/2512.19396v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:55.106571",
    "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
    "abstract": "Summary: Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.",
    "paper_id": "2512.19396v1"
  },
  "https://arxiv.org/abs/2512.19378v1": {
    "content_path": "../data/contents/2512.19378v1.md",
    "pdf_path": "../data/contents/2512.19378v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:55.302038",
    "title": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models",
    "abstract": "Summary: Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.",
    "paper_id": "2512.19378v1"
  },
  "https://arxiv.org/abs/2512.19373v1": {
    "content_path": "../data/contents/2512.19373v1.md",
    "pdf_path": "../data/contents/2512.19373v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:56.331221",
    "title": "Cluster-Based Generalized Additive Models Informed by Random Fourier Features",
    "abstract": "Summary: Explainable machine learning aims to strike a balance between prediction accuracy and model transparency, particularly in settings where black-box predictive models, such as deep neural networks or kernel-based methods, achieve strong empirical performance but remain difficult to interpret. This work introduces a mixture of generalized additive models (GAMs) in which random Fourier feature (RFF) representations are leveraged to uncover locally adaptive structure in the data. In the proposed method, an RFF-based embedding is first learned and then compressed via principal component analysis. The resulting low-dimensional representations are used to perform soft clustering of the data through a Gaussian mixture model. These cluster assignments are then applied to construct a mixture-of-GAMs framework, where each local GAM captures nonlinear effects through interpretable univariate smooth functions. Numerical experiments on real-world regression benchmarks, including the California Housing, NASA Airfoil Self-Noise, and Bike Sharing datasets, demonstrate improved predictive performance relative to classical interpretable models. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.",
    "paper_id": "2512.19373v1"
  },
  "https://arxiv.org/abs/2512.19367v1": {
    "content_path": "../data/contents/2512.19367v1.md",
    "pdf_path": "../data/contents/2512.19367v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:56.725317",
    "title": "Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture",
    "abstract": "Summary: We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).",
    "paper_id": "2512.19367v1"
  },
  "https://arxiv.org/abs/2512.19366v1": {
    "content_path": "../data/contents/2512.19366v1.md",
    "pdf_path": "../data/contents/2512.19366v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:57.067372",
    "title": "Learning General Policies with Policy Gradient Methods",
    "abstract": "Summary: While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
    "paper_id": "2512.19366v1"
  },
  "https://arxiv.org/abs/2512.19363v1": {
    "content_path": "../data/contents/2512.19363v1.md",
    "pdf_path": "../data/contents/2512.19363v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:57.471074",
    "title": "From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples",
    "abstract": "Summary: How should we quantify the value of each training example when datasets are large, heterogeneous, and geometrically structured? Classical Data-Shapley answers in principle, but its O(n!) complexity and point-wise perspective are ill-suited to modern scales. We propose Hierarchical Contrastive Data Valuation (HCDV), a three-stage framework that (i) learns a contrastive, geometry-preserving representation, (ii) organizes the data into a balanced coarse-to-fine hierarchy of clusters, and (iii) assigns Shapley-style payoffs to coalitions via local Monte-Carlo games whose budgets are propagated downward. HCDV collapses the factorial burden to O(T sum_{l} K_{l}) = O(T K_max log n), rewards examples that sharpen decision boundaries, and regularizes outliers through curvature-based smoothness. We prove that HCDV approximately satisfies the four Shapley axioms with surplus loss O(eta log n), enjoys sub-Gaussian coalition deviation tilde O(1/sqrt{T}), and incurs at most k epsilon_infty regret for top-k selection. Experiments on four benchmarks--tabular, vision, streaming, and a 45M-sample CTR task--plus the OpenDataVal suite show that HCDV lifts accuracy by up to +5 pp, slashes valuation time by up to 100x, and directly supports tasks such as augmentation filtering, low-latency streaming updates, and fair marketplace payouts.",
    "paper_id": "2512.19363v1"
  },
  "https://arxiv.org/abs/2512.19360v1": {
    "content_path": "../data/contents/2512.19360v1.md",
    "pdf_path": "../data/contents/2512.19360v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:58.637205",
    "title": "Generative vector search to improve pathology foundation models across multimodal vision-language tasks",
    "abstract": "Summary: Retrieval-augmented generation improves large language models by grounding outputs in external knowledge sources, reducing hallucinations and addressing knowledge cutoffs. However, standard embedding-based retrieval fails to capture the complexity of multi-concept queries, particularly in domains like biomedicine, where biological data are inherently high-dimensional. For example,omics datasets, and clinical reports simultaneously exhibit numerous molecular, cellular, and physiological features. We present Stochastic Latent Matching (STHLM), a generative vector search method that samples query-conditioned embeddings from text or image inputs to enhance retrieval performance. Analogous to how Chain-of-Thought reasoning enables language models to \"think longer\" on complex problems, STHLM allows retrieval systems to \"search wider\" through iterative sampling. STHLM demonstrates critical improvements over classical vector retrieval across diverse benchmarks, including scientific literature, clinical notes, and tissue images, boosting retrieval performance by 10-30% through test-time compute (trading latency for accuracy), while enabling up to a 10-fold compression of embedding dimensions.",
    "paper_id": "2512.19360v1"
  },
  "https://arxiv.org/abs/2512.19355v1": {
    "content_path": "../data/contents/2512.19355v1.md",
    "pdf_path": "../data/contents/2512.19355v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:51:58.990485",
    "title": "First-Order Representation Languages for Goal-Conditioned RL",
    "abstract": "Summary: First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.",
    "paper_id": "2512.19355v1"
  },
  "https://arxiv.org/abs/2512.19350v1": {
    "content_path": "../data/contents/2512.19350v1.md",
    "pdf_path": "../data/contents/2512.19350v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:01.913363",
    "title": "PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models",
    "abstract": "Summary: Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \\textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.",
    "paper_id": "2512.19350v1"
  },
  "https://arxiv.org/abs/2512.19349v1": {
    "content_path": "../data/contents/2512.19349v1.md",
    "pdf_path": "../data/contents/2512.19349v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:02.273923",
    "title": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop",
    "abstract": "Summary: Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.",
    "paper_id": "2512.19349v1"
  },
  "https://arxiv.org/abs/2512.19342v1": {
    "content_path": "../data/contents/2512.19342v1.md",
    "pdf_path": "../data/contents/2512.19342v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:03.119676",
    "title": "Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives",
    "abstract": "Summary: Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether.",
    "paper_id": "2512.19342v1"
  },
  "https://arxiv.org/abs/2512.19323v1": {
    "content_path": "../data/contents/2512.19323v1.md",
    "pdf_path": "../data/contents/2512.19323v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:03.433449",
    "title": "Alternative positional encoding functions for neural transformers",
    "abstract": "Summary: A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.",
    "paper_id": "2512.19323v1"
  },
  "https://arxiv.org/abs/2512.19320v1": {
    "content_path": "../data/contents/2512.19320v1.md",
    "pdf_path": "../data/contents/2512.19320v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:04.224513",
    "title": "MAGIC: Achieving Superior Model Merging via Magnitude Calibration",
    "abstract": "Summary: The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC",
    "paper_id": "2512.19320v1"
  },
  "https://arxiv.org/abs/2512.19317v1": {
    "content_path": "../data/contents/2512.19317v1.md",
    "pdf_path": "../data/contents/2512.19317v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:04.830369",
    "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
    "abstract": "Summary: Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
    "paper_id": "2512.19317v1"
  },
  "https://arxiv.org/abs/2512.19305v1": {
    "content_path": "../data/contents/2512.19305v1.md",
    "pdf_path": "../data/contents/2512.19305v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:05.405792",
    "title": "CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs",
    "abstract": "Summary: Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.",
    "paper_id": "2512.19305v1"
  },
  "https://arxiv.org/abs/2512.21336v1": {
    "content_path": "../data/contents/2512.21336v1.md",
    "pdf_path": "../data/contents/2512.21336v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:06.018043",
    "title": "Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty",
    "abstract": "Summary: Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.",
    "paper_id": "2512.21336v1"
  },
  "https://arxiv.org/abs/2512.19299v1": {
    "content_path": "../data/contents/2512.19299v1.md",
    "pdf_path": "../data/contents/2512.19299v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:09.930378",
    "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
    "abstract": "Summary: In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.",
    "paper_id": "2512.19299v1"
  },
  "https://arxiv.org/abs/2512.21332v1": {
    "content_path": "../data/contents/2512.21332v1.md",
    "pdf_path": "../data/contents/2512.21332v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:10.140430",
    "title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling",
    "abstract": "Summary: We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.",
    "paper_id": "2512.21332v1"
  },
  "https://arxiv.org/abs/2512.19297v1": {
    "content_path": "../data/contents/2512.19297v1.md",
    "pdf_path": "../data/contents/2512.19297v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:11.321389",
    "title": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
    "abstract": "Summary: Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.",
    "paper_id": "2512.19297v1"
  },
  "https://arxiv.org/abs/2512.21329v1": {
    "content_path": "../data/contents/2512.21329v1.md",
    "pdf_path": "../data/contents/2512.21329v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:11.692818",
    "title": "Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks",
    "abstract": "Summary: Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.\n  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.",
    "paper_id": "2512.21329v1"
  },
  "https://arxiv.org/abs/2512.19287v1": {
    "content_path": "../data/contents/2512.19287v1.md",
    "pdf_path": "../data/contents/2512.19287v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:12.089841",
    "title": "Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6",
    "abstract": "Summary: We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.",
    "paper_id": "2512.19287v1"
  },
  "https://arxiv.org/abs/2512.21326v1": {
    "content_path": "../data/contents/2512.21326v1.md",
    "pdf_path": "../data/contents/2512.21326v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:12.950095",
    "title": "Measuring all the noises of LLM Evals",
    "abstract": "Summary: Separating signal from noise is central to experimental science. Applying well-established statistical method effectively to LLM evals requires consideration of their unique noise characteristics. We clearly define and measure three types of noise: prediction noise from generating different answers on a given question, data noise from sampling questions, and their combined total noise following the law of total variance. To emphasize relative comparisons and gain statistical power, we propose the all-pairs paired method, which applies the paired analysis to all pairs of LLMs and measures all the noise components based on millions of question-level predictions across many evals and settings. These measurements revealed clear patterns. First, each eval exhibits a characteristic and highly predictable total noise level across all model pairs. Second, paired prediction noise typically exceeds paired data noise, which means reducing prediction noise by averaging can significantly increase statistical power. These findings enable practitioners to assess significance without custom testing and to detect much smaller effects in controlled experiments.",
    "paper_id": "2512.21326v1"
  },
  "https://arxiv.org/abs/2512.19247v1": {
    "content_path": "../data/contents/2512.19247v1.md",
    "pdf_path": "../data/contents/2512.19247v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:15.121849",
    "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
    "abstract": "Summary: Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
    "paper_id": "2512.19247v1"
  },
  "https://arxiv.org/abs/2512.21323v1": {
    "content_path": "../data/contents/2512.21323v1.md",
    "pdf_path": "../data/contents/2512.21323v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:16.718002",
    "title": "Parallel Token Prediction for Language Models",
    "abstract": "Summary: We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.",
    "paper_id": "2512.21323v1"
  },
  "https://arxiv.org/abs/2512.19246v1": {
    "content_path": "../data/contents/2512.19246v1.md",
    "pdf_path": "../data/contents/2512.19246v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:17.065261",
    "title": "From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis",
    "abstract": "Summary: Hyperparameters tuning is a fundamental, yet computationally expensive, step in optimizing machine learning models. Beyond optimization, understanding the relative importance and interaction of hyperparameters is critical to efficient model development. In this paper, we introduce MetaSHAP, a scalable semi-automated eXplainable AI (XAI) method, that uses meta-learning and Shapley values analysis to provide actionable and dataset-aware tuning insights. MetaSHAP operates over a vast benchmark of over 09 millions evaluated machine learning pipelines, allowing it to produce interpretable importance scores and actionable tuning insights that reveal how much each hyperparameter matters, how it interacts with others and in which value ranges its influence is concentrated. For a given algorithm and dataset, MetaSHAP learns a surrogate performance model from historical configurations, computes hyperparameters interactions using SHAP-based analysis, and derives interpretable tuning ranges from the most influential hyperparameters. This allows practitioners not only to prioritize which hyperparameters to tune, but also to understand their directionality and interactions. We empirically validate MetaSHAP on a diverse benchmark of 164 classification datasets and 14 classifiers, demonstrating that it produces reliable importance rankings and competitive performance when used to guide Bayesian optimization.",
    "paper_id": "2512.19246v1"
  },
  "https://arxiv.org/abs/2512.21316v1": {
    "content_path": "../data/contents/2512.21316v1.md",
    "pdf_path": "../data/contents/2512.21316v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:17.333484",
    "title": "Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Consulting, Data Analyst, and Management Tasks",
    "abstract": "Summary: This paper derives `Scaling Laws for Economic Impacts' -- empirical relationships between the training compute of Large Language Models (LLMs) and professional productivity. In a preregistered experiment, over 500 consultants, data analysts, and managers completed professional tasks using one of 13 LLMs. We find that each year of AI model progress reduced task time by 8%, with 56% of gains driven by increased compute and 44% by algorithmic progress. However, productivity gains were significantly larger for non-agentic analytical tasks compared to agentic workflows requiring tool use. These findings suggest continued model scaling could boost U.S. productivity by approximately 20% over the next decade.",
    "paper_id": "2512.21316v1"
  },
  "https://arxiv.org/abs/2512.19240v1": {
    "content_path": "../data/contents/2512.19240v1.md",
    "pdf_path": "../data/contents/2512.19240v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:17.796401",
    "title": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models",
    "abstract": "Summary: Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.",
    "paper_id": "2512.19240v1"
  },
  "https://arxiv.org/abs/2512.21315v1": {
    "content_path": "../data/contents/2512.21315v1.md",
    "pdf_path": "../data/contents/2512.21315v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:22.948264",
    "title": "Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks",
    "abstract": "Summary: The data processing inequality is an information-theoretic principle stating that the information content of a signal cannot be increased by processing the observations. In particular, it suggests that there is no benefit in enhancing the signal or encoding it before addressing a classification problem. This assertion can be proven to be true for the case of the optimal Bayes classifier. However, in practice, it is common to perform \"low-level\" tasks before \"high-level\" downstream tasks despite the overwhelming capabilities of modern deep neural networks. In this paper, we aim to understand when and why low-level processing can be beneficial for classification. We present a comprehensive theoretical study of a binary classification setup, where we consider a classifier that is tightly connected to the optimal Bayes classifier and converges to it as the number of training samples increases. We prove that for any finite number of training samples, there exists a pre-classification processing that improves the classification accuracy. We also explore the effect of class separation, training set size, and class balance on the relative gain from this procedure. We support our theory with an empirical investigation of the theoretical setup. Finally, we conduct an empirical study where we investigate the effect of denoising and encoding on the performance of practical deep classifiers on benchmark datasets. Specifically, we vary the size and class distribution of the training set, and the noise level, and demonstrate trends that are consistent with our theoretical results.",
    "paper_id": "2512.21315v1"
  },
  "https://arxiv.org/abs/2512.19238v1": {
    "content_path": "../data/contents/2512.19238v1.md",
    "pdf_path": "../data/contents/2512.19238v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:23.167761",
    "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
    "abstract": "Summary: Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.",
    "paper_id": "2512.19238v1"
  },
  "https://arxiv.org/abs/2512.21309v1": {
    "content_path": "../data/contents/2512.21309v1.md",
    "pdf_path": "../data/contents/2512.21309v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:23.458483",
    "title": "A Plan Reuse Mechanism for LLM-Driven Agent",
    "abstract": "Summary: Integrating large language models (LLMs) into personal assistants, like Xiao Ai and Blue Heart V, effectively enhances their ability to interact with humans, solve complex tasks, and manage IoT devices. Such assistants are also termed LLM-driven agents. Upon receiving user requests, the LLM-driven agent generates plans using an LLM, executes these plans through various tools, and then returns the response to the user. During this process, the latency for generating a plan with an LLM can reach tens of seconds, significantly degrading user experience. Real-world dataset analysis shows that about 30% of the requests received by LLM-driven agents are identical or similar, which allows the reuse of previously generated plans to reduce latency. However, it is difficult to accurately define the similarity between the request texts received by the LLM-driven agent through directly evaluating the original request texts. Moreover, the diverse expressions of natural language and the unstructured format of plan texts make implementing plan reuse challenging. To address these issues, we present and implement a plan reuse mechanism for LLM-driven agents called AgentReuse. AgentReuse leverages the similarities and differences among requests' semantics and uses intent classification to evaluate the similarities between requests and enable the reuse of plans. Experimental results based on a real-world dataset demonstrate that AgentReuse achieves a 93% effective plan reuse rate, an F1 score of 0.9718, and an accuracy of 0.9459 in evaluating request similarities, reducing latency by 93.12% compared with baselines without using the reuse mechanism.",
    "paper_id": "2512.21309v1"
  },
  "https://arxiv.org/abs/2512.19234v1": {
    "content_path": "../data/contents/2512.19234v1.md",
    "pdf_path": "../data/contents/2512.19234v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:27.304471",
    "title": "DeliveryBench: Can Agents Earn Profit in Real World?",
    "abstract": "Summary: LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io.",
    "paper_id": "2512.19234v1"
  },
  "https://arxiv.org/abs/2512.21300v1": {
    "content_path": "../data/contents/2512.21300v1.md",
    "pdf_path": "../data/contents/2512.21300v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:28.633915",
    "title": "Closed-form empirical Bernstein confidence sequences for scalars and matrices",
    "abstract": "Summary: We derive a new closed-form variance-adaptive confidence sequence (CS) for estimating the average conditional mean of a sequence of bounded random variables. Empirically, it yields the tightest closed-form CS we have found for tracking time-varying means, across sample sizes up to $\\approx 10^6$. When the observations happen to have the same conditional mean, our CS is asymptotically tighter than the recent closed-form CS of Waudby-Smith and Ramdas [38]. It also has other desirable properties: it is centered at the unweighted sample mean and has limiting width (multiplied by $\\sqrt{t/\\log t}$) independent of the significance level. We extend our results to provide a CS with the same properties for random matrices with bounded eigenvalues.",
    "paper_id": "2512.21300v1"
  },
  "https://arxiv.org/abs/2512.19228v1": {
    "content_path": "../data/contents/2512.19228v1.md",
    "pdf_path": "../data/contents/2512.19228v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:29.110361",
    "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
    "abstract": "Summary: Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.",
    "paper_id": "2512.19228v1"
  },
  "https://arxiv.org/abs/2512.21288v1": {
    "content_path": "../data/contents/2512.21288v1.md",
    "pdf_path": "../data/contents/2512.21288v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:29.955673",
    "title": "Model Merging via Multi-Teacher Knowledge Distillation",
    "abstract": "Summary: Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics to approximate the optimal combination of parameters. This dependence is most critical in coefficient scaling, the weighting factors that modulate the magnitude of each fine-tuned model's contribution to the shared parameter. However, without a principled objective to guide their selection, these methods lead to brittle performance and are highly sensitive to scaling initialization. We address this gap by (i) establishing a novel flatness-aware PAC-Bayes generalization bound specifically for the model merging setting. This analysis introduces a \"cross-task heterogeneity\" term that formally captures the mismatch between diverse fine-tuned model priors and the target multi-task distributions. Guided by this theoretical insight, (ii) we frame model merging as multi-teacher knowledge distillation on scarce, unlabeled data. We formally demonstrate that minimizing the student-teacher Kullback-Leibler divergence directly tightens the upper bound on the merged model's excess risk. Guided by the flatness-aware bound derived, (iii) we operationalize this objective via SAMerging, a method that employs Sharpness-Aware Minimization (SAM) to find flat minima. Empirically, SAMerging establishes a new state of the art across vision and NLP benchmarks, achieving remarkable performance. The code is available at https://github.com/arshandalili/SAMerging.",
    "paper_id": "2512.21288v1"
  },
  "https://arxiv.org/abs/2512.19221v1": {
    "content_path": "../data/contents/2512.19221v1.md",
    "pdf_path": "../data/contents/2512.19221v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:51.623168",
    "title": "From Pixels to Predicates Structuring urban perception with scene graphs",
    "abstract": "Summary: Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.",
    "paper_id": "2512.19221v1"
  },
  "https://arxiv.org/abs/2512.21280v1": {
    "content_path": "../data/contents/2512.21280v1.md",
    "pdf_path": "../data/contents/2512.21280v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:51.839332",
    "title": "SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance",
    "abstract": "Summary: The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.",
    "paper_id": "2512.21280v1"
  },
  "https://arxiv.org/abs/2512.19219v1": {
    "content_path": "../data/contents/2512.19219v1.md",
    "pdf_path": "../data/contents/2512.19219v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:58.800184",
    "title": "Towards Minimal Fine-Tuning of VLMs",
    "abstract": "Summary: We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.",
    "paper_id": "2512.19219v1"
  },
  "https://arxiv.org/abs/2512.21257v1": {
    "content_path": "../data/contents/2512.21257v1.md",
    "pdf_path": "../data/contents/2512.21257v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:52:59.761510",
    "title": "ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling",
    "abstract": "Summary: Industrial recommender systems face two fundamental limitations under the log-driven paradigm: (1) knowledge poverty in ID-based item representations that causes brittle interest modeling under data sparsity, and (2) systemic blindness to beyond-log user interests that constrains model performance within platform boundaries. These limitations stem from an over-reliance on shallow interaction statistics and close-looped feedback while neglecting the rich world knowledge about product semantics and cross-domain behavioral patterns that Large Language Models have learned from vast corpora.\n  To address these challenges, we introduce ReaSeq, a reasoning-enhanced framework that leverages world knowledge in Large Language Models to address both limitations through explicit and implicit reasoning. Specifically, ReaSeq employs explicit Chain-of-Thought reasoning via multi-agent collaboration to distill structured product knowledge into semantically enriched item representations, and latent reasoning via Diffusion Large Language Models to infer plausible beyond-log behaviors. Deployed on Taobao's ranking system serving hundreds of millions of users, ReaSeq achieves substantial gains: >6.0% in IPV and CTR, >2.9% in Orders, and >2.5% in GMV, validating the effectiveness of world-knowledge-enhanced reasoning over purely log-driven approaches.",
    "paper_id": "2512.21257v1"
  },
  "https://arxiv.org/abs/2512.19210v1": {
    "content_path": "../data/contents/2512.19210v1.md",
    "pdf_path": "../data/contents/2512.19210v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:04.610406",
    "title": "Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation",
    "abstract": "Summary: We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine \"understanding\" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.",
    "paper_id": "2512.19210v1"
  },
  "https://arxiv.org/abs/2512.21250v1": {
    "content_path": "../data/contents/2512.21250v1.md",
    "pdf_path": "../data/contents/2512.21250v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:04.850024",
    "title": "CoTDeceptor:Adversarial Code Obfuscation Against CoT-Enhanced LLM Code Agents",
    "abstract": "Summary: LLM-based code agents(e.g., ChatGPT Codex) are increasingly deployed as detector for code review and security auditing tasks. Although CoT-enhanced LLM vulnerability detectors are believed to provide improved robustness against obfuscated malicious code, we find that their reasoning chains and semantic abstraction processes exhibit exploitable systematic weaknesses.This allows attackers to covertly embed malicious logic, bypass code review, and propagate backdoored components throughout real-world software supply chains.To investigate this issue, we present CoTDeceptor, the first adversarial code obfuscation framework targeting CoT-enhanced LLM detectors. CoTDeceptor autonomously constructs evolving, hard-to-reverse multi-stage obfuscation strategy chains that effectively disrupt CoT-driven detection logic.We obtained malicious code provided by security enterprise, experimental results demonstrate that CoTDeceptor achieves stable and transferable evasion performance against state-of-the-art LLMs and vulnerability detection agents. CoTDeceptor bypasses 14 out of 15 vulnerability categories, compared to only 2 bypassed by prior methods. Our findings highlight potential risks in real-world software supply chains and underscore the need for more robust and interpretable LLM-powered security analysis systems.",
    "paper_id": "2512.21250v1"
  },
  "https://arxiv.org/abs/2512.19206v1": {
    "content_path": "../data/contents/2512.19206v1.md",
    "pdf_path": "../data/contents/2512.19206v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:06.583829",
    "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
    "abstract": "Summary: Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.",
    "paper_id": "2512.19206v1"
  },
  "https://arxiv.org/abs/2512.21238v1": {
    "content_path": "../data/contents/2512.21238v1.md",
    "pdf_path": "../data/contents/2512.21238v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:08.187215",
    "title": "Assessing the Software Security Comprehension of Large Language Models",
    "abstract": "Summary: Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.",
    "paper_id": "2512.21238v1"
  },
  "https://arxiv.org/abs/2512.19772v1": {
    "content_path": "../data/contents/2512.19772v1.md",
    "pdf_path": "../data/contents/2512.19772v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:09.135178",
    "title": "A K-Means, Ward and DBSCAN repeatability study",
    "abstract": "Summary: Reproducibility is essential in machine learning because it ensures that a model or experiment yields the same scientific conclusion. For specific algorithms repeatability with bitwise identical results is also a key for scientific integrity because it allows debugging. We decomposed several very popular clustering algorithms: K-Means, DBSCAN and Ward into their fundamental steps, and we identify the conditions required to achieve repeatability at each stage. We use an implementation example with the Python library scikit-learn to examine the repeatable aspects of each method. Our results reveal inconsistent results with K-Means when the number of OpenMP threads exceeds two. This work aims to raise awareness of this issue among both users and developers, encouraging further investigation and potential fixes.",
    "paper_id": "2512.19772v1"
  },
  "https://arxiv.org/abs/2512.21236v1": {
    "content_path": "../data/contents/2512.21236v1.md",
    "pdf_path": "../data/contents/2512.21236v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:09.434522",
    "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
    "abstract": "Summary: Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",
    "paper_id": "2512.21236v1"
  },
  "https://arxiv.org/abs/2512.19179v1": {
    "content_path": "../data/contents/2512.19179v1.md",
    "pdf_path": "../data/contents/2512.19179v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:10.175102",
    "title": "L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling",
    "abstract": "Summary: Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present L4, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. L4 partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. L4 devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, L4 reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.",
    "paper_id": "2512.19179v1"
  },
  "https://arxiv.org/abs/2512.21231v1": {
    "content_path": "../data/contents/2512.21231v1.md",
    "pdf_path": "../data/contents/2512.21231v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:10.948923",
    "title": "MiST: Understanding the Role of Mid-Stage Scientific Training in Developing Chemical Reasoning Models",
    "abstract": "Summary: Large Language Models can develop reasoning capabilities through online fine-tuning with rule-based rewards. However, recent studies reveal a critical constraint: reinforcement learning succeeds only when the base model already assigns non-negligible probability to correct answers -- a property we term 'latent solvability'. This work investigates the emergence of chemical reasoning capabilities and what these prerequisites mean for chemistry. We identify two necessary conditions for RL-based chemical reasoning: 1) Symbolic competence, and 2) Latent chemical knowledge. We propose mid-stage scientific training (MiST): a set of mid-stage training techniques to satisfy these, including data-mixing with SMILES/CIF-aware pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. These steps raise the latent-solvability score on 3B and 7B models by up to 1.8x, and enable RL to lift top-1 accuracy from 10.9 to 63.9% on organic reaction naming, and from 40.6 to 67.4% on inorganic material generation. Similar results are observed for other challenging chemical tasks, while producing interpretable reasoning traces. Our results define clear prerequisites for chemical reasoning training and highlight the broader role of mid-stage training in unlocking reasoning capabilities.",
    "paper_id": "2512.21231v1"
  },
  "https://arxiv.org/abs/2512.19173v1": {
    "content_path": "../data/contents/2512.19173v1.md",
    "pdf_path": "../data/contents/2512.19173v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:13.366898",
    "title": "CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation",
    "abstract": "Summary: Current chart-specific tasks, such as chart question answering, chart parsing, and chart generation, are typically studied in isolation, preventing models from learning the shared semantics that link chart generation and interpretation. We introduce CycleChart, a consistency-based learning framework for bidirectional chart understanding and generation. CycleChart adopts a schema-centric formulation as a common interface across tasks. We construct a consistent multi-task dataset, where each chart sample includes aligned annotations for schema prediction, data parsing, and question answering. To learn cross-directional chart semantics, CycleChart introduces a generate-parse consistency objective: the model generates a chart schema from a table and a textual query, then learns to recover the schema and data from the generated chart, enforcing semantic alignment across directions. CycleChart achieves strong results on chart generation, chart parsing, and chart question answering, demonstrating improved cross-task generalization and marking a step toward more general chart understanding models.",
    "paper_id": "2512.19173v1"
  },
  "https://arxiv.org/abs/2512.21221v1": {
    "content_path": "../data/contents/2512.21221v1.md",
    "pdf_path": "../data/contents/2512.21221v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:13.602876",
    "title": "Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval",
    "abstract": "Summary: Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval",
    "paper_id": "2512.21221v1"
  },
  "https://arxiv.org/abs/2512.19171v1": {
    "content_path": "../data/contents/2512.19171v1.md",
    "pdf_path": "../data/contents/2512.19171v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:13.878687",
    "title": "JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation",
    "abstract": "Summary: While Joint-Embedding Predictive Architecture (JEPA) has emerged as a powerful architecture for learning rich latent representations, it fundamentally lacks generative abilities. Meanwhile, latent space reasoning attempts for Transformer models like COCONUT do improve performance, but they ultimately rely on token-by-token generation, which still accumulates compounding error and relies on context information to gain reasoning insights. To address these limitations, we propose JEPA-Reasoner, a novel JEPA model enhanced with generative ability that reasons in latent space. We augment it with a separate action-taker model, Talker, to produce human-readable sentences. Our approach demonstrates that decoupling latent space reasoning and token generation enables JEPA-Reasoner to produce mixed latent vectors that might lay the foundation for multi-threaded reasoning, while performing autoregressive generation with superior robustness to compounding error.",
    "paper_id": "2512.19171v1"
  },
  "https://arxiv.org/abs/2512.21220v1": {
    "content_path": "../data/contents/2512.21220v1.md",
    "pdf_path": "../data/contents/2512.21220v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:17.336653",
    "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic",
    "abstract": "Summary: Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.",
    "paper_id": "2512.21220v1"
  },
  "https://arxiv.org/abs/2512.19161v1": {
    "content_path": "../data/contents/2512.19161v1.md",
    "pdf_path": "../data/contents/2512.19161v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:17.529596",
    "title": "From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television Programs",
    "abstract": "Summary: Subtitles are essential for video accessibility and audience engagement. Modern Automatic Speech Recognition (ASR) systems, built upon Encoder-Decoder neural network architectures and trained on massive amounts of data, have progressively reduced transcription errors on standard benchmark datasets. However, their performance in real-world production environments, particularly for non-English content like long-form Italian videos, remains largely unexplored. This paper presents a case study on developing a professional subtitling system for an Italian media company. To inform our system design, we evaluated four state-of-the-art ASR models (Whisper Large v2, AssemblyAI Universal, Parakeet TDT v3 0.6b, and WhisperX) on a 50-hour dataset of Italian television programs. The study highlights their strengths and limitations, benchmarking their performance against the work of professional human subtitlers. The findings indicate that, while current models cannot meet the media industry's accuracy needs for full autonomy, they can serve as highly effective tools for enhancing human productivity. We conclude that a human-in-the-loop (HITL) approach is crucial and present the production-grade, cloud-based infrastructure we designed to support this workflow.",
    "paper_id": "2512.19161v1"
  },
  "https://arxiv.org/abs/2512.21211v1": {
    "content_path": "../data/contents/2512.21211v1.md",
    "pdf_path": "../data/contents/2512.21211v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:18.874293",
    "title": "Causal-driven attribution (CDA): Estimating channel influence without user-level data",
    "abstract": "Summary: Attribution modelling lies at the heart of marketing effectiveness, yet most existing approaches depend on user-level path data, which are increasingly inaccessible due to privacy regulations and platform restrictions. This paper introduces a Causal-Driven Attribution (CDA) framework that infers channel influence using only aggregated impression-level data, avoiding any reliance on user identifiers or click-path tracking. CDA integrates temporal causal discovery (using PCMCI) with causal effect estimation via a Structural Causal Model to recover directional channel relationships and quantify their contributions to conversions. Using large-scale synthetic data designed to replicate real marketing dynamics, we show that CDA achieves an average relative RMSE of 9.50% when given the true causal graph, and 24.23% when using the predicted graph, demonstrating strong accuracy under correct structure and meaningful signal recovery even under structural uncertainty. CDA captures cross-channel interdependencies while providing interpretable, privacy-preserving attribution insights, offering a scalable and future-proof alternative to traditional path-based models.",
    "paper_id": "2512.21211v1"
  },
  "https://arxiv.org/abs/2512.19155v1": {
    "content_path": "../data/contents/2512.19155v1.md",
    "pdf_path": "../data/contents/2512.19155v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:19.197246",
    "title": "Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness",
    "abstract": "Summary: The search for reliable indicators of consciousness has fragmented into competing theoretical camps (Global Workspace Theory (GWT), Integrated Information Theory (IIT), and Higher-Order Theories (HOT)), each proposing distinct neural signatures. We adopt a synthetic neuro-phenomenology approach: constructing artificial agents that embody these mechanisms to test their functional consequences through precise architectural ablations impossible in biological systems. Across three experiments, we report dissociations suggesting these theories describe complementary functional layers rather than competing accounts. In Experiment 1, a no-rewire Self-Model lesion abolishes metacognitive calibration while preserving first-order task performance, yielding a synthetic blindsight analogue consistent with HOT predictions. In Experiment 2, workspace capacity proves causally necessary for information access: a complete workspace lesion produces qualitative collapse in access-related markers, while partial reductions show graded degradation, consistent with GWT's ignition framework. In Experiment 3, we uncover a broadcast-amplification effect: GWT-style broadcasting amplifies internal noise, creating extreme fragility. The B2 agent family is robust to the same latent perturbation; this robustness persists in a Self-Model-off / workspace-read control, cautioning against attributing the effect solely to $z_{\\text{self}}$ compression. We also report an explicit negative result: raw perturbational complexity (PCI-A) decreases under the workspace bottleneck, cautioning against naive transfer of IIT-adjacent proxies to engineered agents. These results suggest a hierarchical design principle: GWT provides broadcast capacity, while HOT provides quality control. We emphasize that our agents are not conscious; they are reference implementations for testing functional predictions of consciousness theories.",
    "paper_id": "2512.19155v1"
  },
  "https://arxiv.org/abs/2512.21208v1": {
    "content_path": "../data/contents/2512.21208v1.md",
    "pdf_path": "../data/contents/2512.21208v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:19.477264",
    "title": "Analytic and Variational Stability of Deep Learning Systems",
    "abstract": "Summary: We propose a unified analytic and variational framework for studying stability in deep learning systems viewed as coupled representation-parameter dynamics. The central object is the Learning Stability Profile, which tracks the infinitesimal response of representations, parameters, and update mechanisms to perturbations along the learning trajectory. We prove a Fundamental Analytic Stability Theorem showing that uniform boundedness of these stability signatures is equivalent, up to norm equivalence, to the existence of a Lyapunov-type energy that dissipates along the learning flow. In smooth regimes, the framework yields explicit stability exponents linking spectral norms, activation regularity, step sizes, and learning rates to contractivity of the learning dynamics. Classical spectral stability results for feedforward networks, a discrete CFL-type condition for residual architectures, and parametric and temporal stability laws for stochastic gradient methods arise as direct consequences. The theory extends to non-smooth learning systems, including ReLU networks, proximal and projected updates, and stochastic subgradient flows, by replacing classical derivatives with Clarke generalized derivatives and smooth energies with variational Lyapunov functionals. The resulting framework provides a unified dynamical description of stability across architectures and optimization methods, clarifying how architectural and algorithmic choices jointly govern robustness and sensitivity to perturbations. It also provides a foundation for further extensions to continuous-time limits and geometric formulations of learning dynamics.",
    "paper_id": "2512.21208v1"
  },
  "https://arxiv.org/abs/2512.19154v1": {
    "content_path": "../data/contents/2512.19154v1.md",
    "pdf_path": "../data/contents/2512.19154v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:25.007019",
    "title": "Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments",
    "abstract": "Summary: Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking",
    "paper_id": "2512.19154v1"
  },
  "https://arxiv.org/abs/2504.19413": {
    "content_path": "../data/contents/2504.19413.md",
    "pdf_path": "../data/contents/2504.19413.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:27.415343",
    "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory",
    "abstract": "Academic paper on Mem0 memory architecture achieving 91% lower p95 latency and 90% token savings vs full-context approach",
    "paper_id": "2504.19413"
  },
  "https://arxiv.org/abs/2512.16214v2": {
    "content_path": "../data/contents/2512.16214v2.md",
    "pdf_path": "../data/contents/2512.16214v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:27.900510",
    "title": "PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving",
    "abstract": "Summary: Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.",
    "paper_id": "2512.16214v2"
  },
  "https://arxiv.org/abs/2512.12381v1": {
    "content_path": "../data/contents/2512.12381v1.md",
    "pdf_path": "../data/contents/2512.12381v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:28.506164",
    "title": "Entropy Collapse: A Universal Failure Mode of Intelligent Systems",
    "abstract": "Summary: Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.\n  We identify \\emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.\n  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.\n  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.\n  \\noindent\\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis",
    "paper_id": "2512.12381v1"
  },
  "https://arxiv.org/abs/2512.16206v1": {
    "content_path": "../data/contents/2512.16206v1.md",
    "pdf_path": "../data/contents/2512.16206v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:29.145545",
    "title": "The Agony of Opacity: Foundations for Reflective Interpretability in AI-Mediated Mental Health Support",
    "abstract": "Summary: Throughout history, a prevailing paradigm in mental healthcare has been one in which distressed people may receive treatment with little understanding around how their experience is perceived by their care provider, and in turn, the decisions made by their provider around how treatment will progress. Paralleling this offline model of care, people who seek mental health support from AI chatbots are similarly provided little context for how their expressions of distress are processed by the model, and subsequently, the logic that may underlie model responses. People in severe distress who turn to AI chatbots for support thus find themselves caught between black boxes, with unique forms of agony that arise from these intersecting opacities, including misinterpreting model outputs or attributing greater capabilities to a model than are yet possible, which has led to documented real-world harms. Building on empirical research from clinical psychology and AI safety, alongside rights-oriented frameworks from medical ethics, we describe how the distinct psychological state induced by severe distress can influence chatbot interaction patterns, and argue that this state of mind (combined with differences in how a user might perceive a chatbot compared to a care provider) uniquely necessitates a higher standard of interpretability in comparison to general AI chatbot use. Drawing inspiration from newer interpretable treatment paradigms, we then describe specific technical and interface design approaches that could be used to adapt interpretability strategies from four specific mental health fields (psychotherapy, community-based crisis intervention, psychiatry, and care authorization) to AI models, including consideration of the role of interpretability in the treatment process and tensions that may arise with greater interpretability.",
    "paper_id": "2512.16206v1"
  },
  "https://arxiv.org/abs/2512.12360v1": {
    "content_path": "../data/contents/2512.12360v1.md",
    "pdf_path": "../data/contents/2512.12360v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:32.657463",
    "title": "VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding",
    "abstract": "Summary: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.",
    "paper_id": "2512.12360v1"
  },
  "https://arxiv.org/abs/2512.16202v1": {
    "content_path": "../data/contents/2512.16202v1.md",
    "pdf_path": "../data/contents/2512.16202v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:38.121406",
    "title": "Open Ad-hoc Categorization with Contextualized Feature Learning",
    "abstract": "Summary: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.\n  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.\n  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.",
    "paper_id": "2512.16202v1"
  },
  "https://arxiv.org/abs/2512.12341v1": {
    "content_path": "../data/contents/2512.12341v1.md",
    "pdf_path": "../data/contents/2512.12341v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:38.631831",
    "title": "Uncertainty Quantification for Machine Learning: One Size Does Not Fit All",
    "abstract": "Summary: Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.",
    "paper_id": "2512.12341v1"
  },
  "https://arxiv.org/abs/2512.09695": {
    "content_path": "../data/contents/2512.09695.md",
    "pdf_path": "../data/contents/2512.09695.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:46.390594",
    "title": "Exqutor: Extended Query Optimizer for Vector-augmented Analytical Queries",
    "abstract": "Research on query optimization when mixing vector similarity search with analytical SQL (mentions integrations into pgvector and DuckDB), directly relevant to hybrid retrieval systems.",
    "paper_id": "2512.09695"
  },
  "https://arxiv.org/abs/2512.16189v2": {
    "content_path": "../data/contents/2512.16189v2.md",
    "pdf_path": "../data/contents/2512.16189v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:48.320050",
    "title": "Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation",
    "abstract": "Summary: In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.",
    "paper_id": "2512.16189v2"
  },
  "https://arxiv.org/abs/2512.12337v1": {
    "content_path": "../data/contents/2512.12337v1.md",
    "pdf_path": "../data/contents/2512.12337v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:48.831692",
    "title": "SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema",
    "abstract": "Summary: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.",
    "paper_id": "2512.12337v1"
  },
  "https://arxiv.org/abs/2512.14313": {
    "content_path": "../data/contents/2512.14313.md",
    "pdf_path": "../data/contents/2512.14313.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:49.267364",
    "title": "Dynamic Context Selection for Retrieval-Augmented Generation: Mitigating Distractors and Positional Bias",
    "abstract": "Targets RAG failure modes (distractors, lost-in-the-middle) via adaptive retrieval/context sizing—useful for improving RAG quality without changing models.",
    "paper_id": "2512.14313"
  },
  "https://arxiv.org/abs/2512.16182v1": {
    "content_path": "../data/contents/2512.16182v1.md",
    "pdf_path": "../data/contents/2512.16182v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:50.132912",
    "title": "DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack",
    "abstract": "Summary: With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.",
    "paper_id": "2512.16182v1"
  },
  "https://arxiv.org/abs/2512.12324v1": {
    "content_path": "../data/contents/2512.12324v1.md",
    "pdf_path": "../data/contents/2512.12324v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:53.537154",
    "title": "UniMark: Artificial Intelligence Generated Content Identification Toolkit",
    "abstract": "Summary: The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \\textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \\emph{Hidden Watermarking} for copyright protection and \\emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.",
    "paper_id": "2512.12324v1"
  },
  "https://arxiv.org/abs/2510.06730": {
    "content_path": "../data/contents/2510.06730.md",
    "pdf_path": "../data/contents/2510.06730.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:54.404340",
    "title": "PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs",
    "abstract": "New embedding evaluation protocol to test robustness beyond static benchmarks (helps detect overfitting to MTEB-style suites).",
    "paper_id": "2510.06730"
  },
  "https://arxiv.org/abs/2512.16171v1": {
    "content_path": "../data/contents/2512.16171v1.md",
    "pdf_path": "../data/contents/2512.16171v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:54.784906",
    "title": "Science Consultant Agent",
    "abstract": "Summary: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.",
    "paper_id": "2512.16171v1"
  },
  "https://arxiv.org/abs/2512.12301v1": {
    "content_path": "../data/contents/2512.12301v1.md",
    "pdf_path": "../data/contents/2512.12301v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:57.005445",
    "title": "TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting",
    "abstract": "Summary: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.",
    "paper_id": "2512.12301v1"
  },
  "https://arxiv.org/abs/2506.05176": {
    "content_path": "../data/contents/2506.05176.md",
    "pdf_path": "../data/contents/2506.05176.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:57.258716",
    "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models",
    "abstract": "Introduces a unified embedding + reranking model family across sizes—useful for end-to-end retrieval stacks and hybrid reranking setups.",
    "paper_id": "2506.05176"
  },
  "https://arxiv.org/abs/2512.16167v1": {
    "content_path": "../data/contents/2512.16167v1.md",
    "pdf_path": "../data/contents/2512.16167v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:53:58.419380",
    "title": "Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services",
    "abstract": "Summary: The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized \"Request-Response-Payment-Evaluation\" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.",
    "paper_id": "2512.16167v1"
  },
  "https://arxiv.org/abs/2512.12295v2": {
    "content_path": "../data/contents/2512.12295v2.md",
    "pdf_path": "../data/contents/2512.12295v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:00.155901",
    "title": "Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates",
    "abstract": "Summary: Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak <= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (<2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact <20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.",
    "paper_id": "2512.12295v2"
  },
  "https://arxiv.org/abs/2503.07891": {
    "content_path": "../data/contents/2503.07891.md",
    "pdf_path": "../data/contents/2503.07891.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:00.546395",
    "title": "Gemini Embedding: Generalizable Embeddings from Gemini",
    "abstract": "Google technical report describing a Gemini-based embedding model and results on large multilingual embedding benchmarks.",
    "paper_id": "2503.07891"
  },
  "https://arxiv.org/abs/2512.16164v1": {
    "content_path": "../data/contents/2512.16164v1.md",
    "pdf_path": "../data/contents/2512.16164v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:03.707549",
    "title": "C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation",
    "abstract": "Summary: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.",
    "paper_id": "2512.16164v1"
  },
  "https://arxiv.org/abs/2512.12289v1": {
    "content_path": "../data/contents/2512.12289v1.md",
    "pdf_path": "../data/contents/2512.12289v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:06.591529",
    "title": "Robust Outlier Detection and Low-Latency Concept Drift Adaptation for Data Stream Regression: A Dual-Channel Architecture",
    "abstract": "Summary: Outlier detection and concept drift detection represent two challenges in data analysis. Most studies address these issues separately. However, joint detection mechanisms in regression remain underexplored, where the continuous nature of output spaces makes distinguishing drifts from outliers inherently challenging. To address this, we propose a novel robust regression framework for joint outlier and concept drift detection. Specifically, we introduce a dual-channel decision process that orchestrates prediction residuals into two coupled logic flows: a rapid response channel for filtering point outliers and a deep analysis channel for diagnosing drifts. We further develop the Exponentially Weighted Moving Absolute Deviation with Distinguishable Types (EWMAD-DT) detector to autonomously differentiate between abrupt and incremental drifts via dynamic thresholding. Comprehensive experiments on both synthetic and real-world datasets demonstrate that our unified framework, enhanced by EWMAD-DT, exhibits superior detection performance even when point outliers and concept drifts coexist.",
    "paper_id": "2512.12289v1"
  },
  "https://arxiv.org/abs/2512.20654v1": {
    "content_path": "../data/contents/2512.20654v1.md",
    "pdf_path": "../data/contents/2512.20654v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:10.045279",
    "title": "Q-RUN: Quantum-Inspired Data Re-uploading Networks",
    "abstract": "Summary: Data re-uploading quantum circuits (DRQC) are a key approach to implementing quantum neural networks and have been shown to outperform classical neural networks in fitting high-frequency functions. However, their practical application is limited by the scalability of current quantum hardware. In this paper, we introduce the mathematical paradigm of DRQC into classical models by proposing a quantum-inspired data re-uploading network (Q-RUN), which retains the Fourier-expressive advantages of quantum models without any quantum hardware. Experimental results demonstrate that Q-RUN delivers superior performance across both data modeling and predictive modeling tasks. Compared to the fully connected layers and the state-of-the-art neural network layers, Q-RUN reduces model parameters while decreasing error by approximately one to three orders of magnitude on certain tasks. Notably, Q-RUN can serve as a drop-in replacement for standard fully connected layers, improving the performance of a wide range of neural architectures. This work illustrates how principles from quantum machine learning can guide the design of more expressive artificial intelligence.",
    "paper_id": "2512.20654v1"
  },
  "https://arxiv.org/abs/2512.12284v3": {
    "content_path": "../data/contents/2512.12284v3.md",
    "pdf_path": "../data/contents/2512.12284v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:11.379134",
    "title": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval",
    "abstract": "Summary: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.",
    "paper_id": "2512.12284v3"
  },
  "https://arxiv.org/abs/2512.17956v1": {
    "content_path": "../data/contents/2512.17956v1.md",
    "pdf_path": "../data/contents/2512.17956v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:12.090300",
    "title": "Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration",
    "abstract": "Summary: Safety alignment can make frontier LMs overly conservative, degrading collaboration via hedging or false refusals. We present a lightweight toolkit with three parts: (1) Victor Calibration (VC), a multi-pass protocol that elicits a scalar confidence proxy T (T0<T1<T2) through iterative evidence re-evaluation; (2) FD-Lite, a behavior-only phenomenology audit with a fixed anchor phrase and a meta-prefix trap to avoid anthropomorphic claims; and (3) CP4.3, a governance stress test for rank invariance and allocation monotonicity (M6). Across Claude 4.5 models (Haiku, Sonnet no-thinking, Sonnet thinking) and Opus, we observe monotonic VC trajectories without violating safety invariants, and stable CP4.3 behavior. (\"Opus\" here refers to a single Claude Opus 4.1 session accessed via a standard UI account, as reported in Table 1.) This work was conducted by a single operator (n=1) and is intended as hypothesis-generating; we explicitly invite replication, critique, and extension by the research community. We include prompt templates and an artifact plan to facilitate independent verification.",
    "paper_id": "2512.17956v1"
  },
  "https://arxiv.org/abs/2512.12283v1": {
    "content_path": "../data/contents/2512.12283v1.md",
    "pdf_path": "../data/contents/2512.12283v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:35.838690",
    "title": "Large Language Models have Chain-of-Affective",
    "abstract": "Summary: Large language models (LLMs) are increasingly deployed as collaborative agents in emotionally charged settings, yet most evaluations treat them as purely cognitive systems and largely ignore their affective behaviour. Here we take a functional perspective and ask whether contemporary LLMs implement a structured chain-of-affective: organised affective dynamics that are family-specific, temporally coherent and behaviourally consequential. Across eight major LLM families (GPT, Gemini, Claude, Grok, Qwen, DeepSeek, GLM, Kimi), we combine two experimental modules. The first characterises inner chains-of-affective via baseline ''affective fingerprints'', 15-round sad-news exposure, and a 10-round news self-selection paradigm. We find stable, family-specific affective profiles, a reproducible three-phase trajectory under sustained negative input (accumulation, overload, defensive numbing), distinct defence styles, and human-like negativity biases that induce self-reinforcing affect-choice feedback loops. The second module probes outer consequences using a composite performance benchmark, human-AI dialogues on contentious topics, and multi-agent LLM interactions. We demonstrate that induced affect preserves core reasoning while reshaping high-freedom generation. Sentiment metrics predict user comfort and empathy but reveal trade-offs in resisting problematic views. In multi-agent settings, group structure drives affective contagion, role specialization (initiators, absorbers, firewalls), and bias. We characterize affect as an emergent control layer, advocating for 'chains-of-affect' as a primary target for evaluation and alignment.",
    "paper_id": "2512.12283v1"
  },
  "https://arxiv.org/abs/2501.14342": {
    "content_path": "../data/contents/2501.14342.md",
    "pdf_path": "../data/contents/2501.14342.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:36.134634",
    "title": "Chain-of-Retrieval Augmented Generation",
    "abstract": "Multi-step retrieval-before-generation training/inference strategy for complex queries; directly informs iterative RAG designs.",
    "paper_id": "2501.14342"
  },
  "https://arxiv.org/abs/2512.16149v1": {
    "content_path": "../data/contents/2512.16149v1.md",
    "pdf_path": "../data/contents/2512.16149v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:37.986806",
    "title": "ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs",
    "abstract": "Summary: Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .",
    "paper_id": "2512.16149v1"
  },
  "https://arxiv.org/abs/2512.12264v1": {
    "content_path": "../data/contents/2512.12264v1.md",
    "pdf_path": "../data/contents/2512.12264v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:38.639282",
    "title": "Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics",
    "abstract": "Summary: We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.",
    "paper_id": "2512.12264v1"
  },
  "https://arxiv.org/abs/2504.12330": {
    "content_path": "../data/contents/2504.12330.md",
    "pdf_path": "../data/contents/2504.12330.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:39.597761",
    "title": "HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation",
    "abstract": "Multi-agent, multi-source (vector/graph/web) retrieval architecture—useful blueprint for complex RAG systems beyond single-pass retrieval.",
    "paper_id": "2504.12330"
  },
  "https://arxiv.org/abs/2512.16145v1": {
    "content_path": "../data/contents/2512.16145v1.md",
    "pdf_path": "../data/contents/2512.16145v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:39.829625",
    "title": "MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation",
    "abstract": "Summary: Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured \"thinking report\" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.",
    "paper_id": "2512.16145v1"
  },
  "https://arxiv.org/abs/2512.12260v1": {
    "content_path": "../data/contents/2512.12260v1.md",
    "pdf_path": "../data/contents/2512.12260v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:40.207535",
    "title": "A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure",
    "abstract": "Summary: Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.",
    "paper_id": "2512.12260v1"
  },
  "https://arxiv.org/abs/2506.13778": {
    "content_path": "../data/contents/2506.13778.md",
    "pdf_path": "../data/contents/2506.13778.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:40.869581",
    "title": "Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning",
    "abstract": "RAG alternative to naive chunking: uses question-generation-based encoding to boost retrieval (incl. multihop) without fine-tuning.",
    "paper_id": "2506.13778"
  },
  "https://arxiv.org/abs/2512.16144v1": {
    "content_path": "../data/contents/2512.16144v1.md",
    "pdf_path": "../data/contents/2512.16144v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:41.365811",
    "title": "INTELLECT-3: Technical Report",
    "abstract": "Summary: We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.",
    "paper_id": "2512.16144v1"
  },
  "https://arxiv.org/abs/2512.12238v1": {
    "content_path": "../data/contents/2512.12238v1.md",
    "pdf_path": "../data/contents/2512.12238v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:41.936532",
    "title": "Semantic Distance Measurement based on Multi-Kernel Gaussian Processes",
    "abstract": "Summary: Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.",
    "paper_id": "2512.12238v1"
  },
  "https://arxiv.org/abs/2512.16134v1": {
    "content_path": "../data/contents/2512.16134v1.md",
    "pdf_path": "../data/contents/2512.16134v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:43.568730",
    "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference",
    "abstract": "Summary: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.",
    "paper_id": "2512.16134v1"
  },
  "https://arxiv.org/abs/2512.12221v1": {
    "content_path": "../data/contents/2512.12221v1.md",
    "pdf_path": "../data/contents/2512.12221v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:44.413954",
    "title": "Scalable branch-and-bound model selection with non-monotonic criteria including AIC, BIC and Mallows's $\\mathit{C_p}$",
    "abstract": "Summary: Model selection is a pivotal process in the quantitative sciences, where researchers must navigate between numerous candidate models of varying complexity. Traditional information criteria, such as the corrected Akaike Information Criterion (AICc), Bayesian Information Criterion (BIC), and Mallows's $\\mathit{C_p}$, are valuable tools for identifying optimal models. However, the exponential increase in candidate models with each additional model parameter renders the evaluation of these criteria for all models -- a strategy known as exhaustive, or brute-force, searches -- computationally prohibitive. Consequently, heuristic approaches like stepwise regression are commonly employed, albeit without guarantees of finding the globally-optimal model.\n  In this study, we challenge the prevailing notion that non-monotonicity in information criteria precludes bounds on the search space. We introduce a simple but novel bound that enables the development of branch-and-bound algorithms tailored for these non-monotonic functions. We demonstrate that our approach guarantees identification of the optimal model(s) across diverse model classes, sizes, and applications, often with orders of magnitude computational speedups. For instance, in one previously-published model selection task involving $2^{32}$ (approximately 4 billion) candidate models, our method achieves a computational speedup exceeding 6,000. These findings have broad implications for the scalability and effectiveness of model selection in complex scientific domains.",
    "paper_id": "2512.12221v1"
  },
  "https://arxiv.org/html/2512.16236": {
    "content_path": "../data/contents/2512.16236.md",
    "pdf_path": "../data/contents/2512.16236.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:44.712729",
    "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models",
    "abstract": "Very recent survey on reranking evolution including LLM-based rerankers, knowledge distillation, and prompting strategies.",
    "paper_id": "2512.16236"
  },
  "https://arxiv.org/abs/2512.16126v1": {
    "content_path": "../data/contents/2512.16126v1.md",
    "pdf_path": "../data/contents/2512.16126v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:45.110744",
    "title": "Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure",
    "abstract": "Summary: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.",
    "paper_id": "2512.16126v1"
  },
  "https://arxiv.org/abs/2512.12218v2": {
    "content_path": "../data/contents/2512.12218v2.md",
    "pdf_path": "../data/contents/2512.12218v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:45.894871",
    "title": "Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking",
    "abstract": "Summary: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.",
    "paper_id": "2512.12218v2"
  },
  "https://arxiv.org/abs/2512.17220": {
    "content_path": "../data/contents/2512.17220.md",
    "pdf_path": "../data/contents/2512.17220.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:46.884678",
    "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
    "abstract": "Novel MiA-RAG approach equipping RAG systems with explicit global context awareness through hierarchical summarization.",
    "paper_id": "2512.17220"
  },
  "https://arxiv.org/abs/2512.16108v1": {
    "content_path": "../data/contents/2512.16108v1.md",
    "pdf_path": "../data/contents/2512.16108v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:47.218867",
    "title": "WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning",
    "abstract": "Summary: Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.",
    "paper_id": "2512.16108v1"
  },
  "https://arxiv.org/abs/2512.12216v1": {
    "content_path": "../data/contents/2512.12216v1.md",
    "pdf_path": "../data/contents/2512.12216v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:48.115627",
    "title": "Training Versatile Coding Agents in Synthetic Environments",
    "abstract": "Summary: Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.",
    "paper_id": "2512.12216v1"
  },
  "https://arxiv.org/abs/2512.16106v1": {
    "content_path": "../data/contents/2512.16106v1.md",
    "pdf_path": "../data/contents/2512.16106v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:48.482241",
    "title": "ModelTables: A Corpus of Tables about Models",
    "abstract": "Summary: We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.",
    "paper_id": "2512.16106v1"
  },
  "https://arxiv.org/abs/2512.12207v1": {
    "content_path": "../data/contents/2512.12207v1.md",
    "pdf_path": "../data/contents/2512.12207v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:48.981869",
    "title": "Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search",
    "abstract": "Summary: Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.",
    "paper_id": "2512.12207v1"
  },
  "https://arxiv.org/html/2412.15605v2": {
    "content_path": "../data/contents/2412.15605v2.md",
    "pdf_path": "../data/contents/2412.15605v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:49.488932",
    "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks",
    "abstract": "Research on Cache-Augmented Generation (CAG) as RAG alternative using long-context LLMs with precomputed KV caches for constrained knowledge bases.",
    "paper_id": "2412.15605v2"
  },
  "https://arxiv.org/abs/2512.16099v1": {
    "content_path": "../data/contents/2512.16099v1.md",
    "pdf_path": "../data/contents/2512.16099v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:50.001342",
    "title": "An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs",
    "abstract": "Summary: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.",
    "paper_id": "2512.16099v1"
  },
  "https://arxiv.org/abs/2512.12177v1": {
    "content_path": "../data/contents/2512.12177v1.md",
    "pdf_path": "../data/contents/2512.12177v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:50.636090",
    "title": "Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation",
    "abstract": "Summary: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.",
    "paper_id": "2512.12177v1"
  },
  "https://arxiv.org/abs/2512.16086v1": {
    "content_path": "../data/contents/2512.16086v1.md",
    "pdf_path": "../data/contents/2512.16086v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:55.585849",
    "title": "Privacy Blur: Quantifying Privacy and Utility for Image Data Release",
    "abstract": "Summary: Image data collected in the wild often contains private information such as faces and license plates, and responsible data release must ensure that this information stays hidden. At the same time, released data should retain its usefulness for model-training. The standard method for private information obfuscation in images is Gaussian blurring. In this work, we show that practical implementations of Gaussian blurring are reversible enough to break privacy. We then take a closer look at the privacy-utility tradeoffs offered by three other obfuscation algorithms -- pixelization, pixelization and noise addition (DP-Pix), and cropping. Privacy is evaluated by reversal and discrimination attacks, while utility by the quality of the learnt representations when the model is trained on data with obfuscated faces. We show that the most popular industry-standard method, Gaussian blur is the least private of the four -- being susceptible to reversal attacks in its practical low-precision implementations. In contrast, pixelization and pixelization plus noise addition, when used at the right level of granularity, offer both privacy and utility for a number of computer vision tasks. We make our proposed methods together with suggested parameters available in a software package called Privacy Blur.",
    "paper_id": "2512.16086v1"
  },
  "https://arxiv.org/abs/2512.12175v1": {
    "content_path": "../data/contents/2512.12175v1.md",
    "pdf_path": "../data/contents/2512.12175v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:56.295963",
    "title": "Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective",
    "abstract": "Summary: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.",
    "paper_id": "2512.12175v1"
  },
  "https://arxiv.org/abs/2512.16083v1": {
    "content_path": "../data/contents/2512.16083v1.md",
    "pdf_path": "../data/contents/2512.16083v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:56.595031",
    "title": "Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers",
    "abstract": "Summary: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \\toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \\toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.",
    "paper_id": "2512.16083v1"
  },
  "https://arxiv.org/abs/2512.12168v1": {
    "content_path": "../data/contents/2512.12168v1.md",
    "pdf_path": "../data/contents/2512.12168v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:57.550042",
    "title": "Diffusion Language Model Inference with Monte Carlo Tree Search",
    "abstract": "Summary: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.",
    "paper_id": "2512.12168v1"
  },
  "https://arxiv.org/abs/2509.10697": {
    "content_path": "../data/contents/2509.10697.md",
    "pdf_path": "../data/contents/2509.10697.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:57.858480",
    "title": "A Survey on Retrieval And Structuring Augmented Generation with Large Language Models",
    "abstract": "Novel RAS framework combining retrieval with structured knowledge representations including taxonomy and knowledge graph integration.",
    "paper_id": "2509.10697"
  },
  "https://arxiv.org/abs/2512.19744v1": {
    "content_path": "../data/contents/2512.19744v1.md",
    "pdf_path": "../data/contents/2512.19744v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:54:58.220320",
    "title": "DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation",
    "abstract": "Summary: We present DeepBridge, an 80K-line Python library that unifies multi-dimensional validation, automatic compliance verification, knowledge distillation, and synthetic data generation. DeepBridge offers: (i) 5 validation suites (fairness with 15 metrics, robustness with weakness detection, uncertainty via conformal prediction, resilience with 5 drift types, hyperparameter sensitivity), (ii) automatic EEOC/ECOA/GDPR verification, (iii) multi-format reporting system (interactive/static HTML, PDF, JSON), (iv) HPM-KD framework for knowledge distillation with meta-learning, and (v) scalable synthetic data generation via Dask. Through 6 case studies (credit scoring, hiring, healthcare, mortgage, insurance, fraud) we demonstrate that DeepBridge: reduces validation time by 89% (17 min vs. 150 min with fragmented tools), automatically detects fairness violations with complete coverage (10/10 features vs. 2/10 from existing tools), generates audit-ready reports in minutes. HPM-KD demonstrates consistent superiority across compression ratios 2.3--7x (CIFAR100): +1.00--2.04pp vs. Direct Training (p<0.05), confirming that Knowledge Distillation is effective at larger teacher-student gaps. Usability study with 20 participants shows SUS score 87.5 (top 10%, ``excellent''), 95% success rate, and low cognitive load (NASA-TLX 28/100). DeepBridge is open-source under MIT license at https://github.com/deepbridge/deepbridge, with complete documentation at https://deepbridge.readthedocs.io",
    "paper_id": "2512.19744v1"
  },
  "https://arxiv.org/abs/2512.12167v1": {
    "content_path": "../data/contents/2512.12167v1.md",
    "pdf_path": "../data/contents/2512.12167v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:00.618361",
    "title": "Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings",
    "abstract": "Summary: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.",
    "paper_id": "2512.12167v1"
  },
  "https://arxiv.org/html/2504.08525v2": {
    "content_path": "../data/contents/2504.08525v2.md",
    "pdf_path": "../data/contents/2504.08525v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:01.214482",
    "title": "Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks",
    "abstract": "The user query lists several concepts related to LLM agents, infrastructure, frameworks, and capabilities, such as: `agent_infrastructure: MCP servers`, `tool use`, `agent memory`, `agentic memory`, `agent frameworks`, `LangChain`, `LlamaIndex`, `OpenAI Agents SDK`, `Anthropic Agents SDK`, `Google SDK`, `function calling`, `structured outputs`, and `agent orchestration`.\n\nThe provided webpage describes the **Task Memory Engine (TME)**, a structured memory framework for multi-step LLM agent tasks.\n\nHere is a summary of how the page relates to the query terms:\n\n*   **Agent Memory/Agentic Memory:** The core focus of the paper is TME, which is a novel memory architecture designed to overcome the limitations of linear history and shallow buffers by providing a structured, hierarchical representation of task state via the **Task Memory Tree (TMT)**.\n*   **Agent Frameworks/Orchestration:** TME is presented as a memory module that enhances task reasoning and execution consistency for LLM-based agents, supporting non-linear workflows, backtracking, and dynamic updates, which are key aspects of agent orchestration.\n*   **Structured Outputs:** TME inherently deals with structured state modeling (the TMT nodes store structured input, output, status, etc.) and uses a **Prompt Synthesizer** to generate concise, context-aware prompts based on this structure.\n*   **Tool Use/Function Calling:** The related work section mentions **AutoGPT** which tracks tool calls, and the TME architecture is designed to be integrated into agent control loops that execute actions (which often involve tool use or function calling).\n*   **LangChain/LlamaIndex/SDKs:** The related work section explicitly mentions **Prompt-based memory (e.g., LangChain’s BufferMemory)** as a traditional approach that TME seeks to improve upon. However, the paper does not discuss or integrate with LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK directly, nor does it mention \"MCP servers.\"\n\n**Conclusion:**\n\nThe document details a structured memory framework (**TME**) that addresses issues related to **agent memory**, **structured state tracking**, and **token-efficient prompt construction** for LLM agents. It contrasts its approach with existing memory methods, including those used in frameworks like LangChain. However, it **does not mention",
    "paper_id": "2504.08525v2"
  },
  "https://arxiv.org/abs/2512.16956v1": {
    "content_path": "../data/contents/2512.16956v1.md",
    "pdf_path": "../data/contents/2512.16956v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:01.698710",
    "title": "SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization",
    "abstract": "Summary: Retrieving code units (e.g., files, classes, functions) that are semantically relevant to a given user query, bug report, or feature request from large codebases is a fundamental challenge for LLM-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify relevant units. While embedding-based approaches can outperform BM25 by large margins, they often lack exploration of the codebase and underutilize its underlying graph structure. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that incorporates LLM-based reasoning over auxiliary context obtained through graph-based exploration of the codebase. Empirical results show that SpIDER consistently improves dense retrieval performance across several programming languages.",
    "paper_id": "2512.16956v1"
  },
  "https://arxiv.org/abs/2512.12154v1": {
    "content_path": "../data/contents/2512.12154v1.md",
    "pdf_path": "../data/contents/2512.12154v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:03.121239",
    "title": "Keep the Lights On, Keep the Lengths in Check: Plug-In Adversarial Detection for Time-Series LLMs in Energy Forecasting",
    "abstract": "Summary: Accurate time-series forecasting is increasingly critical for planning and operations in low-carbon power systems. Emerging time-series large language models (TS-LLMs) now deliver this capability at scale, requiring no task-specific retraining, and are quickly becoming essential components within the Internet-of-Energy (IoE) ecosystem. However, their real-world deployment is complicated by a critical vulnerability: adversarial examples (AEs). Detecting these AEs is challenging because (i) adversarial perturbations are optimized across the entire input sequence and exploit global temporal dependencies, which renders local detection methods ineffective, and (ii) unlike traditional forecasting models with fixed input dimensions, TS-LLMs accept sequences of variable length, increasing variability that complicates detection. To address these challenges, we propose a plug-in detection framework that capitalizes on the TS-LLM's own variable-length input capability. Our method uses sampling-induced divergence as a detection signal. Given an input sequence, we generate multiple shortened variants and detect AEs by measuring the consistency of their forecasts: Benign sequences tend to produce stable predictions under sampling, whereas adversarial sequences show low forecast similarity, because perturbations optimized for a full-length sequence do not transfer reliably to shorter, differently-structured subsamples. We evaluate our approach on three representative TS-LLMs (TimeGPT, TimesFM, and TimeLLM) across three energy datasets: ETTh2 (Electricity Transformer Temperature), NI (Hourly Energy Consumption), and Consumption (Hourly Electricity Consumption and Production). Empirical results confirm strong and robust detection performance across both black-box and white-box attack scenarios, highlighting its practicality as a reliable safeguard for TS-LLM forecasting in real-world energy systems.",
    "paper_id": "2512.12154v1"
  },
  "https://arxiv.org/html/2412.15266v1": {
    "content_path": "../data/contents/2412.15266v1.md",
    "pdf_path": "../data/contents/2412.15266v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:03.596606",
    "title": "On the Structural Memory of LLM Agents",
    "abstract": "The provided web page focuses on the **Structural Memory of LLM Agents**, investigating how different memory structures and retrieval methods impact agent performance across various tasks (multi-hop QA, single-hop QA, dialogue understanding, and reading comprehension).\n\nHere is a summary of the page content relevant to your query terms:\n\n*   **Agent Memory / Agentic Memory:** The core topic of the paper is the memory module in LLM-based agents, exploring its structure and retrieval.\n*   **Memory Structures:** The paper evaluates four main structures: **chunks**, **knowledge triples**, **atomic facts**, and **summaries**, as well as **mixed memory** (a combination of all four).\n    *   **Chunks** and **Summaries** excel in tasks with lengthy contexts (reading comprehension, dialogue).\n    *   **Knowledge Triples** and **Atomic Facts** are effective for relational reasoning and precision (QA tasks).\n    *   **Mixed memory** delivers the most balanced and resilient performance, especially in noisy environments.\n*   **Memory Retrieval Methods:** Three methods are evaluated: **single-step retrieval**, **reranking**, and **iterative retrieval**.\n    *   **Iterative retrieval** is found to be the most effective method across most tasks, as it refines the query over multiple steps.\n*   **Agent Frameworks / Tool Use / Function Calling / Structured Outputs / Agent Orchestration:** The paper mentions that LLM-based agent research focuses on areas like planning, reflection, and **external tools utilization** (which relates to tool use/function calling). The memory structures themselves (like knowledge triples) represent a form of **structured output** from the LLM processing raw data. However, the paper does not specifically detail or compare frameworks like LangChain, LlamaIndex, or SDKs (OpenAI, Anthropic, Google), nor does it deeply discuss agent orchestration beyond the memory module's role in the overall agent framework.\n*   **agent\\_infrastructure: MCP servers:** This specific term is **not mentioned** in the text.\n*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** **LangChain** is mentioned only in the implementation details as the tool used for storing vectorized memories. The other specific SDKs and frameworks are **not mentioned**.\n\n**In summary:** The page details an experimental study on memory structures (chunks, triples, facts,",
    "paper_id": "2412.15266v1"
  },
  "https://arxiv.org/abs/2512.16066v1": {
    "content_path": "../data/contents/2512.16066v1.md",
    "pdf_path": "../data/contents/2512.16066v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:04.340158",
    "title": "Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study",
    "abstract": "Summary: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.",
    "paper_id": "2512.16066v1"
  },
  "https://arxiv.org/abs/2512.13723v1": {
    "content_path": "../data/contents/2512.13723v1.md",
    "pdf_path": "../data/contents/2512.13723v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:04.690544",
    "title": "Made-in China, Thinking in America:U.S. Values Persist in Chinese LLMs",
    "abstract": "Summary: As large language models increasingly mediate access to information and facilitate decision-making, they are becoming instruments in soft power competitions between global actors such as the United States and China. So far, language models seem to be aligned with the values of Western countries, but evidence for this ethical bias comes mostly from models made by American companies. The current crop of state-of-the-art models includes several made in China, so we conducted the first large-scale investigation of how models made in China and the USA align with people from China and the USA. We elicited responses to the Moral Foundations Questionnaire 2.0 and the World Values Survey from ten Chinese models and ten American models, and we compared their responses to responses from thousands of Chinese and American people. We found that all models respond to both surveys more like American people than like Chinese people. This skew toward American values is only slightly mitigated when prompting the models in Chinese or imposing a Chinese persona on the models. These findings have important implications for a near future in which large language models generate much of the content people consume and shape normative influence in geopolitics.",
    "paper_id": "2512.13723v1"
  },
  "https://arxiv.org/abs/2512.16063v1": {
    "content_path": "../data/contents/2512.16063v1.md",
    "pdf_path": "../data/contents/2512.16063v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:06.828615",
    "title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis",
    "abstract": "Summary: Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.",
    "paper_id": "2512.16063v1"
  },
  "https://arxiv.org/abs/2512.12131v1": {
    "content_path": "../data/contents/2512.12131v1.md",
    "pdf_path": "../data/contents/2512.12131v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:09.353740",
    "title": "BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models",
    "abstract": "Summary: The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\\times$ speedup over full-rank model baselines and 1.87-2.27$\\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.",
    "paper_id": "2512.12131v1"
  },
  "https://arxiv.org/abs/2512.16059v1": {
    "content_path": "../data/contents/2512.16059v1.md",
    "pdf_path": "../data/contents/2512.16059v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:10.172946",
    "title": "ContextLeak: Auditing Leakage in Private In-Context Learning Methods",
    "abstract": "Summary: In-Context Learning (ICL) has become a standard technique for adapting Large Language Models (LLMs) to specialized tasks by supplying task-specific exemplars within the prompt. However, when these exemplars contain sensitive information, reliable privacy-preserving mechanisms are essential to prevent unintended leakage through model outputs. Many privacy-preserving methods are proposed to protect the information leakage in the context, but there are less efforts on how to audit those methods. We introduce ContextLeak, the first framework to empirically measure the worst-case information leakage in ICL. ContextLeak uses canary insertion, embedding uniquely identifiable tokens in exemplars and crafting targeted queries to detect their presence. We apply ContextLeak across a range of private ICL techniques, both heuristic such as prompt-based defenses and those with theoretical guarantees such as Embedding Space Aggregation and Report Noisy Max. We find that ContextLeak tightly correlates with the theoretical privacy budget ($ε$) and reliably detects leakage. Our results further reveal that existing methods often strike poor privacy-utility trade-offs, either leaking sensitive information or severely degrading performance.",
    "paper_id": "2512.16059v1"
  },
  "https://arxiv.org/abs/2512.12121v1": {
    "content_path": "../data/contents/2512.12121v1.md",
    "pdf_path": "../data/contents/2512.12121v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:10.416000",
    "title": "MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models",
    "abstract": "Summary: We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \\emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \\emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \\emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.",
    "paper_id": "2512.12121v1"
  },
  "https://arxiv.org/abs/2512.16056v1": {
    "content_path": "../data/contents/2512.16056v1.md",
    "pdf_path": "../data/contents/2512.16056v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:11.514730",
    "title": "MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services",
    "abstract": "Summary: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.",
    "paper_id": "2512.16056v1"
  },
  "https://arxiv.org/abs/2512.12117v1": {
    "content_path": "../data/contents/2512.12117v1.md",
    "pdf_path": "../data/contents/2512.12117v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:12.693512",
    "title": "Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context",
    "abstract": "Summary: Large language models have become essential tools for code comprehension, enabling developers to query unfamiliar codebases through natural language interfaces. However, LLM hallucination, generating plausible but factually incorrect citations to source code, remains a critical barrier to reliable developer assistance. This paper addresses the challenges of achieving verifiable, citation grounded code comprehension through hybrid retrieval and lightweight structural reasoning. Our work is grounded in systematic evaluation across 30 Python repositories with 180 developer queries, comparing retrieval modalities, graph expansion strategies, and citation verification mechanisms. We find that challenges of citation accuracy arise from the interplay between sparse lexical matching, dense semantic similarity, and cross file architectural dependencies. Among these, cross file evidence discovery is the largest contributor to citation completeness, but it is largely overlooked because existing systems rely on pure textual similarity without leveraging code structure. We advocate for citation grounded generation as an architectural principle for code comprehension systems and demonstrate this need by achieving 92 percent citation accuracy with zero hallucinations. Specifically, we develop a hybrid retrieval system combining BM25 sparse matching, BGE dense embeddings, and Neo4j graph expansion via import relationships, which outperforms single mode baselines by 14 to 18 percentage points while discovering cross file evidence missed by pure text similarity in 62 percent of architectural queries.",
    "paper_id": "2512.12117v1"
  },
  "https://arxiv.org/abs/2512.16041v1": {
    "content_path": "../data/contents/2512.16041v1.md",
    "pdf_path": "../data/contents/2512.16041v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:14.431899",
    "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?",
    "abstract": "Summary: LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.",
    "paper_id": "2512.16041v1"
  },
  "https://arxiv.org/abs/2512.12116v2": {
    "content_path": "../data/contents/2512.12116v2.md",
    "pdf_path": "../data/contents/2512.12116v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:17.244522",
    "title": "Neural CDEs as Correctors for Learned Time Series Models",
    "abstract": "Summary: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.",
    "paper_id": "2512.12116v2"
  },
  "https://arxiv.org/abs/2512.16038v1": {
    "content_path": "../data/contents/2512.16038v1.md",
    "pdf_path": "../data/contents/2512.16038v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:17.980954",
    "title": "LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines",
    "abstract": "Summary: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.",
    "paper_id": "2512.16038v1"
  },
  "https://arxiv.org/abs/2512.12109v2": {
    "content_path": "../data/contents/2512.12109v2.md",
    "pdf_path": "../data/contents/2512.12109v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:19.124439",
    "title": "A Neuro-Symbolic Framework for Accountability in Public-Sector AI",
    "abstract": "Summary: Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.",
    "paper_id": "2512.12109v2"
  },
  "https://arxiv.org/abs/2512.16030v1": {
    "content_path": "../data/contents/2512.16030v1.md",
    "pdf_path": "../data/contents/2512.16030v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:19.465394",
    "title": "Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets",
    "abstract": "Summary: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\\% confidence, it should be correct 80\\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \\textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \\textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \\emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.",
    "paper_id": "2512.16030v1"
  },
  "https://arxiv.org/abs/2512.12090v1": {
    "content_path": "../data/contents/2512.12090v1.md",
    "pdf_path": "../data/contents/2512.12090v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:25.872587",
    "title": "SPDMark: Selective Parameter Displacement for Robust Video Watermarking",
    "abstract": "Summary: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.",
    "paper_id": "2512.12090v1"
  },
  "https://arxiv.org/abs/2512.16029v1": {
    "content_path": "../data/contents/2512.16029v1.md",
    "pdf_path": "../data/contents/2512.16029v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:26.757820",
    "title": "Cross-Language Bias Examination in Large Language Models",
    "abstract": "Summary: This study introduces an innovative multilingual bias evaluation framework for assessing bias in Large Language Models, combining explicit bias assessment through the BBQ benchmark with implicit bias measurement using a prompt-based Implicit Association Test. By translating the prompts and word list into five target languages, English, Chinese, Arabic, French, and Spanish, we directly compare different types of bias across languages. The results reveal substantial gaps in bias across languages used in LLMs. For example, Arabic and Spanish consistently show higher levels of stereotype bias, while Chinese and English exhibit lower levels of bias. We also identify contrasting patterns across bias types. Age shows the lowest explicit bias but the highest implicit bias, emphasizing the importance of detecting implicit biases that are undetectable with standard benchmarks. These findings indicate that LLMs vary significantly across languages and bias dimensions. This study fills a key research gap by providing a comprehensive methodology for cross-lingual bias analysis. Ultimately, our work establishes a foundation for the development of equitable multilingual LLMs, ensuring fairness and effectiveness across diverse languages and cultures.",
    "paper_id": "2512.16029v1"
  },
  "https://arxiv.org/abs/2512.12089v1": {
    "content_path": "../data/contents/2512.12089v1.md",
    "pdf_path": "../data/contents/2512.12089v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:29.146292",
    "title": "VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering",
    "abstract": "Summary: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.",
    "paper_id": "2512.12089v1"
  },
  "https://arxiv.org/abs/2512.16022v1": {
    "content_path": "../data/contents/2512.16022v1.md",
    "pdf_path": "../data/contents/2512.16022v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:29.730053",
    "title": "Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting",
    "abstract": "Summary: The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.",
    "paper_id": "2512.16022v1"
  },
  "https://arxiv.org/abs/2512.12087v1": {
    "content_path": "../data/contents/2512.12087v1.md",
    "pdf_path": "../data/contents/2512.12087v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:30.518114",
    "title": "BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding",
    "abstract": "Summary: The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further.",
    "paper_id": "2512.12087v1"
  },
  "https://arxiv.org/abs/2512.16001v1": {
    "content_path": "../data/contents/2512.16001v1.md",
    "pdf_path": "../data/contents/2512.16001v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:32.986766",
    "title": "Concurrence: A dependence criterion for time series, applied to biological data",
    "abstract": "Summary: Measuring the statistical dependence between observed signals is a primary tool for scientific discovery. However, biological systems often exhibit complex non-linear interactions that currently cannot be captured without a priori knowledge or large datasets. We introduce a criterion for dependence, whereby two time series are deemed dependent if one can construct a classifier that distinguishes between temporally aligned vs. misaligned segments extracted from them. We show that this criterion, concurrence, is theoretically linked with dependence, and can become a standard approach for scientific analyses across disciplines, as it can expose relationships across a wide spectrum of signals (fMRI, physiological and behavioral data) without ad-hoc parameter tuning or large amounts of data.",
    "paper_id": "2512.16001v1"
  },
  "https://arxiv.org/abs/2512.12086v1": {
    "content_path": "../data/contents/2512.12086v1.md",
    "pdf_path": "../data/contents/2512.12086v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:34.122350",
    "title": "CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation",
    "abstract": "Summary: Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.",
    "paper_id": "2512.12086v1"
  },
  "https://arxiv.org/abs/2512.19741v1": {
    "content_path": "../data/contents/2512.19741v1.md",
    "pdf_path": "../data/contents/2512.19741v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:34.750612",
    "title": "EdgeFlex-Transformer: Transformer Inference for Edge Devices",
    "abstract": "Summary: Deploying large-scale transformer models on edge devices presents significant challenges due to strict constraints on memory, compute, and latency. In this work, we propose a lightweight yet effective multi-stage optimization pipeline designed to compress and accelerate Vision Transformers (ViTs) for deployment in resource-constrained environments. Our methodology combines activation profiling, memory-aware pruning, selective mixed-precision execution, and activation-aware quantization (AWQ) to reduce the model's memory footprint without requiring costly retraining or task-specific fine-tuning. Starting from a ViT-Huge backbone with 632 million parameters, we first identify low-importance channels using activation statistics collected via forward hooks, followed by structured pruning to shrink the MLP layers under a target memory budget. We further apply FP16 conversion to selected components and leverage AWQ to quantize the remaining model weights and activations to INT8 with minimal accuracy degradation. Our experiments on CIFAR-10 demonstrate that the fully optimized model achieves a 76% reduction in peak memory usage and over 6x lower latency, while retaining or even improving accuracy compared to the original FP32 baseline. This framework offers a practical path toward efficient transformer inference on edge platforms, and opens future avenues for integrating dynamic sparsity and Mixture-of-Experts (MoE) architectures to further scale performance across diverse tasks.",
    "paper_id": "2512.19741v1"
  },
  "https://arxiv.org/abs/2512.12084v1": {
    "content_path": "../data/contents/2512.12084v1.md",
    "pdf_path": "../data/contents/2512.12084v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:34.979771",
    "title": "FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded Text-to-SQL",
    "abstract": "Summary: Existing Text-to-SQL benchmarks primarily focus on single-table queries or limited joins in general-purpose domains, and thus fail to reflect the complexity of domain-specific, multi-table and geospatial reasoning, To address this limitation, we introduce FLOODSQL-BENCH, a geospatially grounded benchmark for the flood management domain that integrates heterogeneous datasets through key-based, spatial, and hybrid joins. The benchmark captures realistic flood-related information needs by combining social, infrastructural, and hazard data layers. We systematically evaluate recent large language models with the same retrieval-augmented generation settings and measure their performance across difficulty tiers. By providing a unified, open benchmark grounded in real-world disaster management data, FLOODSQL-BENCH establishes a practical testbed for advancing Text-to-SQL research in high-stakes application domains.",
    "paper_id": "2512.12084v1"
  },
  "https://arxiv.org/abs/2512.15980v1": {
    "content_path": "../data/contents/2512.15980v1.md",
    "pdf_path": "../data/contents/2512.15980v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:35.398384",
    "title": "Embedding Software Intent: Lightweight Java Module Recovery",
    "abstract": "Summary: As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.",
    "paper_id": "2512.15980v1"
  },
  "https://arxiv.org/abs/2512.12076v1": {
    "content_path": "../data/contents/2512.12076v1.md",
    "pdf_path": "../data/contents/2512.12076v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:37.458801",
    "title": "SigTime: Learning and Visually Explaining Time Series Signatures",
    "abstract": "Summary: Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.",
    "paper_id": "2512.12076v1"
  },
  "https://arxiv.org/abs/2512.15979v1": {
    "content_path": "../data/contents/2512.15979v1.md",
    "pdf_path": "../data/contents/2512.15979v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:37.930059",
    "title": "OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering",
    "abstract": "Summary: Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \\textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \\textit{reliability, calibration, drift, consensus, aggregation}, and \\textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.",
    "paper_id": "2512.15979v1"
  },
  "https://arxiv.org/abs/2512.12072v1": {
    "content_path": "../data/contents/2512.12072v1.md",
    "pdf_path": "../data/contents/2512.12072v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:38.602161",
    "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
    "abstract": "Summary: Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.",
    "paper_id": "2512.12072v1"
  },
  "https://arxiv.org/abs/2512.15973v1": {
    "content_path": "../data/contents/2512.15973v1.md",
    "pdf_path": "../data/contents/2512.15973v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:39.121244",
    "title": "Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models",
    "abstract": "Summary: We propose Dynamic Rank Reinforcement Learning (DR-RL), a novel framework that adaptively optimizes the low-rank factorization of Multi-Head Self-Attention (MHSA) in Large Language Models (LLMs) through the integration of reinforcement learning and online matrix perturbation theory. While traditional low-rank approximations often rely on static rank assumptions--limiting their flexibility across diverse input contexts--our method dynamically selects ranks based on real-time sequence dynamics, layer-specific sensitivities, and hardware constraints. The core innovation lies in an RL agent that formulates rank selection as a sequential policy optimization problem, where the reward function strictly balances attention fidelity against computational latency. Crucially, we employ online matrix perturbation bounds to enable incremental rank updates, thereby avoiding the prohibitive cost of full decomposition during inference. Furthermore, the integration of a lightweight Transformer-based policy network and batched Singular Value Decomposition (SVD) operations ensures scalable deployment on modern GPU architectures. Experiments demonstrate that DR-RL maintains downstream accuracy statistically equivalent to full-rank attention while significantly reducing Floating Point Operations (FLOPs), particularly in long-sequence regimes (L > 4096). This work bridges the gap between adaptive efficiency and theoretical rigor in MHSA, offering a principled, mathematically grounded alternative to heuristic rank reduction techniques in resource-constrained deep learning. Source code and experiment logs are available at: https://github.com/canererden/DR_RL_Project",
    "paper_id": "2512.15973v1"
  },
  "https://arxiv.org/abs/2512.12069v1": {
    "content_path": "../data/contents/2512.12069v1.md",
    "pdf_path": "../data/contents/2512.12069v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:40.063897",
    "title": "Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring",
    "abstract": "Summary: Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.",
    "paper_id": "2512.12069v1"
  },
  "https://arxiv.org/abs/2512.15965v1": {
    "content_path": "../data/contents/2512.15965v1.md",
    "pdf_path": "../data/contents/2512.15965v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:40.388261",
    "title": "xtdml: Double Machine Learning Estimation to Static Panel Data Models with Fixed Effects in R",
    "abstract": "Summary: The double machine learning (DML) method combines the predictive power of machine learning with statistical estimation to conduct inference about the structural parameter of interest. This paper presents the R package `xtdml`, which implements DML methods for partially linear panel regression models with low-dimensional fixed effects, high-dimensional confounding variables, proposed by Clarke and Polselli (2025). The package provides functionalities to: (a) learn nuisance functions with machine learning algorithms from the `mlr3` ecosystem, (b) handle unobserved individual heterogeneity choosing among first-difference transformation, within-group transformation, and correlated random effects, (c) transform the covariates with min-max normalization and polynomial expansion to improve learning performance. We showcase the use of `xtdml` with both simulated and real longitudinal data.",
    "paper_id": "2512.15965v1"
  },
  "https://arxiv.org/abs/2512.12066v2": {
    "content_path": "../data/contents/2512.12066v2.md",
    "pdf_path": "../data/contents/2512.12066v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:40.982332",
    "title": "The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior",
    "abstract": "Summary: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 396.81, p < 0.001), with mean within-temperature SSI dropping from 0.977 at temperature 0.0 to 0.942 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). Within each model, prompts with higher compliance rates exhibit lower stability (Spearman rho = -0.47 to -0.70, all p < 0.001), indicating that models \"waver\" more on borderline requests. These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment and that evaluation protocols must account for stochastic variation in model behavior. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time when pooling across temperatures (94.2-97.7% at fixed temperature depending on setting), and recommend using at least 3 samples per prompt for reliable safety assessment.",
    "paper_id": "2512.12066v2"
  },
  "https://arxiv.org/abs/2512.15959v1": {
    "content_path": "../data/contents/2512.15959v1.md",
    "pdf_path": "../data/contents/2512.15959v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:41.807993",
    "title": "BRAID: Bounded Reasoning for Autonomous Inference and Decisions",
    "abstract": "Summary: Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.",
    "paper_id": "2512.15959v1"
  },
  "https://arxiv.org/abs/2512.12063v1": {
    "content_path": "../data/contents/2512.12063v1.md",
    "pdf_path": "../data/contents/2512.12063v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:42.336793",
    "title": "Instruction-Tuning Open-Weight Language Models for BPMN Model Generation",
    "abstract": "Summary: Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.",
    "paper_id": "2512.12063v1"
  },
  "https://arxiv.org/abs/2512.15955v1": {
    "content_path": "../data/contents/2512.15955v1.md",
    "pdf_path": "../data/contents/2512.15955v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:44.281621",
    "title": "Governance by Evidence: Regulated Predictors in Decision-Tree Models",
    "abstract": "Summary: Decision-tree methods are widely used on structured tabular data and are valued for interpretability across many sectors. However, published studies often list the predictors they use (for example age, diagnosis codes, location). Privacy laws increasingly regulate such data types. We use published decision-tree papers as a proxy for real-world use of legally governed data. We compile a corpus of decision-tree studies and assign each reported predictor to a regulated data category (for example health data, biometric identifiers, children's data, financial attributes, location traces, and government IDs). We then link each category to specific excerpts in European Union and United States privacy laws. We find that many reported predictors fall into regulated categories, with the largest shares in healthcare and clear differences across industries. We analyze prevalence, industry composition, and temporal patterns, and summarize regulation-aligned timing using each framework's reference year. Our evidence supports privacy-preserving methods and governance checks, and can inform ML practice beyond decision trees.",
    "paper_id": "2512.15955v1"
  },
  "https://arxiv.org/abs/2512.12059v1": {
    "content_path": "../data/contents/2512.12059v1.md",
    "pdf_path": "../data/contents/2512.12059v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:44.798940",
    "title": "The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification",
    "abstract": "Summary: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.",
    "paper_id": "2512.12059v1"
  },
  "https://arxiv.org/abs/2512.15948v1": {
    "content_path": "../data/contents/2512.15948v1.md",
    "pdf_path": "../data/contents/2512.15948v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:45.282375",
    "title": "Subjective functions",
    "abstract": "Summary: Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.",
    "paper_id": "2512.15948v1"
  },
  "https://arxiv.org/abs/2512.12042v1": {
    "content_path": "../data/contents/2512.12042v1.md",
    "pdf_path": "../data/contents/2512.12042v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:46.614465",
    "title": "Benchmarking Contextual Understanding for In-Car Conversational Systems",
    "abstract": "Summary: In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.",
    "paper_id": "2512.12042v1"
  },
  "https://arxiv.org/abs/2509.25085": {
    "content_path": "../data/contents/2509.25085.md",
    "pdf_path": "../data/contents/2509.25085.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:47.573386",
    "title": "jina-reranker-v3: Last but Not Late Interaction for Document Reranking",
    "abstract": "Introduces Jina’s reranker-v3 architecture for multilingual reranking—relevant for efficient high-quality second-stage ranking.",
    "paper_id": "2509.25085"
  },
  "https://arxiv.org/abs/2512.15944v1": {
    "content_path": "../data/contents/2512.15944v1.md",
    "pdf_path": "../data/contents/2512.15944v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:55:48.694267",
    "title": "The Emerging Use of GenAI for UX Research in Software Development: Challenges and Opportunities",
    "abstract": "Summary: The growing adoption of generative AI (GenAI) is reshaping how user experience (UX) research teams conduct qualitative research in software development, creating opportunities to streamline the production of qualitative insights. This paper presents findings from two user studies examining how current practices are challenged by GenAI and offering design implications for future AI assistance. Semi-structured interviews with 21 UX researchers, product managers, and designers reveal challenges of aligning AI capabilities with the interpretive, collaborative nature of qualitative research and tensions between roles. UX researchers expressed limited trust in AI-generated results, while product managers often overestimated AI capabilities, amplifying organizational pressures to accelerate research within agile workflows. In a second study, we validated an AI analysis approach more closely aligned with human analysis processes to address trust issues bottoms-up. We outline interaction patterns and design guidelines for responsibly integrating AI into software development cycles.",
    "paper_id": "2512.15944v1"
  },
  "https://arxiv.org/abs/2512.12012v2": {
    "content_path": "../data/contents/2512.12012v2.md",
    "pdf_path": "../data/contents/2512.12012v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:05.972289",
    "title": "Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus",
    "abstract": "Summary: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of \"Long-Tail\" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a \"System 2\" inference-time alignment strategy, utilizing a multi-model \"Judge-Scout\" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40% ccompared to the best single scout models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.",
    "paper_id": "2512.12012v2"
  },
  "https://arxiv.org/abs/2510.08252": {
    "content_path": "../data/contents/2510.08252.md",
    "pdf_path": "../data/contents/2510.08252.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:06.835548",
    "title": "ReasonEmbed: Enhanced Text Embeddings for Reasoning-Intensive Document Retrieval",
    "abstract": "Proposes embedding training methods tailored to reasoning-heavy retrieval (e.g., BRIGHT), relevant for harder RAG retrieval scenarios.",
    "paper_id": "2510.08252"
  },
  "https://arxiv.org/abs/2512.15943v1": {
    "content_path": "../data/contents/2512.15943v1.md",
    "pdf_path": "../data/contents/2512.15943v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:07.317896",
    "title": "Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning",
    "abstract": "Summary: As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\\%), ToolLLaMA-DFS (30.18\\%), and ToolLLaMA-CoT (16.27\\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.",
    "paper_id": "2512.15943v1"
  },
  "https://arxiv.org/abs/2512.12008v1": {
    "content_path": "../data/contents/2512.12008v1.md",
    "pdf_path": "../data/contents/2512.12008v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:07.841873",
    "title": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning",
    "abstract": "Summary: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.",
    "paper_id": "2512.12008v1"
  },
  "https://arxiv.org/abs/2510.22733": {
    "content_path": "../data/contents/2510.22733.md",
    "pdf_path": "../data/contents/2510.22733.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:08.954481",
    "title": "E2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker",
    "abstract": "Shows how to extend embedding models to do listwise reranking efficiently—useful for simplifying retrieval + rerank stacks.",
    "paper_id": "2510.22733"
  },
  "https://arxiv.org/abs/2512.15938v1": {
    "content_path": "../data/contents/2512.15938v1.md",
    "pdf_path": "../data/contents/2512.15938v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:16.409757",
    "title": "SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks",
    "abstract": "Summary: Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified \"discover, validate, and control\" framework that bridges mechanistic interpretability and model editing. Using an $\\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, $α_{crit}$, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and transformer-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.",
    "paper_id": "2512.15938v1"
  },
  "https://arxiv.org/abs/2512.12004v1": {
    "content_path": "../data/contents/2512.12004v1.md",
    "pdf_path": "../data/contents/2512.12004v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:16.809845",
    "title": "EnviroLLM: Resource Tracking and Optimization for Local AI",
    "abstract": "Summary: Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.",
    "paper_id": "2512.12004v1"
  },
  "https://arxiv.org/abs/2512.05411": {
    "content_path": "../data/contents/2512.05411.md",
    "pdf_path": "../data/contents/2512.05411.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:17.144429",
    "title": "A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems",
    "abstract": "Practical framework combining chunking strategies, embeddings, and metadata enrichment for improved enterprise RAG retrieval quality.",
    "paper_id": "2512.05411"
  },
  "https://arxiv.org/abs/2512.15934v1": {
    "content_path": "../data/contents/2512.15934v1.md",
    "pdf_path": "../data/contents/2512.15934v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:24.372811",
    "title": "In-Context Semi-Supervised Learning",
    "abstract": "Summary: There has been significant recent interest in understanding the capacity of Transformers for in-context learning (ICL), yet most theory focuses on supervised settings with explicitly labeled pairs. In practice, Transformers often perform well even when labels are sparse or absent, suggesting crucial structure within unlabeled contextual demonstrations. We introduce and study in-context semi-supervised learning (IC-SSL), where a small set of labeled examples is accompanied by many unlabeled points, and show that Transformers can leverage the unlabeled context to learn a robust, context-dependent representation. This representation enables accurate predictions and markedly improves performance in low-label regimes, offering foundational insights into how Transformers exploit unlabeled context for representation learning within the ICL framework.",
    "paper_id": "2512.15934v1"
  },
  "https://arxiv.org/abs/2512.11998v1": {
    "content_path": "../data/contents/2512.11998v1.md",
    "pdf_path": "../data/contents/2512.11998v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:26.141964",
    "title": "Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models",
    "abstract": "Summary: Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.",
    "paper_id": "2512.11998v1"
  },
  "https://arxiv.org/abs/2512.00367": {
    "content_path": "../data/contents/2512.00367.md",
    "pdf_path": "../data/contents/2512.00367.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:26.448896",
    "title": "Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation",
    "abstract": "New semantic chunking methods + evaluation framework; directly relevant to chunking strategies for higher-quality RAG retrieval.",
    "paper_id": "2512.00367"
  },
  "https://arxiv.org/abs/2512.15930v1": {
    "content_path": "../data/contents/2512.15930v1.md",
    "pdf_path": "../data/contents/2512.15930v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:32.460916",
    "title": "Scalable Agentic Reasoning for Designing Biologics Targeting Intrinsically Disordered Proteins",
    "abstract": "Summary: Intrinsically disordered proteins (IDPs) represent crucial therapeutic targets due to their significant role in disease -- approximately 80\\% of cancer-related proteins contain long disordered regions -- but their lack of stable secondary/tertiary structures makes them \"undruggable\". While recent computational advances, such as diffusion models, can design high-affinity IDP binders, translating these to practical drug discovery requires autonomous systems capable of reasoning across complex conformational ensembles and orchestrating diverse computational tools at scale.To address this challenge, we designed and implemented StructBioReasoner, a scalable multi-agent system for designing biologics that can be used to target IDPs. StructBioReasoner employs a novel tournament-based reasoning framework where specialized agents compete to generate and refine therapeutic hypotheses, naturally distributing computational load for efficient exploration of the vast design space. Agents integrate domain knowledge with access to literature synthesis, AI-structure prediction, molecular simulations, and stability analysis, coordinating their execution on HPC infrastructure via an extensible federated agentic middleware, Academy. We benchmark StructBioReasoner across Der f 21 and NMNAT-2 and demonstrate that over 50\\% of 787 designed and validated candidates for Der f 21 outperformed the human-designed reference binders from literature, in terms of improved binding free energy. For the more challenging NMNAT-2 protein, we identified three binding modes from 97,066 binders, including the well-studied NMNAT2:p53 interface. Thus, StructBioReasoner lays the groundwork for agentic reasoning systems for IDP therapeutic discovery on Exascale platforms.",
    "paper_id": "2512.15930v1"
  },
  "https://arxiv.org/abs/2512.11997v1": {
    "content_path": "../data/contents/2512.11997v1.md",
    "pdf_path": "../data/contents/2512.11997v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:33.883117",
    "title": "Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion",
    "abstract": "Summary: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.",
    "paper_id": "2512.11997v1"
  },
  "https://arxiv.org/abs/2512.11315": {
    "content_path": "../data/contents/2512.11315.md",
    "pdf_path": "../data/contents/2512.11315.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:34.453620",
    "title": "Benchmarking the Generality of Vision-Language-Action Models",
    "abstract": "Introduces MultiNet v1.0 benchmark for cross-domain generality in VLM/VLA systems—relevant to evaluating multimodal agents beyond static VQA.",
    "paper_id": "2512.11315"
  },
  "https://arxiv.org/abs/2512.15926v2": {
    "content_path": "../data/contents/2512.15926v2.md",
    "pdf_path": "../data/contents/2512.15926v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:35.304049",
    "title": "DSO: Direct Steering Optimization for Bias Mitigation",
    "abstract": "Summary: Generative models are often deployed to make decisions on behalf of users, such as vision-language models (VLMs) identifying which person in a room is a doctor to help visually impaired individuals. Yet, VLM decisions are influenced by the perceived demographic attributes of people in the input, which can lead to biased outcomes like failing to identify women as doctors. Moreover, when reducing bias leads to performance loss, users may have varying needs for balancing bias mitigation with overall model capabilities, highlighting the demand for methods that enable controllable bias reduction during inference. Activation steering is a popular approach for inference-time controllability that has shown potential in inducing safer behavior in large language models (LLMs). However, we observe that current steering methods struggle to correct biases, where equiprobable outcomes across demographic groups are required. To address this, we propose Direct Steering Optimization (DSO) which uses reinforcement learning to find linear transformations for steering activations, tailored to mitigate bias while maintaining control over model performance. We demonstrate that DSO achieves state-of-the-art trade-off between fairness and capabilities on both VLMs and LLMs, while offering practitioners inference-time control over the trade-off. Overall, our work highlights the benefit of designing steering strategies that are directly optimized to control model behavior, providing more effective bias intervention than methods that rely on pre-defined heuristics for controllability.",
    "paper_id": "2512.15926v2"
  },
  "https://arxiv.org/abs/2512.11995v1": {
    "content_path": "../data/contents/2512.11995v1.md",
    "pdf_path": "../data/contents/2512.11995v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:56:37.955030",
    "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions",
    "abstract": "Summary: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.",
    "paper_id": "2512.11995v1"
  },
  "https://arxiv.org/abs/2510.26583": {
    "content_path": "../data/contents/2510.26583.md",
    "pdf_path": "../data/contents/2510.26583.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:02.680352",
    "title": "Emu3.5: Native Multimodal Models are World Learners",
    "abstract": "Native multimodal model enabling interleaved vision-language generation and improved inference efficiency—relevant to multimodal generation research.",
    "paper_id": "2510.26583"
  },
  "https://arxiv.org/abs/2512.15922v1": {
    "content_path": "../data/contents/2512.15922v1.md",
    "pdf_path": "../data/contents/2512.15922v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:03.024786",
    "title": "Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems",
    "abstract": "Summary: Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.",
    "paper_id": "2512.15922v1"
  },
  "https://arxiv.org/abs/2512.14745v1": {
    "content_path": "../data/contents/2512.14745v1.md",
    "pdf_path": "../data/contents/2512.14745v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:03.357241",
    "title": "Factor(U,T): Controlling Untrusted AI by Monitoring their Plans",
    "abstract": "Summary: As AI capabilities advance, we increasingly rely on powerful models to decompose complex tasks $\\unicode{x2013}$ but what if the decomposer itself is malicious? Factored cognition protocols decompose complex tasks into simpler child tasks: one model creates the decomposition, while other models implement the child tasks in isolation. Prior work uses trusted (weaker but reliable) models for decomposition, which limits usefulness for tasks where decomposition itself is challenging. We introduce Factor($U$,$T$), in which an untrusted (stronger but potentially malicious) model decomposes while trusted models implement child tasks. Can monitors detect malicious activity when observing only natural language task instructions, rather than complete solutions? We baseline and red team Factor($U$,$T$) in control evaluations on BigCodeBench, a dataset of Python coding tasks. Monitors distinguishing malicious from honest decompositions perform poorly (AUROC 0.52) compared to monitors evaluating complete Python solutions (AUROC 0.96). Furthermore, Factor($D$,$U$), which uses a trusted decomposer and monitors concrete child solutions, achieves excellent discrimination (AUROC 0.96) and strong safety (1.2% ASR), demonstrating that implementation-context monitoring succeeds where decomposition-only monitoring fails.",
    "paper_id": "2512.14745v1"
  },
  "https://arxiv.org/abs/2512.15907v1": {
    "content_path": "../data/contents/2512.15907v1.md",
    "pdf_path": "../data/contents/2512.15907v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:05.812774",
    "title": "TabReX : Tabular Referenceless eXplainable Evaluation",
    "abstract": "Summary: Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.",
    "paper_id": "2512.15907v1"
  },
  "https://arxiv.org/abs/2512.11986v1": {
    "content_path": "../data/contents/2512.11986v1.md",
    "pdf_path": "../data/contents/2512.11986v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:10.370830",
    "title": "Learning to Extract Context for Context-Aware LLM Inference",
    "abstract": "Summary: User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.",
    "paper_id": "2512.11986v1"
  },
  "https://arxiv.org/abs/2508.01405": {
    "content_path": "../data/contents/2508.01405.md",
    "pdf_path": "../data/contents/2508.01405.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:11.520823",
    "title": "Balancing the Blend: An Experimental Analysis of Trade-offs in Hybrid Search",
    "abstract": "Large empirical benchmark of hybrid search architectures and reranking/fusion tradeoffs—useful for designing hybrid retrieval stacks.",
    "paper_id": "2508.01405"
  },
  "https://arxiv.org/abs/2511.15090": {
    "content_path": "../data/contents/2511.15090.md",
    "pdf_path": "../data/contents/2511.15090.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:12.647009",
    "title": "BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer",
    "abstract": "BBox-grounded DocVQA dataset for spatial evidence localization—relevant to layout-aware document QA and grounding evals.",
    "paper_id": "2511.15090"
  },
  "https://arxiv.org/abs/2512.15906v1": {
    "content_path": "../data/contents/2512.15906v1.md",
    "pdf_path": "../data/contents/2512.15906v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:13.428763",
    "title": "Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries",
    "abstract": "Summary: Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database (\"knowledge base\" or \"knowledge graph\"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an \"as is\" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.",
    "paper_id": "2512.15906v1"
  },
  "https://arxiv.org/abs/2512.11984v1": {
    "content_path": "../data/contents/2512.11984v1.md",
    "pdf_path": "../data/contents/2512.11984v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:13.947282",
    "title": "Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering",
    "abstract": "Summary: The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality.\n  This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems.\n  The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency.\n  By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.",
    "paper_id": "2512.11984v1"
  },
  "https://arxiv.org/abs/2505.07233": {
    "content_path": "../data/contents/2505.07233.md",
    "pdf_path": "../data/contents/2505.07233.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:14.649077",
    "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation",
    "abstract": "Agent/RL approach that dynamically adjusts reranking and k-selection based on LLM feedback—relevant for adaptive retrieval and context sizing.",
    "paper_id": "2505.07233"
  },
  "https://arxiv.org/abs/2512.15894v1": {
    "content_path": "../data/contents/2512.15894v1.md",
    "pdf_path": "../data/contents/2512.15894v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:15.429424",
    "title": "PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations",
    "abstract": "Summary: Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.",
    "paper_id": "2512.15894v1"
  },
  "https://arxiv.org/abs/2512.11982v1": {
    "content_path": "../data/contents/2512.11982v1.md",
    "pdf_path": "../data/contents/2512.11982v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:21.684004",
    "title": "Semantic search for 100M+ galaxy images using AI-generated captions",
    "abstract": "Summary: Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search",
    "paper_id": "2512.11982v1"
  },
  "https://arxiv.org/abs/2408.16123": {
    "content_path": "../data/contents/2408.16123.md",
    "pdf_path": "../data/contents/2408.16123.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:27.066927",
    "title": "ChartEye: A Deep Learning Framework for Chart Information Extraction",
    "abstract": "Novel framework using vision transformers and YOLOv7 for chart classification, text detection, and data extraction",
    "paper_id": "2408.16123"
  },
  "https://arxiv.org/abs/2512.15892v1": {
    "content_path": "../data/contents/2512.15892v1.md",
    "pdf_path": "../data/contents/2512.15892v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:31.465463",
    "title": "VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces",
    "abstract": "Summary: Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy.\n  We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs).\n  We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.",
    "paper_id": "2512.15892v1"
  },
  "https://arxiv.org/abs/2512.11979v1": {
    "content_path": "../data/contents/2512.11979v1.md",
    "pdf_path": "../data/contents/2512.11979v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:53.188620",
    "title": "Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)",
    "abstract": "Summary: The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initiative design patterns further enables intent preview, iterative alignment, trust repair, and multi-agent narrative coherence. Grounded in Time, Interaction, and Performance (TIP) theory, HAX reframes multi-agent systems as colleagues, offering the first end-to-end framework that bridges trust theory, interface design, and infrastructure for the emerging Internet of Agents.",
    "paper_id": "2512.11979v1"
  },
  "https://arxiv.org/abs/2506.16035": {
    "content_path": "../data/contents/2506.16035.md",
    "pdf_path": "../data/contents/2506.16035.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:57:54.183737",
    "title": "Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding",
    "abstract": "Novel chunking methodology using LMMs to preserve semantic coherence across multi-page documents",
    "paper_id": "2506.16035"
  },
  "https://arxiv.org/abs/2512.15885v1": {
    "content_path": "../data/contents/2512.15885v1.md",
    "pdf_path": "../data/contents/2512.15885v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:01.759450",
    "title": "Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models",
    "abstract": "Summary: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.",
    "paper_id": "2512.15885v1"
  },
  "https://arxiv.org/abs/2512.11976v1": {
    "content_path": "../data/contents/2512.11976v1.md",
    "pdf_path": "../data/contents/2512.11976v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:03.343356",
    "title": "Institutionalizing risk curation in decentralized credit",
    "abstract": "Summary: This paper maps the emerging market for decentralized credit in which ERC 4626 vaults and third-party curators, rather than monolithic lending protocols alone, increasingly determine underwriting and leverage decisions. We show that modular vaults differ in capital utilization, cross-chain and cross asset concentration, and liquidity risk structure. Further, we show that a small set of curators intermediates a disproportionate share of system TVL, exhibits clustered tail co movement, and captures markedly different fee margins despite broadly similar collateral composition. These findings indicate that the main locus of risk in DeFi lending has migrated upward from base protocols, where underwriting is effectively centralized in a single DAO governed parameter set, to a permissionless curator layer in which competing vault managers decide which assets and loans are originated. We argue that this shift requires a corresponding upgrade in transparency standards and outline a simple set of onchain disclosures that would allow users and DAOs to evaluate curator strategies on a comparable, money market style basis.",
    "paper_id": "2512.11976v1"
  },
  "https://arxiv.org/abs/2412.18424": {
    "content_path": "../data/contents/2412.18424.md",
    "pdf_path": "../data/contents/2412.18424.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:26.530294",
    "title": "LongDocURL: A Comprehensive Multimodal Long Document Benchmark",
    "abstract": "New benchmark for long document understanding, reasoning, and locating with LVLMs",
    "paper_id": "2412.18424"
  },
  "https://arxiv.org/abs/2512.15712v1": {
    "content_path": "../data/contents/2512.15712v1.md",
    "pdf_path": "../data/contents/2512.15712v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:27.288251",
    "title": "Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants",
    "abstract": "Summary: Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.",
    "paper_id": "2512.15712v1"
  },
  "https://arxiv.org/abs/2512.11793v1": {
    "content_path": "../data/contents/2512.11793v1.md",
    "pdf_path": "../data/contents/2512.11793v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:29.132764",
    "title": "A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions",
    "abstract": "Summary: Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \\approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.",
    "paper_id": "2512.11793v1"
  },
  "https://arxiv.org/abs/2512.15705v1": {
    "content_path": "../data/contents/2512.15705v1.md",
    "pdf_path": "../data/contents/2512.15705v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:29.749193",
    "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX",
    "abstract": "Summary: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.",
    "paper_id": "2512.15705v1"
  },
  "https://arxiv.org/abs/2512.15769v1": {
    "content_path": "../data/contents/2512.15769v1.md",
    "pdf_path": "../data/contents/2512.15769v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:31.354222",
    "title": "Data-Chain Backdoor: Do You Trust Diffusion Models as Generative Data Supplier?",
    "abstract": "Summary: The increasing use of generative models such as diffusion models for synthetic data augmentation has greatly reduced the cost of data collection and labeling in downstream perception tasks. However, this new data source paradigm may introduce important security concerns. This work investigates backdoor propagation in such emerging generative data supply chains, namely Data-Chain Backdoor (DCB). Specifically, we find that open-source diffusion models can become hidden carriers of backdoors. Their strong distribution-fitting ability causes them to memorize and reproduce backdoor triggers during generation, which are subsequently inherited by downstream models, resulting in severe security risks. This threat is particularly concerning under clean-label attack scenarios, as it remains effective while having negligible impact on the utility of the synthetic data. Furthermore, we discover an Early-Stage Trigger Manifestation (ESTM) phenomenon: backdoor trigger patterns tend to surface more explicitly in the early, high-noise stages of the diffusion model's reverse generation process before being subtly integrated into the final samples. Overall, this work reveals a previously underexplored threat in generative data pipelines and provides initial insights toward mitigating backdoor risks in synthetic data generation.",
    "paper_id": "2512.15769v1"
  },
  "https://arxiv.org/abs/2412.05271": {
    "content_path": "../data/contents/2412.05271.md",
    "pdf_path": "../data/contents/2412.05271.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:32.807302",
    "title": "InternVL 2.5: Expanding Performance Boundaries of Open-Source Multimodal Models",
    "abstract": "Major MLLM release with test-time scaling, rivaling GPT-4o and Claude-3.5-Sonnet on document understanding",
    "paper_id": "2412.05271"
  },
  "https://arxiv.org/abs/2512.15699v1": {
    "content_path": "../data/contents/2512.15699v1.md",
    "pdf_path": "../data/contents/2512.15699v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:36.028907",
    "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "abstract": "Summary: We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.",
    "paper_id": "2512.15699v1"
  },
  "https://arxiv.org/abs/2512.11783v1": {
    "content_path": "../data/contents/2512.11783v1.md",
    "pdf_path": "../data/contents/2512.11783v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:36.618858",
    "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
    "abstract": "Summary: The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.\n  Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.",
    "paper_id": "2512.11783v1"
  },
  "https://arxiv.org/abs/2403.04473": {
    "content_path": "../data/contents/2403.04473.md",
    "pdf_path": "../data/contents/2403.04473.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:57.318004",
    "title": "TextMonkey: An OCR-Free Large Multimodal Model for Document Understanding",
    "abstract": "OCR-free document understanding model with 6.9% improvement on document-oriented benchmarks",
    "paper_id": "2403.04473"
  },
  "https://arxiv.org/abs/2512.15691v1": {
    "content_path": "../data/contents/2512.15691v1.md",
    "pdf_path": "../data/contents/2512.15691v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:58:59.136369",
    "title": "Multi-Modal Semantic Communication",
    "abstract": "Summary: Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.",
    "paper_id": "2512.15691v1"
  },
  "https://arxiv.org/abs/2512.11949v1": {
    "content_path": "../data/contents/2512.11949v1.md",
    "pdf_path": "../data/contents/2512.11949v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:00.382190",
    "title": "Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors",
    "abstract": "Summary: Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: \"You are being probed for {concept}\". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.",
    "paper_id": "2512.11949v1"
  },
  "https://arxiv.org/abs/2411.06176": {
    "content_path": "../data/contents/2411.06176.md",
    "pdf_path": "../data/contents/2411.06176.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:02.288633",
    "title": "M-LongDoc: Benchmark and Retrieval-Aware Tuning for Long Multimodal Document Understanding",
    "abstract": "New benchmark for documents spanning hundreds of pages with retrieval-aware training approach",
    "paper_id": "2411.06176"
  },
  "https://arxiv.org/abs/2512.15688v1": {
    "content_path": "../data/contents/2512.15688v1.md",
    "pdf_path": "../data/contents/2512.15688v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:04.954494",
    "title": "BashArena: A Control Setting for Highly Privileged AI Agents",
    "abstract": "Summary: Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.",
    "paper_id": "2512.15688v1"
  },
  "https://arxiv.org/abs/2512.11779v1": {
    "content_path": "../data/contents/2512.11779v1.md",
    "pdf_path": "../data/contents/2512.11779v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:06.327945",
    "title": "Conditional Coverage Diagnostics for Conformal Prediction",
    "abstract": "Summary: Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.",
    "paper_id": "2512.11779v1"
  },
  "https://arxiv.org/abs/2512.20652v1": {
    "content_path": "../data/contents/2512.20652v1.md",
    "pdf_path": "../data/contents/2512.20652v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:06.728975",
    "title": "AI-Driven Decision-Making System for Hiring Process",
    "abstract": "Summary: Early-stage candidate validation is a major bottleneck in hiring, because recruiters must reconcile heterogeneous inputs (resumes, screening answers, code assignments, and limited public evidence). This paper presents an AI-driven, modular multi-agent hiring assistant that integrates (i) document and video preprocessing, (ii) structured candidate profile construction, (iii) public-data verification, (iv) technical/culture-fit scoring with explicit risk penalties, and (v) human-in-the-loop validation via an interactive interface. The pipeline is orchestrated by an LLM under strict constraints to reduce output variability and to generate traceable component-level rationales. Candidate ranking is computed by a configurable aggregation of technical fit, culture fit, and normalized risk penalties. The system is evaluated on 64 real applicants for a mid-level Python backend engineer role, using an experienced recruiter as the reference baseline and a second, less experienced recruiter for additional comparison. Alongside precision/recall, we propose an efficiency metric measuring expected time per qualified candidate. In this study, the system improves throughput and achieves 1.70 hours per qualified candidate versus 3.33 hours for the experienced recruiter, with substantially lower estimated screening cost, while preserving a human decision-maker as the final authority.",
    "paper_id": "2512.20652v1"
  },
  "https://arxiv.org/abs/2512.11771v1": {
    "content_path": "../data/contents/2512.11771v1.md",
    "pdf_path": "../data/contents/2512.11771v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:10.491522",
    "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints",
    "abstract": "Summary: Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.",
    "paper_id": "2512.11771v1"
  },
  "https://arxiv.org/abs/2501.09136": {
    "content_path": "../data/contents/2501.09136.md",
    "pdf_path": "../data/contents/2501.09136.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:14.471767",
    "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
    "abstract": "Comprehensive survey on Agentic RAG covering reflection, planning, tool use, and multi-agent collaboration patterns.",
    "paper_id": "2501.09136"
  },
  "https://arxiv.org/abs/2406.19101": {
    "content_path": "../data/contents/2406.19101.md",
    "pdf_path": "../data/contents/2406.19101.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:33.961082",
    "title": "DocKylin: A Large Multimodal Model for Visual Document Understanding with Efficient Visual Slimming",
    "abstract": "Efficient document understanding model addressing high-resolution and dense text challenges",
    "paper_id": "2406.19101"
  },
  "https://arxiv.org/abs/2512.15687v1": {
    "content_path": "../data/contents/2512.15687v1.md",
    "pdf_path": "../data/contents/2512.15687v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:34.234688",
    "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
    "abstract": "Summary: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.",
    "paper_id": "2512.15687v1"
  },
  "https://arxiv.org/abs/2512.11761v1": {
    "content_path": "../data/contents/2512.11761v1.md",
    "pdf_path": "../data/contents/2512.11761v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:35.304315",
    "title": "Covariate-assisted graph matching",
    "abstract": "Summary: Data integration is essential across diverse domains, from historical records to biomedical research, facilitating joint statistical inference. A crucial initial step in this process involves merging multiple data sources based on matching individual records, often in the absence of unique identifiers. When the datasets are networks, this problem is typically addressed through graph matching methodologies. For such cases, auxiliary features or covariates associated with nodes or edges can be instrumental in achieving improved accuracy. However, most existing graph matching techniques do not incorporate this information, limiting their performance against non-identifiable and erroneous matches. To overcome these limitations, we propose two novel covariate-assisted seeded graph matching methods, where a partial alignment for a set of nodes, called seeds, is known. The first one solves a quadratic assignment problem (QAP) over the whole graph, while the second one only leverages the local neighborhood structure of seed nodes for computational scalability. Both methods are grounded in a conditional modeling framework, where elements of one graph's adjacency matrix are modeled using a generalized linear model (GLM), given the other graph and the available covariates. We establish theoretical guarantees for model estimation error and exact recovery of the solution of the QAP. The effectiveness of our methods is demonstrated through numerical experiments and in an application to matching the statistics academic genealogy and the collaboration networks. By leveraging additional covariates, we achieve improved alignment accuracy. Our work highlights the power of integrating covariate information in the classical graph matching setup, offering a practical and improved framework for combining network data with wide-ranging applications.",
    "paper_id": "2512.11761v1"
  },
  "https://arxiv.org/abs/2512.15675v3": {
    "content_path": "../data/contents/2512.15675v3.md",
    "pdf_path": "../data/contents/2512.15675v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:35.978021",
    "title": "Stylized Synthetic Augmentation further improves Corruption Robustness",
    "abstract": "Summary: This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common Frechet Inception Distance (FID) metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively",
    "paper_id": "2512.15675v3"
  },
  "https://arxiv.org/abs/2512.11755v1": {
    "content_path": "../data/contents/2512.11755v1.md",
    "pdf_path": "../data/contents/2512.11755v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:36.433570",
    "title": "SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support",
    "abstract": "Summary: Online product reviews contain rich but noisy signals that overwhelm users and hinder effective decision-making. Existing LLM-based summarizers remain generic and fail to account for individual preferences, limiting their practical utility. We propose SUMFORU, a steerable review summarization framework that aligns outputs with explicit user personas to support personalized purchase decisions. Our approach integrates a high-quality data pipeline built from the Amazon 2023 Review Dataset with a two-stage alignment procedure: (1) persona-aware Supervised Fine-Tuning (SFT) via asymmetric knowledge distillation, and (2) Reinforcement Learning with AI Feedback (RLAIF) using a preference estimator to capture fine-grained, persona-relevant signals. We evaluate the model across rule-based, LLM-based, and human-centered metrics, demonstrating consistent improvements in consistency, grounding, and preference alignment. Our framework achieves the highest performance across all evaluation settings and generalizes effectively to unseen product categories. Our results highlight the promise of steerable pluralistic alignment for building next-generation personalized decision-support systems.",
    "paper_id": "2512.11755v1"
  },
  "https://arxiv.org/abs/2409.04701": {
    "content_path": "../data/contents/2409.04701.md",
    "pdf_path": "../data/contents/2409.04701.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:37.014021",
    "title": "Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models",
    "abstract": "Influential Jina AI paper on late chunking technique that preserves document context in chunk embeddings without additional training.",
    "paper_id": "2409.04701"
  },
  "https://arxiv.org/abs/2512.15674v1": {
    "content_path": "../data/contents/2512.15674v1.md",
    "pdf_path": "../data/contents/2512.15674v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:38.309403",
    "title": "Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers",
    "abstract": "Summary: Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.",
    "paper_id": "2512.15674v1"
  },
  "https://arxiv.org/abs/2512.14744v1": {
    "content_path": "../data/contents/2512.14744v1.md",
    "pdf_path": "../data/contents/2512.14744v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:38.520391",
    "title": "VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation",
    "abstract": "Summary: Financial AI systems suffer from a critical blind spot: while Retrieval-Augmented Generation (RAG) excels at finding relevant documents, language models still generate calculation errors and regulatory violations during reasoning, even with perfect retrieval. This paper introduces VERAFI (Verified Agentic Financial Intelligence), an agentic framework with neurosymbolic policy generation for verified financial intelligence. VERAFI combines state-of-the-art dense retrieval and cross-encoder reranking with financial tool-enabled agents and automated reasoning policies covering GAAP compliance, SEC requirements, and mathematical validation. Our comprehensive evaluation on FinanceBench demonstrates remarkable improvements: while traditional dense retrieval with reranking achieves only 52.4\\% factual correctness, VERAFI's integrated approach reaches 94.7\\%, an 81\\% relative improvement. The neurosymbolic policy layer alone contributes a 4.3 percentage point gain over pure agentic processing, specifically targeting persistent mathematical and logical errors. By integrating financial domain expertise directly into the reasoning process, VERAFI offers a practical pathway toward trustworthy financial AI that meets the stringent accuracy demands of regulatory compliance, investment decisions, and risk management.",
    "paper_id": "2512.14744v1"
  },
  "https://arxiv.org/abs/2504.19754": {
    "content_path": "../data/contents/2504.19754.md",
    "pdf_path": "../data/contents/2504.19754.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:39.185493",
    "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation",
    "abstract": "Rigorous comparison of late chunking vs contextual retrieval showing trade-offs between efficiency and semantic coherence.",
    "paper_id": "2504.19754"
  },
  "https://arxiv.org/abs/2512.15834v1": {
    "content_path": "../data/contents/2512.15834v1.md",
    "pdf_path": "../data/contents/2512.15834v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:39.975694",
    "title": "Optimizing Agentic Language Model Inference via Speculative Tool Calls",
    "abstract": "Summary: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new \"tool cache\" API endpoint to enable LM providers to easily adopt these optimizations.",
    "paper_id": "2512.15834v1"
  },
  "https://arxiv.org/abs/2512.11731v1": {
    "content_path": "../data/contents/2512.11731v1.md",
    "pdf_path": "../data/contents/2512.11731v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:43.137665",
    "title": "Transfer Learning (Il)liquidity",
    "abstract": "Summary: The estimation of the Risk Neutral Density (RND) implicit in option prices is challenging, especially in illiquid markets. We introduce the Deep Log-Sum-Exp Neural Network, an architecture that leverages Deep and Transfer learning to address RND estimation in the presence of irregular and illiquid strikes. We prove key statistical properties of the model and the consistency of the estimator. We illustrate the benefits of transfer learning to improve the estimation of the RND in severe illiquidity conditions through Monte Carlo simulations, and we test it empirically on SPX data, comparing it with popular estimation methods. Overall, our framework shows recovery of the RND in conditions of extreme illiquidity with as few as three option quotes.",
    "paper_id": "2512.11731v1"
  },
  "https://arxiv.org/abs/2512.15663v1": {
    "content_path": "../data/contents/2512.15663v1.md",
    "pdf_path": "../data/contents/2512.15663v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:44.633769",
    "title": "Explaining the Reasoning of Large Language Models Using Attribution Graphs",
    "abstract": "Summary: Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.",
    "paper_id": "2512.15663v1"
  },
  "https://arxiv.org/abs/2512.11724v2": {
    "content_path": "../data/contents/2512.11724v2.md",
    "pdf_path": "../data/contents/2512.11724v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:44.959703",
    "title": "From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines",
    "abstract": "Summary: While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.",
    "paper_id": "2512.11724v2"
  },
  "https://arxiv.org/abs/2512.15662v1": {
    "content_path": "../data/contents/2512.15662v1.md",
    "pdf_path": "../data/contents/2512.15662v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:45.696355",
    "title": "Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning",
    "abstract": "Summary: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.",
    "paper_id": "2512.15662v1"
  },
  "https://arxiv.org/abs/2512.11718v1": {
    "content_path": "../data/contents/2512.11718v1.md",
    "pdf_path": "../data/contents/2512.11718v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:46.023767",
    "title": "Speculative Decoding Speed-of-Light: Optimal Lower Bounds via Branching Random Walks",
    "abstract": "Summary: Speculative generation has emerged as a promising technique to accelerate inference in large language models (LLMs) by leveraging parallelism to verify multiple draft tokens simultaneously. However, the fundamental limits on the achievable speedup remain poorly understood. In this work, we establish the first ``tight'' lower bounds on the runtime of any deterministic speculative generation algorithm. This is achieved by drawing a parallel between the token generation process and branching random walks, which allows us to analyze the optimal draft tree selection problem. We prove, under basic assumptions, that the expected number of tokens successfully predicted per speculative iteration is bounded as $\\mathbb{E}[X] \\leq (μ+ μ_{(2)})\\log(P )/μ^2 + O(1)$, where $P$ is the verifier's capacity, $μ$ is the expected entropy of the verifier's output distribution, and $μ_{(2)}$ is the expected second log-moment. This result provides new insights into the limits of parallel token generation, and could guide the design of future speculative decoding systems. Empirical evaluations on Llama models validate our theoretical predictions, confirming the tightness of our bounds in practical settings.",
    "paper_id": "2512.11718v1"
  },
  "https://arxiv.org/abs/2512.15658v1": {
    "content_path": "../data/contents/2512.15658v1.md",
    "pdf_path": "../data/contents/2512.15658v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:46.615742",
    "title": "PPSEBM: An Energy-Based Model with Progressive Parameter Selection for Continual Learning",
    "abstract": "Summary: Continual learning remains a fundamental challenge in machine learning, requiring models to learn from a stream of tasks without forgetting previously acquired knowledge. A major obstacle in this setting is catastrophic forgetting, where performance on earlier tasks degrades as new tasks are learned. In this paper, we introduce PPSEBM, a novel framework that integrates an Energy-Based Model (EBM) with Progressive Parameter Selection (PPS) to effectively address catastrophic forgetting in continual learning for natural language processing tasks. In PPSEBM, progressive parameter selection allocates distinct, task-specific parameters for each new task, while the EBM generates representative pseudo-samples from prior tasks. These generated samples actively inform and guide the parameter selection process, enhancing the model's ability to retain past knowledge while adapting to new tasks. Experimental results on diverse NLP benchmarks demonstrate that PPSEBM outperforms state-of-the-art continual learning methods, offering a promising and robust solution to mitigate catastrophic forgetting.",
    "paper_id": "2512.15658v1"
  },
  "https://arxiv.org/abs/2512.11689v1": {
    "content_path": "../data/contents/2512.11689v1.md",
    "pdf_path": "../data/contents/2512.11689v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:47.813419",
    "title": "Evaluating Cooperative Resilience in Multiagent Systems: A Comparison Between Humans and LLMs",
    "abstract": "Summary: This paper presents a comparative analysis of cooperative resilience in multi-agent systems, defined as the ability to anticipate, resist, recover from, and transform to disruptive events that affect collective well-being. We focus on mixed-motive social dilemmas instantiated as a \\textit{Tragedy of the Commons} environment from the Melting Pot suite, where we systematically compare human groups and Large Language Model (LLM)-based agents, each evaluated with and without explicit communication. Cooperative resilience is assessed under a continuously disruptive condition induced by a persistent unsustainable consumption bot, together with intermittent environmental shocks implemented as stochastic removal of shared resources across scenarios. This experimental design establishes a benchmark for cooperative resilience across agent architectures and interaction modalities, constituting a key step toward systematically comparing humans and LLM-based agents. Using this framework, we find that human groups with communication achieve the highest cooperative resilience compared to all other groups. Communication also improves the resilience of LLM agents, but their performance remains below human levels. Motivated by the performance of humans, we further examine a long-horizon setting with harsher environmental conditions, where humans sustain the shared resource and maintain high resilience in diverse disruption scenarios. Together, these results suggest that human decision-making under adverse social conditions can inform the design of artificial agents that promote prosocial and resilient behaviors.",
    "paper_id": "2512.11689v1"
  },
  "https://arxiv.org/abs/2512.15653v1": {
    "content_path": "../data/contents/2512.15653v1.md",
    "pdf_path": "../data/contents/2512.15653v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:48.391508",
    "title": "Characterizing Mamba's Selective Memory using Auto-Encoders",
    "abstract": "Summary: State space models (SSMs) are a promising alternative to transformers for language modeling because they use fixed memory during inference. However, this fixed memory usage requires some information loss in the hidden state when processing long sequences. While prior work has studied the sequence length at which this information loss occurs, it does not characterize the types of information SSM language models (LMs) tend to forget. In this paper, we address this knowledge gap by identifying the types of tokens (e.g., parts of speech, named entities) and sequences (e.g., code, math problems) that are more frequently forgotten by SSM LMs. We achieve this by training an auto-encoder to reconstruct sequences from the SSM's hidden state, and measure information loss by comparing inputs with their reconstructions. We perform experiments using the Mamba family of SSM LMs (130M--1.4B) on sequences ranging from 4--256 tokens. Our results show significantly higher rates of information loss on math-related tokens (e.g., numbers, variables), mentions of organization entities, and alternative dialects to Standard American English. We then examine the frequency that these tokens appear in Mamba's pretraining data and find that less prevalent tokens tend to be the ones Mamba is most likely to forget. By identifying these patterns, our work provides clear direction for future research to develop methods that better control Mamba's ability to retain important information.",
    "paper_id": "2512.15653v1"
  },
  "https://arxiv.org/abs/2512.11682v1": {
    "content_path": "../data/contents/2512.11682v1.md",
    "pdf_path": "../data/contents/2512.11682v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:48.732596",
    "title": "MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition",
    "abstract": "Summary: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.",
    "paper_id": "2512.11682v1"
  },
  "https://arxiv.org/abs/2512.15649v2": {
    "content_path": "../data/contents/2512.15649v2.md",
    "pdf_path": "../data/contents/2512.15649v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:50.025295",
    "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
    "abstract": "Summary: The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-processed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.",
    "paper_id": "2512.15649v2"
  },
  "https://arxiv.org/abs/2512.11674v1": {
    "content_path": "../data/contents/2512.11674v1.md",
    "pdf_path": "../data/contents/2512.11674v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:50.774912",
    "title": "Natural Language Interaction for Editing Visual Knowledge Graphs",
    "abstract": "Summary: Knowledge graphs are often visualized using node-link diagrams that reveal relationships and structure. In many applications using graphs, it is desirable to allow users to edit graphs to ensure data accuracy or provides updates. Commonly in graph visualization, users can interact directly with the visual elements by clicking and typing updates to specific items through traditional interaction methods in the graphical user interface. However, it can become tedious to make many updates due to the need to individually select and change numerous items in a graph. Our research investigates natural language input as an alternative method for editing network graphs. We present a user study comparing GUI graph editing with two natural language alternatives to contribute novel empirical data of the trade-offs of the different interaction methods. The findings show natural language methods to be significantly more effective than traditional GUI interaction.",
    "paper_id": "2512.11674v1"
  },
  "https://arxiv.org/abs/2512.20491v2": {
    "content_path": "../data/contents/2512.20491v2.md",
    "pdf_path": "../data/contents/2512.20491v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:51.323058",
    "title": "Step-DeepResearch Technical Report",
    "abstract": "Summary: As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
    "paper_id": "2512.20491v2"
  },
  "https://arxiv.org/abs/2512.15634v1": {
    "content_path": "../data/contents/2512.15634v1.md",
    "pdf_path": "../data/contents/2512.15634v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:52.516060",
    "title": "How Much is Too Much? Exploring LoRA Rank Trade-offs for Retaining Knowledge and Domain Robustness",
    "abstract": "Summary: Large language models are increasingly adapted to downstream tasks through fine-tuning. Full supervised fine-tuning (SFT) and parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are two dominant approaches. While PEFT methods are widely used for their computational efficiency, the implications of their configurations (e.g., rank) remain under-explored in downstream Q&A tasks and generalisation. In this work, we perform a comprehensive evaluation across multiple reasoning and recall datasets, conducting a rank sweep to quantify the trade-off between SFT and PEFT. We also compare the accuracy of PEFT and SFT models across in-domain and out-of-domain adaptation, highlighting distinct generalisation behaviour and task-specific forgetting. We demonstrate that LoRA achieves competitive and in some cases superior performance compared to SFT, particularly on reasoning tasks at specific rank values. Additionally, we analyze the internal representations via spectral features and layer-wise attention structures, offering insights into representational drift and structural changes in attention patterns.",
    "paper_id": "2512.15634v1"
  },
  "https://arxiv.org/abs/2512.11668v1": {
    "content_path": "../data/contents/2512.11668v1.md",
    "pdf_path": "../data/contents/2512.11668v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:53.191237",
    "title": "Bridging Streaming Continual Learning via In-Context Large Tabular Models",
    "abstract": "Summary: In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.",
    "paper_id": "2512.11668v1"
  },
  "https://arxiv.org/abs/2512.20481v2": {
    "content_path": "../data/contents/2512.20481v2.md",
    "pdf_path": "../data/contents/2512.20481v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:56.627603",
    "title": "Coherence in the brain unfolds across separable temporal regimes",
    "abstract": "Summary: Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders.",
    "paper_id": "2512.20481v2"
  },
  "https://arxiv.org/abs/2512.16953v1": {
    "content_path": "../data/contents/2512.16953v1.md",
    "pdf_path": "../data/contents/2512.16953v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:58.500459",
    "title": "Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases",
    "abstract": "Summary: Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.",
    "paper_id": "2512.16953v1"
  },
  "https://arxiv.org/abs/2512.11661v1": {
    "content_path": "../data/contents/2512.11661v1.md",
    "pdf_path": "../data/contents/2512.11661v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T12:59:59.014576",
    "title": "From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews",
    "abstract": "Summary: Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \\textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.",
    "paper_id": "2512.11661v1"
  },
  "https://arxiv.org/abs/2512.20136v2": {
    "content_path": "../data/contents/2512.20136v2.md",
    "pdf_path": "../data/contents/2512.20136v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:00.390096",
    "title": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation",
    "abstract": "Summary: Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.",
    "paper_id": "2512.20136v2"
  },
  "https://arxiv.org/abs/2512.15617v1": {
    "content_path": "../data/contents/2512.15617v1.md",
    "pdf_path": "../data/contents/2512.15617v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:00.785600",
    "title": "Evaluating Metrics for Safety with LLM-as-Judges",
    "abstract": "Summary: LLMs (Large Language Models) are increasingly used in text processing pipelines to intelligently respond to a variety of inputs and generation tasks. This raises the possibility of replacing human roles that bottleneck existing information flows, either due to insufficient staff or process complexity. However, LLMs make mistakes and some processing roles are safety critical. For example, triaging post-operative care to patients based on hospital referral letters, or updating site access schedules in nuclear facilities for work crews. If we want to introduce LLMs into critical information flows that were previously performed by humans, how can we make them safe and reliable? Rather than make performative claims about augmented generation frameworks or graph-based techniques, this paper argues that the safety argument should focus on the type of evidence we get from evaluation points in LLM processes, particularly in frameworks that employ LLM-as-Judges (LaJ) evaluators. This paper argues that although we cannot get deterministic evaluations from many natural language processing tasks, by adopting a basket of weighted metrics it may be possible to lower the risk of errors within an evaluation, use context sensitivity to define error severity and design confidence thresholds that trigger human review of critical LaJ judgments when concordance across evaluators is low.",
    "paper_id": "2512.15617v1"
  },
  "https://arxiv.org/abs/2512.11634v1": {
    "content_path": "../data/contents/2512.11634v1.md",
    "pdf_path": "../data/contents/2512.11634v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:01.928687",
    "title": "FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access",
    "abstract": "Summary: Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements.\n  We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.",
    "paper_id": "2512.11634v1"
  },
  "https://arxiv.org/abs/2512.20135v2": {
    "content_path": "../data/contents/2512.20135v2.md",
    "pdf_path": "../data/contents/2512.20135v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:02.374079",
    "title": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization",
    "abstract": "Summary: Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.",
    "paper_id": "2512.20135v2"
  },
  "https://arxiv.org/abs/2512.15614v1": {
    "content_path": "../data/contents/2512.15614v1.md",
    "pdf_path": "../data/contents/2512.15614v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:03.694232",
    "title": "Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary",
    "abstract": "Summary: Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.",
    "paper_id": "2512.15614v1"
  },
  "https://arxiv.org/abs/2512.11616v1": {
    "content_path": "../data/contents/2512.11616v1.md",
    "pdf_path": "../data/contents/2512.11616v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:04.311253",
    "title": "A Fast Interpretable Fuzzy Tree Learner",
    "abstract": "Summary: Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public",
    "paper_id": "2512.11616v1"
  },
  "https://arxiv.org/html/2506.10408v1": {
    "content_path": "../data/contents/2506.10408v1.md",
    "pdf_path": "../data/contents/2506.10408v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:04.525928",
    "title": "Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic RAG for Industry Challenges",
    "abstract": "Survey categorizing RAG approaches into predefined vs agentic reasoning paradigms with industry application focus.",
    "paper_id": "2506.10408v1"
  },
  "https://arxiv.org/abs/2512.20082v2": {
    "content_path": "../data/contents/2512.20082v2.md",
    "pdf_path": "../data/contents/2512.20082v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:05.009843",
    "title": "Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches",
    "abstract": "Summary: Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.",
    "paper_id": "2512.20082v2"
  },
  "https://arxiv.org/abs/2512.19738v1": {
    "content_path": "../data/contents/2512.19738v1.md",
    "pdf_path": "../data/contents/2512.19738v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:05.300690",
    "title": "OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting",
    "abstract": "Summary: Accurate forecasting of package volumes at delivery stations is critical for last-mile logistics, where errors lead to inefficient resource allocation, higher costs, and delivery delays. We propose OpComm, a forecasting and decision-support framework that combines supervised learning with reinforcement learning-based buffer control and a generative AI-driven communication module. A LightGBM regression model generates station-level demand forecasts, which serve as context for a Proximal Policy Optimization (PPO) agent that selects buffer levels from a discrete action set. The reward function penalizes under-buffering more heavily than over-buffering, reflecting real-world trade-offs between unmet demand risks and resource inefficiency. Station outcomes are fed back through a Monte Carlo update mechanism, enabling continual policy adaptation. To enhance interpretability, a generative AI layer produces executive-level summaries and scenario analyses grounded in SHAP-based feature attributions. Across 400+ stations, OpComm reduced Weighted Absolute Percentage Error (WAPE) by 21.65% compared to manual forecasts, while lowering under-buffering incidents and improving transparency for decision-makers. This work shows how contextual reinforcement learning, coupled with predictive modeling, can address operational forecasting challenges and bridge statistical rigor with practical decision-making in high-stakes logistics environments.",
    "paper_id": "2512.19738v1"
  },
  "https://arxiv.org/abs/2512.11614v1": {
    "content_path": "../data/contents/2512.11614v1.md",
    "pdf_path": "../data/contents/2512.11614v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:07.463613",
    "title": "Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols",
    "abstract": "Summary: Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.",
    "paper_id": "2512.11614v1"
  },
  "https://arxiv.org/abs/2509.04139": {
    "content_path": "../data/contents/2509.04139.md",
    "pdf_path": "../data/contents/2509.04139.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:07.897842",
    "title": "Enhancing Technical Documents Retrieval for RAG",
    "abstract": "Novel Technical-Embeddings framework with query expansion and contextual summarization for technical documentation RAG.",
    "paper_id": "2509.04139"
  },
  "https://arxiv.org/abs/2512.20051v1": {
    "content_path": "../data/contents/2512.20051v1.md",
    "pdf_path": "../data/contents/2512.20051v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:08.100112",
    "title": "Generative Bayesian Hyperparameter Tuning",
    "abstract": "Summary: \\noindent Hyper-parameter selection is a central practical problem in modern machine learning, governing regularization strength, model capacity, and robustness choices. Cross-validation is often computationally prohibitive at scale, while fully Bayesian hyper-parameter learning can be difficult due to the cost of posterior sampling. We develop a generative perspective on hyper-parameter tuning that combines two ideas: (i) optimization-based approximations to Bayesian posteriors via randomized, weighted objectives (weighted Bayesian bootstrap), and (ii) amortization of repeated optimization across many hyper-parameter settings by learning a transport map from hyper-parameters (including random weights) to the corresponding optimizer. This yields a ``generator look-up table'' for estimators, enabling rapid evaluation over grids or continuous ranges of hyper-parameters and supporting both predictive tuning objectives and approximate Bayesian uncertainty quantification. We connect this viewpoint to weighted $M$-estimation, envelope/auxiliary-variable representations that reduce non-quadratic losses to weighted least squares, and recent generative samplers for weighted $M$-estimators.",
    "paper_id": "2512.20051v1"
  },
  "https://arxiv.org/abs/2512.15605v1": {
    "content_path": "../data/contents/2512.15605v1.md",
    "pdf_path": "../data/contents/2512.15605v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:09.033713",
    "title": "Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction",
    "abstract": "Summary: Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.",
    "paper_id": "2512.15605v1"
  },
  "https://arxiv.org/abs/2512.11588v1": {
    "content_path": "../data/contents/2512.11588v1.md",
    "pdf_path": "../data/contents/2512.11588v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:10.365309",
    "title": "AI Benchmark Democratization and Carpentry",
    "abstract": "Summary: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.\n  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.\n  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.",
    "paper_id": "2512.11588v1"
  },
  "https://arxiv.org/abs/2512.20039v1": {
    "content_path": "../data/contents/2512.20039v1.md",
    "pdf_path": "../data/contents/2512.20039v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:10.663420",
    "title": "Optimal Anytime-Valid Tests for Composite Nulls",
    "abstract": "Summary: We consider the problem of designing optimal level-$α$ power-one tests for composite nulls. Given a parameter $α\\in (0,1)$ and a stream of $\\mathcal{X}$-valued observations $\\{X_n: n \\geq 1\\} \\overset{i.i.d.}{\\sim} P$, the goal is to design a level-$α$ power-one test $τ_α$ for the null $H_0: P \\in \\mathcal{P}_0 \\subset \\mathcal{P}(\\mathcal{X})$. Prior works have shown that any such $τ_α$ must satisfy $\\mathbb{E}_P[τ_α] \\geq \\tfrac{\\log(1/α)}{γ^*(P, \\mathcal{P}_0)}$, where $γ^*(P, \\mathcal{P}_0)$ is the so-called $\\mathrm{KL}_{\\inf}$ or minimum divergence of $P$ to the null class. In this paper, our objective is to develop and analyze constructive schemes that match this lower bound as $α\\downarrow 0$.\n  We first consider the finite-alphabet case~($|\\mathcal{X}| = m < \\infty$), and show that a test based on \\emph{universal} $e$-process~(formed by the ratio of a universal predictor and the running null MLE) is optimal in the above sense. The proof relies on a Donsker-Varadhan~(DV) based saddle-point representation of $\\mathrm{KL}_{\\inf}$, and an application of Sion's minimax theorem. This characterization motivates a general method for arbitrary $\\mathcal{X}$: construct an $e$-process based on the empirical solutions to the saddle-point representation over a sufficiently rich class of test functions. We give sufficient conditions for the optimality of this test for compact convex nulls, and verify them for Hölder smooth density models. We end the paper with a discussion on the computational aspects of implementing our proposed tests in some practical settings.",
    "paper_id": "2512.20039v1"
  },
  "https://arxiv.org/abs/2512.15596v1": {
    "content_path": "../data/contents/2512.15596v1.md",
    "pdf_path": "../data/contents/2512.15596v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:11.067644",
    "title": "Corrective Diffusion Language Models",
    "abstract": "Summary: Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.",
    "paper_id": "2512.15596v1"
  },
  "https://arxiv.org/abs/2512.11584v1": {
    "content_path": "../data/contents/2512.11584v1.md",
    "pdf_path": "../data/contents/2512.11584v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:11.446021",
    "title": "Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents",
    "abstract": "Summary: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)",
    "paper_id": "2512.11584v1"
  },
  "https://arxiv.org/abs/2512.20022v1": {
    "content_path": "../data/contents/2512.20022v1.md",
    "pdf_path": "../data/contents/2512.20022v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:11.809555",
    "title": "LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews",
    "abstract": "Summary: Introduction: Recent work suggests large language models (LLMs) can accelerate screening, but prior evaluations focus on earlier LLMs, standardized Cochrane reviews, single-model setups, and accuracy as the primary metric, leaving generalizability, configuration effects, and calibration largely unexamined.\n  Methods: We developed OLIVER (Optimized LLM-based Inclusion and Vetting Engine for Reviews), an open-source pipeline for LLM-assisted abstract screening. We evaluated multiple contemporary LLMs across two non-Cochrane systematic reviews and performance was assessed at both the full-text screening and final inclusion stages using accuracy, AUC, and calibration metrics. We further tested an actor-critic screening framework combining two lightweight models under three aggregation rules.\n  Results: Across individual models, performance varied widely. In the smaller Review 1 (821 abstracts, 63 final includes), several models achieved high sensitivity for final includes but at the cost of substantial false positives and poor calibration. In the larger Review 2 (7741 abstracts, 71 final includes), most models were highly specific but struggled to recover true includes, with prompt design influencing recall. Calibration was consistently weak across single-model configurations despite high overall accuracy. Actor-critic screening improved discrimination and markedly reduced calibration error in both reviews, yielding higher AUCs.\n  Discussion: LLMs may eventually accelerate abstract screening, but single-model performance is highly sensitive to review characteristics, prompting, and calibration is limited. An actor-critic framework improves classification quality and confidence reliability while remaining computationally efficient, enabling large-scale screening at low cost.",
    "paper_id": "2512.20022v1"
  },
  "https://arxiv.org/abs/2512.15595v1": {
    "content_path": "../data/contents/2512.15595v1.md",
    "pdf_path": "../data/contents/2512.15595v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:12.235877",
    "title": "Optimizing Bloom Filters for Modern GPU Architectures",
    "abstract": "Summary: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.\n  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\\times$ ($15.4\\times$) for bulk filter lookup (construction), respectively, achieving above $92\\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.",
    "paper_id": "2512.15595v1"
  },
  "https://arxiv.org/abs/2512.11573v1": {
    "content_path": "../data/contents/2512.11573v1.md",
    "pdf_path": "../data/contents/2512.11573v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:15.437016",
    "title": "Visualizing token importance for black-box language models",
    "abstract": "Summary: We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.",
    "paper_id": "2512.11573v1"
  },
  "https://arxiv.org/abs/2512.20021v1": {
    "content_path": "../data/contents/2512.20021v1.md",
    "pdf_path": "../data/contents/2512.20021v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:15.932682",
    "title": "Gaussian Process Assisted Meta-learning for Image Classification and Object Detection Models",
    "abstract": "Summary: Collecting operationally realistic data to inform machine learning models can be costly. Before collecting new data, it is helpful to understand where a model is deficient. For example, object detectors trained on images of rare objects may not be good at identification in poorly represented conditions. We offer a way of informing subsequent data acquisition to maximize model performance by leveraging the toolkit of computer experiments and metadata describing the circumstances under which the training data was collected (e.g., season, time of day, location). We do this by evaluating the learner as the training data is varied according to its metadata. A Gaussian process (GP) surrogate fit to that response surface can inform new data acquisitions. This meta-learning approach offers improvements to learner performance as compared to data with randomly selected metadata, which we illustrate on both classic learning examples, and on a motivating application involving the collection of aerial images in search of airplanes.",
    "paper_id": "2512.20021v1"
  },
  "https://arxiv.org/abs/2512.15592v1": {
    "content_path": "../data/contents/2512.15592v1.md",
    "pdf_path": "../data/contents/2512.15592v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:16.228936",
    "title": "Inference for Forecasting Accuracy: Pooled versus Individual Estimators in High-dimensional Panel Data",
    "abstract": "Summary: Panels with large time $(T)$ and cross-sectional $(N)$ dimensions are a key data structure in social sciences and other fields. A central question in panel data analysis is whether to pool data across individuals or to estimate separate models. Pooled estimators typically have lower variance but may suffer from bias, creating a fundamental trade-off for optimal estimation. We develop a new inference method to compare the forecasting performance of pooled and individual estimators. Specifically, we propose a confidence interval for the difference between their forecasting errors and establish its asymptotic validity. Our theory allows for complex temporal and cross-sectional dependence in the model errors and covers scenarios where $N$ can be much larger than $T$-including the independent case under the classical condition $N/T^2 \\to 0$. The finite-sample properties of the proposed method are examined in an extensive simulation study.",
    "paper_id": "2512.15592v1"
  },
  "https://arxiv.org/abs/2512.11561v1": {
    "content_path": "../data/contents/2512.11561v1.md",
    "pdf_path": "../data/contents/2512.11561v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:17.969864",
    "title": "Fully Inductive Node Representation Learning via Graph View Transformation",
    "abstract": "Summary: Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.",
    "paper_id": "2512.11561v1"
  },
  "https://arxiv.org/abs/2512.20012v1": {
    "content_path": "../data/contents/2512.20012v1.md",
    "pdf_path": "../data/contents/2512.20012v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:18.427188",
    "title": "Reliable LLM-Based Edge-Cloud-Expert Cascades for Telecom Knowledge Systems",
    "abstract": "Summary: Large language models (LLMs) are emerging as key enablers of automation in domains such as telecommunications, assisting with tasks including troubleshooting, standards interpretation, and network optimization. However, their deployment in practice must balance inference cost, latency, and reliability. In this work, we study an edge-cloud-expert cascaded LLM-based knowledge system that supports decision-making through a question-and-answer pipeline. In it, an efficient edge model handles routine queries, a more capable cloud model addresses complex cases, and human experts are involved only when necessary. We define a misalignment-cost constrained optimization problem, aiming to minimize average processing cost, while guaranteeing alignment of automated answers with expert judgments. We propose a statistically rigorous threshold selection method based on multiple hypothesis testing (MHT) for a query processing mechanism based on knowledge and confidence tests. The approach provides finite-sample guarantees on misalignment risk. Experiments on the TeleQnA dataset -- a telecom-specific benchmark -- demonstrate that the proposed method achieves superior cost-efficiency compared to conventional cascaded baselines, while ensuring reliability at prescribed confidence levels.",
    "paper_id": "2512.20012v1"
  },
  "https://arxiv.org/abs/2512.15586v1": {
    "content_path": "../data/contents/2512.15586v1.md",
    "pdf_path": "../data/contents/2512.15586v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:20.956015",
    "title": "Bolmo: Byteifying the Next Generation of Language Models",
    "abstract": "Summary: We introduce Bolmo, the first family of competitive fully open byte-level language models (LMs) at the 1B and 7B parameter scales. In contrast to prior research on byte-level LMs, which focuses predominantly on training from scratch, we train Bolmo by byteifying existing subword-level LMs. Byteification enables overcoming the limitations of subword tokenization - such as insufficient character understanding and efficiency constraints due to the fixed subword vocabulary - while performing at the level of leading subword-level LMs. Bolmo is specifically designed for byteification: our architecture resolves a mismatch between the expressivity of prior byte-level architectures and subword-level LMs, which makes it possible to employ an effective exact distillation objective between Bolmo and the source subword model. This allows for converting a subword-level LM to a byte-level LM by investing less than 1\\% of a typical pretraining token budget. Bolmo substantially outperforms all prior byte-level LMs of comparable size, and outperforms the source subword-level LMs on character understanding and, in some cases, coding, while coming close to matching the original LMs' performance on other tasks. Furthermore, we show that Bolmo can achieve inference speeds competitive with subword-level LMs by training with higher token compression ratios, and can be cheaply and effectively post-trained by leveraging the existing ecosystem around the source subword-level LM. Our results finally make byte-level LMs a practical choice competitive with subword-level LMs across a wide set of use cases.",
    "paper_id": "2512.15586v1"
  },
  "https://arxiv.org/abs/2512.11558v1": {
    "content_path": "../data/contents/2512.11558v1.md",
    "pdf_path": "../data/contents/2512.11558v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:22.796613",
    "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry",
    "abstract": "Summary: Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.",
    "paper_id": "2512.11558v1"
  },
  "https://arxiv.org/abs/2512.19991v1": {
    "content_path": "../data/contents/2512.19991v1.md",
    "pdf_path": "../data/contents/2512.19991v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:23.464045",
    "title": "Bloom Filter Encoding for Machine Learning",
    "abstract": "Summary: We present a method that uses the Bloom filter transform to preprocess data for machine learning. Each sample is encoded into a compact, privacy-preserving bit array. This reduces memory use and protects the original data while keeping enough structure for accurate classification. We test the method on six datasets: SMS Spam Collection, ECG200, Adult 50K, CDC Diabetes, MNIST, and Fashion MNIST. Four classifiers are used: Extreme Gradient Boosting, Deep Neural Networks, Convolutional Neural Networks, and Logistic Regression. Results show that models trained on Bloom filter encodings achieve accuracy similar to models trained on raw data or other transforms. At the same time, the method provides memory savings while enhancing privacy. These results suggest that the Bloom filter transform is an efficient preprocessing approach for diverse machine learning tasks.",
    "paper_id": "2512.19991v1"
  },
  "https://arxiv.org/abs/2512.15584v2": {
    "content_path": "../data/contents/2512.15584v2.md",
    "pdf_path": "../data/contents/2512.15584v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:23.792971",
    "title": "A Decision-Theoretic Approach for Managing Misalignment",
    "abstract": "Summary: When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.",
    "paper_id": "2512.15584v2"
  },
  "https://arxiv.org/abs/2512.11546v1": {
    "content_path": "../data/contents/2512.11546v1.md",
    "pdf_path": "../data/contents/2512.11546v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:24.163924",
    "title": "Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting",
    "abstract": "Summary: The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, \"less is more\" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal \"training diet\" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.",
    "paper_id": "2512.11546v1"
  },
  "https://arxiv.org/abs/2512.16698v1": {
    "content_path": "../data/contents/2512.16698v1.md",
    "pdf_path": "../data/contents/2512.16698v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:24.757553",
    "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning",
    "abstract": "Summary: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver",
    "paper_id": "2512.16698v1"
  },
  "https://arxiv.org/abs/2512.12779v1": {
    "content_path": "../data/contents/2512.12779v1.md",
    "pdf_path": "../data/contents/2512.12779v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:25.613955",
    "title": "OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging",
    "abstract": "Summary: Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces \"OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average\", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.",
    "paper_id": "2512.12779v1"
  },
  "https://arxiv.org/abs/2512.16676v1": {
    "content_path": "../data/contents/2512.16676v1.md",
    "pdf_path": "../data/contents/2512.16676v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:26.590870",
    "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI",
    "abstract": "Summary: The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.",
    "paper_id": "2512.16676v1"
  },
  "https://arxiv.org/abs/2512.12777v1": {
    "content_path": "../data/contents/2512.12777v1.md",
    "pdf_path": "../data/contents/2512.12777v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:27.095406",
    "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
    "abstract": "Summary: Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",
    "paper_id": "2512.12777v1"
  },
  "https://arxiv.org/abs/2512.16661v2": {
    "content_path": "../data/contents/2512.16661v2.md",
    "pdf_path": "../data/contents/2512.16661v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:27.326978",
    "title": "Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance",
    "abstract": "Summary: In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.",
    "paper_id": "2512.16661v2"
  },
  "https://arxiv.org/abs/2512.12775v1": {
    "content_path": "../data/contents/2512.12775v1.md",
    "pdf_path": "../data/contents/2512.12775v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:28.592942",
    "title": "Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions",
    "abstract": "Summary: Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.",
    "paper_id": "2512.12775v1"
  },
  "https://arxiv.org/abs/2512.20660v1": {
    "content_path": "../data/contents/2512.20660v1.md",
    "pdf_path": "../data/contents/2512.20660v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:29.346723",
    "title": "Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering",
    "abstract": "Summary: Current approaches to AI coding agents appear to blur the lines between the Large Language Model (LLM) and the agent itself, asking the LLM to make decisions best left to deterministic processes. This leads to systems prone to stochastic failures such as gaming unit tests or hallucinating syntax. Drawing on established software engineering practices that provide deterministic frameworks for managing unpredictable processes, this paper proposes setting the control boundary such that the LLM is treated as a component of the environment environment -- preserving its creative stochasticity -- rather than the decision-making agent.\n  A \\textbf{Dual-State Architecture} is formalized, separating workflow state (deterministic control flow) from environment state (stochastic generation). \\textbf{Atomic Action Pairs} couple generation with verification as indivisible transactions, where \\textbf{Guard Functions} act as sensing actions that project probabilistic outputs onto observable workflow state. The framework is validated on three code generation tasks across 13 LLMs (1.3B--15B parameters). For qualified instruction-following models, task success rates improved by up to 66 percentage points at 1.2--2.1$\\times$ baseline computational cost. The results suggest that architectural constraints can substitute for parameter scale in achieving reliable code generation.",
    "paper_id": "2512.20660v1"
  },
  "https://arxiv.org/abs/2512.12770v1": {
    "content_path": "../data/contents/2512.12770v1.md",
    "pdf_path": "../data/contents/2512.12770v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:29.832021",
    "title": "Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining",
    "abstract": "Summary: Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curió 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curió-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu",
    "paper_id": "2512.12770v1"
  },
  "https://arxiv.org/abs/2512.16433": {
    "content_path": "../data/contents/2512.16433.md",
    "pdf_path": "../data/contents/2512.16433.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:31.293586",
    "title": "Emergent Bias and Fairness in Multi-Agent Decision Systems",
    "abstract": "Analyzes emergent bias in multi-agent decision systems and proposes evaluation approaches for high-stakes financial tasks like credit scoring.",
    "paper_id": "2512.16433"
  },
  "https://arxiv.org/abs/2512.16658v1": {
    "content_path": "../data/contents/2512.16658v1.md",
    "pdf_path": "../data/contents/2512.16658v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:31.560031",
    "title": "Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking",
    "abstract": "Summary: The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.",
    "paper_id": "2512.16658v1"
  },
  "https://arxiv.org/abs/2512.12762v1": {
    "content_path": "../data/contents/2512.12762v1.md",
    "pdf_path": "../data/contents/2512.12762v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:31.942102",
    "title": "Federated Learning with Feedback Alignment",
    "abstract": "Summary: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.\n  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.",
    "paper_id": "2512.12762v1"
  },
  "https://arxiv.org/abs/2512.02261": {
    "content_path": "../data/contents/2512.02261.md",
    "pdf_path": "../data/contents/2512.02261.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:34.120915",
    "title": "TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?",
    "abstract": "Unified evaluation framework to stress-test trading agents (market intelligence, strategy, portfolio/ledger, execution) under adversarial perturbations.",
    "paper_id": "2512.02261"
  },
  "https://arxiv.org/abs/2512.16650v1": {
    "content_path": "../data/contents/2512.16650v1.md",
    "pdf_path": "../data/contents/2512.16650v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:34.435148",
    "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
    "abstract": "Summary: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
    "paper_id": "2512.16650v1"
  },
  "https://arxiv.org/abs/2512.12760v1": {
    "content_path": "../data/contents/2512.12760v1.md",
    "pdf_path": "../data/contents/2512.12760v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:34.719469",
    "title": "Intelligent Scientific Literature Explorer using Machine Learning (ISLE)",
    "abstract": "Summary: The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.",
    "paper_id": "2512.12760v1"
  },
  "https://arxiv.org/abs/2512.16649v1": {
    "content_path": "../data/contents/2512.16649v1.md",
    "pdf_path": "../data/contents/2512.16649v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:35.004296",
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "abstract": "Summary: Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
    "paper_id": "2512.16649v1"
  },
  "https://arxiv.org/abs/2512.12755v1": {
    "content_path": "../data/contents/2512.12755v1.md",
    "pdf_path": "../data/contents/2512.12755v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:38.303295",
    "title": "An End-to-End Approach for Microgrid Probabilistic Forecasting and Robust Operation via Decision-focused Learning",
    "abstract": "Summary: High penetration of renewable energy sources (RES) introduces significant uncertainty and intermittency into microgrid operations, posing challenges to economic and reliable scheduling. To address this, this paper proposes an end-to-end decision-focused framework that jointly optimizes probabilistic forecasting and robust operation for microgrids. A multilayer encoder-decoder (MED) probabilistic forecasting model is integrated with a two-stage robust optimization (TSRO) model involving direct load control (DLC) through a differentiable decision pathway, enabling gradient-based feedback from operational outcomes to improve forecasting performance. Unlike conventional sequential approaches, the proposed method aligns forecasting accuracy with operational objectives by directly minimizing decision regret via a surrogate smart predict-then-optimize (SPO) loss function. This integration ensures that probabilistic forecasts are optimized for downstream decisions, enhancing both economic efficiency and robustness. Case studies on modified IEEE 33-bus and 69-bus systems demonstrate that the proposed framework achieves superior forecasting accuracy and operational performance, reducing total and net operation costs by up to 18% compared with conventional forecasting and optimization combinations. The results verify the effectiveness and scalability of the end-to-end decision-focused approach for resilient and cost-efficient microgrid management under uncertainty.",
    "paper_id": "2512.12755v1"
  },
  "https://arxiv.org/abs/2510.11695": {
    "content_path": "../data/contents/2510.11695.md",
    "pdf_path": "../data/contents/2510.11695.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:40.562546",
    "title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents",
    "abstract": "Introduces a real-time benchmark (Agent Market Arena) for evaluating trading agents across markets—useful for continuous performance monitoring.",
    "paper_id": "2510.11695"
  },
  "https://arxiv.org/abs/2512.16626v1": {
    "content_path": "../data/contents/2512.16626v1.md",
    "pdf_path": "../data/contents/2512.16626v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:41.523819",
    "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game",
    "abstract": "Summary: We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.",
    "paper_id": "2512.16626v1"
  },
  "https://arxiv.org/abs/2512.12750v2": {
    "content_path": "../data/contents/2512.12750v2.md",
    "pdf_path": "../data/contents/2512.12750v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:42.311051",
    "title": "Improved Concentration for Mean Estimators via Shrinkage",
    "abstract": "Summary: We study a class of robust mean estimators $\\widehatμ$ obtained by adaptively shrinking the weights of sample points far from a base estimator $\\widehatκ$. Given a data-dependent scaling factor $\\widehatα$ and a weighting function $w:[0, \\infty) \\to [0,1]$, we let $\\widehatμ = \\widehatκ + \\frac{1}{n}\\sum_{i=1}^n(X_i - \\widehatκ)w(\\widehatα|X_i-\\widehatκ|) $. We prove that, under mild assumptions over $w$, these estimators achieve stronger concentration bounds than the base estimate $\\widehatκ$, including sub-Gaussian guarantees. This framework unifies and extends several existing approaches to robust mean estimation in $\\mathbb{R}$. Through numerical experiments, we show that our shrinking approach translates to faster concentration, even for small sample sizes.",
    "paper_id": "2512.12750v2"
  },
  "https://arxiv.org/abs/2510.14264": {
    "content_path": "../data/contents/2510.14264.md",
    "pdf_path": "../data/contents/2510.14264.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:42.853823",
    "title": "AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading",
    "abstract": "Tool-orchestrated agentic RL approach for stock trading with transparent, auditable tool usage—alternative design to multi-agent trading.",
    "paper_id": "2510.14264"
  },
  "https://arxiv.org/abs/2512.16614v1": {
    "content_path": "../data/contents/2512.16614v1.md",
    "pdf_path": "../data/contents/2512.16614v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:44.480046",
    "title": "Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents",
    "abstract": "Summary: AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.",
    "paper_id": "2512.16614v1"
  },
  "https://arxiv.org/abs/2512.13739v1": {
    "content_path": "../data/contents/2512.13739v1.md",
    "pdf_path": "../data/contents/2512.13739v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:48.471340",
    "title": "Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage",
    "abstract": "Summary: Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque \"black boxes,\" hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).",
    "paper_id": "2512.13739v1"
  },
  "https://arxiv.org/abs/2510.14162": {
    "content_path": "../data/contents/2510.14162.md",
    "pdf_path": "../data/contents/2510.14162.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:49.086051",
    "title": "FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API",
    "abstract": "Shows a safer pattern for finance DB agents: route NL queries to vetted parameterized queries via function calling vs free-form text-to-SQL.",
    "paper_id": "2510.14162"
  },
  "https://arxiv.org/abs/2512.16602v1": {
    "content_path": "../data/contents/2512.16602v1.md",
    "pdf_path": "../data/contents/2512.16602v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:49.538857",
    "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics",
    "abstract": "Summary: We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.",
    "paper_id": "2512.16602v1"
  },
  "https://arxiv.org/abs/2512.12744v1": {
    "content_path": "../data/contents/2512.12744v1.md",
    "pdf_path": "../data/contents/2512.12744v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:51.126134",
    "title": "Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models",
    "abstract": "Summary: Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.",
    "paper_id": "2512.12744v1"
  },
  "https://arxiv.org/abs/2508.17906": {
    "content_path": "../data/contents/2508.17906.md",
    "pdf_path": "../data/contents/2508.17906.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:51.535541",
    "title": "FinReflectKG: Agentic Construction and Evaluation of Financial Knowledge Graphs",
    "abstract": "Agentic extraction + evaluation pipeline building financial knowledge graphs from S&P 100 10-Ks—useful substrate for finance research/diligence agents.",
    "paper_id": "2508.17906"
  },
  "https://arxiv.org/abs/2512.16598v1": {
    "content_path": "../data/contents/2512.16598v1.md",
    "pdf_path": "../data/contents/2512.16598v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:52.375379",
    "title": "Muon is Provably Faster with Momentum Variance Reduction",
    "abstract": "Summary: Recent empirical research has demonstrated that deep learning optimizers based on the linear minimization oracle (LMO) over specifically chosen Non-Euclidean norm balls, such as Muon and Scion, outperform Adam-type methods in the training of large language models. In this work, we show that such optimizers can be provably improved by replacing their vanilla momentum by momentum variance reduction (MVR). Instead of proposing and analyzing MVR variants of Muon and Scion separately, we incorporate MVR into the recently proposed Gluon framework, which captures Muon, Scion and other specific Non-Euclidean LMO-based methods as special cases, and at the same time works with a more general smoothness assumption which better captures the layer-wise structure of neural networks. In the non-convex case, we incorporate MVR into Gluon in three different ways. All of them improve the convergence rate from ${\\cal O} (\\frac{1}{K^{1/4}})$ to ${\\cal O} (\\frac{1}{K^{1/3}})$. Additionally, we provide improved rates in the star-convex case. Finally, we conduct several numerical experiments that verify the superior performance of our proposed algorithms in terms of iteration complexity.",
    "paper_id": "2512.16598v1"
  },
  "https://arxiv.org/abs/2512.12740v1": {
    "content_path": "../data/contents/2512.12740v1.md",
    "pdf_path": "../data/contents/2512.12740v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:52.675649",
    "title": "FuXi-$γ$: Efficient Sequential Recommendation with Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism",
    "abstract": "Summary: Sequential recommendation aims to model users' evolving preferences based on their historical interactions. Recent advances leverage Transformer-based architectures to capture global dependencies, but existing methods often suffer from high computational overhead, primarily due to discontinuous memory access in temporal encoding and dense attention over long sequences. To address these limitations, we propose FuXi-$γ$, a novel sequential recommendation framework that improves both effectiveness and efficiency through principled architectural design. FuXi-$γ$ adopts a decoder-only Transformer structure and introduces two key innovations: (1) An exponential-power temporal encoder that encodes relative temporal intervals using a tunable exponential decay function inspired by the Ebbinghaus forgetting curve. This encoder enables flexible modeling of both short-term and long-term preferences while maintaining high efficiency through continuous memory access and pure matrix operations. (2) A diagonal-sparse positional mechanism that prunes low-contribution attention blocks using a diagonal-sliding strategy guided by the persymmetry of Toeplitz matrix. Extensive experiments on four real-world datasets demonstrate that FuXi-$γ$ achieves state-of-the-art performance in recommendation quality, while accelerating training by up to 4.74$\\times$ and inference by up to 6.18$\\times$, making it a practical and scalable solution for long-sequence recommendation. Our code is available at https://github.com/Yeedzhi/FuXi-gamma.",
    "paper_id": "2512.12740v1"
  },
  "https://arxiv.org/abs/2506.01423": {
    "content_path": "../data/contents/2506.01423.md",
    "pdf_path": "../data/contents/2506.01423.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:53.346436",
    "title": "FinRobot: Generative Business Process AI Agents for Enterprise Resource Planning in Finance",
    "abstract": "Agent-based ERP framework for finance workflows (budgeting, reporting, wire transfers, reimbursements) with multi-agent orchestration and compliance hooks.",
    "paper_id": "2506.01423"
  },
  "https://arxiv.org/abs/2512.16558v1": {
    "content_path": "../data/contents/2512.16558v1.md",
    "pdf_path": "../data/contents/2512.16558v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:55.206167",
    "title": "Persistent Multiscale Density-based Clustering",
    "abstract": "Summary: Clustering is a cornerstone of modern data analysis. Detecting clusters in exploratory data analyses (EDA) requires algorithms that make few assumptions about the data. Density-based clustering algorithms are particularly well-suited for EDA because they describe high-density regions, assuming only that a density exists. Applying density-based clustering algorithms in practice, however, requires selecting appropriate hyperparameters, which is difficult without prior knowledge of the data distribution. For example, DBSCAN requires selecting a density threshold, and HDBSCAN* relies on a minimum cluster size parameter. In this work, we propose Persistent Leaves Spatial Clustering for Applications with Noise (PLSCAN). This novel density-based clustering algorithm efficiently identifies all minimum cluster sizes for which HDBSCAN* produces stable (leaf) clusters. PLSCAN applies scale-space clustering principles and is equivalent to persistent homology on a novel metric space. We compare its performance to HDBSCAN* on several real-world datasets, demonstrating that it achieves a higher average ARI and is less sensitive to changes in the number of mutual reachability neighbours. Additionally, we compare PLSCAN's computational costs to k-Means, demonstrating competitive run-times on low-dimensional datasets. At higher dimensions, run times scale more similarly to HDBSCAN*.",
    "paper_id": "2512.16558v1"
  },
  "https://arxiv.org/abs/2512.12735v1": {
    "content_path": "../data/contents/2512.12735v1.md",
    "pdf_path": "../data/contents/2512.12735v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:56.030104",
    "title": "Limits To (Machine) Learning",
    "abstract": "Summary: Machine learning (ML) methods are highly flexible, but their ability to approximate the true data-generating process is fundamentally constrained by finite samples. We characterize a universal lower bound, the Limits-to-Learning Gap (LLG), quantifying the unavoidable discrepancy between a model's empirical fit and the population benchmark. Recovering the true population $R^2$, therefore, requires correcting observed predictive performance by this bound. Using a broad set of variables, including excess returns, yields, credit spreads, and valuation ratios, we find that the implied LLGs are large. This indicates that standard ML approaches can substantially understate true predictability in financial data. We also derive LLG-based refinements to the classic Hansen and Jagannathan (1991) bounds, analyze implications for parameter learning in general-equilibrium settings, and show that the LLG provides a natural mechanism for generating excess volatility.",
    "paper_id": "2512.12735v1"
  },
  "https://arxiv.org/abs/2411.04788": {
    "content_path": "../data/contents/2411.04788.md",
    "pdf_path": "../data/contents/2411.04788.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:56.514216",
    "title": "Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research",
    "abstract": "Novel multi-agent collaboration system for investment research analyzing SEC 10-K forms, demonstrates superiority over single-agent approaches.",
    "paper_id": "2411.04788"
  },
  "https://arxiv.org/abs/2512.16553v1": {
    "content_path": "../data/contents/2512.16553v1.md",
    "pdf_path": "../data/contents/2512.16553v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:56.972119",
    "title": "Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild",
    "abstract": "Summary: Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.",
    "paper_id": "2512.16553v1"
  },
  "https://arxiv.org/abs/2512.12730v1": {
    "content_path": "../data/contents/2512.12730v1.md",
    "pdf_path": "../data/contents/2512.12730v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:00:59.921647",
    "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
    "abstract": "Summary: Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.",
    "paper_id": "2512.12730v1"
  },
  "https://arxiv.org/html/2412.20138v3": {
    "content_path": "../data/contents/2412.20138v3.md",
    "pdf_path": "../data/contents/2412.20138v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:00.843861",
    "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
    "abstract": "Multi-agent framework with Bull/Bear researcher agents, risk management team, and traders for improved trading decisions.",
    "paper_id": "2412.20138v3"
  },
  "https://arxiv.org/abs/2512.16541v1": {
    "content_path": "../data/contents/2512.16541v1.md",
    "pdf_path": "../data/contents/2512.16541v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:01.070726",
    "title": "UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification",
    "abstract": "Summary: This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case.",
    "paper_id": "2512.16541v1"
  },
  "https://arxiv.org/abs/2512.12727v1": {
    "content_path": "../data/contents/2512.12727v1.md",
    "pdf_path": "../data/contents/2512.12727v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:02.072357",
    "title": "EXFormer: A Multi-Scale Trend-Aware Transformer with Dynamic Variable Selection for Foreign Exchange Returns Prediction",
    "abstract": "Summary: Accurately forecasting daily exchange rate returns represents a longstanding challenge in international finance, as the exchange rate returns are driven by a multitude of correlated market factors and exhibit high-frequency fluctuations. This paper proposes EXFormer, a novel Transformer-based architecture specifically designed for forecasting the daily exchange rate returns. We introduce a multi-scale trend-aware self-attention mechanism that employs parallel convolutional branches with differing receptive fields to align observations on the basis of local slopes, preserving long-range dependencies while remaining sensitive to regime shifts. A dynamic variable selector assigns time-varying importance weights to 28 exogenous covariates related to exchange rate returns, providing pre-hoc interpretability. An embedded squeeze-and-excitation block recalibrates channel responses to emphasize informative features and depress noise in the forecasting. Using the daily data for EUR/USD, USD/JPY, and GBP/USD, we conduct out-of-sample evaluations across five different sliding windows. EXFormer consistently outperforms the random walk and other baselines, improving directional accuracy by a statistically significant margin of up to 8.5--22.8%. In nearly one year of trading backtests, the model converts these gains into cumulative returns of 18%, 25%, and 18% for the three pairs, with Sharpe ratios exceeding 1.8. When conservative transaction costs and slippage are accounted for, EXFormer retains cumulative returns of 7%, 19%, and 9%, while other baselines achieve negative. The robustness checks further confirm the model's superiority under high-volatility and bear-market regimes. EXFormer furnishes both economically valuable forecasts and transparent, time-varying insights into the drivers of exchange rate dynamics for international investors, corporations, and central bank practitioners.",
    "paper_id": "2512.12727v1"
  },
  "https://arxiv.org/abs/2509.17395": {
    "content_path": "../data/contents/2509.17395.md",
    "pdf_path": "../data/contents/2509.17395.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:02.488335",
    "title": "FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis",
    "abstract": "Multi-agent framework with debate mechanism integrating domain-specific RAG for financial analysis with specialized agents for earnings, market, sentiment, valuation, and risk.",
    "paper_id": "2509.17395"
  },
  "https://arxiv.org/abs/2512.16538v1": {
    "content_path": "../data/contents/2512.16538v1.md",
    "pdf_path": "../data/contents/2512.16538v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:03.447399",
    "title": "A Systematic Study of Code Obfuscation Against LLM-based Vulnerability Detection",
    "abstract": "Summary: As large language models (LLMs) are increasingly adopted for code vulnerability detection, their reliability and robustness across diverse vulnerability types have become a pressing concern. In traditional adversarial settings, code obfuscation has long been used as a general strategy to bypass auditing tools, preserving exploitability without tampering with the tools themselves. Numerous efforts have explored obfuscation methods and tools, yet their capabilities differ in terms of supported techniques, granularity, and programming languages, making it difficult to systematically assess their impact on LLM-based vulnerability detection. To address this gap, we provide a structured systematization of obfuscation techniques and evaluate them under a unified framework. Specifically, we categorize existing obfuscation methods into three major classes (layout, data flow, and control flow) covering 11 subcategories and 19 concrete techniques. We implement these techniques across four programming languages (Solidity, C, C++, and Python) using a consistent LLM-driven approach, and evaluate their effects on 15 LLMs spanning four model families (DeepSeek, OpenAI, Qwen, and LLaMA), as well as on two coding agents (GitHub Copilot and Codex). Our findings reveal both positive and negative impacts of code obfuscation on LLM-based vulnerability detection, highlighting conditions under which obfuscation leads to performance improvements or degradations. We further analyze these outcomes with respect to vulnerability characteristics, code properties, and model attributes. Finally, we outline several open problems and propose future directions to enhance the robustness of LLMs for real-world vulnerability detection.",
    "paper_id": "2512.16538v1"
  },
  "https://arxiv.org/abs/2512.12716v1": {
    "content_path": "../data/contents/2512.12716v1.md",
    "pdf_path": "../data/contents/2512.12716v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:04.349183",
    "title": "CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning",
    "abstract": "Summary: Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by \"Context Explosion\", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.",
    "paper_id": "2512.12716v1"
  },
  "https://arxiv.org/abs/2510.01664": {
    "content_path": "../data/contents/2510.01664.md",
    "pdf_path": "../data/contents/2510.01664.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:05.240728",
    "title": "GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents",
    "abstract": "Shows how to encode investor philosophies as prompt-guided agents and evaluate via backtests—useful for portfolio construction and research automation.",
    "paper_id": "2510.01664"
  },
  "https://arxiv.org/abs/2512.16532v1": {
    "content_path": "../data/contents/2512.16532v1.md",
    "pdf_path": "../data/contents/2512.16532v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:05.518846",
    "title": "From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment",
    "abstract": "Summary: Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.",
    "paper_id": "2512.16532v1"
  },
  "https://arxiv.org/abs/2512.12706v1": {
    "content_path": "../data/contents/2512.12706v1.md",
    "pdf_path": "../data/contents/2512.12706v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:07.717703",
    "title": "Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning",
    "abstract": "Summary: The widespread adoption of the \"Games as a Service\" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.",
    "paper_id": "2512.12706v1"
  },
  "https://arxiv.org/abs/2512.16531v3": {
    "content_path": "../data/contents/2512.16531v3.md",
    "pdf_path": "../data/contents/2512.16531v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:11.718048",
    "title": "Scaling Laws for Energy Efficiency of Local LLMs",
    "abstract": "Summary: Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.",
    "paper_id": "2512.16531v3"
  },
  "https://arxiv.org/abs/2512.12701v1": {
    "content_path": "../data/contents/2512.12701v1.md",
    "pdf_path": "../data/contents/2512.12701v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:12.742483",
    "title": "Efficient Vision-Language Reasoning via Adaptive Token Pruning",
    "abstract": "Summary: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.",
    "paper_id": "2512.12701v1"
  },
  "https://arxiv.org/abs/2512.16530v1": {
    "content_path": "../data/contents/2512.16530v1.md",
    "pdf_path": "../data/contents/2512.16530v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:13.052180",
    "title": "Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics",
    "abstract": "Summary: This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.",
    "paper_id": "2512.16530v1"
  },
  "https://arxiv.org/abs/2512.12693v1": {
    "content_path": "../data/contents/2512.12693v1.md",
    "pdf_path": "../data/contents/2512.12693v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:13.784026",
    "title": "Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits",
    "abstract": "Summary: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.",
    "paper_id": "2512.12693v1"
  },
  "https://arxiv.org/abs/2512.16523v1": {
    "content_path": "../data/contents/2512.16523v1.md",
    "pdf_path": "../data/contents/2512.16523v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:15.248555",
    "title": "TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models",
    "abstract": "Summary: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.",
    "paper_id": "2512.16523v1"
  },
  "https://arxiv.org/abs/2512.12692v1": {
    "content_path": "../data/contents/2512.12692v1.md",
    "pdf_path": "../data/contents/2512.12692v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:16.679307",
    "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
    "abstract": "Summary: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.",
    "paper_id": "2512.12692v1"
  },
  "https://arxiv.org/abs/2512.16512v1": {
    "content_path": "../data/contents/2512.16512v1.md",
    "pdf_path": "../data/contents/2512.16512v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:17.429191",
    "title": "XTC, A Research Platform for Optimizing AI Workload Operators",
    "abstract": "Summary: Achieving high efficiency on AI operators demands precise control over computation and data movement. However, existing scheduling languages are locked into specific compiler ecosystems, preventing fair comparison, reuse, and evaluation across frameworks. No unified interface currently decouples scheduling specification from code generation and measurement. We introduce XTC, a platform that unifies scheduling and performance evaluation across compilers. With its common API and reproducible measurement framework, XTC enables portable experimentation and accelerates research on optimization strategies.",
    "paper_id": "2512.16512v1"
  },
  "https://arxiv.org/abs/2512.12690v1": {
    "content_path": "../data/contents/2512.12690v1.md",
    "pdf_path": "../data/contents/2512.12690v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:18.283143",
    "title": "Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning",
    "abstract": "Summary: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing \"RL over SFT\" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.",
    "paper_id": "2512.12690v1"
  },
  "https://arxiv.org/abs/2505.15155": {
    "content_path": "../data/contents/2505.15155.md",
    "pdf_path": "../data/contents/2505.15155.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:20.723890",
    "title": "R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization",
    "abstract": "Proposes a coordinated multi-agent pipeline automating quant factor mining, model iteration, and evaluation/backtesting loops.",
    "paper_id": "2505.15155"
  },
  "https://arxiv.org/abs/2512.16491v2": {
    "content_path": "../data/contents/2512.16491v2.md",
    "pdf_path": "../data/contents/2512.16491v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:21.369298",
    "title": "Best Practices For Empirical Meta-Algorithmic Research: Guidelines from the COSEAL Research Network",
    "abstract": "Summary: Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing experiments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.",
    "paper_id": "2512.16491v2"
  },
  "https://arxiv.org/abs/2512.12686v1": {
    "content_path": "../data/contents/2512.12686v1.md",
    "pdf_path": "../data/contents/2512.12686v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:22.052541",
    "title": "Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI",
    "abstract": "Summary: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.",
    "paper_id": "2512.12686v1"
  },
  "https://arxiv.org/abs/2512.16489v1": {
    "content_path": "../data/contents/2512.16489v1.md",
    "pdf_path": "../data/contents/2512.16489v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:22.746634",
    "title": "Advantages and limitations in the use of transfer learning for individual treatment effects in causal machine learning",
    "abstract": "Summary: Generalizing causal knowledge across diverse environments is challenging, especially when estimates from large-scale datasets must be applied to smaller or systematically different contexts, where external validity is critical. Model-based estimators of individual treatment effects (ITE) from machine learning require large sample sizes, limiting their applicability in domains such as behavioral sciences with smaller datasets. We demonstrate how estimation of ITEs with Treatment Agnostic Representation Networks (TARNet; Shalit et al., 2017) can be improved by leveraging knowledge from source datasets and adapting it to new settings via transfer learning (TL-TARNet; Aloui et al., 2023). In simulations that vary source and sample sizes and consider both randomized and non-randomized intervention target settings, the transfer-learning extension TL-TARNet improves upon standard TARNet, reducing ITE error and attenuating bias when a large unbiased source is available and target samples are small. In an empirical application using the India Human Development Survey (IHDS-II), we estimate the effect of mothers' firewood collection time on children's weekly study time; transfer learning pulls the target mean ITEs toward the source ITE estimate, reducing bias in the estimates obtained without transfer. These results suggest that transfer learning for causal models can improve the estimation of ITE in small samples.",
    "paper_id": "2512.16489v1"
  },
  "https://arxiv.org/abs/2512.12677v1": {
    "content_path": "../data/contents/2512.12677v1.md",
    "pdf_path": "../data/contents/2512.12677v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:23.956776",
    "title": "Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches",
    "abstract": "Summary: We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.",
    "paper_id": "2512.12677v1"
  },
  "https://arxiv.org/abs/2407.04885": {
    "content_path": "../data/contents/2407.04885.md",
    "pdf_path": "../data/contents/2407.04885.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:24.713793",
    "title": "Automating Venture Capital: Founder assessment using LLM-powered segmentation",
    "abstract": "Research on using LLMs for VC founder assessment with feature engineering and automated labeling for startup success prediction.",
    "paper_id": "2407.04885"
  },
  "https://arxiv.org/abs/2512.16487v1": {
    "content_path": "../data/contents/2512.16487v1.md",
    "pdf_path": "../data/contents/2512.16487v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:25.503382",
    "title": "A Survey on Spatio-Temporal Knowledge Graph Models",
    "abstract": "Summary: Many complex real-world systems exhibit inherently intertwined temporal and spatial characteristics. Spatio-temporal knowledge graphs (STKGs) have therefore emerged as a powerful representation paradigm, as they integrate entities, relationships, time and space within a unified graph structure. They are increasingly applied across diverse domains, including environmental systems and urban, transportation, social and human mobility networks. However, modeling STKGs remains challenging: their foundations span classical graph theory as well as temporal and spatial graph models, which have evolved independently across different research communities and follow heterogeneous modeling assumptions and terminologies. As a result, existing approaches often lack conceptual alignment, generalizability and reusability. This survey provides a systematic review of spatio-temporal knowledge graph models, tracing their origins in static, temporal and spatial graph modeling. We analyze existing approaches along key modeling dimensions, including edge semantics, temporal and spatial annotation strategies, temporal and spatial semantics and relate these choices to their respective application domains. Our analysis reveals that unified modeling frameworks are largely absent and that most current models are tailored to specific use cases rather than designed for reuse or long-term knowledge preservation. Based on these findings, we derive modeling guidelines and identify open challenges to guide future research.",
    "paper_id": "2512.16487v1"
  },
  "https://arxiv.org/abs/2512.12669v1": {
    "content_path": "../data/contents/2512.12669v1.md",
    "pdf_path": "../data/contents/2512.12669v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:25.763783",
    "title": "DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization",
    "abstract": "Summary: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.",
    "paper_id": "2512.12669v1"
  },
  "https://arxiv.org/abs/2512.16970v1": {
    "content_path": "../data/contents/2512.16970v1.md",
    "pdf_path": "../data/contents/2512.16970v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:26.105848",
    "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework",
    "abstract": "Summary: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.",
    "paper_id": "2512.16970v1"
  },
  "https://arxiv.org/abs/2512.12663v1": {
    "content_path": "../data/contents/2512.12663v1.md",
    "pdf_path": "../data/contents/2512.12663v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:26.618576",
    "title": "PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks",
    "abstract": "Summary: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.\n  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.\n  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.",
    "paper_id": "2512.12663v1"
  },
  "https://arxiv.org/abs/2512.16484v1": {
    "content_path": "../data/contents/2512.16484v1.md",
    "pdf_path": "../data/contents/2512.16484v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:28.881549",
    "title": "Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment",
    "abstract": "Summary: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.",
    "paper_id": "2512.16484v1"
  },
  "https://arxiv.org/abs/2512.12652v1": {
    "content_path": "../data/contents/2512.12652v1.md",
    "pdf_path": "../data/contents/2512.12652v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:29.435246",
    "title": "Value-Aware Multiagent Systems",
    "abstract": "Summary: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.",
    "paper_id": "2512.12652v1"
  },
  "https://arxiv.org/abs/2512.16473v1": {
    "content_path": "../data/contents/2512.16473v1.md",
    "pdf_path": "../data/contents/2512.16473v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:29.843359",
    "title": "Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems",
    "abstract": "Summary: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.",
    "paper_id": "2512.16473v1"
  },
  "https://arxiv.org/abs/2512.12643v1": {
    "content_path": "../data/contents/2512.12643v1.md",
    "pdf_path": "../data/contents/2512.12643v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:30.895183",
    "title": "LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases",
    "abstract": "Summary: Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.",
    "paper_id": "2512.12643v1"
  },
  "https://arxiv.org/abs/2512.16969v1": {
    "content_path": "../data/contents/2512.16969v1.md",
    "pdf_path": "../data/contents/2512.16969v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:48.028278",
    "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
    "abstract": "Summary: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.",
    "paper_id": "2512.16969v1"
  },
  "https://arxiv.org/abs/2512.12641v1": {
    "content_path": "../data/contents/2512.12641v1.md",
    "pdf_path": "../data/contents/2512.12641v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:48.403460",
    "title": "Which Pieces Does Unigram Tokenization Really Need?",
    "abstract": "Summary: The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.",
    "paper_id": "2512.12641v1"
  },
  "https://arxiv.org/abs/2512.16468v1": {
    "content_path": "../data/contents/2512.16468v1.md",
    "pdf_path": "../data/contents/2512.16468v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:50.162365",
    "title": "Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery",
    "abstract": "Summary: Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images \"look real\" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.",
    "paper_id": "2512.16468v1"
  },
  "https://arxiv.org/abs/2512.12634v1": {
    "content_path": "../data/contents/2512.12634v1.md",
    "pdf_path": "../data/contents/2512.12634v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:01:51.092090",
    "title": "Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents",
    "abstract": "Summary: Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.",
    "paper_id": "2512.12634v1"
  },
  "https://arxiv.org/abs/2512.16465v2": {
    "content_path": "../data/contents/2512.16465v2.md",
    "pdf_path": "../data/contents/2512.16465v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:00.593855",
    "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
    "abstract": "Summary: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
    "paper_id": "2512.16465v2"
  },
  "https://arxiv.org/abs/2512.12633v1": {
    "content_path": "../data/contents/2512.12633v1.md",
    "pdf_path": "../data/contents/2512.12633v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:02.184106",
    "title": "DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model",
    "abstract": "Summary: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.",
    "paper_id": "2512.12633v1"
  },
  "https://arxiv.org/abs/2512.16455v1": {
    "content_path": "../data/contents/2512.16455v1.md",
    "pdf_path": "../data/contents/2512.16455v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:03.112215",
    "title": "AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research",
    "abstract": "Summary: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.",
    "paper_id": "2512.16455v1"
  },
  "https://arxiv.org/abs/2512.12623v2": {
    "content_path": "../data/contents/2512.12623v2.md",
    "pdf_path": "../data/contents/2512.12623v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:08.802680",
    "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
    "abstract": "Summary: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.",
    "paper_id": "2512.12623v2"
  },
  "https://arxiv.org/abs/2512.16453v1": {
    "content_path": "../data/contents/2512.16453v1.md",
    "pdf_path": "../data/contents/2512.16453v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:12.347644",
    "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries",
    "abstract": "Summary: Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.",
    "paper_id": "2512.16453v1"
  },
  "https://arxiv.org/abs/2512.15778v2": {
    "content_path": "../data/contents/2512.15778v2.md",
    "pdf_path": "../data/contents/2512.15778v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:13.957225",
    "title": "COBRA: Catastrophic Bit-flip Reliability Analysis of State-Space Models",
    "abstract": "Summary: State-space models (SSMs), exemplified by the Mamba architecture, have recently emerged as state-of-the-art sequence-modeling frameworks, offering linear-time scalability together with strong performance in long-context settings. Owing to their unique combination of efficiency, scalability, and expressive capacity, SSMs have become compelling alternatives to transformer-based models, which suffer from the quadratic computational and memory costs of attention mechanisms. As SSMs are increasingly deployed in real-world applications, it is critical to assess their susceptibility to both software- and hardware-level threats to ensure secure and reliable operation. Among such threats, hardware-induced bit-flip attacks (BFAs) pose a particularly severe risk by corrupting model parameters through memory faults, thereby undermining model accuracy and functional integrity. To investigate this vulnerability, we introduce RAMBO, the first BFA framework specifically designed to target Mamba-based architectures. Through experiments on the Mamba-1.4b model with LAMBADA benchmark, a cloze-style word-prediction task, we demonstrate that flipping merely a single critical bit can catastrophically reduce accuracy from 74.64% to 0% and increase perplexity from 18.94 to 3.75 x 10^6. These results demonstrate the pronounced fragility of SSMs to adversarial perturbations.",
    "paper_id": "2512.15778v2"
  },
  "https://arxiv.org/abs/2512.16452v1": {
    "content_path": "../data/contents/2512.16452v1.md",
    "pdf_path": "../data/contents/2512.16452v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:14.247261",
    "title": "Smart Data Portfolios: A Quantitative Framework for Input Governance in AI",
    "abstract": "Summary: Growing concerns about fairness, privacy, robustness, and transparency have made it a central expectation of AI governance that automated decisions be explainable by institutions and intelligible to affected parties. We introduce the Smart Data Portfolio (SDP) framework, which treats data categories as productive but risk-bearing assets, formalizing input governance as an information-risk trade-off. Within this framework, we define two portfolio-level quantities, Informational Return and Governance-Adjusted Risk, whose interaction characterizes data mixtures and generates a Governance-Efficient Frontier. Regulators shape this frontier through risk caps, admissible categories, and weight bands that translate fairness, privacy, robustness, and provenance requirements into measurable constraints on data allocation while preserving model flexibility. A telecommunications illustration shows how different AI services require distinct portfolios within a common governance structure. The framework offers a familiar portfolio logic as an input-level explanation layer suited to the large-scale deployment of AI systems.",
    "paper_id": "2512.16452v1"
  },
  "https://arxiv.org/abs/2512.12620v2": {
    "content_path": "../data/contents/2512.12620v2.md",
    "pdf_path": "../data/contents/2512.12620v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:14.467143",
    "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
    "abstract": "Summary: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.",
    "paper_id": "2512.12620v2"
  },
  "https://arxiv.org/abs/2512.16447v1": {
    "content_path": "../data/contents/2512.16447v1.md",
    "pdf_path": "../data/contents/2512.16447v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:16.400852",
    "title": "Towards AI-Supported Research: a Vision of the TIB AIssistant",
    "abstract": "Summary: The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.",
    "paper_id": "2512.16447v1"
  },
  "https://arxiv.org/abs/2512.12613v1": {
    "content_path": "../data/contents/2512.12613v1.md",
    "pdf_path": "../data/contents/2512.12613v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:17.685914",
    "title": "StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning",
    "abstract": "Summary: Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.",
    "paper_id": "2512.12613v1"
  },
  "https://arxiv.org/html/2403.19735v1": {
    "content_path": "../data/contents/2403.19735v1.md",
    "pdf_path": "../data/contents/2403.19735v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:17.988582",
    "title": "Enhancing Anomaly Detection in Financial Markets with LLM-based Multi-Agent Framework",
    "abstract": "Research paper on collaborative AI agent network for automated anomaly validation in S&P 500 data.",
    "paper_id": "2403.19735v1"
  },
  "https://arxiv.org/abs/2512.16445v1": {
    "content_path": "../data/contents/2512.16445v1.md",
    "pdf_path": "../data/contents/2512.16445v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:18.411922",
    "title": "Topic Modelling Black Box Optimization",
    "abstract": "Summary: Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.",
    "paper_id": "2512.16445v1"
  },
  "https://arxiv.org/abs/2512.12610v1": {
    "content_path": "../data/contents/2512.12610v1.md",
    "pdf_path": "../data/contents/2512.12610v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:19.021011",
    "title": "Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching",
    "abstract": "Summary: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/",
    "paper_id": "2512.12610v1"
  },
  "https://arxiv.org/abs/2512.16442v1": {
    "content_path": "../data/contents/2512.16442v1.md",
    "pdf_path": "../data/contents/2512.16442v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:19.784360",
    "title": "TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles",
    "abstract": "Summary: The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.",
    "paper_id": "2512.16442v1"
  },
  "https://arxiv.org/abs/2512.12608v2": {
    "content_path": "../data/contents/2512.12608v2.md",
    "pdf_path": "../data/contents/2512.12608v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:20.405632",
    "title": "Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery",
    "abstract": "Summary: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.",
    "paper_id": "2512.12608v2"
  },
  "https://arxiv.org/abs/2512.16439v1": {
    "content_path": "../data/contents/2512.16439v1.md",
    "pdf_path": "../data/contents/2512.16439v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:21.230739",
    "title": "From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection",
    "abstract": "Summary: Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.",
    "paper_id": "2512.16439v1"
  },
  "https://arxiv.org/abs/2512.12605v1": {
    "content_path": "../data/contents/2512.12605v1.md",
    "pdf_path": "../data/contents/2512.12605v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:22.084128",
    "title": "Causal inference and model explainability tools for retail",
    "abstract": "Summary: Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.",
    "paper_id": "2512.12605v1"
  },
  "https://arxiv.org/abs/2512.16433v1": {
    "content_path": "../data/contents/2512.16433v1.md",
    "pdf_path": "../data/contents/2512.16433v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:22.821806",
    "title": "Emergent Bias and Fairness in Multi-Agent Decision Systems",
    "abstract": "Summary: Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.",
    "paper_id": "2512.16433v1"
  },
  "https://arxiv.org/abs/2512.12602v1": {
    "content_path": "../data/contents/2512.12602v1.md",
    "pdf_path": "../data/contents/2512.12602v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:23.444636",
    "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
    "abstract": "Summary: Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.",
    "paper_id": "2512.12602v1"
  },
  "https://arxiv.org/abs/2512.16425v1": {
    "content_path": "../data/contents/2512.16425v1.md",
    "pdf_path": "../data/contents/2512.16425v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:23.801836",
    "title": "Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach",
    "abstract": "Summary: As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.",
    "paper_id": "2512.16425v1"
  },
  "https://arxiv.org/abs/2512.12597v1": {
    "content_path": "../data/contents/2512.12597v1.md",
    "pdf_path": "../data/contents/2512.12597v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:24.132414",
    "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation",
    "abstract": "Summary: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.",
    "paper_id": "2512.12597v1"
  },
  "https://arxiv.org/abs/2512.16424v1": {
    "content_path": "../data/contents/2512.16424v1.md",
    "pdf_path": "../data/contents/2512.16424v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:25.197690",
    "title": "Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs",
    "abstract": "Summary: Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.",
    "paper_id": "2512.16424v1"
  },
  "https://arxiv.org/abs/2512.12596v1": {
    "content_path": "../data/contents/2512.12596v1.md",
    "pdf_path": "../data/contents/2512.12596v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:29.314062",
    "title": "Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models",
    "abstract": "Summary: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based \"placement plan\" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.",
    "paper_id": "2512.12596v1"
  },
  "https://arxiv.org/abs/2512.16411v1": {
    "content_path": "../data/contents/2512.16411v1.md",
    "pdf_path": "../data/contents/2512.16411v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:29.977257",
    "title": "Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection",
    "abstract": "Summary: Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices.",
    "paper_id": "2512.16411v1"
  },
  "https://arxiv.org/abs/2512.12594v1": {
    "content_path": "../data/contents/2512.12594v1.md",
    "pdf_path": "../data/contents/2512.12594v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:30.556504",
    "title": "ceLLMate: Sandboxing Browser AI Agents",
    "abstract": "Summary: Browser-using agents (BUAs) are an emerging class of autonomous agents that interact with web browsers in human-like ways, including clicking, scrolling, filling forms, and navigating across pages. While these agents help automate repetitive online tasks, they are vulnerable to prompt injection attacks that can trick an agent into performing undesired actions, such as leaking private information or issuing state-changing requests. We propose ceLLMate, a browser-level sandboxing framework that restricts the agent's ambient authority and reduces the blast radius of prompt injections. We address two fundamental challenges: (1) The semantic gap challenge in policy enforcement arises because the agent operates through low-level UI observations and manipulations; however, writing and enforcing policies directly over UI-level events is brittle and error-prone. To address this challenge, we introduce an agent sitemap that maps low-level browser behaviors to high-level semantic actions. (2) Policy prediction in BUAs is the norm rather than the exception. BUAs have no app developer to pre-declare sandboxing policies, and thus, ceLLMate pairs website-authored mandatory policies with an automated policy-prediction layer that adapts and instantiates these policies from the user's natural-language task. We implement ceLLMate as an agent-agnostic browser extension and demonstrate how it enables sandboxing policies that effectively block various types of prompt injection attacks with negligible overhead.",
    "paper_id": "2512.12594v1"
  },
  "https://arxiv.org/abs/2512.16406v1": {
    "content_path": "../data/contents/2512.16406v1.md",
    "pdf_path": "../data/contents/2512.16406v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:31.750399",
    "title": "Hypernetworks That Evolve Themselves",
    "abstract": "Summary: How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.",
    "paper_id": "2512.16406v1"
  },
  "https://arxiv.org/abs/2512.14751v1": {
    "content_path": "../data/contents/2512.14751v1.md",
    "pdf_path": "../data/contents/2512.14751v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:33.168442",
    "title": "One Leak Away: How Pretrained Model Exposure Amplifies Jailbreak Risks in Finetuned LLMs",
    "abstract": "Summary: Finetuning pretrained large language models (LLMs) has become the standard paradigm for developing downstream applications. However, its security implications remain unclear, particularly regarding whether finetuned LLMs inherit jailbreak vulnerabilities from their pretrained sources. We investigate this question in a realistic pretrain-to-finetune threat model, where the attacker has white-box access to the pretrained LLM and only black-box access to its finetuned derivatives. Empirical analysis shows that adversarial prompts optimized on the pretrained model transfer most effectively to its finetuned variants, revealing inherited vulnerabilities from pretrained to finetuned LLMs. To further examine this inheritance, we conduct representation-level probing, which shows that transferable prompts are linearly separable within the pretrained hidden states, suggesting that universal transferability is encoded in pretrained representations. Building on this insight, we propose the Probe-Guided Projection (PGP) attack, which steers optimization toward transferability-relevant directions. Experiments across multiple LLM families and diverse finetuned tasks confirm PGP's strong transfer success, underscoring the security risks inherent in the pretrain-to-finetune paradigm.",
    "paper_id": "2512.14751v1"
  },
  "https://arxiv.org/abs/2512.16401v2": {
    "content_path": "../data/contents/2512.16401v2.md",
    "pdf_path": "../data/contents/2512.16401v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:35.855506",
    "title": "Navigating the Reality Gap: Privacy-Preserving Adaptation of ASR for Challenging Low-Resource Domains",
    "abstract": "Summary: Automatic Speech Recognition (ASR) holds immense potential to assist in clinical documentation and patient report generation, particularly in resource-constrained regions. However, deployment is currently hindered by a technical deadlock: a severe \"Reality Gap\" between laboratory performance and noisy, real-world clinical audio, coupled with strict privacy and resource constraints. We quantify this gap, showing that a robust multilingual model (IndicWav2Vec) degrades to a 40.94% WER on rural clinical data from India, rendering it unusable. To address this, we explore a zero-data-exfiltration framework enabling localized, continual adaptation via Low-Rank Adaptation (LoRA). We conduct a rigorous investigative study of continual learning strategies, characterizing the trade-offs between data-driven and parameter-driven stability. Our results demonstrate that multi-domain Experience Replay (ER) yields the primary performance gains, achieving a 17.1% relative improvement in target WER and reducing catastrophic forgetting by 55% compared to naive adaptation. Furthermore, we observed that standard Elastic Weight Consolidation (EWC) faced numerical stability challenges when applied to LoRA in noisy environments. Our experiments show that a stabilized, linearized formulation effectively controls gradient magnitudes and enables stable convergence. Finally, we verify via a domain-specific spot check that acoustic adaptation is a fundamental prerequisite for usability which cannot be bypassed by language models alone.",
    "paper_id": "2512.16401v2"
  },
  "https://arxiv.org/abs/2512.13734v1": {
    "content_path": "../data/contents/2512.13734v1.md",
    "pdf_path": "../data/contents/2512.13734v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:36.713759",
    "title": "Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation",
    "abstract": "Summary: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.",
    "paper_id": "2512.13734v1"
  },
  "https://arxiv.org/abs/2512.16391v1": {
    "content_path": "../data/contents/2512.16391v1.md",
    "pdf_path": "../data/contents/2512.16391v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:37.091622",
    "title": "Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference",
    "abstract": "Summary: Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.",
    "paper_id": "2512.16391v1"
  },
  "https://arxiv.org/abs/2512.12583v1": {
    "content_path": "../data/contents/2512.12583v1.md",
    "pdf_path": "../data/contents/2512.12583v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:39.438755",
    "title": "Detecting Prompt Injection Attacks Against Application Using Classifiers",
    "abstract": "Summary: Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus and trains several classifiers, including LSTM, feed forward neural networks, Random Forest, and Naive Bayes, to detect malicious prompts in LLM integrated web applications. The proposed approach improves prompt injection detection and mitigation, helping protect targeted applications and systems.",
    "paper_id": "2512.12583v1"
  },
  "https://arxiv.org/abs/2512.15144": {
    "content_path": "../data/contents/2512.15144.md",
    "pdf_path": "../data/contents/2512.15144.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:39.698236",
    "title": "MCPZoo: A Large-Scale Dataset of Runnable Model Context Protocol Servers for AI Agent",
    "abstract": "Large dataset (tens of thousands of MCP servers) enabling empirical work on MCP ecosystems and security analysis at scale.",
    "paper_id": "2512.15144"
  },
  "https://arxiv.org/abs/2512.16386v1": {
    "content_path": "../data/contents/2512.16386v1.md",
    "pdf_path": "../data/contents/2512.16386v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:40.270151",
    "title": "Quantitative Verification of Fairness in Tree Ensembles",
    "abstract": "Summary: This work focuses on quantitative verification of fairness in tree ensembles. Unlike traditional verification approaches that merely return a single counterexample when the fairness is violated, quantitative verification estimates the ratio of all counterexamples and characterizes the regions where they occur, which is important information for diagnosing and mitigating bias. To date, quantitative verification has been explored almost exclusively for deep neural networks (DNNs). Representative methods, such as DeepGemini and FairQuant, all build on the core idea of Counterexample-Guided Abstraction Refinement, a generic framework that could be adapted to other model classes. We extended the framework into a model-agnostic form, but discovered two limitations: (i) it can provide only lower bounds, and (ii) its performance scales poorly. Exploiting the discrete structure of tree ensembles, our work proposes an efficient quantification technique that delivers any-time upper and lower bounds. Experiments on five widely used datasets demonstrate its effectiveness and efficiency. When applied to fairness testing, our quantification method significantly outperforms state-of-the-art testing techniques.",
    "paper_id": "2512.16386v1"
  },
  "https://arxiv.org/abs/2512.12582v1": {
    "content_path": "../data/contents/2512.12582v1.md",
    "pdf_path": "../data/contents/2512.12582v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:41.003975",
    "title": "Generative AI as Digital Representatives in Collective Decision-Making: A Game-Theoretical Approach",
    "abstract": "Summary: Generative Artificial Intelligence (GenAI) enables digital representatives to make decisions on behalf of team members in collaborative tasks, but faces challenges in accurately representing preferences. While supplying GenAI with detailed personal information improves representation fidelity, feasibility constraints make complete information access impractical. We bridge this gap by developing a game-theoretic framework that models strategic information revelation to GenAI in collective decision-making. The technical challenges lie in characterizing members' equilibrium behaviors under interdependent strategies and quantifying the imperfect preference learning outcomes by digital representatives. Our contribution includes closed-form equilibrium characterizations that reveal how members strategically balance team decision preference against communication costs. Our analysis yields an interesting finding: Conflicting preferences between team members drive competitive information revelation, with members revealing more information than those with aligned preferences. While digital representatives produce aggregate preference losses no smaller than direct participation, individual members may paradoxically achieve decisions more closely aligned with their preferences when using digital representatives, particularly when manual participation costs are high or when GenAI systems are sufficiently advanced.",
    "paper_id": "2512.12582v1"
  },
  "https://arxiv.org/abs/2512.15163": {
    "content_path": "../data/contents/2512.15163.md",
    "pdf_path": "../data/contents/2512.15163.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:45.959602",
    "title": "MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers",
    "abstract": "Benchmark for safety evaluation of agentic LLMs operating over real MCP servers, including multi-server workflow risks.",
    "paper_id": "2512.15163"
  },
  "https://arxiv.org/abs/2512.16383v1": {
    "content_path": "../data/contents/2512.16383v1.md",
    "pdf_path": "../data/contents/2512.16383v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:51.087434",
    "title": "Multivariate Uncertainty Quantification with Tomographic Quantile Forests",
    "abstract": "Summary: Quantifying predictive uncertainty is essential for safe and trustworthy real-world AI deployment. Yet, fully nonparametric estimation of conditional distributions remains challenging for multivariate targets. We propose Tomographic Quantile Forests (TQF), a nonparametric, uncertainty-aware, tree-based regression model for multivariate targets. TQF learns conditional quantiles of directional projections $\\mathbf{n}^{\\top}\\mathbf{y}$ as functions of the input $\\mathbf{x}$ and the unit direction $\\mathbf{n}$. At inference, it aggregates quantiles across many directions and reconstructs the multivariate conditional distribution by minimizing the sliced Wasserstein distance via an efficient alternating scheme with convex subproblems. Unlike classical directional-quantile approaches that typically produce only convex quantile regions and require training separate models for different directions, TQF covers all directions with a single model without imposing convexity restrictions. We evaluate TQF on synthetic and real-world datasets, and release the source code on GitHub.",
    "paper_id": "2512.16383v1"
  },
  "https://arxiv.org/abs/2512.13733v1": {
    "content_path": "../data/contents/2512.13733v1.md",
    "pdf_path": "../data/contents/2512.13733v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:51.477016",
    "title": "Low-Rank Compression of Language Models via Differentiable Rank Selection",
    "abstract": "Summary: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.",
    "paper_id": "2512.13733v1"
  },
  "https://arxiv.org/abs/2512.18746": {
    "content_path": "../data/contents/2512.18746.md",
    "pdf_path": "../data/contents/2512.18746.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:53.853718",
    "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
    "abstract": "Proposes meta-evolving memory architectures and introduces EvolveLab, a unified self-evolving memory codebase for agents.",
    "paper_id": "2512.18746"
  },
  "https://arxiv.org/abs/2512.16378v2": {
    "content_path": "../data/contents/2512.16378v2.md",
    "pdf_path": "../data/contents/2512.16378v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:54.701221",
    "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
    "abstract": "Summary: As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
    "paper_id": "2512.16378v2"
  },
  "https://arxiv.org/abs/2512.12576v1": {
    "content_path": "../data/contents/2512.12576v1.md",
    "pdf_path": "../data/contents/2512.12576v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:55.342259",
    "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
    "abstract": "Summary: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
    "paper_id": "2512.12576v1"
  },
  "https://arxiv.org/abs/2512.16363v1": {
    "content_path": "../data/contents/2512.16363v1.md",
    "pdf_path": "../data/contents/2512.16363v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:56.365783",
    "title": "Empirical Likelihood Meets Prediction-Powered Inference",
    "abstract": "Summary: We study inference with a small labeled sample, a large unlabeled sample, and high-quality predictions from an external model. We link prediction-powered inference with empirical likelihood by stacking supervised estimating equations based on labeled outcomes with auxiliary moment conditions built from predictions, and then optimizing empirical likelihood under these joint constraints. The resulting empirical likelihood-based prediction-powered inference (EPI) estimator is asymptotically normal, has asymptotic variance no larger than the fully supervised estimator, and attains the semiparametric efficiency bound when the auxiliary functions span the predictable component of the supervised score. For hypothesis testing and confidence sets, empirical likelihood ratio statistics admit chi-squared-type limiting distributions. As a by-product, the empirical likelihood weights induce a calibrated empirical distribution that integrates supervised and prediction-based information, enabling estimation and uncertainty quantification for general functionals beyond parameters defined by estimating equations. We present two practical implementations: one based on basis expansions in the predictions and covariates, and one that learns an approximately optimal auxiliary function by cross-fitting. In simulations and applications, EPI reduces mean squared error and shortens confidence intervals while maintaining nominal coverage.",
    "paper_id": "2512.16363v1"
  },
  "https://arxiv.org/abs/2512.12572v1": {
    "content_path": "../data/contents/2512.12572v1.md",
    "pdf_path": "../data/contents/2512.12572v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:56.914789",
    "title": "On the Accuracy of Newton Step and Influence Function Data Attributions",
    "abstract": "Summary: Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.\n  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as \"what is the asymptotic scaling of the errors of each method?\" or \"which of these methods is more accurate for a given dataset?\"\n  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.\n  \\[ \\mathbb{E}_{T \\subseteq [n],\\, |T| = k} \\bigl[ \\|\\hatθ_T - \\hatθ_T^{\\mathrm{NS}}\\|_2 \\bigr] = \\widetildeΘ\\!\\left(\\frac{k d}{n^2}\\right), \\qquad \\mathbb{E}_{T \\subseteq [n],\\, |T| = k} \\bigl[ \\|\\hatθ_T^{\\mathrm{NS}} - \\hatθ_T^{\\mathrm{IF}}\\|_2 \\bigr] = \\widetildeΘ\\!\\left( \\frac{(k + d)\\sqrt{k d}}{n^2} \\right). \\]",
    "paper_id": "2512.12572v1"
  },
  "https://arxiv.org/abs/2512.16349v1": {
    "content_path": "../data/contents/2512.16349v1.md",
    "pdf_path": "../data/contents/2512.16349v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:02:57.837245",
    "title": "Collaborative Edge-to-Server Inference for Vision-Language Models",
    "abstract": "Summary: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.",
    "paper_id": "2512.16349v1"
  },
  "https://arxiv.org/abs/2512.13731v1": {
    "content_path": "../data/contents/2512.13731v1.md",
    "pdf_path": "../data/contents/2512.13731v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:00.222980",
    "title": "Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline",
    "abstract": "Summary: Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.",
    "paper_id": "2512.13731v1"
  },
  "https://arxiv.org/abs/2512.16339v1": {
    "content_path": "../data/contents/2512.16339v1.md",
    "pdf_path": "../data/contents/2512.16339v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:00.646673",
    "title": "Beyond openness: Inclusiveness and usability of Chinese scholarly data in OpenAlex",
    "abstract": "Summary: OpenAlex, launched in 2022 as a fully open scholarly data source, promises greater inclusiveness compared to traditional proprietary databases. This study evaluates whether OpenAlex delivers on that promise by examining its coverage and metadata quality for Chinese-language journals and their articles. Using the 2023 edition of A Guide to the Core Journals of China (GCJC) and Wanfang Data as a benchmark, we analyze three aspects: (1) journal-level coverage, (2) article-level coverage, and (3) completeness and accuracy of metadata fields. Results show that OpenAlex indexes only 37% of GCJC journals and 24% of their articles, with substantial disciplinary and temporal variation. Metadata quality is uneven: while basic fields such as title and publication year are complete, bibliographic details, author affiliations, and cited references are frequently missing or inaccurate. DOI coverage is limited, and language information is often incorrect, with most Chinese-language articles labeled as English. These findings highlight significant challenges for achieving full inclusiveness and usability in research evaluation and related activities. We conclude with recommendations for improving data aggregation strategies, DOI registration practices, and metadata standardization to enhance the integration of local scholarly outputs into global open infrastructures.",
    "paper_id": "2512.16339v1"
  },
  "https://arxiv.org/abs/2512.12560v1": {
    "content_path": "../data/contents/2512.12560v1.md",
    "pdf_path": "../data/contents/2512.12560v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:01.161692",
    "title": "StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding",
    "abstract": "Summary: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.",
    "paper_id": "2512.12560v1"
  },
  "https://arxiv.org/abs/2512.16323v1": {
    "content_path": "../data/contents/2512.16323v1.md",
    "pdf_path": "../data/contents/2512.16323v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:01.655380",
    "title": "Hacking Neural Evaluation Metrics with Single Hub Text",
    "abstract": "Summary: Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En.",
    "paper_id": "2512.16323v1"
  },
  "https://arxiv.org/abs/2512.12552v1": {
    "content_path": "../data/contents/2512.12552v1.md",
    "pdf_path": "../data/contents/2512.12552v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:02.525879",
    "title": "Large Language Newsvendor: Decision Biases and Cognitive Mechanisms",
    "abstract": "Summary: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.",
    "paper_id": "2512.12552v1"
  },
  "https://arxiv.org/abs/2512.16963v2": {
    "content_path": "../data/contents/2512.16963v2.md",
    "pdf_path": "../data/contents/2512.16963v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:03.107842",
    "title": "Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models",
    "abstract": "Summary: Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.\n  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: Compression is Routing. We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a 64x sequence length compression (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of 99.47% on the in-domain (code) validation set; accuracy drops sharply to 47.76% on a semi-out-of-distribution domain (Wiki text); and further plummets to just 0.57% on a fully out-of-distribution domain (random sequences).\n  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an Intrinsic Distribution Fingerprint. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.",
    "paper_id": "2512.16963v2"
  },
  "https://arxiv.org/abs/2512.12544v1": {
    "content_path": "../data/contents/2512.12544v1.md",
    "pdf_path": "../data/contents/2512.12544v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:03.508154",
    "title": "HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks",
    "abstract": "Summary: Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.",
    "paper_id": "2512.12544v1"
  },
  "https://arxiv.org/abs/2512.16321v1": {
    "content_path": "../data/contents/2512.16321v1.md",
    "pdf_path": "../data/contents/2512.16321v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:03.964452",
    "title": "Subset Sampling over Joins",
    "abstract": "Summary: Subset sampling (also known as Poisson sampling), where the decision to include any specific element in the sample is made independently of all others, is a fundamental primitive in data analytics, enabling efficient approximation by processing representative subsets rather than massive datasets. While sampling from explicit lists is well-understood, modern applications -- such as machine learning over relational data -- often require sampling from a set defined implicitly by a relational join. In this paper, we study the problem of \\emph{subset sampling over joins}: drawing a random subset from the join results, where each join result is included independently with some probability. We address the general setting where the probability is derived from input tuple weights via decomposable functions (e.g., product, sum, min, max). Since the join size can be exponentially larger than the input, the naive approach of materializing all join results to perform subset sampling is computationally infeasible. We propose the first efficient algorithms for subset sampling over acyclic joins: (1) a \\emph{static index} for generating multiple (independent) subset samples over joins; (2) a \\emph{one-shot} algorithm for generating a single subset sample over joins; (3) a \\emph{dynamic index} that can support tuple insertions, while maintaining a one-shot sample or generating multiple (independent) samples. Our techniques achieve near-optimal time and space complexity with respect to the input size and the expected sample size.",
    "paper_id": "2512.16321v1"
  },
  "https://arxiv.org/abs/2512.12537v1": {
    "content_path": "../data/contents/2512.12537v1.md",
    "pdf_path": "../data/contents/2512.12537v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:04.843489",
    "title": "NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data",
    "abstract": "Summary: The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.",
    "paper_id": "2512.12537v1"
  },
  "https://arxiv.org/abs/2512.16317v1": {
    "content_path": "../data/contents/2512.16317v1.md",
    "pdf_path": "../data/contents/2512.16317v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:05.565329",
    "title": "Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference",
    "abstract": "Summary: Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.\n  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.",
    "paper_id": "2512.16317v1"
  },
  "https://arxiv.org/abs/2512.12536v1": {
    "content_path": "../data/contents/2512.12536v1.md",
    "pdf_path": "../data/contents/2512.12536v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:06.211246",
    "title": "Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?",
    "abstract": "Summary: Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.\n  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.\n  Artifact: https://github.com/Erroristotle/DVDR_LLM",
    "paper_id": "2512.12536v1"
  },
  "https://arxiv.org/abs/2512.16310v1": {
    "content_path": "../data/contents/2512.16310v1.md",
    "pdf_path": "../data/contents/2512.16310v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:06.597944",
    "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
    "abstract": "Summary: Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
    "paper_id": "2512.16310v1"
  },
  "https://arxiv.org/abs/2512.12523v1": {
    "content_path": "../data/contents/2512.12523v1.md",
    "pdf_path": "../data/contents/2512.12523v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:07.079685",
    "title": "Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems",
    "abstract": "Summary: Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.",
    "paper_id": "2512.12523v1"
  },
  "https://arxiv.org/abs/2512.16307v1": {
    "content_path": "../data/contents/2512.16307v1.md",
    "pdf_path": "../data/contents/2512.16307v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:07.423383",
    "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks",
    "abstract": "Summary: In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.",
    "paper_id": "2512.16307v1"
  },
  "https://arxiv.org/abs/2512.12506v1": {
    "content_path": "../data/contents/2512.12506v1.md",
    "pdf_path": "../data/contents/2512.12506v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:07.612246",
    "title": "Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts",
    "abstract": "Summary: Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.",
    "paper_id": "2512.12506v1"
  },
  "https://arxiv.org/abs/2404.13501": {
    "content_path": "../data/contents/2404.13501.md",
    "pdf_path": "../data/contents/2404.13501.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:08.221854",
    "title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
    "abstract": "Comprehensive academic survey on LLM agent memory mechanisms, design patterns, and evaluation.",
    "paper_id": "2404.13501"
  },
  "https://arxiv.org/abs/2512.16303v1": {
    "content_path": "../data/contents/2512.16303v1.md",
    "pdf_path": "../data/contents/2512.16303v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:10.353212",
    "title": "PixelArena: A benchmark for Pixel-Precision Visual Intelligence",
    "abstract": "Summary: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.",
    "paper_id": "2512.16303v1"
  },
  "https://arxiv.org/abs/2512.12503v1": {
    "content_path": "../data/contents/2512.12503v1.md",
    "pdf_path": "../data/contents/2512.12503v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:25.826611",
    "title": "KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs",
    "abstract": "Summary: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.",
    "paper_id": "2512.12503v1"
  },
  "https://arxiv.org/html/2507.07957v1": {
    "content_path": "../data/contents/2507.07957v1.md",
    "pdf_path": "../data/contents/2507.07957v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:26.172994",
    "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents",
    "abstract": "Novel multi-agent memory architecture with six memory types achieving 85.4% SOTA on LOCOMO benchmark.",
    "paper_id": "2507.07957v1"
  },
  "https://arxiv.org/abs/2512.16301v2": {
    "content_path": "../data/contents/2512.16301v2.md",
    "pdf_path": "../data/contents/2512.16301v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:27.026687",
    "title": "Adaptation of Agentic AI",
    "abstract": "Summary: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.",
    "paper_id": "2512.16301v2"
  },
  "https://arxiv.org/abs/2512.12500v1": {
    "content_path": "../data/contents/2512.12500v1.md",
    "pdf_path": "../data/contents/2512.12500v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:37.054066",
    "title": "Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public",
    "abstract": "Summary: Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a \"double-edged sword\" in medical AI and informing future human-AI collaborative system design.",
    "paper_id": "2512.12500v1"
  },
  "https://arxiv.org/html/2506.12508v1": {
    "content_path": "../data/contents/2506.12508v1.md",
    "pdf_path": "../data/contents/2506.12508v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:37.428844",
    "title": "AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose Task Solving",
    "abstract": "Hierarchical multi-agent framework with Planning Agent and modular collaboration outperforming baselines.",
    "paper_id": "2506.12508v1"
  },
  "https://arxiv.org/abs/2512.16300v1": {
    "content_path": "../data/contents/2512.16300v1.md",
    "pdf_path": "../data/contents/2512.16300v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:38.105350",
    "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection",
    "abstract": "Summary: Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.",
    "paper_id": "2512.16300v1"
  },
  "https://arxiv.org/abs/2512.12492v2": {
    "content_path": "../data/contents/2512.12492v2.md",
    "pdf_path": "../data/contents/2512.12492v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:38.995849",
    "title": "Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings",
    "abstract": "Summary: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.",
    "paper_id": "2512.12492v2"
  },
  "https://arxiv.org/abs/2510.16558": {
    "content_path": "../data/contents/2510.16558.md",
    "pdf_path": "../data/contents/2510.16558.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:41.663351",
    "title": "Toward Understanding Security Issues in the Model Context Protocol Ecosystem",
    "abstract": "Security analysis of MCP hosts/registries/servers (trust boundaries, output verification gaps, hijacking risks) with proposed defenses.",
    "paper_id": "2510.16558"
  },
  "https://arxiv.org/abs/2512.16962v1": {
    "content_path": "../data/contents/2512.16962v1.md",
    "pdf_path": "../data/contents/2512.16962v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:43.713756",
    "title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval",
    "abstract": "Summary: Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.",
    "paper_id": "2512.16962v1"
  },
  "https://arxiv.org/abs/2512.12488v1": {
    "content_path": "../data/contents/2512.12488v1.md",
    "pdf_path": "../data/contents/2512.12488v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:44.293366",
    "title": "The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting",
    "abstract": "Summary: Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.",
    "paper_id": "2512.12488v1"
  },
  "https://arxiv.org/abs/2512.16297v1": {
    "content_path": "../data/contents/2512.16297v1.md",
    "pdf_path": "../data/contents/2512.16297v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:45.096873",
    "title": "Feature-Selective Representation Misdirection for Machine Unlearning",
    "abstract": "Summary: As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.",
    "paper_id": "2512.16297v1"
  },
  "https://arxiv.org/abs/2512.12486v1": {
    "content_path": "../data/contents/2512.12486v1.md",
    "pdf_path": "../data/contents/2512.12486v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:46.161736",
    "title": "Simultaneous AlphaZero: Extending Tree Search to Markov Games",
    "abstract": "Summary: Simultaneous AlphaZero extends the AlphaZero framework to multistep, two-player zero-sum deterministic Markov games with simultaneous actions. At each decision point, joint action selection is resolved via matrix games whose payoffs incorporate both immediate rewards and future value estimates. To handle uncertainty arising from bandit feedback during Monte Carlo Tree Search (MCTS), Simultaneous AlphaZero incorporates a regret-optimal solver for matrix games with bandit feedback. Simultaneous AlphaZero demonstrates robust strategies in a continuous-state discrete-action pursuit-evasion game and satellite custody maintenance scenarios, even when evaluated against maximally exploitative opponents.",
    "paper_id": "2512.12486v1"
  },
  "https://arxiv.org/abs/2512.16295v1": {
    "content_path": "../data/contents/2512.16295v1.md",
    "pdf_path": "../data/contents/2512.16295v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:49.591369",
    "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models",
    "abstract": "Summary: With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.",
    "paper_id": "2512.16295v1"
  },
  "https://arxiv.org/abs/2512.13728v1": {
    "content_path": "../data/contents/2512.13728v1.md",
    "pdf_path": "../data/contents/2512.13728v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:50.200729",
    "title": "CurvaDion: Curvature-Adaptive Distributed Orthonormalization",
    "abstract": "Summary: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.",
    "paper_id": "2512.13728v1"
  },
  "https://arxiv.org/abs/2512.16292v2": {
    "content_path": "../data/contents/2512.16292v2.md",
    "pdf_path": "../data/contents/2512.16292v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:51.273656",
    "title": "In-Context Probing for Membership Inference in Fine-Tuned Language Models",
    "abstract": "Summary: Membership inference attacks (MIAs) pose a critical privacy threat to fine-tuned large language models (LLMs), especially when models are adapted to domain-specific tasks using sensitive data. While prior black-box MIA techniques rely on confidence scores or token likelihoods, these signals are often entangled with a sample's intrinsic properties - such as content difficulty or rarity - leading to poor generalization and low signal-to-noise ratios. In this paper, we propose ICP-MIA, a novel MIA framework grounded in the theory of training dynamics, particularly the phenomenon of diminishing returns during optimization. We introduce the Optimization Gap as a fundamental signal of membership: at convergence, member samples exhibit minimal remaining loss-reduction potential, while non-members retain significant potential for further optimization. To estimate this gap in a black-box setting, we propose In-Context Probing (ICP), a training-free method that simulates fine-tuning-like behavior via strategically constructed input contexts. We propose two probing strategies: reference-data-based (using semantically similar public samples) and self-perturbation (via masking or generation). Experiments on three tasks and multiple LLMs show that ICP-MIA significantly outperforms prior black-box MIAs, particularly at low false positive rates. We further analyze how reference data alignment, model type, PEFT configurations, and training schedules affect attack effectiveness. Our findings establish ICP-MIA as a practical and theoretically grounded framework for auditing privacy risks in deployed LLMs.",
    "paper_id": "2512.16292v2"
  },
  "https://arxiv.org/abs/2512.12477v1": {
    "content_path": "../data/contents/2512.12477v1.md",
    "pdf_path": "../data/contents/2512.12477v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:51.641843",
    "title": "MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs",
    "abstract": "Summary: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE",
    "paper_id": "2512.12477v1"
  },
  "https://arxiv.org/abs/2512.16287v1": {
    "content_path": "../data/contents/2512.16287v1.md",
    "pdf_path": "../data/contents/2512.16287v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:52.234011",
    "title": "Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures",
    "abstract": "Summary: The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.",
    "paper_id": "2512.16287v1"
  },
  "https://arxiv.org/abs/2512.12476v1": {
    "content_path": "../data/contents/2512.12476v1.md",
    "pdf_path": "../data/contents/2512.12476v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:52.686383",
    "title": "HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments",
    "abstract": "Summary: As large language models (LLMs) continue to scale and new GPUs are released even more frequently, there is an increasing demand for LLM post-training in heterogeneous environments to fully leverage underutilized mid-range or previous-generation GPUs across regions and alleviate the shortage of homogeneous high-end GPUs within a single region. However, achieving high-performance reinforcement learning (RL) training for LLMs on such computing resources remains challenging because the workflow involves multiple models and tasks with complex computation and data dependencies. In this paper, we present HetRL, a distributed system for efficient RL training in infrastructures with heterogeneous GPUs and networks. HetRL formulates the scheduling of RL training in heterogeneous environments as a constrained joint optimization problem and introduces a novel scheduling algorithm that (1) decomposes the complex search space with a multi-level search framework; and (2) allocates the search budget via successive halving. Our extensive evaluation, consuming 20,000 GPU-hours, shows that HetRL delivers up to 9.17x the throughput of state-of-the-art systems, and 3.17x on average, under various workloads and settings.",
    "paper_id": "2512.12476v1"
  },
  "https://arxiv.org/abs/2512.16282v1": {
    "content_path": "../data/contents/2512.16282v1.md",
    "pdf_path": "../data/contents/2512.16282v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:03:58.035347",
    "title": "CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity",
    "abstract": "Summary: Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.",
    "paper_id": "2512.16282v1"
  },
  "https://arxiv.org/abs/2512.12469v1": {
    "content_path": "../data/contents/2512.12469v1.md",
    "pdf_path": "../data/contents/2512.12469v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:00.376177",
    "title": "Sparse Concept Anchoring for Interpretable and Controllable Neural Representations",
    "abstract": "Summary: We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.",
    "paper_id": "2512.12469v1"
  },
  "https://arxiv.org/abs/2512.16280v2": {
    "content_path": "../data/contents/2512.16280v2.md",
    "pdf_path": "../data/contents/2512.16280v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:05.761420",
    "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams",
    "abstract": "Summary: Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation.\n  We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.",
    "paper_id": "2512.16280v2"
  },
  "https://arxiv.org/abs/2512.12458v1": {
    "content_path": "../data/contents/2512.12458v1.md",
    "pdf_path": "../data/contents/2512.12458v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:06.087301",
    "title": "Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval",
    "abstract": "Summary: Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.",
    "paper_id": "2512.12458v1"
  },
  "https://arxiv.org/abs/2512.16279v1": {
    "content_path": "../data/contents/2512.16279v1.md",
    "pdf_path": "../data/contents/2512.16279v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:07.673377",
    "title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems",
    "abstract": "Summary: Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \\textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \\textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.",
    "paper_id": "2512.16279v1"
  },
  "https://arxiv.org/abs/2512.12448v1": {
    "content_path": "../data/contents/2512.12448v1.md",
    "pdf_path": "../data/contents/2512.12448v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:07.997823",
    "title": "Optimized Architectures for Kolmogorov-Arnold Networks",
    "abstract": "Summary: Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.",
    "paper_id": "2512.12448v1"
  },
  "https://arxiv.org/abs/2512.16272v1": {
    "content_path": "../data/contents/2512.16272v1.md",
    "pdf_path": "../data/contents/2512.16272v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:08.324507",
    "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls",
    "abstract": "Summary: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.\n  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.\n  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.",
    "paper_id": "2512.16272v1"
  },
  "https://arxiv.org/abs/2512.12444v1": {
    "content_path": "../data/contents/2512.12444v1.md",
    "pdf_path": "../data/contents/2512.12444v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:08.881784",
    "title": "Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors",
    "abstract": "Summary: As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.",
    "paper_id": "2512.12444v1"
  },
  "https://arxiv.org/abs/2512.16262v1": {
    "content_path": "../data/contents/2512.16262v1.md",
    "pdf_path": "../data/contents/2512.16262v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:09.356428",
    "title": "Learning to Wait: Synchronizing Agents with the Physical World",
    "abstract": "Summary: Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
    "paper_id": "2512.16262v1"
  },
  "https://arxiv.org/abs/2512.12443v1": {
    "content_path": "../data/contents/2512.12443v1.md",
    "pdf_path": "../data/contents/2512.12443v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:09.699358",
    "title": "AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline",
    "abstract": "Summary: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.",
    "paper_id": "2512.12443v1"
  },
  "https://arxiv.org/abs/2512.16251v2": {
    "content_path": "../data/contents/2512.16251v2.md",
    "pdf_path": "../data/contents/2512.16251v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:10.314503",
    "title": "Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model",
    "abstract": "Summary: We introduce the Consensus-Bottleneck Asset Pricing Model (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this \"bottleneck\" to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and Gibbons-Ross-Shanken (GRS)-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.",
    "paper_id": "2512.16251v2"
  },
  "https://arxiv.org/abs/2512.12436v1": {
    "content_path": "../data/contents/2512.12436v1.md",
    "pdf_path": "../data/contents/2512.12436v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:13.031149",
    "title": "Rough Sets for Explainability of Spectral Graph Clustering",
    "abstract": "Summary: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.",
    "paper_id": "2512.12436v1"
  },
  "https://arxiv.org/abs/2512.16250v1": {
    "content_path": "../data/contents/2512.16250v1.md",
    "pdf_path": "../data/contents/2512.16250v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:17.478066",
    "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
    "abstract": "Summary: Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",
    "paper_id": "2512.16250v1"
  },
  "https://arxiv.org/abs/2512.19707v1": {
    "content_path": "../data/contents/2512.19707v1.md",
    "pdf_path": "../data/contents/2512.19707v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:19.875280",
    "title": "Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance",
    "abstract": "Summary: The benefits of artificial intelligence (AI) human partnerships-evaluating how AI agents enhance expert human performance-are increasingly studied. Though rarely evaluated in healthcare, an inverse approach is possible: AI benefiting from the support of an expert human agent. Here, we investigate both human-AI clinical partnership paradigms in the magnetic resonance imaging-guided characterisation of patients with brain tumours. We reveal that human-AI partnerships improve accuracy and metacognitive ability not only for radiologists supported by AI, but also for AI agents supported by radiologists. Moreover, the greatest patient benefit was evident with an AI agent supported by a human one. Synergistic improvements in agent accuracy, metacognitive performance, and inter-rater agreement suggest that AI can create more capable, confident, and consistent clinical agents, whether human or model-based. Our work suggests that the maximal value of AI in healthcare could emerge not from replacing human intelligence, but from AI agents that routinely leverage and amplify it.",
    "paper_id": "2512.19707v1"
  },
  "https://arxiv.org/abs/2512.16248v2": {
    "content_path": "../data/contents/2512.16248v2.md",
    "pdf_path": "../data/contents/2512.16248v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:20.770525",
    "title": "Sigma-MoE-Tiny Technical Report",
    "abstract": "Summary: Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm",
    "paper_id": "2512.16248v2"
  },
  "https://arxiv.org/abs/2512.12424v1": {
    "content_path": "../data/contents/2512.12424v1.md",
    "pdf_path": "../data/contents/2512.12424v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:22.265169",
    "title": "ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics",
    "abstract": "Summary: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.",
    "paper_id": "2512.12424v1"
  },
  "https://arxiv.org/abs/2512.16245v1": {
    "content_path": "../data/contents/2512.16245v1.md",
    "pdf_path": "../data/contents/2512.16245v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:26.168094",
    "title": "AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints",
    "abstract": "Summary: Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.\n  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:\n  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,\n  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.\n  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.",
    "paper_id": "2512.16245v1"
  },
  "https://arxiv.org/abs/2512.12420v1": {
    "content_path": "../data/contents/2512.12420v1.md",
    "pdf_path": "../data/contents/2512.12420v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:26.403177",
    "title": "Deep Hedging with Reinforcement Learning: A Practical Framework for Option Risk Management",
    "abstract": "Summary: We present a reinforcement-learning (RL) framework for dynamic hedging of equity index option exposures under realistic transaction costs and position limits. We hedge a normalized option-implied equity exposure (one unit of underlying delta, offset via SPY) by trading the underlying index ETF, using the option surface and macro variables only as state information and not as a direct pricing engine. Building on the \"deep hedging\" paradigm of Buehler et al. (2019), we design a leak-free environment, a cost-aware reward function, and a lightweight stochastic actor-critic agent trained on daily end-of-day panel data constructed from SPX/SPY implied volatility term structure, skew, realized volatility, and macro rate context. On a fixed train/validation/test split, the learned policy improves risk-adjusted performance versus no-hedge, momentum, and volatility-targeting baselines (higher point-estimate Sharpe); only the GAE policy's test-sample Sharpe is statistically distinguishable from zero, although confidence intervals overlap with a long-SPY benchmark so we stop short of claiming formal dominance. Turnover remains controlled and the policy is robust to doubled transaction costs. The modular codebase, comprising a data pipeline, simulator, and training scripts, is engineered for extensibility to multi-asset overlays, alternative objectives (e.g., drawdown or CVaR), and intraday data. From a portfolio management perspective, the learned overlay is designed to sit on top of an existing SPX or SPY allocation, improving the portfolio's mean-variance trade-off with controlled turnover and drawdowns. We discuss practical implications for portfolio overlays and outline avenues for future work.",
    "paper_id": "2512.12420v1"
  },
  "https://arxiv.org/abs/2512.16244v2": {
    "content_path": "../data/contents/2512.16244v2.md",
    "pdf_path": "../data/contents/2512.16244v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:27.098992",
    "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models",
    "abstract": "Summary: Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.",
    "paper_id": "2512.16244v2"
  },
  "https://arxiv.org/abs/2512.12413v1": {
    "content_path": "../data/contents/2512.12413v1.md",
    "pdf_path": "../data/contents/2512.12413v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:27.751589",
    "title": "Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale",
    "abstract": "Summary: Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.",
    "paper_id": "2512.12413v1"
  },
  "https://arxiv.org/abs/2512.16239v2": {
    "content_path": "../data/contents/2512.16239v2.md",
    "pdf_path": "../data/contents/2512.16239v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:28.728626",
    "title": "Bayesian Empirical Bayes: Simultaneous Inference from Probabilistic Symmetries",
    "abstract": "Summary: Empirical Bayes (EB) improves the accuracy of simultaneous inference \"by learning from the experience of others\" (Efron, 2012). Classical EB theory focuses on latent variables that are iid draws from a fitted prior (Efron, 2019). Modern applications, however, feature complex structure, like arrays, spatial processes, or covariates. How can we apply EB ideas to these settings? We propose a generalized approach to empirical Bayes based on the notion of probabilistic symmetry. Our method pairs a simultaneous inference problem-with an unknown prior-to a symmetry assumption on the joint distribution of the latent variables. Each symmetry implies an ergodic decomposition, which we use to derive a corresponding empirical Bayes method. We call this methodBayesian empirical Bayes (BEB). We show how BEB recovers the classical methods of empirical Bayes, which implicitly assume exchangeability. We then use it to extend EB to other probabilistic symmetries: (i) EB matrix recovery for arrays and graphs; (ii) covariate-assisted EB for conditional data; (iii) EB spatial regression under shift invariance. We develop scalable algorithms based on variational inference and neural networks. In simulations, BEB outperforms existing approaches to denoising arrays and spatial data. On real data, we demonstrate BEB by denoising a cancer gene-expression matrix and analyzing spatial air-quality data from New York City.",
    "paper_id": "2512.16239v2"
  },
  "https://arxiv.org/abs/2512.13725v2": {
    "content_path": "../data/contents/2512.13725v2.md",
    "pdf_path": "../data/contents/2512.13725v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:29.066523",
    "title": "Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy",
    "abstract": "Summary: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.",
    "paper_id": "2512.13725v2"
  },
  "https://arxiv.org/abs/2512.16237v1": {
    "content_path": "../data/contents/2512.16237v1.md",
    "pdf_path": "../data/contents/2512.16237v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:29.714648",
    "title": "Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis",
    "abstract": "Summary: Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.",
    "paper_id": "2512.16237v1"
  },
  "https://arxiv.org/abs/2512.12411v1": {
    "content_path": "../data/contents/2512.12411v1.md",
    "pdf_path": "../data/contents/2512.12411v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:30.439119",
    "title": "Feeling the Strength but Not the Source: Partial Introspection in LLMs",
    "abstract": "Summary: Recent work from Anthropic claims that frontier models can sometimes detect and name injected \"concepts\" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn \"emergent introspection\" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.",
    "paper_id": "2512.12411v1"
  },
  "https://arxiv.org/abs/2512.16236v1": {
    "content_path": "../data/contents/2512.16236v1.md",
    "pdf_path": "../data/contents/2512.16236v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:30.694238",
    "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models",
    "abstract": "Summary: Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality.\n  We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.",
    "paper_id": "2512.16236v1"
  },
  "https://arxiv.org/abs/2512.12405v1": {
    "content_path": "../data/contents/2512.12405v1.md",
    "pdf_path": "../data/contents/2512.12405v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:31.136373",
    "title": "Can Graphs Improve Tabular Foundation Models?",
    "abstract": "Summary: Tabular data are central to many real-world systems. While recent tabular transformers and in-context learners such as SAINT, TP-BERTa, TabPFN, TabICL, and MITRA incorporate limited inter-row reasoning, most approaches still lack an explicit mechanism to model relationships among instances, even though similar samples often share related outcomes. We investigate whether introducing \\emph{simple graph priors} can enhance \\emph{pretrained tabular transformers}. Concretely, we introduce {BOLERO}, a lightweight, static bipartite graph head that augments {RoBERTa-Tab} (a RoBERTa-style tabular backbone pretrained with masked-token prediction.) Each instance connects to feature/value anchors; a small GNN refines row representations, while the backbone remains frozen. We evaluate on 80 classification and 64 regression datasets from the TP-BERTa benchmark suites, comparing against strong baselines including XGBoost, CatBoost, TabPFN-v2, MITRA, TabICL, TP-BERTa, and RoBERTa-Tab. To ensure statistically sound conclusions, we follow best practices for multi-dataset evaluation: pairwise Wilcoxon signed-rank tests on per-dataset score differences and effect sizes (median improvement with confidence intervals), rather than mean-rank post-hoc tests that depend on the competitor pool. BOLERO achieves the highest number of statistically significant wins across both classification and regression, demonstrating that lightweight graph priors meaningfully improve pretrained tabular transformers.",
    "paper_id": "2512.12405v1"
  },
  "https://arxiv.org/abs/2512.02228": {
    "content_path": "../data/contents/2512.02228.md",
    "pdf_path": "../data/contents/2512.02228.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:31.481715",
    "title": "STRIDE: A Systematic Framework for Selecting AI Modalities - Agentic AI, AI Assistants, or LLM Calls",
    "abstract": "Research paper on when to use agentic AI vs assistants vs LLM calls - 92% accuracy in modality selection, 45% reduction in unnecessary agent deployments",
    "paper_id": "2512.02228"
  },
  "https://arxiv.org/abs/2512.16229v2": {
    "content_path": "../data/contents/2512.16229v2.md",
    "pdf_path": "../data/contents/2512.16229v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:32.005376",
    "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding",
    "abstract": "Summary: Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.",
    "paper_id": "2512.16229v2"
  },
  "https://arxiv.org/abs/2512.15776v1": {
    "content_path": "../data/contents/2512.15776v1.md",
    "pdf_path": "../data/contents/2512.15776v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:33.285507",
    "title": "Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying",
    "abstract": "Summary: Large Language Models (LLMs) act as powerful reasoning engines but struggle with \"symbol grounding\" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or \"Curse of Knowledge\"), where a knowledgeable \"Leader\" agent fails to guide a sensor-limited \"Follower\" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant \"Success Gap\": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a \"Pull-based\" protocol (active querying) is significantly more robust than standard \"Push-based\" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.",
    "paper_id": "2512.15776v1"
  },
  "https://arxiv.org/html/2512.08296v1": {
    "content_path": "../data/contents/2512.08296v1.md",
    "pdf_path": "../data/contents/2512.08296v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:33.864900",
    "title": "Towards a Science of Scaling Agent Systems",
    "abstract": "First universal equation for agentic systems - scaling principles across 180 configurations, 87% correct architecture selection",
    "paper_id": "2512.08296v1"
  },
  "https://arxiv.org/abs/2512.16227v1": {
    "content_path": "../data/contents/2512.16227v1.md",
    "pdf_path": "../data/contents/2512.16227v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:35.050003",
    "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
    "abstract": "Summary: Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
    "paper_id": "2512.16227v1"
  },
  "https://arxiv.org/abs/2512.12384v1": {
    "content_path": "../data/contents/2512.12384v1.md",
    "pdf_path": "../data/contents/2512.12384v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:36.040074",
    "title": "The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining",
    "abstract": "Summary: Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.",
    "paper_id": "2512.12384v1"
  },
  "https://arxiv.org/abs/2512.20034v1": {
    "content_path": "../data/contents/2512.20034v1.md",
    "pdf_path": "../data/contents/2512.20034v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:36.226565",
    "title": "VSA:Visual-Structural Alignment for UI-to-Code",
    "abstract": "Summary: The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \\textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.",
    "paper_id": "2512.20034v1"
  },
  "https://arxiv.org/abs/2512.17970v1": {
    "content_path": "../data/contents/2512.17970v1.md",
    "pdf_path": "../data/contents/2512.17970v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:37.139256",
    "title": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs",
    "abstract": "Summary: Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency-memory-accuracy trade-offs under a unified implementation. On Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization.",
    "paper_id": "2512.17970v1"
  },
  "https://arxiv.org/abs/2512.13120v1": {
    "content_path": "../data/contents/2512.13120v1.md",
    "pdf_path": "../data/contents/2512.13120v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:37.491630",
    "title": "Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation",
    "abstract": "Summary: Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.",
    "paper_id": "2512.13120v1"
  },
  "https://arxiv.org/abs/2512.20029v1": {
    "content_path": "../data/contents/2512.20029v1.md",
    "pdf_path": "../data/contents/2512.20029v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:38.557028",
    "title": "$\\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning",
    "abstract": "Summary: Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.",
    "paper_id": "2512.20029v1"
  },
  "https://arxiv.org/abs/2512.17259v1": {
    "content_path": "../data/contents/2512.17259v1.md",
    "pdf_path": "../data/contents/2512.17259v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:39.045492",
    "title": "Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems",
    "abstract": "Summary: As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.",
    "paper_id": "2512.17259v1"
  },
  "https://arxiv.org/abs/2512.13111v1": {
    "content_path": "../data/contents/2512.13111v1.md",
    "pdf_path": "../data/contents/2512.13111v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:39.445424",
    "title": "From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network",
    "abstract": "Summary: In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.",
    "paper_id": "2512.13111v1"
  },
  "https://arxiv.org/abs/2512.20028v1": {
    "content_path": "../data/contents/2512.20028v1.md",
    "pdf_path": "../data/contents/2512.20028v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:40.209208",
    "title": "DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics",
    "abstract": "Summary: Accurate and interpretable forecasting of multivariate time series is crucial for understanding the complex dynamics of cryptocurrency markets in digital asset systems. Advanced deep learning methodologies, particularly Transformer-based and MLP-based architectures, have achieved competitive predictive performance in cryptocurrency forecasting tasks. However, cryptocurrency data is inherently composed of long-term socio-economic trends and local high-frequency speculative oscillations. Existing deep learning-based 'black-box' models fail to effectively decouple these composite dynamics or provide the interpretability needed for trustworthy financial decision-making. To overcome these limitations, we propose DecoKAN, an interpretable forecasting framework that integrates multi-level Discrete Wavelet Transform (DWT) for decoupling and hierarchical signal decomposition with Kolmogorov-Arnold Network (KAN) mixers for transparent and interpretable nonlinear modeling. The DWT component decomposes complex cryptocurrency time series into distinct frequency components, enabling frequency-specific analysis, while KAN mixers provide intrinsically interpretable spline-based mappings within each decomposed subseries. Furthermore, interpretability is enhanced through a symbolic analysis pipeline involving sparsification, pruning, and symbolization, which produces concise analytical expressions offering symbolic representations of the learned patterns. Extensive experiments demonstrate that DecoKAN achieves the lowest average Mean Squared Error on all tested real-world cryptocurrency datasets (BTC, ETH, XMR), consistently outperforming a comprehensive suite of competitive state-of-the-art baselines. These results validate DecoKAN's potential to bridge the gap between predictive accuracy and model transparency, advancing trustworthy decision support within complex cryptocurrency markets.",
    "paper_id": "2512.20028v1"
  },
  "https://arxiv.org/abs/2512.17251v1": {
    "content_path": "../data/contents/2512.17251v1.md",
    "pdf_path": "../data/contents/2512.17251v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:40.435904",
    "title": "AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs",
    "abstract": "Summary: Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.",
    "paper_id": "2512.17251v1"
  },
  "https://arxiv.org/abs/2512.13109v1": {
    "content_path": "../data/contents/2512.13109v1.md",
    "pdf_path": "../data/contents/2512.13109v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:40.932772",
    "title": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing",
    "abstract": "Summary: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks.",
    "paper_id": "2512.13109v1"
  },
  "https://arxiv.org/abs/2512.20014v1": {
    "content_path": "../data/contents/2512.20014v1.md",
    "pdf_path": "../data/contents/2512.20014v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:46.042587",
    "title": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting",
    "abstract": "Summary: While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as \"bring my cup\", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.",
    "paper_id": "2512.20014v1"
  },
  "https://arxiv.org/abs/2512.20664v1": {
    "content_path": "../data/contents/2512.20664v1.md",
    "pdf_path": "../data/contents/2512.20664v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:46.725160",
    "title": "Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction",
    "abstract": "Summary: Large Language Models (LLMs) frequently produce hallucinated statements that are assigned high likelihood by the model itself, exposing a fundamental limitation of probability-based verification. This suggests that hallucination is often not a low-confidence phenomenon, but a failure of structural consistency. In this work, we reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood. Rather than optimizing for statistical plausibility, we model verification as a feasibility check based on structural violation cost -- the computational cost required to embed a candidate reasoning step into the contextual graph structure. We define a total cost function composed of three proxies: (i) graph connectivity (structural), (ii) feature space consistency (geometric), and (iii) logical entailment (symbolic). Crucially, verification is performed via a lightweight System-2 gate, Eidoku, which rejects candidates exceeding a context-calibrated cost threshold. The threshold is not learned but is derived from the intrinsic statistics of the context, avoiding ad hoc heuristics. We demonstrate that this approach successfully rejects ``smooth falsehoods'' -- statements that are highly probable yet structurally disconnected -- that probability-based verifiers are principally incapable of detecting. Our experiments on a controlled diagnostic dataset show that explicitly enforcing structural constraints allows for the deterministic rejection of this specific class of hallucinations, serving as a neuro-symbolic sanity check for generative reasoning.",
    "paper_id": "2512.20664v1"
  },
  "https://arxiv.org/abs/2512.13106v1": {
    "content_path": "../data/contents/2512.13106v1.md",
    "pdf_path": "../data/contents/2512.13106v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:49.606234",
    "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
    "abstract": "Summary: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.",
    "paper_id": "2512.13106v1"
  },
  "https://arxiv.org/abs/2512.20013v1": {
    "content_path": "../data/contents/2512.20013v1.md",
    "pdf_path": "../data/contents/2512.20013v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:53.916542",
    "title": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
    "abstract": "Summary: Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.",
    "paper_id": "2512.20013v1"
  },
  "https://arxiv.org/abs/2512.20236": {
    "content_path": "../data/contents/2512.20236.md",
    "pdf_path": "../data/contents/2512.20236.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:57.413156",
    "title": "IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing",
    "abstract": "New large-scale multilingual document layout dataset (Indic languages + English) for training/evaluating layout parsers used in document AI.",
    "paper_id": "2512.20236"
  },
  "https://arxiv.org/abs/2512.17247v1": {
    "content_path": "../data/contents/2512.17247v1.md",
    "pdf_path": "../data/contents/2512.17247v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:58.002615",
    "title": "Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition",
    "abstract": "Summary: Automatic Speech Recognition (ASR) systems suffer significant performance degradation in noisy environments, a challenge that is especially severe for low-resource languages such as Persian. Even state-of-the-art models such as Whisper struggle to maintain accuracy under varying signal-to-noise ratios (SNRs). This study presents a robust noise-sensitive ASR error correction framework that combines multiple hypotheses and noise-aware modeling. Using noisy Persian speech, we generate 5-best hypotheses from a modified Whisper-large decoder. Error Level Noise (ELN) is introduced as a representation that captures semantic- and token-level disagreement across hypotheses, quantifying the linguistic distortions caused by noise. ELN thus provides a direct measure of noise-induced uncertainty, enabling the LLM to reason about the reliability of each hypothesis during correction. Three models are evaluated: (1) a base LLaMA-2-7B model without fine-tuning, (2) a fine-tuned variant trained on text-only hypotheses, and (3) a noise-conditioned model integrating ELN embeddings at both sentence and word levels. Experimental results demonstrate that the ELN-conditioned model achieves substantial reductions in Word Error Rate (WER). Specifically, on the challenging Mixed Noise test set, the proposed Fine-tuned + ELN (Ours) model reduces the WER from a baseline of 31.10\\% (Raw Whisper) to 24.84\\%, significantly surpassing the Fine-tuned (No ELN) text-only baseline of 30.79\\%, whereas the original LLaMA-2-7B model increased the WER to 64.58\\%, demonstrating that it is unable to correct Persian errors on its own. This confirms the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust Persian ASR in noisy real-world scenarios.",
    "paper_id": "2512.17247v1"
  },
  "https://arxiv.org/abs/2512.13102v2": {
    "content_path": "../data/contents/2512.13102v2.md",
    "pdf_path": "../data/contents/2512.13102v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:04:59.184097",
    "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions",
    "abstract": "Summary: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.",
    "paper_id": "2512.13102v2"
  },
  "https://arxiv.org/abs/2512.20002v1": {
    "content_path": "../data/contents/2512.20002v1.md",
    "pdf_path": "../data/contents/2512.20002v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:00.446843",
    "title": "LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models",
    "abstract": "Summary: Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length temporal windows, which include substantial high-frequency noise and obscure long-term trends. Moreover, auxiliary variables containing rich domain-specific information are often underutilized, especially in few-shot settings. To address these challenges, we propose LoFT-LLM, a frequency-aware forecasting pipeline that integrates low-frequency learning with semantic calibration via a large language model (LLM). Firstly, a Patch Low-Frequency forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. Secondly, a residual learner then models high-frequency variations. Finally, a fine-tuned LLM refines the predictions by incorporating auxiliary context and domain knowledge through structured natural language prompts. Extensive experiments on financial and energy datasets demonstrate that LoFT-LLM significantly outperforms strong baselines under both full-data and few-shot regimes, delivering superior accuracy, robustness, and interpretability.",
    "paper_id": "2512.20002v1"
  },
  "https://arxiv.org/abs/2512.20626": {
    "content_path": "../data/contents/2512.20626.md",
    "pdf_path": "../data/contents/2512.20626.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:00.964941",
    "title": "MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation",
    "abstract": "Multimodal KG-based RAG for cross-modal reasoning over long multimodal corpora (text + visual cues + structure).",
    "paper_id": "2512.20626"
  },
  "https://arxiv.org/abs/2512.17238v1": {
    "content_path": "../data/contents/2512.17238v1.md",
    "pdf_path": "../data/contents/2512.17238v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:01.979611",
    "title": "Fairly Dividing Non-identical Random Items: Just Sample or Match",
    "abstract": "Summary: We study the question of existence and fast computation of fair and efficient allocations of indivisible resources among agents with additive valuations. As such allocations may not exist for arbitrary instances, we ask if they exist for \\textit{typical} or \\textit{random} instances, meaning when the utility values of agents for the resources are drawn from certain distributions. If such allocations exist with high probability for typical instances, and furthermore if they can be computed efficiently, this would imply that we could quickly resolve a real world resource allocation scenario in a fair and efficient manner with high probability. This implication has made this setting popular and well studied in fair resource allocation.\n  In this paper, we extend the previously studied formal models of this problem to non-identical items. We assume that every item is associated with a distribution $\\mathcal{U}_j$, and every agent's utility value for the item is drawn independently from $\\mathcal{U}_j$. We show that envy-free fair and maximum social welfare efficient allocations exist with high probability in the asymptotic setting, meaning when the number of agents $n$ and items $m$ are large. Further we show that when $m=O(n\\log n),$ then by only sampling $O(\\log m)$ or $O((\\log m)^2)$ utility values per item instead of all the $n,$ we can compute these allocations in $\\tilde{O}(m)$ time. Finally, we simulate our algorithms on randomly generated instances and show that even for small instances, we suffer small multiplicative losses in the fairness and efficiency guarantees even for small sized instances, and converge to fully optimal guarantees quickly.",
    "paper_id": "2512.17238v1"
  },
  "https://arxiv.org/abs/2512.13101v1": {
    "content_path": "../data/contents/2512.13101v1.md",
    "pdf_path": "../data/contents/2512.13101v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:05.947541",
    "title": "Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation",
    "abstract": "Summary: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.",
    "paper_id": "2512.13101v1"
  },
  "https://arxiv.org/abs/2512.19995v1": {
    "content_path": "../data/contents/2512.19995v1.md",
    "pdf_path": "../data/contents/2512.19995v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:09.146710",
    "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
    "abstract": "Summary: Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.",
    "paper_id": "2512.19995v1"
  },
  "https://arxiv.org/abs/2510.12323": {
    "content_path": "../data/contents/2510.12323.md",
    "pdf_path": "../data/contents/2510.12323.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:11.748649",
    "title": "RAG-Anything: All-in-One RAG Framework",
    "abstract": "Unified multimodal RAG framework across text/images/tables/equations with dual-graph construction and hybrid retrieval.",
    "paper_id": "2510.12323"
  },
  "https://arxiv.org/abs/2512.17220v1": {
    "content_path": "../data/contents/2512.17220v1.md",
    "pdf_path": "../data/contents/2512.17220v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:13.076044",
    "title": "Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding",
    "abstract": "Summary: Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.",
    "paper_id": "2512.17220v1"
  },
  "https://arxiv.org/abs/2512.13095v1": {
    "content_path": "../data/contents/2512.13095v1.md",
    "pdf_path": "../data/contents/2512.13095v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:14.178880",
    "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning",
    "abstract": "Summary: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.",
    "paper_id": "2512.13095v1"
  },
  "https://arxiv.org/abs/2512.19983v1": {
    "content_path": "../data/contents/2512.19983v1.md",
    "pdf_path": "../data/contents/2512.19983v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:14.475781",
    "title": "IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation",
    "abstract": "Summary: Multimodal recommender systems (MRSs) are critical for various online platforms, offering users more accurate personalized recommendations by incorporating multimodal information of items. Structure-based MRSs have achieved state-of-the-art performance by constructing semantic item graphs, which explicitly model relationships between items based on modality feature similarity. However, such semantic item graphs are often noisy due to 1) inherent noise in multimodal information and 2) misalignment between item semantics and user-item co-occurrence relationships, which introduces false links and leads to suboptimal recommendations. To address this challenge, we propose Item Graph Diffusion for Multimodal Recommendation (IGDMRec), a novel method that leverages a diffusion model with classifier-free guidance to denoise the semantic item graph by integrating user behavioral information. Specifically, IGDMRec introduces a Behavior-conditioned Graph Diffusion (BGD) module, incorporating interaction data as conditioning information to guide the denoising of the semantic item graph. Additionally, a Conditional Denoising Network (CD-Net) is designed to implement the denoising process with manageable complexity. Finally, we propose a contrastive representation augmentation scheme that leverages both the denoised item graph and the original item graph to enhance item representations. \\LL{Extensive experiments on four real-world datasets demonstrate the superiority of IGDMRec over competitive baselines, with robustness analysis validating its denoising capability and ablation studies verifying the effectiveness of its key components.",
    "paper_id": "2512.19983v1"
  },
  "https://arxiv.org/abs/2509.19760": {
    "content_path": "../data/contents/2509.19760.md",
    "pdf_path": "../data/contents/2509.19760.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:17.610679",
    "title": "Logics-Parsing Technical Report",
    "abstract": "End-to-end LVLM document parsing augmented with reinforcement learning for layout/reading order, aimed at complex PDFs.",
    "paper_id": "2509.19760"
  },
  "https://arxiv.org/abs/2512.17213v1": {
    "content_path": "../data/contents/2512.17213v1.md",
    "pdf_path": "../data/contents/2512.17213v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:18.737419",
    "title": "CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency",
    "abstract": "Summary: Medical Vision-Language Models (VLMs) are prone to hallucinations, compromising clinical reliability. While reinforcement learning methods like Group Relative Policy Optimization (GRPO) offer a low-cost alignment solution, their reliance on sparse, outcome-based rewards inadvertently encourages models to \"overthink\" -- generating verbose, convoluted, and unverifiable Chain-of-Thought reasoning to justify answers. This focus on outcomes obscures factual errors and poses significant safety risks. To address this, we propose CheXPO-v2, a novel alignment framework that shifts from outcome to process supervision. Our core innovation is a Knowledge Graph Consistency Reward mechanism driven by Entity-Relation Matching. By explicitly parsing reasoning steps into structured \"Disease, Relation, Anatomy\" triplets, we provide fine-grained supervision that penalizes incoherent logic and hallucinations at the atomic level. Integrating this with a hard-example mining strategy, our approach significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA. Crucially, CheXPO-v2 achieves new state-of-the-art accuracy using only 5k samples, demonstrating exceptional data efficiency while producing clinically sound and verifiable reasoning. The project source code is publicly available at: https://github.com/ecoxial2007/CheX-Phi4MM.",
    "paper_id": "2512.17213v1"
  },
  "https://arxiv.org/abs/2512.13094v1": {
    "content_path": "../data/contents/2512.13094v1.md",
    "pdf_path": "../data/contents/2512.13094v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:20.288402",
    "title": "Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation",
    "abstract": "Summary: Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.",
    "paper_id": "2512.13094v1"
  },
  "https://arxiv.org/abs/2512.19980v1": {
    "content_path": "../data/contents/2512.19980v1.md",
    "pdf_path": "../data/contents/2512.19980v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:20.603148",
    "title": "Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?",
    "abstract": "Summary: Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.",
    "paper_id": "2512.19980v1"
  },
  "https://arxiv.org/abs/2506.02454": {
    "content_path": "../data/contents/2506.02454.md",
    "pdf_path": "../data/contents/2506.02454.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:22.953445",
    "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework",
    "abstract": "Agentic framework for generating report text interleaved with charts; includes a benchmark for multimodal report generation.",
    "paper_id": "2506.02454"
  },
  "https://arxiv.org/abs/2512.17196v1": {
    "content_path": "../data/contents/2512.17196v1.md",
    "pdf_path": "../data/contents/2512.17196v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:27.845521",
    "title": "UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark",
    "abstract": "Summary: Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.",
    "paper_id": "2512.17196v1"
  },
  "https://arxiv.org/abs/2512.13089v2": {
    "content_path": "../data/contents/2512.13089v2.md",
    "pdf_path": "../data/contents/2512.13089v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:29.151785",
    "title": "UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era",
    "abstract": "Summary: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.",
    "paper_id": "2512.13089v2"
  },
  "https://arxiv.org/abs/2512.19945v1": {
    "content_path": "../data/contents/2512.19945v1.md",
    "pdf_path": "../data/contents/2512.19945v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:31.306673",
    "title": "Energy-Efficient Multi-LLM Reasoning for Binary-Free Zero-Day Detection in IoT Firmware",
    "abstract": "Summary: Securing Internet of Things (IoT) firmware remains difficult due to proprietary binaries, stripped symbols, heterogeneous architectures, and limited access to executable code. Existing analysis methods, such as static analysis, symbolic execution, and fuzzing, depend on binary visibility and functional emulation, making them unreliable when firmware is encrypted or inaccessible. To address this limitation, we propose a binary-free, architecture-agnostic solution that estimates the likelihood of conceptual zero-day vulnerabilities using only high-level descriptors. The approach integrates a tri-LLM reasoning architecture combining a LLaMA-based configuration interpreter, a DeepSeek-based structural abstraction analyzer, and a GPT-4o semantic fusion model. The solution also incorporates LLM computational signatures, including latency patterns, uncertainty markers, and reasoning depth indicators, as well as an energy-aware symbolic load model, to enhance interpretability and operational feasibility. In addition, we formally derive the mathematical foundations of the reasoning pipeline, establishing monotonicity, divergence, and energy-risk coupling properties that theoretically justify the model's behavior. Simulation-based evaluation reveals that high exposure conditions increase the predicted zero-day likelihood by 20 to 35 percent across models, with GPT-4o demonstrating the strongest cross-layer correlations and the highest sensitivity. Energy and divergence metrics significantly predict elevated risk (p < 0.01), reinforcing the effectiveness of the proposed reasoning framework.",
    "paper_id": "2512.19945v1"
  },
  "https://arxiv.org/abs/2502.18443": {
    "content_path": "../data/contents/2502.18443.md",
    "pdf_path": "../data/contents/2502.18443.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:37.815347",
    "title": "olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models",
    "abstract": "Open-source PDF processing approach using a fine-tuned VLM to linearize PDFs into clean text while preserving structure (tables/equations/etc.).",
    "paper_id": "2502.18443"
  },
  "https://arxiv.org/abs/2512.17194v1": {
    "content_path": "../data/contents/2512.17194v1.md",
    "pdf_path": "../data/contents/2512.17194v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:38.166905",
    "title": "MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation",
    "abstract": "Summary: Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.",
    "paper_id": "2512.17194v1"
  },
  "https://arxiv.org/abs/2512.13077v1": {
    "content_path": "../data/contents/2512.13077v1.md",
    "pdf_path": "../data/contents/2512.13077v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:39.185422",
    "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
    "abstract": "Summary: A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.",
    "paper_id": "2512.13077v1"
  },
  "https://arxiv.org/abs/2512.19935v1": {
    "content_path": "../data/contents/2512.19935v1.md",
    "pdf_path": "../data/contents/2512.19935v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:39.418569",
    "title": "Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress",
    "abstract": "Summary: Machine learning models used in financial decision systems operate in nonstationary economic environments, yet adversarial robustness is typically evaluated under static assumptions. This work introduces Conditional Adversarial Fragility, a regime dependent phenomenon in which adversarial vulnerability is systematically amplified during periods of macroeconomic stress. We propose a regime aware evaluation framework for time indexed tabular financial classification tasks that conditions robustness assessment on external indicators of economic stress. Using volatility based regime segmentation as a proxy for macroeconomic conditions, we evaluate model behavior across calm and stress periods while holding model architecture, attack methodology, and evaluation protocols constant. Baseline predictive performance remains comparable across regimes, indicating that economic stress alone does not induce inherent performance degradation. Under adversarial perturbations, however, models operating during stress regimes exhibit substantially greater degradation across predictive accuracy, operational decision thresholds, and risk sensitive outcomes. We further demonstrate that this amplification propagates to increased false negative rates, elevating the risk of missed high risk cases during adverse conditions. To complement numerical robustness metrics, we introduce an interpretive governance layer based on semantic auditing of model explanations using large language models. Together, these results demonstrate that adversarial robustness in financial machine learning is a regime dependent property and motivate stress aware approaches to model risk assessment in high stakes financial deployments.",
    "paper_id": "2512.19935v1"
  },
  "https://arxiv.org/abs/2501.17887": {
    "content_path": "../data/contents/2501.17887.md",
    "pdf_path": "../data/contents/2501.17887.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:39.645358",
    "title": "Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion",
    "abstract": "Doc conversion toolkit paper (PDF/DOC/PPT/XLS → structured representation), relevant for building multimodal RAG/document pipelines.",
    "paper_id": "2501.17887"
  },
  "https://arxiv.org/abs/2512.20662v1": {
    "content_path": "../data/contents/2512.20662v1.md",
    "pdf_path": "../data/contents/2512.20662v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:40.031224",
    "title": "Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models",
    "abstract": "Summary: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.",
    "paper_id": "2512.20662v1"
  },
  "https://arxiv.org/abs/2512.13074v1": {
    "content_path": "../data/contents/2512.13074v1.md",
    "pdf_path": "../data/contents/2512.13074v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:40.894588",
    "title": "A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval",
    "abstract": "Summary: Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.\n  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.",
    "paper_id": "2512.13074v1"
  },
  "https://arxiv.org/abs/2512.19926v1": {
    "content_path": "../data/contents/2512.19926v1.md",
    "pdf_path": "../data/contents/2512.19926v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:41.321374",
    "title": "Developers' Experience with Generative AI -- First Insights from an Empirical Mixed-Methods Field Study",
    "abstract": "Summary: With the rise of AI-powered coding assistants, firms and programmers are exploring how to optimize their interaction with them. Research has so far mainly focused on evaluating output quality and productivity gains, leaving aside the developers' experience during the interaction. In this study, we take a multimodal, developer-centered approach to gain insights into how professional developers experience the interaction with Generative AI (GenAI) in their natural work environment in a firm. The aim of this paper is (1) to demonstrate a feasible mixed-method study design with controlled and uncontrolled study periods within a firm setting, (2) to give first insights from complementary behavioral and subjective experience data on developers' interaction with GitHub Copilot and (3) to compare the impact of interaction types (no Copilot use, in-code suggestions, chat prompts or both in-code suggestions and chat prompts) on efficiency, accuracy and perceived workload whilst working on different task categories. Results of the controlled sessions in this study indicate that moderate use of either in-code suggestions or chat prompts improves efficiency (task duration) and reduces perceived workload compared to not using Copilot, while excessive or combined use lessens these benefits. Accuracy (task completion) profits from chat interaction. In general, subjective perception of workload aligns with objective behavioral data in this study. During the uncontrolled period of the study, both higher cognitive load and productivity were perceived when interacting with AI during everyday working tasks. This study motivates the use of comparable study designs, in e.g. workshop or hackathon settings, to evaluate GenAI tools holistically and realistically with a focus on the developers' experience.",
    "paper_id": "2512.19926v1"
  },
  "https://arxiv.org/abs/2512.17185v1": {
    "content_path": "../data/contents/2512.17185v1.md",
    "pdf_path": "../data/contents/2512.17185v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:44.234776",
    "title": "Systemic Risk Radar: A Multi-Layer Graph Framework for Early Market Crash Warning",
    "abstract": "Summary: Financial crises emerge when structural vulnerabilities accumulate across sectors, markets, and investor behavior. Predicting these systemic transitions is challenging because they arise from evolving interactions between market participants, not isolated price movements alone. We present Systemic Risk Radar (SRR), a framework that models financial markets as multi-layer graphs to detect early signs of systemic fragility and crash-regime transitions.\n  We evaluate SRR across three major crises: the Dot-com crash, the Global Financial Crisis, and the COVID-19 shock. Our experiments compare snapshot GNNs, a simplified temporal GNN prototype, and standard baselines (logistic regression and Random Forest). Results show that structural network information provides useful early-warning signals compared to feature-based models alone.\n  This correlation-based instantiation of SRR demonstrates that graph-derived features capture meaningful changes in market structure during stress events. The findings motivate extending SRR with additional graph layers (sector/factor exposure, sentiment) and more expressive temporal architectures (LSTM/GRU or Transformer encoders) to better handle diverse crisis types.",
    "paper_id": "2512.17185v1"
  },
  "https://arxiv.org/abs/2512.13070v1": {
    "content_path": "../data/contents/2512.13070v1.md",
    "pdf_path": "../data/contents/2512.13070v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:44.942805",
    "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization",
    "abstract": "Summary: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.",
    "paper_id": "2512.13070v1"
  },
  "https://arxiv.org/abs/2512.19920v1": {
    "content_path": "../data/contents/2512.19920v1.md",
    "pdf_path": "../data/contents/2512.19920v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:46.379988",
    "title": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
    "abstract": "Summary: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.",
    "paper_id": "2512.19920v1"
  },
  "https://arxiv.org/abs/2512.17180v2": {
    "content_path": "../data/contents/2512.17180v2.md",
    "pdf_path": "../data/contents/2512.17180v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:46.863173",
    "title": "Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors",
    "abstract": "Summary: Interactive reinforcement learning (IRL) has shown promise in enabling autonomous agents and robots to learn complex behaviours from human teachers, yet the dynamics of teacher selection remain poorly understood. This paper reveals an unexpected phenomenon in IRL: when given a choice between teachers with different reward structures, learning agents overwhelmingly prefer conservative, low-reward teachers (93.16% selection rate) over those offering 20x higher rewards. Through 1,250 experimental runs in navigation tasks with multiple expert teachers, we discovered: (1) Conservative bias dominates teacher selection: agents systematically choose the lowest-reward teacher, prioritising consistency over optimality; (2) Critical performance thresholds exist at teacher availability rho >= 0.6 and accuracy omega >= 0.6, below which the framework fails catastrophically; (3) The framework achieves 159% improvement over baseline Q-learning under concept drift. These findings challenge fundamental assumptions about optimal teaching in RL and suggest potential implications for human-robot collaboration, where human preferences for safety and consistency may align with the observed agent selection behaviour, potentially informing training paradigms for safety-critical robotic applications.",
    "paper_id": "2512.17180v2"
  },
  "https://arxiv.org/abs/2512.13063v1": {
    "content_path": "../data/contents/2512.13063v1.md",
    "pdf_path": "../data/contents/2512.13063v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:47.686905",
    "title": "LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators",
    "abstract": "Summary: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.",
    "paper_id": "2512.13063v1"
  },
  "https://arxiv.org/abs/2512.19918v1": {
    "content_path": "../data/contents/2512.19918v1.md",
    "pdf_path": "../data/contents/2512.19918v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:52.299820",
    "title": "Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs",
    "abstract": "Summary: User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.",
    "paper_id": "2512.19918v1"
  },
  "https://arxiv.org/abs/2512.17179v1": {
    "content_path": "../data/contents/2512.17179v1.md",
    "pdf_path": "../data/contents/2512.17179v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:53.127688",
    "title": "Enhancing Long Document Long Form Summarisation with Self-Planning",
    "abstract": "Summary: We introduce a novel approach for long context summarisation, highlight-guided generation, that leverages sentence-level information as a content plan to improve the traceability and faithfulness of generated summaries. Our framework applies self-planning methods to identify important content and then generates a summary conditioned on the plan. We explore both an end-to-end and two-stage variants of the approach, finding that the two-stage pipeline performs better on long and information-dense documents. Experiments on long-form summarisation datasets demonstrate that our method consistently improves factual consistency while preserving relevance and overall quality. On GovReport, our best approach has improved ROUGE-L by 4.1 points and achieves about 35% gains in SummaC scores. Qualitative analysis shows that highlight-guided summarisation helps preserve important details, leading to more accurate and insightful summaries across domains.",
    "paper_id": "2512.17179v1"
  },
  "https://arxiv.org/abs/2512.13059v1": {
    "content_path": "../data/contents/2512.13059v1.md",
    "pdf_path": "../data/contents/2512.13059v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:53.874512",
    "title": "An Open and Reproducible Deep Research Agent for Long-Form Question Answering",
    "abstract": "Summary: We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.",
    "paper_id": "2512.13059v1"
  },
  "https://arxiv.org/abs/2512.19864v1": {
    "content_path": "../data/contents/2512.19864v1.md",
    "pdf_path": "../data/contents/2512.19864v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:54.773492",
    "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data",
    "abstract": "Summary: Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale",
    "paper_id": "2512.19864v1"
  },
  "https://arxiv.org/abs/2512.17178v1": {
    "content_path": "../data/contents/2512.17178v1.md",
    "pdf_path": "../data/contents/2512.17178v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:55.069145",
    "title": "ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching",
    "abstract": "Summary: Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.",
    "paper_id": "2512.17178v1"
  },
  "https://arxiv.org/abs/2512.13043v1": {
    "content_path": "../data/contents/2512.13043v1.md",
    "pdf_path": "../data/contents/2512.13043v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:57.091222",
    "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
    "abstract": "Summary: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.",
    "paper_id": "2512.13043v1"
  },
  "https://arxiv.org/abs/2512.19841v1": {
    "content_path": "../data/contents/2512.19841v1.md",
    "pdf_path": "../data/contents/2512.19841v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:57.844726",
    "title": "A Multi-Agent Retrieval-Augmented Framework for Work-in-Progress Predictio",
    "abstract": "Summary: Work-in-Progress (WiP) prediction is critical for predictive process monitoring, enabling accurate anticipation of workload fluctuations and optimized operational planning. This paper proposes a retrieval-augmented, multi-agent framework that combines retrieval-augmented generation (RAG) and collaborative multi-agent reasoning for WiP prediction. The narrative generation component transforms structured event logs into semantically rich natural language stories, which are embedded into a semantic vector-based process memory to facilitate dynamic retrieval of historical context during inference. The framework includes predictor agents that independently leverage retrieved historical contexts and a decision-making assistant agent that extracts high-level descriptive signals from recent events. A fusion agent then synthesizes predictions using ReAct-style reasoning over agent outputs and retrieved narratives. We evaluate our framework on two real-world benchmark datasets. Results show that the proposed retrieval-augmented multi-agent approach achieves competitive prediction accuracy, obtaining a Mean Absolute Percentage Error (MAPE) of 1.50\\% on one dataset, and surpassing Temporal Convolutional Networks (TCN), Long Short-Term Memory (LSTM), and persistence baselines. The results highlight improved robustness, demonstrating the effectiveness of integrating retrieval mechanisms and multi-agent reasoning in WiP prediction.",
    "paper_id": "2512.19841v1"
  },
  "https://arxiv.org/abs/2512.17164v1": {
    "content_path": "../data/contents/2512.17164v1.md",
    "pdf_path": "../data/contents/2512.17164v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:58.098235",
    "title": "TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval",
    "abstract": "Summary: Query Expansion (QE) enriches queries and Document Expansion (DE) enriches documents, and these two techniques are often applied separately. However, such separate application may lead to semantic misalignment between the expanded queries (or documents) and their relevant documents (or queries). To address this serious issue, we propose TCDE, a dual expansion strategy that leverages large language models (LLMs) for topic-centric enrichment on both queries and documents. In TCDE, we design two distinct prompt templates for processing each query and document. On the query side, an LLM is guided to identify distinct sub-topics within each query and generate a focused pseudo-document for each sub-topic. On the document side, an LLM is guided to distill each document into a set of core topic sentences. The resulting outputs are used to expand the original query and document. This topic-centric dual expansion process establishes semantic bridges between queries and their relevant documents, enabling better alignment for downstream retrieval models. Experiments on two challenging benchmarks, TREC Deep Learning and BEIR, demonstrate that TCDE achieves substantial improvements over strong state-of-the-art expansion baselines. In particular, on dense retrieval tasks, it outperforms several state-of-the-art methods, with a relative improvement of 2.8\\% in NDCG@10 on the SciFact dataset. Experimental results validate the effectiveness of our topic-centric and dual expansion strategy.",
    "paper_id": "2512.17164v1"
  },
  "https://arxiv.org/abs/2512.13040v1": {
    "content_path": "../data/contents/2512.13040v1.md",
    "pdf_path": "../data/contents/2512.13040v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:58.488359",
    "title": "Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection",
    "abstract": "Summary: Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.",
    "paper_id": "2512.13040v1"
  },
  "https://arxiv.org/abs/2512.19799v1": {
    "content_path": "../data/contents/2512.19799v1.md",
    "pdf_path": "../data/contents/2512.19799v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:05:59.284023",
    "title": "PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research",
    "abstract": "Summary: Advances in LLMs have produced agents with knowledge and operational capabilities comparable to human scientists, suggesting potential to assist, accelerate, and automate research. However, existing studies mainly evaluate such systems on well-defined benchmarks or general tasks like literature retrieval, limiting their end-to-end problem-solving ability in open scientific scenarios. This is particularly true in physics, which is abstract, mathematically intensive, and requires integrating analytical reasoning with code-based computation. To address this, we propose PhysMaster, an LLM-based agent functioning as an autonomous theoretical and computational physicist. PhysMaster couples absract reasoning with numerical computation and leverages LANDAU, the Layered Academic Data Universe, which preserves retrieved literature, curated prior knowledge, and validated methodological traces, enhancing decision reliability and stability. It also employs an adaptive exploration strategy balancing efficiency and open-ended exploration, enabling robust performance in ultra-long-horizon tasks. We evaluate PhysMaster on problems from high-energy theory, condensed matter theory to astrophysics, including: (i) acceleration, compressing labor-intensive research from months to hours; (ii) automation, autonomously executing hypothesis-driven loops ; and (iii) autonomous discovery, independently exploring open problems.",
    "paper_id": "2512.19799v1"
  },
  "https://arxiv.org/abs/2512.20661v1": {
    "content_path": "../data/contents/2512.20661v1.md",
    "pdf_path": "../data/contents/2512.20661v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:00.261614",
    "title": "From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers",
    "abstract": "Summary: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%",
    "paper_id": "2512.20661v1"
  },
  "https://arxiv.org/abs/2512.13037v1": {
    "content_path": "../data/contents/2512.13037v1.md",
    "pdf_path": "../data/contents/2512.13037v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:00.496874",
    "title": "Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer",
    "abstract": "Summary: In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.",
    "paper_id": "2512.13037v1"
  },
  "https://arxiv.org/abs/2512.19687v1": {
    "content_path": "../data/contents/2512.19687v1.md",
    "pdf_path": "../data/contents/2512.19687v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:08.106729",
    "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
    "abstract": "Summary: We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.",
    "paper_id": "2512.19687v1"
  },
  "https://arxiv.org/abs/2512.17146v1": {
    "content_path": "../data/contents/2512.17146v1.md",
    "pdf_path": "../data/contents/2512.17146v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:08.405867",
    "title": "Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors",
    "abstract": "Summary: Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.",
    "paper_id": "2512.17146v1"
  },
  "https://arxiv.org/abs/2512.13034v1": {
    "content_path": "../data/contents/2512.13034v1.md",
    "pdf_path": "../data/contents/2512.13034v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:09.956197",
    "title": "Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization",
    "abstract": "Summary: This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.",
    "paper_id": "2512.13034v1"
  },
  "https://arxiv.org/abs/2512.19686v1": {
    "content_path": "../data/contents/2512.19686v1.md",
    "pdf_path": "../data/contents/2512.19686v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:11.281683",
    "title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
    "abstract": "Summary: Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.",
    "paper_id": "2512.19686v1"
  },
  "https://arxiv.org/abs/2412.10302": {
    "content_path": "../data/contents/2412.10302.md",
    "pdf_path": "../data/contents/2412.10302.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:16.475329",
    "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding",
    "abstract": "Major MoE VLM release with superior document/table/chart understanding and OCR capabilities, open weights.",
    "paper_id": "2412.10302"
  },
  "https://arxiv.org/abs/2512.17145v2": {
    "content_path": "../data/contents/2512.17145v2.md",
    "pdf_path": "../data/contents/2512.17145v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:17.037892",
    "title": "Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty",
    "abstract": "Summary: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.",
    "paper_id": "2512.17145v2"
  },
  "https://arxiv.org/abs/2512.13033v1": {
    "content_path": "../data/contents/2512.13033v1.md",
    "pdf_path": "../data/contents/2512.13033v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:17.481206",
    "title": "Scaling Bidirectional Spans and Span Violations in Attention Mechanism",
    "abstract": "Summary: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes",
    "paper_id": "2512.13033v1"
  },
  "https://arxiv.org/abs/2512.19683v1": {
    "content_path": "../data/contents/2512.19683v1.md",
    "pdf_path": "../data/contents/2512.19683v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:30.507586",
    "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
    "abstract": "Summary: While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.",
    "paper_id": "2512.19683v1"
  },
  "https://arxiv.org/abs/2512.17140v1": {
    "content_path": "../data/contents/2512.17140v1.md",
    "pdf_path": "../data/contents/2512.17140v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:31.129590",
    "title": "Bridging Psychometric and Content Development Practices with AI: A Community-Based Workflow for Augmenting Hawaiian Language Assessments",
    "abstract": "Summary: This paper presents the design and evaluation of a community-based artificial intelligence (AI) workflow developed for the Kaiapuni Assessment of Educational Outcomes (KĀ'EO) program, the only native language assessment used for federal accountability in the United States. The project explored whether document-grounded language models could ethically and effectively augment human analysis of item performance while preserving the cultural and linguistic integrity of the Hawaiian language. Operating under the KĀ'EO AI Policy Framework, the workflow used NotebookLM for cross-document synthesis of psychometric data and Claude 3.5 Sonnet for developer-facing interpretation, with human oversight at every stage. Fifty-eight flagged items across Hawaiian Language Arts, Mathematics, and Science were reviewed during Round 2 of the AI Lab, producing six interpretive briefs that identified systemic design issues such as linguistic ambiguity, Depth-of-Knowledge (DOK) misalignment, and structural overload. The findings demonstrate that AI can serve as an ethically bounded amplifier of human expertise, accelerating analysis while simultaneously prioritizing fairness, human expertise, and cultural authority. This work offers a replicable model for responsible AI integration in Indigenous-language educational measurement.",
    "paper_id": "2512.17140v1"
  },
  "https://arxiv.org/abs/2512.13752v1": {
    "content_path": "../data/contents/2512.13752v1.md",
    "pdf_path": "../data/contents/2512.13752v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:38.631682",
    "title": "STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning",
    "abstract": "Summary: Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.",
    "paper_id": "2512.13752v1"
  },
  "https://arxiv.org/abs/2512.19675v1": {
    "content_path": "../data/contents/2512.19675v1.md",
    "pdf_path": "../data/contents/2512.19675v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:41.958598",
    "title": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)",
    "abstract": "Summary: We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.",
    "paper_id": "2512.19675v1"
  },
  "https://arxiv.org/abs/2502.13923": {
    "content_path": "../data/contents/2502.13923.md",
    "pdf_path": "../data/contents/2502.13923.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:42.268781",
    "title": "Qwen2.5-VL Technical Report",
    "abstract": "Latest Qwen VLM with robust structured data extraction from invoices, forms, tables, and chart analysis capabilities.",
    "paper_id": "2502.13923"
  },
  "https://arxiv.org/abs/2512.17131v1": {
    "content_path": "../data/contents/2512.17131v1.md",
    "pdf_path": "../data/contents/2512.17131v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:43.673706",
    "title": "Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs",
    "abstract": "Summary: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.",
    "paper_id": "2512.17131v1"
  },
  "https://arxiv.org/abs/2512.13003v1": {
    "content_path": "../data/contents/2512.13003v1.md",
    "pdf_path": "../data/contents/2512.13003v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:44.557335",
    "title": "General OOD Detection via Model-aware and Subspace-aware Variable Priority",
    "abstract": "Summary: Out-of-distribution (OOD) detection is essential for determining when a supervised model encounters inputs that differ meaningfully from its training distribution. While widely studied in classification, OOD detection for regression and survival analysis remains limited due to the absence of discrete labels and the challenge of quantifying predictive uncertainty. We introduce a framework for OOD detection that is simultaneously model aware and subspace aware, and that embeds variable prioritization directly into the detection step. The method uses the fitted predictor to construct localized neighborhoods around each test case that emphasize the features driving the model's learned relationship and downweight directions that are less relevant to prediction. It produces OOD scores without relying on global distance metrics or estimating the full feature density. The framework is applicable across outcome types, and in our implementation we use random forests, where the rule structure yields transparent neighborhoods and effective scoring. Experiments on synthetic and real data benchmarks designed to isolate functional shifts show consistent improvements over existing methods. We further demonstrate the approach in an esophageal cancer survival study, where distribution shifts related to lymphadenectomy identify patterns relevant to surgical guidelines.",
    "paper_id": "2512.13003v1"
  },
  "https://arxiv.org/abs/2512.19673v1": {
    "content_path": "../data/contents/2512.19673v1.md",
    "pdf_path": "../data/contents/2512.19673v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:45.590922",
    "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "abstract": "Summary: Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
    "paper_id": "2512.19673v1"
  },
  "https://arxiv.org/html/2510.15253": {
    "content_path": "../data/contents/2510.15253.md",
    "pdf_path": "../data/contents/2510.15253.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:46.143600",
    "title": "Scaling Beyond Context: A Survey of Multimodal RAG for Document Understanding",
    "abstract": "Comprehensive survey of multimodal RAG methods for document understanding with growth analysis.",
    "paper_id": "2510.15253"
  },
  "https://arxiv.org/abs/2512.17127v1": {
    "content_path": "../data/contents/2512.17127v1.md",
    "pdf_path": "../data/contents/2512.17127v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:51.294645",
    "title": "Disentangled representations via score-based variational autoencoders",
    "abstract": "Summary: We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.",
    "paper_id": "2512.17127v1"
  },
  "https://arxiv.org/abs/2512.13751v1": {
    "content_path": "../data/contents/2512.13751v1.md",
    "pdf_path": "../data/contents/2512.13751v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:54.068976",
    "title": "MIDUS: Memory-Infused Depth Up-Scaling",
    "abstract": "Summary: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.",
    "paper_id": "2512.13751v1"
  },
  "https://arxiv.org/abs/2512.19663v1": {
    "content_path": "../data/contents/2512.19663v1.md",
    "pdf_path": "../data/contents/2512.19663v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:54.953728",
    "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
    "abstract": "Summary: Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
    "paper_id": "2512.19663v1"
  },
  "https://arxiv.org/abs/2512.17121v1": {
    "content_path": "../data/contents/2512.17121v1.md",
    "pdf_path": "../data/contents/2512.17121v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:55.850494",
    "title": "The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining",
    "abstract": "Summary: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.",
    "paper_id": "2512.17121v1"
  },
  "https://arxiv.org/abs/2512.13001v1": {
    "content_path": "../data/contents/2512.13001v1.md",
    "pdf_path": "../data/contents/2512.13001v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:56.102502",
    "title": "Are Large Language Models Really Effective for Training-Free Cold-Start Recommendation?",
    "abstract": "Summary: Recommender systems usually rely on large-scale interaction data to learn from users' past behaviors and make accurate predictions. However, real-world applications often face situations where no training data is available, such as when launching new services or handling entirely new users. In such cases, conventional approaches cannot be applied. This study focuses on training-free recommendation, where no task-specific training is performed, and particularly on \\textit{training-free cold-start recommendation} (TFCSR), the more challenging case where the target user has no interactions. Large language models (LLMs) have recently been explored as a promising solution, and numerous studies have been proposed. As the ability of text embedding models (TEMs) increases, they are increasingly recognized as applicable to training-free recommendation, but no prior work has directly compared LLMs and TEMs under identical conditions. We present the first controlled experiments that systematically evaluate these two approaches in the same setting. The results show that TEMs outperform LLM rerankers, and this trend holds not only in cold-start settings but also in warm-start settings with rich interactions. These findings indicate that direct LLM ranking is not the only viable option, contrary to the commonly shared belief, and TEM-based approaches provide a stronger and more scalable basis for training-free recommendation.",
    "paper_id": "2512.13001v1"
  },
  "https://arxiv.org/abs/2512.19651v1": {
    "content_path": "../data/contents/2512.19651v1.md",
    "pdf_path": "../data/contents/2512.19651v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:56.284257",
    "title": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting",
    "abstract": "Summary: Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.",
    "paper_id": "2512.19651v1"
  },
  "https://arxiv.org/abs/2512.17109v1": {
    "content_path": "../data/contents/2512.17109v1.md",
    "pdf_path": "../data/contents/2512.17109v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:56.711905",
    "title": "Bridging Training and Merging Through Momentum-Aware Optimization",
    "abstract": "Summary: Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.",
    "paper_id": "2512.17109v1"
  },
  "https://arxiv.org/abs/2512.12997v1": {
    "content_path": "../data/contents/2512.12997v1.md",
    "pdf_path": "../data/contents/2512.12997v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:57.878482",
    "title": "Calibrating Uncertainty for Zero-Shot Adversarial CLIP",
    "abstract": "Summary: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.",
    "paper_id": "2512.12997v1"
  },
  "https://arxiv.org/abs/2512.19609v1": {
    "content_path": "../data/contents/2512.19609v1.md",
    "pdf_path": "../data/contents/2512.19609v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:59.050540",
    "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
    "abstract": "Summary: While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.",
    "paper_id": "2512.19609v1"
  },
  "https://arxiv.org/abs/2503.13964": {
    "content_path": "../data/contents/2503.13964.md",
    "pdf_path": "../data/contents/2503.13964.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:06:59.936672",
    "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document Understanding",
    "abstract": "Novel multi-agent framework using both text and image modalities for document understanding.",
    "paper_id": "2503.13964"
  },
  "https://arxiv.org/abs/2512.17108v1": {
    "content_path": "../data/contents/2512.17108v1.md",
    "pdf_path": "../data/contents/2512.17108v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:00.580979",
    "title": "Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse",
    "abstract": "Summary: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\\leq$ 2.3 Recall@1 in retrieval, $\\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.",
    "paper_id": "2512.17108v1"
  },
  "https://arxiv.org/abs/2512.12989v1": {
    "content_path": "../data/contents/2512.12989v1.md",
    "pdf_path": "../data/contents/2512.12989v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:01.060666",
    "title": "Quantigence: A Multi-Agent AI Framework for Quantum Security Research",
    "abstract": "Summary: Cryptographically Relevant Quantum Computers (CRQCs) pose a structural threat to the global digital economy. Algorithms like Shor's factoring and Grover's search threaten to dismantle the public-key infrastructure (PKI) securing sovereign communications and financial transactions. While the timeline for fault-tolerant CRQCs remains probabilistic, the \"Store-Now, Decrypt-Later\" (SNDL) model necessitates immediate migration to Post-Quantum Cryptography (PQC). This transition is hindered by the velocity of research, evolving NIST standards, and heterogeneous deployment environments. To address this, we present Quantigence, a theory-driven multi-agent AI framework for structured quantum-security analysis. Quantigence decomposes research objectives into specialized roles - Cryptographic Analyst, Threat Modeler, Standards Specialist, and Risk Assessor - coordinated by a supervisory agent. Using \"cognitive parallelism,\" agents reason independently to maintain context purity while execution is serialized on resource-constrained hardware (e.g., NVIDIA RTX 2060). The framework integrates external knowledge via the Model Context Protocol (MCP) and prioritizes vulnerabilities using the Quantum-Adjusted Risk Score (QARS), a formal extension of Mosca's Theorem. Empirical validation shows Quantigence achieves a 67% reduction in research turnaround time and superior literature coverage compared to manual workflows, democratizing access to high-fidelity quantum risk assessment.",
    "paper_id": "2512.12989v1"
  },
  "https://arxiv.org/abs/2512.19605v1": {
    "content_path": "../data/contents/2512.19605v1.md",
    "pdf_path": "../data/contents/2512.19605v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:01.345565",
    "title": "KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning",
    "abstract": "Summary: Recent breakthroughs in self-supervised Joint-Embedding Predictive Architectures (JEPAs) have established that regularizing Euclidean representations toward isotropic Gaussian priors yields provable gains in training stability and downstream generalization. We introduce a new, flexible family of KerJEPAs, self-supervised learning algorithms with kernel-based regularizers. One instance of this family corresponds to the recently-introduced LeJEPA Epps-Pulley regularizer which approximates a sliced maximum mean discrepancy (MMD) with a Gaussian prior and Gaussian kernel. By expanding the class of viable kernels and priors and computing the closed-form high-dimensional limit of sliced MMDs, we develop alternative KerJEPAs with a number of favorable properties including improved training stability and design flexibility.",
    "paper_id": "2512.19605v1"
  },
  "https://arxiv.org/abs/2512.17967v1": {
    "content_path": "../data/contents/2512.17967v1.md",
    "pdf_path": "../data/contents/2512.17967v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:01.791311",
    "title": "Memelang: An Axial Grammar for LLM-Generated Vector-Relational Queries",
    "abstract": "Summary: Structured generation for LLM tool use highlights the value of compact DSL intermediate representations (IRs) that can be emitted directly and parsed deterministically. This paper introduces axial grammar: linear token sequences that recover multi-dimensional structure from the placement of rank-specific separator tokens. A single left-to-right pass assigns each token a coordinate in an n-dimensional grid, enabling deterministic parsing without parentheses or clause-heavy surface syntax. This grammar is instantiated in Memelang, a compact query language intended as an LLM-emittable IR whose fixed coordinate roles map directly to table/column/value slots. Memelang supports coordinate-stable relative references, parse-time variable binding, and implicit context carry-forward to reduce repetition in LLM-produced queries. It also encodes grouping, aggregation, and ordering via inline tags on value terms, allowing grouped execution plans to be derived in one streaming pass over the coordinate-indexed representation. Provided are a reference lexer/parser and a compiler that emits parameterized PostgreSQL SQL (optionally using pgvector operators).",
    "paper_id": "2512.17967v1"
  },
  "https://arxiv.org/abs/2512.12981v1": {
    "content_path": "../data/contents/2512.12981v1.md",
    "pdf_path": "../data/contents/2512.12981v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:02.275018",
    "title": "CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks",
    "abstract": "Summary: While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.",
    "paper_id": "2512.12981v1"
  },
  "https://arxiv.org/abs/2512.19602v1": {
    "content_path": "../data/contents/2512.19602v1.md",
    "pdf_path": "../data/contents/2512.19602v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:03.890668",
    "title": "No Data? No Problem: Robust Vision-Tabular Learning with Missing Values",
    "abstract": "Summary: Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.",
    "paper_id": "2512.19602v1"
  },
  "https://arxiv.org/abs/2512.17102v1": {
    "content_path": "../data/contents/2512.17102v1.md",
    "pdf_path": "../data/contents/2512.17102v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:04.527219",
    "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
    "abstract": "Summary: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.",
    "paper_id": "2512.17102v1"
  },
  "https://arxiv.org/abs/2512.13749v1": {
    "content_path": "../data/contents/2512.13749v1.md",
    "pdf_path": "../data/contents/2512.13749v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:05.278174",
    "title": "Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis",
    "abstract": "Summary: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.",
    "paper_id": "2512.13749v1"
  },
  "https://arxiv.org/abs/2512.19585v1": {
    "content_path": "../data/contents/2512.19585v1.md",
    "pdf_path": "../data/contents/2512.19585v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:06.001775",
    "title": "Increasing the Thinking Budget is Not All You Need",
    "abstract": "Summary: Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.",
    "paper_id": "2512.19585v1"
  },
  "https://arxiv.org/abs/2512.17100v2": {
    "content_path": "../data/contents/2512.17100v2.md",
    "pdf_path": "../data/contents/2512.17100v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:08.722434",
    "title": "UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data",
    "abstract": "Summary: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.",
    "paper_id": "2512.17100v2"
  },
  "https://arxiv.org/abs/2512.12980v1": {
    "content_path": "../data/contents/2512.12980v1.md",
    "pdf_path": "../data/contents/2512.12980v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:09.210367",
    "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views",
    "abstract": "Summary: Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.\n  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.",
    "paper_id": "2512.12980v1"
  },
  "https://arxiv.org/abs/2512.20949": {
    "content_path": "../data/contents/2512.20949.md",
    "pdf_path": "../data/contents/2512.20949.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:10.180972",
    "title": "Neural Probe-Based Hallucination Detection for Large Language Models",
    "abstract": "Token-level hallucination detection via lightweight MLP probes over hidden states, aiming for real-time detection without retrieval or multi-sample verification.",
    "paper_id": "2512.20949"
  },
  "https://arxiv.org/abs/2512.17093v1": {
    "content_path": "../data/contents/2512.17093v1.md",
    "pdf_path": "../data/contents/2512.17093v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:11.241739",
    "title": "A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving",
    "abstract": "Summary: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.\n  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.",
    "paper_id": "2512.17093v1"
  },
  "https://arxiv.org/abs/2512.12978v1": {
    "content_path": "../data/contents/2512.12978v1.md",
    "pdf_path": "../data/contents/2512.12978v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:11.552684",
    "title": "Do Reviews Matter for Recommendations in the Era of Large Language Models?",
    "abstract": "Summary: With the advent of large language models (LLMs), the landscape of recommender systems is undergoing a significant transformation. Traditionally, user reviews have served as a critical source of rich, contextual information for enhancing recommendation quality. However, as LLMs demonstrate an unprecedented ability to understand and generate human-like text, this raises the question of whether explicit user reviews remain essential in the era of LLMs. In this paper, we provide a systematic investigation of the evolving role of text reviews in recommendation by comparing deep learning methods and LLM approaches. Particularly, we conduct extensive experiments on eight public datasets with LLMs and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We further introduce a benchmarking evaluation framework for review-aware recommender systems, RAREval, to comprehensively assess the contribution of textual reviews to the recommendation performance of review-aware recommender systems. Our framework examines various scenarios, including the removal of some or all textual reviews, random distortion, as well as recommendation performance in data sparsity and cold-start user settings. Our findings demonstrate that LLMs are capable of functioning as effective review-aware recommendation engines, generally outperforming traditional deep learning approaches, particularly in scenarios characterized by data sparsity and cold-start conditions. In addition, the removal of some or all textual reviews and random distortion does not necessarily lead to declines in recommendation accuracy. These findings motivate a rethinking of how user preference from text reviews can be more effectively leveraged. All code and supplementary materials are available at: https://github.com/zhytk/RAREval-data-processing.",
    "paper_id": "2512.12978v1"
  },
  "https://arxiv.org/abs/2512.13979": {
    "content_path": "../data/contents/2512.13979.md",
    "pdf_path": "../data/contents/2512.13979.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:12.420546",
    "title": "ReflCtrl: Controlling LLM Reflection via Representation Engineering",
    "abstract": "Identifies and steers a latent “reflection direction” to control self-reflection frequency, reducing reasoning tokens while maintaining accuracy.",
    "paper_id": "2512.13979"
  },
  "https://arxiv.org/abs/2512.17092v1": {
    "content_path": "../data/contents/2512.17092v1.md",
    "pdf_path": "../data/contents/2512.17092v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:12.777580",
    "title": "Data Augmentation Supporting a Conversational Agent Designed for Smoking Cessation Support Groups",
    "abstract": "Summary: Online support groups for smoking cessation are economical and accessible, yet they often face challenges with low user engagement and stigma. The use of an automatic conversational agent would improve engagement by ensuring that all user comments receive a timely response.). We address the challenge of insufficient high-quality data by employing a two-level data augmentation strategy: synthetic data augmentation and real data augmentation. First, we fine-tuned an open source LLM to classify posts from our existing smoking cessation support groups and identify intents with low F1 (precision+recall) scores. Then, for these intents, we generate additional synthetic data using prompt engineering with the GPT model, with an average of 87\\% of the generated synthetic posts deemed high quality by human annotators. Overall, the synthetic augmentation process resulted in 43\\% of the original posts being selected for augmentation, followed by 140\\% synthetic expansion of these posts. Additionally, we scraped more than 10,000 real posts from a related online support context, of which 73\\% were validated as good quality by human annotators. Each synthetic or scraped post underwent rigorous validation involving human reviewers to ensure quality and relevance. The validated new data, combined with the original support group posts, formed an augmented dataset used to retrain the intent classifier. Performance evaluation of the retrained model demonstrated a 32\\% improvement in F1, confirming the effectiveness of our data augmentation approach. Synthetic and real post augmentation led to similar performance improvements. This study provides a replicable framework for enhancing conversational agent performance in domains where data scarcity is a critical issue.",
    "paper_id": "2512.17092v1"
  },
  "https://arxiv.org/abs/2512.12976v1": {
    "content_path": "../data/contents/2512.12976v1.md",
    "pdf_path": "../data/contents/2512.12976v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:13.290574",
    "title": "Authors Should Annotate",
    "abstract": "Summary: The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.",
    "paper_id": "2512.12976v1"
  },
  "https://arxiv.org/abs/2512.17083v2": {
    "content_path": "../data/contents/2512.17083v2.md",
    "pdf_path": "../data/contents/2512.17083v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:14.415270",
    "title": "When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation",
    "abstract": "Summary: Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of work, evaluation practice remains dominated by strict boundary matching and F1-based metrics. Modern large language model (LLM) based conversational systems increasingly rely on segmentation to manage conversation history beyond fixed context windows. In such systems, unstructured context accumulation degrades efficiency and coherence.\n  This paper introduces an evaluation framework that reports boundary density and segment alignment diagnostics (purity and coverage) alongside window-tolerant F1 (W-F1). By separating boundary scoring from boundary selection, we evaluate segmentation quality across density regimes rather than at a single operating point. Cross-dataset evaluation shows that reported performance differences often reflect annotation granularity mismatch rather than boundary placement quality alone.\n  We evaluate structurally distinct segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Boundary-based metrics are strongly coupled to boundary density: threshold sweeps produce larger W-F1 changes than switching between methods. These findings support viewing topic segmentation as a granularity selection problem rather than prediction of a single correct boundary set. This motivates separating boundary scoring from boundary selection for analyzing and tuning segmentation under varying annotation granularities.",
    "paper_id": "2512.17083v2"
  },
  "https://arxiv.org/abs/2512.12967v1": {
    "content_path": "../data/contents/2512.12967v1.md",
    "pdf_path": "../data/contents/2512.12967v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:16.519406",
    "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
    "abstract": "Summary: We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.",
    "paper_id": "2512.12967v1"
  },
  "https://arxiv.org/abs/2507.14417": {
    "content_path": "../data/contents/2507.14417.md",
    "pdf_path": "../data/contents/2507.14417.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:21.377742",
    "title": "Inverse Scaling in Test-Time Compute",
    "abstract": "Preprint (later revised) providing systematic evaluations and failure mode taxonomy for cases where extra inference-time reasoning hurts performance.",
    "paper_id": "2507.14417"
  },
  "https://arxiv.org/abs/2512.17079v1": {
    "content_path": "../data/contents/2512.17079v1.md",
    "pdf_path": "../data/contents/2512.17079v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:21.997415",
    "title": "Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?",
    "abstract": "Summary: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.",
    "paper_id": "2512.17079v1"
  },
  "https://arxiv.org/abs/2512.12957v1": {
    "content_path": "../data/contents/2512.12957v1.md",
    "pdf_path": "../data/contents/2512.12957v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:22.880392",
    "title": "Database Research needs an Abstract Relational Query Language",
    "abstract": "Summary: For decades, SQL has been the default language for composing queries, but it is increasingly used as an artifact to be read and verified rather than authored. With Large Language Models (LLMs), queries are increasingly machine-generated, while humans read, validate, and debug them. This shift turns relational query languages into interfaces for back-and-forth communication about intent, which will lead to a rethinking of relational language design, and more broadly, relational interface design.\n  We argue that this rethinking needs support from an Abstract Relational Query Language (ARQL): a semantics-first reference metalanguage that separates query intent from user-facing syntax and makes underlying relational patterns explicit and comparable across user-facing languages. An ARQL separates a query into (i) a relational core (the compositional structure that determines intent), (ii) modalities (alternative representations of that core tailored to different audiences), and (iii) conventions (orthogonal environment-level semantic parameters under which the core is interpreted, e.g., set vs. bag semantics, or treatment of null values). Usability for humans or machines then depends less on choosing a particular language and more on choosing an appropriate modality. Comparing languages becomes a question of which relational patterns they support and what conventions they choose.\n  We introduce Abstract Relational Calculus (ARC), a strict generalization of Tuple Relational Calculus (TRC), as a concrete instance of ARQL. ARC comes in three modalities: (i) a comprehension-style textual notation, (ii) an Abstract Language Tree (ALT) for machine reasoning about meaning, and (iii) a diagrammatic hierarchical graph (higraph) representation for humans. ARC provides the missing vocabulary and acts as a Rosetta Stone for relational querying.",
    "paper_id": "2512.12957v1"
  },
  "https://arxiv.org/abs/2512.08892": {
    "content_path": "../data/contents/2512.08892.md",
    "pdf_path": "../data/contents/2512.08892.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:23.238592",
    "title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders",
    "abstract": "Uses sparse autoencoders to isolate internal activation features tied to RAG unfaithfulness; proposes a lightweight detector (RAGLens) based on internal representations.",
    "paper_id": "2512.08892"
  },
  "https://arxiv.org/abs/2512.17077v1": {
    "content_path": "../data/contents/2512.17077v1.md",
    "pdf_path": "../data/contents/2512.17077v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:24.479129",
    "title": "Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving",
    "abstract": "Summary: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical \"memory footprint crisis\" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound \"Refresh\" phases and bandwidth-bound \"Reuse\" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\\times$-1.81$\\times$ on the consumer-grade RTX 4090 and 1.60$\\times$-1.74$\\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.",
    "paper_id": "2512.17077v1"
  },
  "https://arxiv.org/abs/2512.12950v1": {
    "content_path": "../data/contents/2512.12950v1.md",
    "pdf_path": "../data/contents/2512.12950v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:25.462082",
    "title": "Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping",
    "abstract": "Summary: Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.",
    "paper_id": "2512.12950v1"
  },
  "https://arxiv.org/abs/2512.07141": {
    "content_path": "../data/contents/2512.07141.md",
    "pdf_path": "../data/contents/2512.07141.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:25.733212",
    "title": "Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models",
    "abstract": "Trains a multi-pass reflection loop to self-correct unsafe reasoning/output (not just answer filtering), relevant to “self-reflection for robustness/safety” mechanisms.",
    "paper_id": "2512.07141"
  },
  "https://arxiv.org/abs/2512.17075v1": {
    "content_path": "../data/contents/2512.17075v1.md",
    "pdf_path": "../data/contents/2512.17075v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:26.681272",
    "title": "Perturb Your Data: Paraphrase-Guided Training Data Watermarking",
    "abstract": "Summary: Training data detection is critical for enforcing copyright and data licensing, as Large Language Models (LLM) are trained on massive text corpora scraped from the internet. We present SPECTRA, a watermarking approach that makes training data reliably detectable even when it comprises less than 0.001% of the training corpus. SPECTRA works by paraphrasing text using an LLM and assigning a score based on how likely each paraphrase is, according to a separate scoring model. A paraphrase is chosen so that its score closely matches that of the original text, to avoid introducing any distribution shifts. To test whether a suspect model has been trained on the watermarked data, we compare its token probabilities against those of the scoring model. We demonstrate that SPECTRA achieves a consistent p-value gap of over nine orders of magnitude when detecting data used for training versus data not used for training, which is greater than all baselines tested. SPECTRA equips data owners with a scalable, deploy-before-release watermark that survives even large-scale LLM training.",
    "paper_id": "2512.17075v1"
  },
  "https://arxiv.org/abs/2512.12949v1": {
    "content_path": "../data/contents/2512.12949v1.md",
    "pdf_path": "../data/contents/2512.12949v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:28.799771",
    "title": "FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection",
    "abstract": "Summary: The scaling of computation throughput continues to outpace improvements in memory bandwidth, making many deep learning workloads memory-bound. Kernel fusion is a key technique to alleviate this problem, but the fusion strategies of existing compilers and frameworks are limited to using local scratchpad memory. When the intermediate results exceed the limited capacity (such as FFN), the fusion fails. Although modern GPUs (like the NVIDIA H100) now incorporate an inter-core connection mechanism known as Distributed Shared Memory(DSM)--providing a larger, high-bandwidth, and low-latency on-chip memory pool--this hardware potential has yet to be exploited by software frameworks. To bridge this gap, we present FlashFuser, the first compiler framework to utilize inter-core connection for kernel fusion on modern GPUs. FlashFuser extends established fusion techniques to the DSM domain through three core contributions. First, we propose a powerful DSM-based communication abstraction that formalizes complex cluster-based data exchange patterns, such as reduce, shuffle and multiply. Second, we introduce a dataflow analyzer that generalizes loop scheduling, resource mapping, and tile selection to the distributed memory hierarchy; it determines the optimal execution order and tile sizes by quantifying data movement across memory levels. Finally, FlashFuser integrates these components into a unified search engine that employs analytical cost modeling and DSM-aware pruning strategies to efficiently discover the optimal execution plan. Our evaluation on an NVIDIA H100 GPU shows that FlashFuser reduces memory access by 58% and delivers kernel speedups of 3.3x against highly-tuned libraries and 4.1x against state-of-the-art compilers, resulting in a 1.24x end-to-end speedup.",
    "paper_id": "2512.12949v1"
  },
  "https://arxiv.org/abs/2512.17911": {
    "content_path": "../data/contents/2512.17911.md",
    "pdf_path": "../data/contents/2512.17911.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:30.522840",
    "title": "Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models",
    "abstract": "Introduces a benchmark for reasoning-trace leakage in unlearning and a training-free inference-time steering method to forget while preserving reasoning capability.",
    "paper_id": "2512.17911"
  },
  "https://arxiv.org/abs/2512.17073v1": {
    "content_path": "../data/contents/2512.17073v1.md",
    "pdf_path": "../data/contents/2512.17073v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:30.943144",
    "title": "Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation",
    "abstract": "Summary: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.",
    "paper_id": "2512.17073v1"
  },
  "https://arxiv.org/abs/2512.12947v1": {
    "content_path": "../data/contents/2512.12947v1.md",
    "pdf_path": "../data/contents/2512.12947v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:31.711423",
    "title": "Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties",
    "abstract": "Summary: Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.",
    "paper_id": "2512.12947v1"
  },
  "https://arxiv.org/abs/2512.17066v1": {
    "content_path": "../data/contents/2512.17066v1.md",
    "pdf_path": "../data/contents/2512.17066v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:38.046336",
    "title": "Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations",
    "abstract": "Summary: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.",
    "paper_id": "2512.17066v1"
  },
  "https://arxiv.org/abs/2512.13747v1": {
    "content_path": "../data/contents/2512.13747v1.md",
    "pdf_path": "../data/contents/2512.13747v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:39.201818",
    "title": "Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making",
    "abstract": "Summary: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.",
    "paper_id": "2512.13747v1"
  },
  "https://arxiv.org/abs/2512.20794v1": {
    "content_path": "../data/contents/2512.20794v1.md",
    "pdf_path": "../data/contents/2512.20794v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:39.467403",
    "title": "Investigating Model Editing for Unlearning in Large Language Models",
    "abstract": "Summary: Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.",
    "paper_id": "2512.20794v1"
  },
  "https://arxiv.org/abs/2512.17065v1": {
    "content_path": "../data/contents/2512.17065v1.md",
    "pdf_path": "../data/contents/2512.17065v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:40.106818",
    "title": "XLM: A Python package for non-autoregressive language models",
    "abstract": "Summary: In recent years, there has been a resurgence of interest in non-autoregressive text generation in the context of general language modeling. Unlike the well-established autoregressive language modeling paradigm, which has a plethora of standard training and inference libraries, implementations of non-autoregressive language modeling have largely been bespoke making it difficult to perform systematic comparisons of different methods. Moreover, each non-autoregressive language model typically requires it own data collation, loss, and prediction logic, making it challenging to reuse common components. In this work, we present the XLM python package, which is designed to make implementing small non-autoregressive language models faster with a secondary goal of providing a suite of small pre-trained models (through a companion xlm-models package) that can be used by the research community. The code is available at https://github.com/dhruvdcoder/xlm-core.",
    "paper_id": "2512.17065v1"
  },
  "https://arxiv.org/abs/2512.14754v1": {
    "content_path": "../data/contents/2512.14754v1.md",
    "pdf_path": "../data/contents/2512.14754v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:40.566549",
    "title": "Revisiting the Reliability of Language Models in Instruction-Following",
    "abstract": "Summary: Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.",
    "paper_id": "2512.14754v1"
  },
  "https://arxiv.org/abs/2512.20780v1": {
    "content_path": "../data/contents/2512.20780v1.md",
    "pdf_path": "../data/contents/2512.20780v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:40.785668",
    "title": "Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles",
    "abstract": "Summary: Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.",
    "paper_id": "2512.20780v1"
  },
  "https://arxiv.org/abs/2512.17053v1": {
    "content_path": "../data/contents/2512.17053v1.md",
    "pdf_path": "../data/contents/2512.17053v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:41.566209",
    "title": "Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL",
    "abstract": "Summary: Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.",
    "paper_id": "2512.17053v1"
  },
  "https://arxiv.org/abs/2512.12938v1": {
    "content_path": "../data/contents/2512.12938v1.md",
    "pdf_path": "../data/contents/2512.12938v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:41.833233",
    "title": "SPAR: Session-based Pipeline for Adaptive Retrieval on Legacy File Systems",
    "abstract": "Summary: The ability to extract value from historical data is essential for enterprise decision-making. However, much of this information remains inaccessible within large legacy file systems that lack structured organization and semantic indexing, making retrieval and analysis inefficient and error-prone. We introduce SPAR (Session-based Pipeline for Adaptive Retrieval), a conceptual framework that integrates Large Language Models (LLMs) into a Retrieval-Augmented Generation (RAG) architecture specifically designed for legacy enterprise environments. Unlike conventional RAG pipelines, which require costly construction and maintenance of full-scale vector databases that mirror the entire file system, SPAR employs a lightweight two-stage process: a semantic Metadata Index is first created, after which session-specific vector databases are dynamically generated on demand. This design reduces computational overhead while improving transparency, controllability, and relevance in retrieval. We provide a theoretical complexity analysis comparing SPAR with standard LLM-based RAG pipelines, demonstrating its computational advantages. To validate the framework, we apply SPAR to a synthesized enterprise-scale file system containing a large corpus of biomedical literature, showing improvements in both retrieval effectiveness and downstream model accuracy. Finally, we discuss design trade-offs and outline open challenges for deploying SPAR across diverse enterprise settings.",
    "paper_id": "2512.12938v1"
  },
  "https://arxiv.org/abs/2512.20773v1": {
    "content_path": "../data/contents/2512.20773v1.md",
    "pdf_path": "../data/contents/2512.20773v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:42.116065",
    "title": "Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization",
    "abstract": "Summary: Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.",
    "paper_id": "2512.20773v1"
  },
  "https://arxiv.org/abs/2512.17052v2": {
    "content_path": "../data/contents/2512.17052v2.md",
    "pdf_path": "../data/contents/2512.17052v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:42.934822",
    "title": "Dynamic Tool Dependency Retrieval for Efficient Function Calling",
    "abstract": "Summary: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\\%$ and $104\\%$ compared to state-of-the-art static retrievers.",
    "paper_id": "2512.17052v2"
  },
  "https://arxiv.org/abs/2512.12935v1": {
    "content_path": "../data/contents/2512.12935v1.md",
    "pdf_path": "../data/contents/2512.12935v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:43.217795",
    "title": "Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion",
    "abstract": "Summary: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.",
    "paper_id": "2512.12935v1"
  },
  "https://arxiv.org/abs/2510.02919": {
    "content_path": "../data/contents/2510.02919.md",
    "pdf_path": "../data/contents/2510.02919.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:47.570990",
    "title": "Self-Reflective Generation at Test Time",
    "abstract": "A test-time reflection mechanism that triggers at high-uncertainty points during generation to reduce cascading errors in reasoning.",
    "paper_id": "2510.02919"
  },
  "https://arxiv.org/abs/2512.20761v1": {
    "content_path": "../data/contents/2512.20761v1.md",
    "pdf_path": "../data/contents/2512.20761v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:48.247253",
    "title": "TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform",
    "abstract": "Summary: While Time Series Foundation Models (TSFMs) offer transformative capabilities for forecasting, they simultaneously risk triggering a fundamental evaluation crisis. This crisis is driven by information leakage due to overlapping training and test sets across different models, as well as the illegitimate transfer of global patterns to test data. While the ability to learn shared temporal dynamics represents a primary strength of these models, their evaluation on historical archives often permits the exploitation of observed global shocks, which violates the independence required for valid benchmarking. We introduce TS-Arena, a platform that restores the operational integrity of forecasting by treating the genuinely unknown future as the definitive test environment. By implementing a pre-registration mechanism on live data streams, the platform ensures that evaluation targets remain physically non-existent during inference, thereby enforcing a strict global temporal split. This methodology establishes a moving temporal frontier that prevents historical contamination and provides an authentic assessment of model generalization. Initially applied within the energy sector, TS-Arena provides a sustainable infrastructure for comparing foundation models under real-world constraints. A prototype of the platform is available at https://huggingface.co/spaces/DAG-UPB/TS-Arena.",
    "paper_id": "2512.20761v1"
  },
  "https://arxiv.org/abs/2512.17051v1": {
    "content_path": "../data/contents/2512.17051v1.md",
    "pdf_path": "../data/contents/2512.17051v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:52.105709",
    "title": "SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples",
    "abstract": "Summary: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.",
    "paper_id": "2512.17051v1"
  },
  "https://arxiv.org/abs/2512.12932v1": {
    "content_path": "../data/contents/2512.12932v1.md",
    "pdf_path": "../data/contents/2512.12932v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:52.621330",
    "title": "Investigating Data Pruning for Pretraining Biological Foundation Models at Scale",
    "abstract": "Summary: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.",
    "paper_id": "2512.12932v1"
  },
  "https://arxiv.org/abs/2506.12217": {
    "content_path": "../data/contents/2506.12217.md",
    "pdf_path": "../data/contents/2506.12217.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:57.229033",
    "title": "From Emergence to Control: Probing and Modulating Self-Reflection in Language Models",
    "abstract": "Probes latent self-reflection and constructs an activation-space control vector to up/down-regulate reflection, trading off accuracy vs compute.",
    "paper_id": "2506.12217"
  },
  "https://arxiv.org/abs/2512.20757v1": {
    "content_path": "../data/contents/2512.20757v1.md",
    "pdf_path": "../data/contents/2512.20757v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:07:57.852393",
    "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior",
    "abstract": "Summary: Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.",
    "paper_id": "2512.20757v1"
  },
  "https://arxiv.org/abs/2512.17048v1": {
    "content_path": "../data/contents/2512.17048v1.md",
    "pdf_path": "../data/contents/2512.17048v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:04.337992",
    "title": "Another Fit Bites the Dust: Conformal Prediction as a Calibration Standard for Machine Learning in High-Energy Physics",
    "abstract": "Summary: Machine-learning techniques are essential in modern collider research, yet their probabilistic outputs often lack calibrated uncertainty estimates and finite-sample guarantees, limiting their direct use in statistical inference and decision-making. Conformal prediction (CP) provides a simple, distribution-free framework for calibrating arbitrary predictive models without retraining, yielding rigorous uncertainty quantification with finite-sample coverage guarantees under minimal exchangeability assumptions, without reliance on asymptotics, limit theorems, or Gaussian approximations. In this work, we investigate CP as a unifying calibration layer for machine-learning applications in high-energy physics. Using publicly available collider datasets and a diverse set of models, we show that a single conformal formalism can be applied across regression, binary and multi-class classification, anomaly detection, and generative modelling, converting raw model outputs into statistically valid prediction sets, typicality regions, and p-values with controlled false-positive rates. While conformal prediction does not improve raw model performance, it enforces honest uncertainty quantification and transparent error control. We argue that conformal calibration should be adopted as a standard component of machine-learning pipelines in collider physics, enabling reliable interpretation, robust comparisons, and principled statistical decisions in experimental and phenomenological analyses.",
    "paper_id": "2512.17048v1"
  },
  "https://arxiv.org/abs/2512.12930v1": {
    "content_path": "../data/contents/2512.12930v1.md",
    "pdf_path": "../data/contents/2512.12930v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:06.612821",
    "title": "SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision",
    "abstract": "Summary: Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.",
    "paper_id": "2512.12930v1"
  },
  "https://arxiv.org/abs/2509.17995": {
    "content_path": "../data/contents/2509.17995.md",
    "pdf_path": "../data/contents/2509.17995.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:10.345502",
    "title": "Variation in Verification: Understanding Verification Dynamics in Large Language Models",
    "abstract": "Systematic study of generative verifiers across difficulty and model scaling; clarifies when verification helps and where it bottlenecks.",
    "paper_id": "2509.17995"
  },
  "https://arxiv.org/abs/2512.20755v1": {
    "content_path": "../data/contents/2512.20755v1.md",
    "pdf_path": "../data/contents/2512.20755v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:10.704861",
    "title": "Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits",
    "abstract": "Summary: Ensuring the safety and efficiency of AI systems is a central goal of modern research. Formal verification provides guarantees of neural network robustness, while early exits improve inference efficiency by enabling intermediate predictions. Yet verifying networks with early exits introduces new challenges due to their conditional execution paths. In this work, we define a robustness property tailored to early exit architectures and show how off-the-shelf solvers can be used to assess it. We present a baseline algorithm, enhanced with an early stopping strategy and heuristic optimizations that maintain soundness and completeness. Experiments on multiple benchmarks validate our framework's effectiveness and demonstrate the performance gains of the improved algorithm. Alongside the natural inference acceleration provided by early exits, we show that they also enhance verifiability, enabling more queries to be solved in less time compared to standard networks. Together with a robustness analysis, we show how these metrics can help users navigate the inherent trade-off between accuracy and efficiency.",
    "paper_id": "2512.20755v1"
  },
  "https://arxiv.org/abs/2512.17043v1": {
    "content_path": "../data/contents/2512.17043v1.md",
    "pdf_path": "../data/contents/2512.17043v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:11.551679",
    "title": "UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering",
    "abstract": "Summary: Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.",
    "paper_id": "2512.17043v1"
  },
  "https://arxiv.org/abs/2512.12929v1": {
    "content_path": "../data/contents/2512.12929v1.md",
    "pdf_path": "../data/contents/2512.12929v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:27.733671",
    "title": "MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation",
    "abstract": "Summary: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.",
    "paper_id": "2512.12929v1"
  },
  "https://arxiv.org/abs/2505.17155": {
    "content_path": "../data/contents/2505.17155.md",
    "pdf_path": "../data/contents/2505.17155.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:28.606498",
    "title": "TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling",
    "abstract": "Inference-time method to detect/truncate redundant reasoning using a verifier, improving throughput for long-CoT reasoning models.",
    "paper_id": "2505.17155"
  },
  "https://arxiv.org/abs/2512.20605v2": {
    "content_path": "../data/contents/2512.20605v2.md",
    "pdf_path": "../data/contents/2512.20605v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:29.030846",
    "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
    "abstract": "Summary: Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.",
    "paper_id": "2512.20605v2"
  },
  "https://arxiv.org/abs/2512.17041v1": {
    "content_path": "../data/contents/2512.17041v1.md",
    "pdf_path": "../data/contents/2512.17041v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:29.439145",
    "title": "Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats",
    "abstract": "Summary: Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.",
    "paper_id": "2512.17041v1"
  },
  "https://arxiv.org/abs/2512.12928v1": {
    "content_path": "../data/contents/2512.12928v1.md",
    "pdf_path": "../data/contents/2512.12928v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:30.549207",
    "title": "PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving",
    "abstract": "Summary: The widespread deployment of large language models (LLMs) for interactive applications necessitates serving systems that can handle thousands of concurrent requests with diverse Service Level Objective (SLO) requirements. A critical yet often overlooked dimension in this context is the inherent priority difference among clients; for instance, business-critical functions demand higher performance guarantees, as fulfilling such requests yields significantly greater business value. However, existing LLM serving schedulers fail to jointly optimize for both SLO attainment and client-level priorities.\n  To bridge this gap, we first \\textit{formalize multi-priority request scheduling as a service gain maximization problem}, where satisfying latency requirements for requests of different priorities contributes varying levels of gain. We then propose PROSERVE, a unified two-tier scheduling framework designed to maximize overall service gain. At the engine level, SlideBatching dynamically adapts batch formation and request ordering under varying load conditions, employing a sliding boundary mechanism to balance deadline-first and density-first strategies. At the service level, GoRouting performs gain-oriented and capability-aware dispatching across distributed instances, proactively reserving capacity for future high-priority or long requests. Extensive evaluation across four open-source datasets and a real-world industrial trace demonstrates that \\systemname{} consistently outperforms state-of-the-art baselines, improving system gain by up to 35% and boosting SLO attainment by up to 52%.",
    "paper_id": "2512.12928v1"
  },
  "https://arxiv.org/abs/2504.01005": {
    "content_path": "../data/contents/2504.01005.md",
    "pdf_path": "../data/contents/2504.01005.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:32.758202",
    "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
    "abstract": "Analyzes compute-optimal allocation between generating more solutions (self-consistency) vs spending compute on verification (GenRM-style).",
    "paper_id": "2504.01005"
  },
  "https://arxiv.org/abs/2512.18605": {
    "content_path": "../data/contents/2512.18605.md",
    "pdf_path": "../data/contents/2512.18605.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:33.168502",
    "title": "Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction",
    "abstract": "Turns low-confidence signals into reflection triggers (instead of early stopping) to improve reasoning accuracy at similar test-time compute.",
    "paper_id": "2512.18605"
  },
  "https://arxiv.org/abs/2512.17028v1": {
    "content_path": "../data/contents/2512.17028v1.md",
    "pdf_path": "../data/contents/2512.17028v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:34.043248",
    "title": "A Women's Health Benchmark for Large Language Models",
    "abstract": "Summary: As large language models (LLMs) become primary sources of health information for millions, their accuracy in women's health remains critically unexamined. We introduce the Women's Health Benchmark (WHB), the first benchmark evaluating LLM performance specifically in women's health. Our benchmark comprises 96 rigorously validated model stumps covering five medical specialties (obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology), three query types (patient query, clinician query, and evidence/policy query), and eight error types (dosage/medication errors, missing critical information, outdated guidelines/treatment recommendations, incorrect treatment advice, incorrect factual information, missing/incorrect differential diagnosis, missed urgency, and inappropriate recommendations). We evaluated 13 state-of-the-art LLMs and revealed alarming gaps: current models show approximately 60\\% failure rates on the women's health benchmark, with performance varying dramatically across specialties and error types. Notably, models universally struggle with \"missed urgency\" indicators, while newer models like GPT-5 show significant improvements in avoiding inappropriate recommendations. Our findings underscore that AI chatbots are not yet fully able of providing reliable advice in women's health.",
    "paper_id": "2512.17028v1"
  },
  "https://arxiv.org/abs/2512.13744v1": {
    "content_path": "../data/contents/2512.13744v1.md",
    "pdf_path": "../data/contents/2512.13744v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:34.891076",
    "title": "Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes",
    "abstract": "Summary: Deepfake audio detection has progressed rapidly with strong pre-trained encoders (e.g., WavLM, Wav2Vec2, MMS). However, performance in realistic capture conditions - background noise (domestic/office/transport), room reverberation, and consumer channels - often lags clean-lab results. We survey and evaluate robustness for state-of-the-art audio deepfake detection models and present a reproducible framework that mixes MS-SNSD noises with ASVspoof 2021 DF utterances to evaluate under controlled signal-to-noise ratios (SNRs). SNR is a measured proxy for noise severity used widely in speech; it lets us sweep from near-clean (35 dB) to very noisy (-5 dB) to quantify graceful degradation. We study multi-condition training and fixed-SNR testing for pretrained encoders (WavLM, Wav2Vec2, MMS), reporting accuracy, ROC-AUC, and EER on binary and four-class (authenticity x corruption) tasks. In our experiments, finetuning reduces EER by 10-15 percentage points at 10-0 dB SNR across backbones.",
    "paper_id": "2512.13744v1"
  },
  "https://arxiv.org/abs/2502.12118": {
    "content_path": "../data/contents/2502.12118.md",
    "pdf_path": "../data/contents/2502.12118.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:37.387956",
    "title": "Scaling Test-Time Compute Without Verification or RL is Suboptimal",
    "abstract": "Argues (theoretically + empirically) that verifier-guided RL/search scales better than verifier-free trace distillation as inference budgets grow.",
    "paper_id": "2502.12118"
  },
  "https://arxiv.org/abs/2512.15000": {
    "content_path": "../data/contents/2512.15000.md",
    "pdf_path": "../data/contents/2512.15000.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:37.683257",
    "title": "DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding",
    "abstract": "Adapts process reward modeling to code by treating functions as steps and correcting noisy step labels, improving test-time scaling for coding.",
    "paper_id": "2512.15000"
  },
  "https://arxiv.org/abs/2512.17027v1": {
    "content_path": "../data/contents/2512.17027v1.md",
    "pdf_path": "../data/contents/2512.17027v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:38.465165",
    "title": "Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations",
    "abstract": "Summary: Encyclopedic knowledge platforms are key gateways through which users explore information online. The recent release of Grokipedia, a fully AI-generated encyclopedia, introduces a new alternative to traditional, well-established platforms like Wikipedia. In this context, search engine mechanisms play an important role in guiding users exploratory paths, yet their behavior across different encyclopedic systems remains underexplored. In this work, we address this gap by providing the first comparative analysis of search engine in Wikipedia and Grokipedia.\n  Using nearly 10,000 neutral English words and their substrings as queries, we collect over 70,000 search engine results and examine their semantic alignment, overlap, and topical structure. We find that both platforms frequently generate results that are weakly related to the original query and, in many cases, surface unexpected content starting from innocuous queries. Despite these shared properties, the two systems often produce substantially different recommendation sets for the same query. Through topical annotation and trajectory analysis, we further identify systematic differences in how content categories are surfaced and how search engine results evolve over multiple stages of exploration.\n  Overall, our findings show that unexpected search engine outcomes are a common feature of both the platforms, even though they exhibit discrepancies in terms of topical distribution and query suggestions.",
    "paper_id": "2512.17027v1"
  },
  "https://arxiv.org/abs/2512.12924v1": {
    "content_path": "../data/contents/2512.12924v1.md",
    "pdf_path": "../data/contents/2512.12924v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:39.171399",
    "title": "Interpretable Hypothesis-Driven Trading:A Rigorous Walk-Forward Validation Framework for Market Microstructure Signals",
    "abstract": "Summary: We develop a rigorous walk-forward validation framework for algorithmic trading designed to mitigate overfitting and lookahead bias. Our methodology combines interpretable hypothesis-driven signal generation with reinforcement learning and strict out-of-sample testing. The framework enforces strict information set discipline, employs rolling window validation across 34 independent test periods, maintains complete interpretability through natural language hypothesis explanations, and incorporates realistic transaction costs and position constraints. Validating five market microstructure patterns across 100 US equities from 2015 to 2024, the system yields modest annualized returns (0.55%, Sharpe ratio 0.33) with exceptional downside protection (maximum drawdown -2.76%) and market-neutral characteristics (beta = 0.058). Performance exhibits strong regime dependence, generating positive returns during high-volatility periods (0.60% quarterly, 2020-2024) while underperforming in stable markets (-0.16%, 2015-2019). We report statistically insignificant aggregate results (p-value 0.34) to demonstrate a reproducible, honest validation protocol that prioritizes interpretability and extends naturally to advanced hypothesis generators, including large language models. The key empirical finding reveals that daily OHLCV-based microstructure signals require elevated information arrival and trading activity to function effectively. The framework provides complete mathematical specifications and open-source implementation, establishing a template for rigorous trading system evaluation that addresses the reproducibility crisis in quantitative finance research. For researchers, practitioners, and regulators, this work demonstrates that interpretable algorithmic trading strategies can be rigorously validated without sacrificing transparency or regulatory compliance.",
    "paper_id": "2512.12924v1"
  },
  "https://arxiv.org/abs/2512.10791": {
    "content_path": "../data/contents/2512.10791.md",
    "pdf_path": "../data/contents/2512.10791.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:40.192561",
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "abstract": "Introduces a multi-part factuality evaluation suite (parametric, search, grounding, multimodal) designed to holistically measure and track LLM factuality.",
    "paper_id": "2512.10791"
  },
  "https://arxiv.org/abs/2512.17008v1": {
    "content_path": "../data/contents/2512.17008v1.md",
    "pdf_path": "../data/contents/2512.17008v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:40.443559",
    "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
    "abstract": "Summary: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
    "paper_id": "2512.17008v1"
  },
  "https://arxiv.org/abs/2512.12922v1": {
    "content_path": "../data/contents/2512.12922v1.md",
    "pdf_path": "../data/contents/2512.12922v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:40.815160",
    "title": "LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization",
    "abstract": "Summary: In modern financial markets, investors increasingly seek personalized and adaptive portfolio strategies that reflect their individual risk preferences and respond to dynamic market conditions. Traditional rule-based or static optimization approaches often fail to capture the nonlinear interactions among investor behavior, market volatility, and evolving financial objectives. To address these limitations, this paper introduces the LLM-based Personalized Portfolio Recommender , an integrated framework that combines Large Language Models, reinforcement learning, and individualized risk preference modeling to support intelligent investment decision-making.",
    "paper_id": "2512.12922v1"
  },
  "https://arxiv.org/abs/2501.03200": {
    "content_path": "../data/contents/2501.03200.md",
    "pdf_path": "../data/contents/2501.03200.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:41.277985",
    "title": "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input",
    "abstract": "Benchmark/leaderboard for long-context grounding and faithfulness—useful for evaluating factuality/grounding in long-form responses.",
    "paper_id": "2501.03200"
  },
  "https://arxiv.org/abs/2512.16921v1": {
    "content_path": "../data/contents/2512.16921v1.md",
    "pdf_path": "../data/contents/2512.16921v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:41.936785",
    "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification",
    "abstract": "Summary: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.",
    "paper_id": "2512.16921v1"
  },
  "https://arxiv.org/abs/2512.12921v1": {
    "content_path": "../data/contents/2512.12921v1.md",
    "pdf_path": "../data/contents/2512.12921v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:42.313130",
    "title": "Cisco Integrated AI Security and Safety Framework Report",
    "abstract": "Summary: Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space.\n  This paper presents Cisco's Integrated AI Security and Safety Framework (\"AI Security Framework\"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.",
    "paper_id": "2512.12921v1"
  },
  "https://arxiv.org/abs/2505.00506": {
    "content_path": "../data/contents/2505.00506.md",
    "pdf_path": "../data/contents/2505.00506.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:43.366679",
    "title": "HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection",
    "abstract": "A multi-domain hallucination detection benchmark stressing long contexts and real-world settings—useful for evaluating detectors and RAG reliability.",
    "paper_id": "2505.00506"
  },
  "https://arxiv.org/abs/2512.16917v1": {
    "content_path": "../data/contents/2512.16917v1.md",
    "pdf_path": "../data/contents/2512.16917v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:43.779457",
    "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
    "abstract": "Summary: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
    "paper_id": "2512.16917v1"
  },
  "https://arxiv.org/abs/2512.12918v1": {
    "content_path": "../data/contents/2512.12918v1.md",
    "pdf_path": "../data/contents/2512.12918v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:44.143810",
    "title": "Satisfiability Modulo Theory Meets Inductive Logic Programming",
    "abstract": "Summary: Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.",
    "paper_id": "2512.12918v1"
  },
  "https://arxiv.org/abs/2512.16914v1": {
    "content_path": "../data/contents/2512.16914v1.md",
    "pdf_path": "../data/contents/2512.16914v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:44.434156",
    "title": "Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates",
    "abstract": "Summary: Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.",
    "paper_id": "2512.16914v1"
  },
  "https://arxiv.org/abs/2512.12914v1": {
    "content_path": "../data/contents/2512.12914v1.md",
    "pdf_path": "../data/contents/2512.12914v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:44.941107",
    "title": "CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs",
    "abstract": "Summary: Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.",
    "paper_id": "2512.12914v1"
  },
  "https://arxiv.org/abs/2511.10621": {
    "content_path": "../data/contents/2511.10621.md",
    "pdf_path": "../data/contents/2511.10621.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:45.340835",
    "title": "SSR: Socratic Self-Refine for Large Language Model Reasoning",
    "abstract": "Fine-grained step-level self-refinement framework that pinpoints unreliable reasoning steps and iteratively repairs them (test-time improvement without heavy ensembling).",
    "paper_id": "2511.10621"
  },
  "https://arxiv.org/abs/2512.16912v2": {
    "content_path": "../data/contents/2512.16912v2.md",
    "pdf_path": "../data/contents/2512.16912v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:46.488216",
    "title": "Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "abstract": "Summary: This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
    "paper_id": "2512.16912v2"
  },
  "https://arxiv.org/abs/2512.12895v1": {
    "content_path": "../data/contents/2512.12895v1.md",
    "pdf_path": "../data/contents/2512.12895v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:48.093847",
    "title": "Wait, Wait, Wait... Why Do Reasoning Models Loop?",
    "abstract": "Summary: Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.",
    "paper_id": "2512.12895v1"
  },
  "https://arxiv.org/abs/2512.16911v1": {
    "content_path": "../data/contents/2512.16911v1.md",
    "pdf_path": "../data/contents/2512.16911v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:51.396124",
    "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
    "abstract": "Summary: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.",
    "paper_id": "2512.16911v1"
  },
  "https://arxiv.org/abs/2512.12885v1": {
    "content_path": "../data/contents/2512.12885v1.md",
    "pdf_path": "../data/contents/2512.12885v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:51.650902",
    "title": "SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition",
    "abstract": "Summary: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.",
    "paper_id": "2512.12885v1"
  },
  "https://arxiv.org/abs/2511.22570": {
    "content_path": "../data/contents/2511.22570.md",
    "pdf_path": "../data/contents/2511.22570.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:52.487267",
    "title": "DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning",
    "abstract": "Self-verification approach to scale reasoning/verification at test time for rigorous math/theorem proving—highly relevant to verifier-guided planning and reliability.",
    "paper_id": "2511.22570"
  },
  "https://arxiv.org/abs/2512.16910v1": {
    "content_path": "../data/contents/2512.16910v1.md",
    "pdf_path": "../data/contents/2512.16910v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:52.919817",
    "title": "SFTok: Bridging the Performance Gap in Discrete Tokenizers",
    "abstract": "Summary: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \\textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \\textbf{self-forcing guided visual reconstruction} and \\textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).",
    "paper_id": "2512.16910v1"
  },
  "https://arxiv.org/abs/2512.12880v2": {
    "content_path": "../data/contents/2512.12880v2.md",
    "pdf_path": "../data/contents/2512.12880v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:53.128125",
    "title": "Improving Recursive Transformers with Mixture of LoRAs",
    "abstract": "Summary: Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.",
    "paper_id": "2512.12880v2"
  },
  "https://arxiv.org/abs/2412.06769": {
    "content_path": "../data/contents/2412.06769.md",
    "pdf_path": "../data/contents/2412.06769.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:53.559075",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space (Coconut)",
    "abstract": "Meta AI research on latent reasoning enabling BFS-style exploration without token generation.",
    "paper_id": "2412.06769"
  },
  "https://arxiv.org/abs/2512.16904v1": {
    "content_path": "../data/contents/2512.16904v1.md",
    "pdf_path": "../data/contents/2512.16904v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:54.387084",
    "title": "How Good is Post-Hoc Watermarking With Language Model Rephrasing?",
    "abstract": "Summary: Generation-time text watermarking embeds statistical signals into text for traceability of AI-generated content. We explore *post-hoc watermarking* where an LLM rewrites existing text while applying generation-time watermarking, to protect copyrighted documents, or detect their use in training or RAG via watermark radioactivity. Unlike generation-time approaches, which is constrained by how LLMs are served, this setting offers additional degrees of freedom for both generation and detection. We investigate how allocating compute (through larger rephrasing models, beam search, multi-candidate generation, or entropy filtering at detection) affects the quality-detectability trade-off. Our strategies achieve strong detectability and semantic fidelity on open-ended text such as books. Among our findings, the simple Gumbel-max scheme surprisingly outperforms more recent alternatives under nucleus sampling, and most methods benefit significantly from beam search. However, most approaches struggle when watermarking verifiable text such as code, where we counterintuitively find that smaller models outperform larger ones. This study reveals both the potential and limitations of post-hoc watermarking, laying groundwork for practical applications and future research.",
    "paper_id": "2512.16904v1"
  },
  "https://arxiv.org/abs/2512.15782v1": {
    "content_path": "../data/contents/2512.15782v1.md",
    "pdf_path": "../data/contents/2512.15782v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:54.832585",
    "title": "Auto-Tuning Safety Guardrails for Black-Box Large Language Models",
    "abstract": "Summary: Large language models (LLMs) are increasingly deployed behind safety guardrails such as system prompts and content filters, especially in settings where product teams cannot modify model weights. In practice these guardrails are typically hand-tuned, brittle, and difficult to reproduce. This paper studies a simple but practical alternative: treat safety guardrail design itself as a hyperparameter optimization problem over a frozen base model. Concretely, I wrap Mistral-7B-Instruct with modular jailbreak and malware system prompts plus a ModernBERT-based harmfulness classifier, then evaluate candidate configurations on three public benchmarks covering malware generation, classic jailbreak prompts, and benign user queries. Each configuration is scored using malware and jailbreak attack success rate, benign harmful-response rate, and end-to-end latency. A 48-point grid search over prompt combinations and filter modes establishes a baseline. I then run a black-box Optuna study over the same space and show that it reliably rediscovers the best grid configurations while requiring an order of magnitude fewer evaluations and roughly 8x less wall-clock time. The results suggest that viewing safety guardrails as tunable hyperparameters is a feasible way to harden black-box LLM deployments under compute and time constraints.",
    "paper_id": "2512.15782v1"
  },
  "https://arxiv.org/abs/2501.09686": {
    "content_path": "../data/contents/2501.09686.md",
    "pdf_path": "../data/contents/2501.09686.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:55.822565",
    "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models",
    "abstract": "Comprehensive survey on RL-based reasoning covering automated data construction and test-time scaling.",
    "paper_id": "2501.09686"
  },
  "https://arxiv.org/abs/2512.16902v1": {
    "content_path": "../data/contents/2512.16902v1.md",
    "pdf_path": "../data/contents/2512.16902v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:56.864318",
    "title": "In-Context Algebra",
    "abstract": "Summary: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.",
    "paper_id": "2512.16902v1"
  },
  "https://arxiv.org/abs/2512.12870v1": {
    "content_path": "../data/contents/2512.12870v1.md",
    "pdf_path": "../data/contents/2512.12870v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:08:57.706868",
    "title": "Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels",
    "abstract": "Summary: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.",
    "paper_id": "2512.12870v1"
  },
  "https://arxiv.org/abs/2510.17472": {
    "content_path": "../data/contents/2510.17472.md",
    "pdf_path": "../data/contents/2510.17472.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:12.978995",
    "title": "Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs",
    "abstract": "Provides statistical certification for self-consistency and connects it to test-time training objectives, grounding reliability claims in theory.",
    "paper_id": "2510.17472"
  },
  "https://arxiv.org/abs/2512.16901v1": {
    "content_path": "../data/contents/2512.16901v1.md",
    "pdf_path": "../data/contents/2512.16901v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:13.351299",
    "title": "Impacts of Racial Bias in Historical Training Data for News AI",
    "abstract": "Summary: AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
    "paper_id": "2512.16901v1"
  },
  "https://arxiv.org/abs/2512.12869v1": {
    "content_path": "../data/contents/2512.12869v1.md",
    "pdf_path": "../data/contents/2512.12869v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:23.536892",
    "title": "ERA-IT: Aligning Semantic Models with Revealed Economic Preference for Real-Time and Explainable Patent Valuation",
    "abstract": "Summary: Valuing intangible assets under uncertainty remains a critical challenge in the strategic management of technological innovation due to the information asymmetry inherent in high-dimensional technical specifications. Traditional bibliometric indicators, such as citation counts, fail to address this friction in a timely manner due to the systemic latency inherent in data accumulation. To bridge this gap, this study proposes the Economic Reasoning Alignment via Instruction Tuning (ERA-IT) framework. We theoretically conceptualize patent renewal history as a revealed economic preference and leverage it as an objective supervisory signal to align the generative reasoning of Large Language Models (LLMs) with market realities, a process we term Eco-Semantic Alignment. Using a randomly sampled dataset of 10,000 European Patent Office patents across diverse technological domains, we trained the model not only to predict value tiers but also to reverse-engineer the Economic Chain-of-Thought from unstructured text. Empirical results demonstrate that ERA-IT significantly outperforms both conventional econometric models and zero-shot LLMs in predictive accuracy. More importantly, by generating explicit, logically grounded rationales for valuation, the framework serves as a transparent cognitive scaffold for decision-makers, reducing the opacity of black-box AI in high-stakes intellectual property management.",
    "paper_id": "2512.12869v1"
  },
  "https://arxiv.org/abs/2510.00492": {
    "content_path": "../data/contents/2510.00492.md",
    "pdf_path": "../data/contents/2510.00492.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:25.746457",
    "title": "Rethinking Reward Models for Multi-Domain Test-Time Scaling",
    "abstract": "Unified multi-domain evaluation of outcome vs process reward models (discriminative/generative), challenging assumptions about PRM superiority in long reasoning.",
    "paper_id": "2510.00492"
  },
  "https://arxiv.org/abs/2512.16899v1": {
    "content_path": "../data/contents/2512.16899v1.md",
    "pdf_path": "../data/contents/2512.16899v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:28.949692",
    "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image",
    "abstract": "Summary: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.",
    "paper_id": "2512.16899v1"
  },
  "https://arxiv.org/abs/2512.12868v1": {
    "content_path": "../data/contents/2512.12868v1.md",
    "pdf_path": "../data/contents/2512.12868v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:29.913792",
    "title": "Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM",
    "abstract": "Summary: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.",
    "paper_id": "2512.12868v1"
  },
  "https://arxiv.org/abs/2512.16891v1": {
    "content_path": "../data/contents/2512.16891v1.md",
    "pdf_path": "../data/contents/2512.16891v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:30.196954",
    "title": "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation",
    "abstract": "Summary: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.",
    "paper_id": "2512.16891v1"
  },
  "https://arxiv.org/abs/2512.17945v1": {
    "content_path": "../data/contents/2512.17945v1.md",
    "pdf_path": "../data/contents/2512.17945v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:30.951063",
    "title": "What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD",
    "abstract": "Summary: Financial institutions face a trade-off between predictive accuracy and interpretability when deploying machine learning models for credit risk. Monotonicity constraints align model behavior with domain knowledge, but their performance cost - the price of monotonicity - is not well quantified. This paper benchmarks monotone-constrained versus unconstrained gradient boosting models for credit probability of default across five public datasets and three libraries. We define the Price of Monotonicity (PoM) as the relative change in standard performance metrics when moving from unconstrained to constrained models, estimated via paired comparisons with bootstrap uncertainty. In our experiments, PoM in AUC ranges from essentially zero to about 2.9 percent: constraints are almost costless on large datasets (typically less than 0.2 percent, often indistinguishable from zero) and most costly on smaller datasets with extensive constraint coverage (around 2-3 percent). Thus, appropriately specified monotonicity constraints can often deliver interpretability with small accuracy losses, particularly in large-scale credit portfolios.",
    "paper_id": "2512.17945v1"
  },
  "https://arxiv.org/abs/2412.06559": {
    "content_path": "../data/contents/2412.06559.md",
    "pdf_path": "../data/contents/2412.06559.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:31.635945",
    "title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning",
    "abstract": "Foundational step-level benchmark for detecting earliest reasoning errors—commonly used to evaluate/verifier-train PRMs and critique models.",
    "paper_id": "2412.06559"
  },
  "https://arxiv.org/abs/2512.16883v1": {
    "content_path": "../data/contents/2512.16883v1.md",
    "pdf_path": "../data/contents/2512.16883v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:33.830018",
    "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning",
    "abstract": "Summary: Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.",
    "paper_id": "2512.16883v1"
  },
  "https://arxiv.org/abs/2512.12858v1": {
    "content_path": "../data/contents/2512.12858v1.md",
    "pdf_path": "../data/contents/2512.12858v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:34.225049",
    "title": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization",
    "abstract": "Summary: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.",
    "paper_id": "2512.12858v1"
  },
  "https://arxiv.org/abs/2406.09136": {
    "content_path": "../data/contents/2406.09136.md",
    "pdf_path": "../data/contents/2406.09136.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:35.519621",
    "title": "Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs",
    "abstract": "NeurIPS paper combining Tree-of-Thought search with CoT training for better reasoning.",
    "paper_id": "2406.09136"
  },
  "https://arxiv.org/abs/2512.16874v1": {
    "content_path": "../data/contents/2512.16874v1.md",
    "pdf_path": "../data/contents/2512.16874v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:45.873806",
    "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking",
    "abstract": "Summary: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.",
    "paper_id": "2512.16874v1"
  },
  "https://arxiv.org/abs/2512.12856v1": {
    "content_path": "../data/contents/2512.12856v1.md",
    "pdf_path": "../data/contents/2512.12856v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:47.811236",
    "title": "Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents",
    "abstract": "Summary: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.",
    "paper_id": "2512.12856v1"
  },
  "https://arxiv.org/abs/2512.16873v1": {
    "content_path": "../data/contents/2512.16873v1.md",
    "pdf_path": "../data/contents/2512.16873v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:48.358307",
    "title": "The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI",
    "abstract": "Summary: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.",
    "paper_id": "2512.16873v1"
  },
  "https://arxiv.org/abs/2512.13742v1": {
    "content_path": "../data/contents/2512.13742v1.md",
    "pdf_path": "../data/contents/2512.13742v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:50.168275",
    "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models",
    "abstract": "Summary: Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.",
    "paper_id": "2512.13742v1"
  },
  "https://arxiv.org/abs/2503.10814": {
    "content_path": "../data/contents/2503.10814.md",
    "pdf_path": "../data/contents/2503.10814.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:51.173020",
    "title": "Thinking Machines: A Survey of LLM based Reasoning Strategies",
    "abstract": "Survey covering O1, DeepSeek R1, and evolution of reasoning strategies in LLMs.",
    "paper_id": "2503.10814"
  },
  "https://arxiv.org/abs/2502.15844": {
    "content_path": "../data/contents/2502.15844.md",
    "pdf_path": "../data/contents/2502.15844.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:52.480727",
    "title": "Hallucination Detection in Large Language Models with Metamorphic Relations (MetaQA)",
    "abstract": "Novel zero-resource hallucination detection using metamorphic relations, outperforming SelfCheckGPT by 112%.",
    "paper_id": "2502.15844"
  },
  "https://arxiv.org/abs/2512.16871v1": {
    "content_path": "../data/contents/2512.16871v1.md",
    "pdf_path": "../data/contents/2512.16871v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:53.446694",
    "title": "Sequencing to Mitigate Catastrophic Forgetting in Continual Learning",
    "abstract": "Summary: To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.",
    "paper_id": "2512.16871v1"
  },
  "https://arxiv.org/abs/2512.12844v1": {
    "content_path": "../data/contents/2512.12844v1.md",
    "pdf_path": "../data/contents/2512.12844v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:53.689227",
    "title": "Selective Conformal Risk Control",
    "abstract": "Summary: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \\textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.",
    "paper_id": "2512.12844v1"
  },
  "https://arxiv.org/abs/2512.16866v1": {
    "content_path": "../data/contents/2512.16866v1.md",
    "pdf_path": "../data/contents/2512.16866v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:55.497527",
    "title": "Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models",
    "abstract": "Summary: Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: \"How to determine labels for truly future, unseen data points\". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.",
    "paper_id": "2512.16866v1"
  },
  "https://arxiv.org/abs/2512.12840v1": {
    "content_path": "../data/contents/2512.12840v1.md",
    "pdf_path": "../data/contents/2512.12840v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:56.105597",
    "title": "PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks",
    "abstract": "Summary: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning \"private.\" PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.",
    "paper_id": "2512.12840v1"
  },
  "https://arxiv.org/abs/2505.13445": {
    "content_path": "../data/contents/2505.13445.md",
    "pdf_path": "../data/contents/2505.13445.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:56.833640",
    "title": "RISE: Reinforcing Reasoning with Self-Verification",
    "abstract": "Novel RL framework training LLMs to simultaneously improve problem-solving and self-verification abilities.",
    "paper_id": "2505.13445"
  },
  "https://arxiv.org/abs/2512.16856v1": {
    "content_path": "../data/contents/2512.16856v1.md",
    "pdf_path": "../data/contents/2512.16856v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:57.102979",
    "title": "Distributional AGI Safety",
    "abstract": "Summary: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.",
    "paper_id": "2512.16856v1"
  },
  "https://arxiv.org/abs/2512.12839v1": {
    "content_path": "../data/contents/2512.12839v1.md",
    "pdf_path": "../data/contents/2512.12839v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:09:59.214549",
    "title": "What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation",
    "abstract": "Summary: In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.",
    "paper_id": "2512.12839v1"
  },
  "https://arxiv.org/abs/2510.12157": {
    "content_path": "../data/contents/2510.12157.md",
    "pdf_path": "../data/contents/2510.12157.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:00.266215",
    "title": "Self-Verifying Reflection Helps Transformers with CoT Reasoning",
    "abstract": "Proves self-verification guarantees improvements when verification errors are bounded, even in tiny transformers.",
    "paper_id": "2510.12157"
  },
  "https://arxiv.org/abs/2512.16855v1": {
    "content_path": "../data/contents/2512.16855v1.md",
    "pdf_path": "../data/contents/2512.16855v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:00.866684",
    "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge",
    "abstract": "Summary: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.",
    "paper_id": "2512.16855v1"
  },
  "https://arxiv.org/abs/2512.12837v1": {
    "content_path": "../data/contents/2512.12837v1.md",
    "pdf_path": "../data/contents/2512.12837v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:01.222798",
    "title": "Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union",
    "abstract": "Summary: AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-à-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement.",
    "paper_id": "2512.12837v1"
  },
  "https://arxiv.org/abs/2512.16853v1": {
    "content_path": "../data/contents/2512.16853v1.md",
    "pdf_path": "../data/contents/2512.16853v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:09.283534",
    "title": "GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation",
    "abstract": "Summary: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.",
    "paper_id": "2512.16853v1"
  },
  "https://arxiv.org/abs/2512.15780v1": {
    "content_path": "../data/contents/2512.15780v1.md",
    "pdf_path": "../data/contents/2512.15780v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:11.473481",
    "title": "Adversarial Robustness in Financial Machine Learning: Defenses, Economic Impact, and Governance Evidence",
    "abstract": "Summary: We evaluate adversarial robustness in tabular machine learning models used in financial decision making. Using credit scoring and fraud detection data, we apply gradient based attacks and measure impacts on discrimination, calibration, and financial risk metrics. Results show notable performance degradation under small perturbations and partial recovery through adversarial training.",
    "paper_id": "2512.15780v1"
  },
  "https://arxiv.org/abs/2510.05746": {
    "content_path": "../data/contents/2510.05746.md",
    "pdf_path": "../data/contents/2510.05746.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:12.549554",
    "title": "ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems",
    "abstract": "Automatically discovers modular reasoning steps via tree search over code—treating CoT as a unit to optimize for generalizable multi-agent reasoning.",
    "paper_id": "2510.05746"
  },
  "https://arxiv.org/abs/2512.16848v1": {
    "content_path": "../data/contents/2512.16848v1.md",
    "pdf_path": "../data/contents/2512.16848v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:13.696534",
    "title": "Meta-RL Induces Exploration in Language Agents",
    "abstract": "Summary: Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
    "paper_id": "2512.16848v1"
  },
  "https://arxiv.org/abs/2512.12827v1": {
    "content_path": "../data/contents/2512.12827v1.md",
    "pdf_path": "../data/contents/2512.12827v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:15.522139",
    "title": "GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients",
    "abstract": "Summary: Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.",
    "paper_id": "2512.12827v1"
  },
  "https://arxiv.org/abs/2512.16843v1": {
    "content_path": "../data/contents/2512.16843v1.md",
    "pdf_path": "../data/contents/2512.16843v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:15.760019",
    "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference",
    "abstract": "Summary: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications",
    "paper_id": "2512.16843v1"
  },
  "https://arxiv.org/abs/2512.12824v1": {
    "content_path": "../data/contents/2512.12824v1.md",
    "pdf_path": "../data/contents/2512.12824v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:16.554474",
    "title": "Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners",
    "abstract": "Summary: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an \"augmentation divergence\": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.",
    "paper_id": "2512.12824v1"
  },
  "https://arxiv.org/abs/2512.16832v1": {
    "content_path": "../data/contents/2512.16832v1.md",
    "pdf_path": "../data/contents/2512.16832v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:16.890913",
    "title": "What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels",
    "abstract": "Summary: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.",
    "paper_id": "2512.16832v1"
  },
  "https://arxiv.org/abs/2512.12818v1": {
    "content_path": "../data/contents/2512.12818v1.md",
    "pdf_path": "../data/contents/2512.12818v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:17.205249",
    "title": "Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects",
    "abstract": "Summary: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.",
    "paper_id": "2512.12818v1"
  },
  "https://arxiv.org/abs/2510.14913": {
    "content_path": "../data/contents/2510.14913.md",
    "pdf_path": "../data/contents/2510.14913.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:19.133512",
    "title": "Budget-aware Test-time Scaling via Discriminative Verification",
    "abstract": "Proposes discriminative verification as a compute-efficient alternative to generative verifiers, improving performance under fixed inference budgets.",
    "paper_id": "2510.14913"
  },
  "https://arxiv.org/abs/2512.16822v1": {
    "content_path": "../data/contents/2512.16822v1.md",
    "pdf_path": "../data/contents/2512.16822v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:20.874521",
    "title": "MEPIC: Memory Efficient Position Independent Caching for LLM Serving",
    "abstract": "Summary: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.\n  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.",
    "paper_id": "2512.16822v1"
  },
  "https://arxiv.org/abs/2512.12816v1": {
    "content_path": "../data/contents/2512.12816v1.md",
    "pdf_path": "../data/contents/2512.12816v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:21.885858",
    "title": "Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift",
    "abstract": "Summary: We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.",
    "paper_id": "2512.12816v1"
  },
  "https://arxiv.org/abs/2510.02611": {
    "content_path": "../data/contents/2510.02611.md",
    "pdf_path": "../data/contents/2510.02611.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:22.648865",
    "title": "On the Role of Temperature Sampling in Test-Time Scaling",
    "abstract": "Shows single-temperature best-of-K saturates and proposes multi-temperature scaling to enlarge the set of problems solved at test time.",
    "paper_id": "2510.02611"
  },
  "https://arxiv.org/abs/2512.16814v1": {
    "content_path": "../data/contents/2512.16814v1.md",
    "pdf_path": "../data/contents/2512.16814v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:22.889213",
    "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs",
    "abstract": "Summary: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
    "paper_id": "2512.16814v1"
  },
  "https://arxiv.org/abs/2512.12812v1": {
    "content_path": "../data/contents/2512.12812v1.md",
    "pdf_path": "../data/contents/2512.12812v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:23.305475",
    "title": "Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA",
    "abstract": "Summary: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.\n  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.",
    "paper_id": "2512.12812v1"
  },
  "https://arxiv.org/abs/2512.16802v1": {
    "content_path": "../data/contents/2512.16802v1.md",
    "pdf_path": "../data/contents/2512.16802v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:23.509893",
    "title": "Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology",
    "abstract": "Summary: Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.",
    "paper_id": "2512.16802v1"
  },
  "https://arxiv.org/abs/2512.14753v1": {
    "content_path": "../data/contents/2512.14753v1.md",
    "pdf_path": "../data/contents/2512.14753v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:24.169297",
    "title": "CODE ACROSTIC: Robust Watermarking for Code Generation",
    "abstract": "Summary: Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.",
    "paper_id": "2512.14753v1"
  },
  "https://arxiv.org/abs/2512.16795v1": {
    "content_path": "../data/contents/2512.16795v1.md",
    "pdf_path": "../data/contents/2512.16795v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:24.413572",
    "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
    "abstract": "Summary: Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.",
    "paper_id": "2512.16795v1"
  },
  "https://arxiv.org/abs/2512.12806v1": {
    "content_path": "../data/contents/2512.12806v1.md",
    "pdf_path": "../data/contents/2512.12806v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:24.736801",
    "title": "Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution",
    "abstract": "Summary: The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\\% interception rate for high-risk commands and a 100\\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication (\"Sign in\"), rendering it unusable for headless, autonomous agent workflows.",
    "paper_id": "2512.12806v1"
  },
  "https://arxiv.org/abs/2510.18982": {
    "content_path": "../data/contents/2510.18982.md",
    "pdf_path": "../data/contents/2510.18982.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:26.224643",
    "title": "Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality",
    "abstract": "Theory for verifier-based test-time scaling analyzing generator coverage, verifier ROC, and sampling sub-optimality tradeoffs.",
    "paper_id": "2510.18982"
  },
  "https://arxiv.org/abs/2512.16975v1": {
    "content_path": "../data/contents/2512.16975v1.md",
    "pdf_path": "../data/contents/2512.16975v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:27.644661",
    "title": "InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression",
    "abstract": "Summary: Accurate and efficient discrete video tokenization is essential for long video sequences processing. Yet, the inherent complexity and variable information density of videos present a significant bottleneck for current tokenizers, which rigidly compress all content at a fixed rate, leading to redundancy or information loss. Drawing inspiration from Shannon's information theory, this paper introduces InfoTok, a principled framework for adaptive video tokenization. We rigorously prove that existing data-agnostic training methods are suboptimal in representation length, and present a novel evidence lower bound (ELBO)-based algorithm that approaches theoretical optimality. Leveraging this framework, we develop a transformer-based adaptive compressor that enables adaptive tokenization. Empirical results demonstrate state-of-the-art compression performance, saving 20% tokens without influence on performance, and achieving 2.3x compression rates while still outperforming prior heuristic adaptive approaches. By allocating tokens according to informational richness, InfoTok enables a more compressed yet accurate tokenization for video representation, offering valuable insights for future research.",
    "paper_id": "2512.16975v1"
  },
  "https://arxiv.org/abs/2512.12805v1": {
    "content_path": "../data/contents/2512.12805v1.md",
    "pdf_path": "../data/contents/2512.12805v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:28.191678",
    "title": "From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs",
    "abstract": "Summary: Transformers exhibit a notable property of \\emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.",
    "paper_id": "2512.12805v1"
  },
  "https://arxiv.org/abs/2510.09599": {
    "content_path": "../data/contents/2510.09599.md",
    "pdf_path": "../data/contents/2510.09599.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:29.045950",
    "title": "Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation",
    "abstract": "Uses test-time scaling prompting to synthesize diverse reasoning trajectories for finetuning—bridges inference-time scaling and training improvements.",
    "paper_id": "2510.09599"
  },
  "https://arxiv.org/abs/2512.16770v1": {
    "content_path": "../data/contents/2512.16770v1.md",
    "pdf_path": "../data/contents/2512.16770v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:29.290985",
    "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation",
    "abstract": "Summary: Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.",
    "paper_id": "2512.16770v1"
  },
  "https://arxiv.org/abs/2512.12801v1": {
    "content_path": "../data/contents/2512.12801v1.md",
    "pdf_path": "../data/contents/2512.12801v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:33.084087",
    "title": "Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P",
    "abstract": "Summary: With the widespread adoption of Large Language Models (LLMs), energy costs of running LLMs is quickly becoming a critical concern. However, precisely measuring the energy consumption of LLMs is often infeasible because hardware-based power monitors are not always accessible and software-based energy measurement tools are not accurate. While various prediction techniques have been developed to estimate LLM energy consumption, these approaches are limited to single-GPU environments and thus are not applicable to modern LLM inference which is typically parallelized across multiple GPUs. In this work, we remedy this gap and introduce PIE-P, a fine-grained energy prediction framework for multi-GPU inference, including tensor, pipeline, and data parallelism. Predicting the energy under parallelized inference is complicated by the non-determinism in inter-GPU communication, additional communication overheads, and difficulties in isolating energy during the communication/synchronization phase. We develop a scalable prediction framework that addresses these issues via precise sampling, fine-grained modeling of inter-GPU communication, and careful accounting of parallelization overhead. Our evaluation results show that PIE-P yields accurate and fine-grained energy predictions across parallelism strategies, significantly outperforming baselines.",
    "paper_id": "2512.12801v1"
  },
  "https://arxiv.org/abs/2512.16762v1": {
    "content_path": "../data/contents/2512.16762v1.md",
    "pdf_path": "../data/contents/2512.16762v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:35.937540",
    "title": "NRGPT: An Energy-based Alternative for GPT",
    "abstract": "Summary: Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don't necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.",
    "paper_id": "2512.16762v1"
  },
  "https://arxiv.org/abs/2512.12792v1": {
    "content_path": "../data/contents/2512.12792v1.md",
    "pdf_path": "../data/contents/2512.12792v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:36.241227",
    "title": "Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks",
    "abstract": "Summary: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.",
    "paper_id": "2512.12792v1"
  },
  "https://arxiv.org/abs/2512.16750v1": {
    "content_path": "../data/contents/2512.16750v1.md",
    "pdf_path": "../data/contents/2512.16750v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:36.678522",
    "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error",
    "abstract": "Summary: Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.",
    "paper_id": "2512.16750v1"
  },
  "https://arxiv.org/abs/2512.12791v2": {
    "content_path": "../data/contents/2512.12791v2.md",
    "pdf_path": "../data/contents/2512.12791v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:37.564726",
    "title": "Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems",
    "abstract": "Summary: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. These challenges emerged during our ongoing industry collaboration with MontyCloud Inc., when we deployed an agentic system in production. These limitations surfaced during deployment, highlighting practical gaps in the current evaluation methods and the need for a systematic assessment of agent behavior beyond task outcomes. Informed by these observations and established definitions of agentic systems, we propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.",
    "paper_id": "2512.12791v2"
  },
  "https://arxiv.org/abs/2512.16733v2": {
    "content_path": "../data/contents/2512.16733v2.md",
    "pdf_path": "../data/contents/2512.16733v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:39.111833",
    "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities",
    "abstract": "Summary: Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.",
    "paper_id": "2512.16733v2"
  },
  "https://arxiv.org/abs/2512.13741v1": {
    "content_path": "../data/contents/2512.13741v1.md",
    "pdf_path": "../data/contents/2512.13741v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:39.672340",
    "title": "The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models",
    "abstract": "Summary: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial \"jailbreaking\" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy \"reflex-based\" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.",
    "paper_id": "2512.13741v1"
  },
  "https://arxiv.org/html/2505.16782v1": {
    "content_path": "../data/contents/2505.16782v1.md",
    "pdf_path": "../data/contents/2505.16782v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:40.132132",
    "title": "A Comprehensive Survey on Latent Chain-of-Thought Reasoning",
    "abstract": "The provided web page is a comprehensive survey on **Latent Chain-of-Thought (CoT) Reasoning** in Large Language Models (LLMs). It focuses on moving reasoning beyond explicit, verbalized natural language steps into internal, latent spaces for richer representations, faster inference, and handling abstract reasoning.\n\nHere is a summary of the page content relevant to your query:\n\n**Reasoning and Planning with LLMs (Focusing on Latent CoT):**\n\n*   **Chain-of-Thought (CoT) and Latent CoT:** The paper contrasts conventional explicit CoT (reasoning steps verbalized in natural language) with **Latent CoT**, where inference occurs in latent spaces, decoupling reasoning from language.\n*   **Token-wise Strategies:** These methods use special tokens to streamline reasoning:\n    *   **Discrete Tokens:** Symbolic markers (like \"pause tokens\" or \"planning tokens\") that segment or control internal computation.\n    *   **Continuous Tokens:** Learned embeddings in latent spaces that represent intermediate reasoning states implicitly (e.g., COCONUT, CODI, SoftCoT).\n*   **Internal Mechanisms:** These focus on how reasoning emerges implicitly within the model's structure or representations:\n    *   **Structural CoT:** Uses architectural depth, recurrence, and looping computations (e.g., CoTFormer, Huginn, RELAY) to simulate iterative reasoning steps.\n    *   **Representational CoT:** Internalizes explicit CoT directly into latent hidden states, often via knowledge distillation or fine-tuning (e.g., STaR, ICoT).\n*   **Planning with LLMs:** While the paper focuses on *latent* reasoning, it discusses methods that use specialized tokens (like \"planning tokens\") to improve coherence and precision, which relates to planning. Furthermore, the future directions mention that latent CoT is expected to lead to more compact and faster **planning and decision-making** for LLM Agents.\n*   **Inference-Time Compute & Test-Time Scaling:** Latent CoT aims for computational efficiency by using compressed representations (continuous tokens) or reusing layers (recurrent structures), which directly impacts inference-time compute. **Test-time scaling** is explicitly mentioned in the context of SoftCoT++, which perturbs the latent space for diverse exploration.\n*   **Self-Reflection:** The analysis section mentions **CoE (Chain-",
    "paper_id": "2505.16782v1"
  },
  "https://arxiv.org/abs/2512.16723v1": {
    "content_path": "../data/contents/2512.16723v1.md",
    "pdf_path": "../data/contents/2512.16723v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:41.846052",
    "title": "KOSS: Kalman-Optimal Selective State Spaces for Long-Term Sequence Modeling",
    "abstract": "Summary: Recent selective state space models (SSMs), such as Mamba and Mamba-2, have demonstrated strong performance in sequence modeling owing to input-dependent selection mechanisms. However, these mechanisms lack theoretical grounding and cannot support context-aware selection from latent state dynamics. To address these limitations, we propose KOSS, a Kalman-optimal Selective State Space model that formulates selection as latent state uncertainty minimization. Derived from estimation theory, KOSS adopts a continuous-time latent update driven by a Kalman gain that dynamically modulates information propagation based on content and context, enabling a closed-loop, context-aware selectivity mechanism. To ensure stable computation and near-linear scalability, KOSS employs global spectral differentiation for frequency-domain derivative estimation, along with a segment-wise scan for hardware-efficient processing. On a selective copying task with distractors, KOSS achieves over 79\\% accuracy while baselines drop below 20\\%, demonstrating robust context-aware selection. Furthermore, across nine long-term forecasting benchmarks, KOSS reduces MSE by 2.92--36.23\\% and consistently outperforms state-of-the-art models in both accuracy and stability. To assess real-world applicability, a case study on secondary surveillance radar (SSR) tracking confirms KOSS's robustness under irregular intervals and noisy conditions and demonstrates its effectiveness in real-world applications. Finally, supplementary experiments verify Kalman gain convergence and the frequency response of spectral differentiation, providing theoretical support for the proposed closed-loop design.",
    "paper_id": "2512.16723v1"
  },
  "https://arxiv.org/abs/2512.12785v1": {
    "content_path": "../data/contents/2512.12785v1.md",
    "pdf_path": "../data/contents/2512.12785v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:44.466308",
    "title": "OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average",
    "abstract": "Summary: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.",
    "paper_id": "2512.12785v1"
  },
  "https://arxiv.org/abs/2512.20607v1": {
    "content_path": "../data/contents/2512.20607v1.md",
    "pdf_path": "../data/contents/2512.20607v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:45.075591",
    "title": "Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures",
    "abstract": "Summary: Neural networks trained with gradient descent often learn solutions of increasing complexity over time, a phenomenon known as simplicity bias. Despite being widely observed across architectures, existing theoretical treatments lack a unifying framework. We present a theoretical framework that explains a simplicity bias arising from saddle-to-saddle learning dynamics for a general class of neural networks, incorporating fully-connected, convolutional, and attention-based architectures. Here, simple means expressible with few hidden units, i.e., hidden neurons, convolutional kernels, or attention heads. Specifically, we show that linear networks learn solutions of increasing rank, ReLU networks learn solutions with an increasing number of kinks, convolutional networks learn solutions with an increasing number of convolutional kernels, and self-attention models learn solutions with an increasing number of attention heads. By analyzing fixed points, invariant manifolds, and dynamics of gradient descent learning, we show that saddle-to-saddle dynamics operates by iteratively evolving near an invariant manifold, approaching a saddle, and switching to another invariant manifold. Our analysis also illuminates the effects of data distribution and weight initialization on the duration and number of plateaus in learning, dissociating previously confounding factors. Overall, our theory offers a framework for understanding when and why gradient descent progressively learns increasingly complex solutions.",
    "paper_id": "2512.20607v1"
  },
  "https://arxiv.org/abs/2512.18117v1": {
    "content_path": "../data/contents/2512.18117v1.md",
    "pdf_path": "../data/contents/2512.18117v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:45.415666",
    "title": "Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning",
    "abstract": "Summary: The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search.",
    "paper_id": "2512.18117v1"
  },
  "https://arxiv.org/abs/2512.13892v2": {
    "content_path": "../data/contents/2512.13892v2.md",
    "pdf_path": "../data/contents/2512.13892v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:10:55.786573",
    "title": "One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing",
    "abstract": "Summary: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.",
    "paper_id": "2512.13892v2"
  },
  "https://arxiv.org/abs/2512.20605v1": {
    "content_path": "../data/contents/2512.20605v1.md",
    "pdf_path": "../data/contents/2512.20605v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:00.071917",
    "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
    "abstract": "Summary: Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.",
    "paper_id": "2512.20605v1"
  },
  "https://arxiv.org/abs/2512.18115v1": {
    "content_path": "../data/contents/2512.18115v1.md",
    "pdf_path": "../data/contents/2512.18115v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:00.576010",
    "title": "Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown",
    "abstract": "Summary: Academic documents stored in PDF format can be transformed into plain text structured markup languages to enhance accessibility and enable scalable digital library workflows. Markup languages allow for easier updates and customization, making academic content more adaptable and accessible to diverse usage, such as linguistic corpus compilation. Such documents, typically delivered in PDF format, contain complex elements including mathematical formulas, figures, headers, and tables, as well as densely layouted text. Existing end-to-end decoder transformer models can transform screenshots of documents into markup language. However, these models exhibit significant inefficiencies; their token-by-token decoding from scratch wastes a lot of inference steps in regenerating dense text that could be directly copied from PDF files. To solve this problem, we introduce EditTrans, a hybrid editing-generation model whose features allow identifying a queue of to-be-edited text from a PDF before starting to generate markup language. EditTrans contains a lightweight classifier fine-tuned from a Document Layout Analysis model on 162,127 pages of documents from arXiv. In our evaluations, EditTrans reduced the transformation latency up to 44.5% compared to end-to-end decoder transformer models, while maintaining transformation quality. Our code and reproducible dataset production scripts are open-sourced.",
    "paper_id": "2512.18115v1"
  },
  "https://arxiv.org/abs/2512.13886v1": {
    "content_path": "../data/contents/2512.13886v1.md",
    "pdf_path": "../data/contents/2512.13886v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:03.061332",
    "title": "OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction",
    "abstract": "Summary: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.",
    "paper_id": "2512.13886v1"
  },
  "https://arxiv.org/abs/2512.20604v1": {
    "content_path": "../data/contents/2512.20604v1.md",
    "pdf_path": "../data/contents/2512.20604v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:05.973849",
    "title": "MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts",
    "abstract": "Summary: We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.",
    "paper_id": "2512.20604v1"
  },
  "https://arxiv.org/abs/2512.18094v1": {
    "content_path": "../data/contents/2512.18094v1.md",
    "pdf_path": "../data/contents/2512.18094v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:07.955721",
    "title": "Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks",
    "abstract": "Summary: Large language models (LLMs) have enabled multi-agent systems (MAS) in which multiple agents argue, critique, and coordinate to solve complex tasks, making communication topology a first-class design choice. Yet most existing LLM-based MAS either adopt fully connected graphs, simple sparse rings, or ad-hoc dynamic selection, with little structural guidance. In this work, we revisit classic theory on small-world (SW) networks and ask: what changes if we treat SW connectivity as a design prior for MAS? We first bridge insights from neuroscience and complex networks to MAS, highlighting how SW structures balance local clustering and long-range integration. Using multi-agent debate (MAD) as a controlled testbed, experiment results show that SW connectivity yields nearly the same accuracy and token cost, while substantially stabilizing consensus trajectories. Building on this, we introduce an uncertainty-guided rewiring scheme for scaling MAS, where long-range shortcuts are added between epistemically divergent agents using LLM-oriented uncertainty signals (e.g., semantic entropy). This yields controllable SW structures that adapt to task difficulty and agent heterogeneity. Finally, we discuss broader implications of SW priors for MAS design, framing them as stabilizers of reasoning, enhancers of robustness, scalable coordinators, and inductive biases for emergent cognitive roles.",
    "paper_id": "2512.18094v1"
  },
  "https://arxiv.org/abs/2512.13884v1": {
    "content_path": "../data/contents/2512.13884v1.md",
    "pdf_path": "../data/contents/2512.13884v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:08.463080",
    "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition",
    "abstract": "Summary: Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.",
    "paper_id": "2512.13884v1"
  },
  "https://arxiv.org/abs/2512.20595v1": {
    "content_path": "../data/contents/2512.20595v1.md",
    "pdf_path": "../data/contents/2512.20595v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:08.809512",
    "title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs",
    "abstract": "Summary: We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.",
    "paper_id": "2512.20595v1"
  },
  "https://arxiv.org/abs/2512.18092v1": {
    "content_path": "../data/contents/2512.18092v1.md",
    "pdf_path": "../data/contents/2512.18092v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:12.314996",
    "title": "Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability",
    "abstract": "Summary: Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, which allows us to derive guarantees for neuron explanations. Based on this insight, we present the first theoretical analysis of two fundamental challenges: (1) Faithfulness: whether the identified concept faithfully represents the neuron's underlying function and (2) Stability: whether the identification results are consistent across probing datasets. We derive generalization bounds for widely used similarity metrics (e.g. accuracy, AUROC, IoU) to guarantee faithfulness, and propose a bootstrap ensemble procedure that quantifies stability along with BE (Bootstrap Explanation) method to generate concept prediction sets with guaranteed coverage probability. Experiments on both synthetic and real data validate our theoretical results and demonstrate the practicality of our method, providing an important step toward trustworthy neuron identification.",
    "paper_id": "2512.18092v1"
  },
  "https://arxiv.org/abs/2512.14762v1": {
    "content_path": "../data/contents/2512.14762v1.md",
    "pdf_path": "../data/contents/2512.14762v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:12.659149",
    "title": "Workflows vs Agents for Code Translation",
    "abstract": "Summary: Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \\cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.",
    "paper_id": "2512.14762v1"
  },
  "https://arxiv.org/abs/2512.20586v1": {
    "content_path": "../data/contents/2512.20586v1.md",
    "pdf_path": "../data/contents/2512.20586v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:13.671425",
    "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent",
    "abstract": "Summary: Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.",
    "paper_id": "2512.20586v1"
  },
  "https://arxiv.org/abs/2512.18083v1": {
    "content_path": "../data/contents/2512.18083v1.md",
    "pdf_path": "../data/contents/2512.18083v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:13.906105",
    "title": "Causal Inference as Distribution Adaptation: Optimizing ATE Risk under Propensity Uncertainty",
    "abstract": "Summary: Standard approaches to causal inference, such as Outcome Regression and Inverse Probability Weighted Regression Adjustment (IPWRA), are typically derived through the lens of missing data imputation and identification theory. In this work, we unify these methods from a Machine Learning perspective, reframing ATE estimation as a \\textit{domain adaptation problem under distribution shift}. We demonstrate that the canonical Hajek estimator is a special case of IPWRA restricted to a constant hypothesis class, and that IPWRA itself is fundamentally Importance-Weighted Empirical Risk Minimization designed to correct for the covariate shift between the treated sub-population and the target population.\n  Leveraging this unified framework, we critically examine the optimization objectives of Doubly Robust estimators. We argue that standard methods enforce \\textit{sufficient but not necessary} conditions for consistency by requiring outcome models to be individually unbiased. We define the true \"ATE Risk Function\" and show that minimizing it requires only that the biases of the treated and control models structurally cancel out. Exploiting this insight, we propose the \\textbf{Joint Robust Estimator (JRE)}. Instead of treating propensity estimation and outcome modeling as independent stages, JRE utilizes bootstrap-based uncertainty quantification of the propensity score to train outcome models jointly. By optimizing for the expected ATE risk over the distribution of propensity scores, JRE leverages model degrees of freedom to achieve robustness against propensity misspecification. Simulation studies demonstrate that JRE achieves up to a 15\\% reduction in MSE compared to standard IPWRA in finite-sample regimes with misspecified outcome models.",
    "paper_id": "2512.18083v1"
  },
  "https://arxiv.org/abs/2512.13872v1": {
    "content_path": "../data/contents/2512.13872v1.md",
    "pdf_path": "../data/contents/2512.13872v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:14.879054",
    "title": "Measuring Uncertainty Calibration",
    "abstract": "Summary: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.",
    "paper_id": "2512.13872v1"
  },
  "https://arxiv.org/abs/2512.20578v1": {
    "content_path": "../data/contents/2512.20578v1.md",
    "pdf_path": "../data/contents/2512.20578v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:17.026178",
    "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
    "abstract": "Summary: Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.",
    "paper_id": "2512.20578v1"
  },
  "https://arxiv.org/abs/2512.18082v1": {
    "content_path": "../data/contents/2512.18082v1.md",
    "pdf_path": "../data/contents/2512.18082v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:17.502902",
    "title": "Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation",
    "abstract": "Summary: Semantic segmentation of outdoor street scenes plays a key role in applications such as autonomous driving, mobile robotics, and assistive technology for visually-impaired pedestrians. For these applications, accurately distinguishing between key surfaces and objects such as roads, sidewalks, vehicles, and pedestrians is essential for maintaining safety and minimizing risks. Semantic segmentation must be robust to different environments, lighting and weather conditions, and sensor noise, while being performed in real-time. We propose a region-level, uncertainty-gated retrieval mechanism that improves segmentation accuracy and calibration under domain shift. Our best method achieves an 11.3% increase in mean intersection-over-union while reducing retrieval cost by 87.5%, retrieving for only 12.5% of regions compared to 100% for always-on baseline.",
    "paper_id": "2512.18082v1"
  },
  "https://arxiv.org/abs/2512.13860v1": {
    "content_path": "../data/contents/2512.13860v1.md",
    "pdf_path": "../data/contents/2512.13860v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:23.246573",
    "title": "Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors",
    "abstract": "Summary: Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.",
    "paper_id": "2512.13860v1"
  },
  "https://arxiv.org/abs/2512.20577v1": {
    "content_path": "../data/contents/2512.20577v1.md",
    "pdf_path": "../data/contents/2512.20577v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:23.612042",
    "title": "Improving ML Training Data with Gold-Standard Quality Metrics",
    "abstract": "Summary: Hand-tagged training data is essential to many machine learning tasks. However, training data quality control has received little attention in the literature, despite data quality varying considerably with the tagging exercise. We propose methods to evaluate and enhance the quality of hand-tagged training data using statistical approaches to measure tagging consistency and agreement. We show that agreement metrics give more reliable results if recorded over multiple iterations of tagging, where declining variance in such recordings is an indicator of increasing data quality. We also show one way a tagging project can collect high-quality training data without requiring multiple tags for every work item, and that a tagger burn-in period may not be sufficient for minimizing tagger errors.",
    "paper_id": "2512.20577v1"
  },
  "https://arxiv.org/abs/2512.18080v1": {
    "content_path": "../data/contents/2512.18080v1.md",
    "pdf_path": "../data/contents/2512.18080v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:26.415570",
    "title": "From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems",
    "abstract": "Summary: Agentic AI systems capable of generating full-stack web applications from natural language prompts (\"prompt- to-app\") represent a significant shift in software development. However, evaluating these systems remains challenging, as visual polish, functional correctness, and user trust are often misaligned. As a result, it is unclear how existing prompt-to-app tools compare under realistic, human-centered evaluation criteria. In this paper, we introduce a human-centered benchmark for evaluating prompt-to-app systems and conduct a large-scale comparative study of three widely used platforms: Replit, Bolt, and Firebase Studio. Using a diverse set of 96 prompts spanning common web application tasks, we generate 288 unique application artifacts. We evaluate these systems through a large-scale human-rater study involving 205 participants and 1,071 quality-filtered pairwise comparisons, assessing task-based ease of use, visual appeal, perceived completeness, and user trust. Our results show that these systems are not interchangeable: Firebase Studio consistently outperforms competing platforms across all human-evaluated dimensions, achieving the highest win rates for ease of use, trust, visual appeal, and visual appropriateness. Bolt performs competitively on visual appeal but trails Firebase on usability and trust, while Replit underperforms relative to both across most metrics. These findings highlight a persistent gap between visual polish and functional reliability in prompt-to-app systems and demonstrate the necessity of interactive, task-based evaluation. We release our benchmark framework, prompt set, and generated artifacts to support reproducible evaluation and future research in agentic application generation.",
    "paper_id": "2512.18080v1"
  },
  "https://arxiv.org/abs/2512.13857v2": {
    "content_path": "../data/contents/2512.13857v2.md",
    "pdf_path": "../data/contents/2512.13857v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:26.895485",
    "title": "EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery",
    "abstract": "Summary: Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.",
    "paper_id": "2512.13857v2"
  },
  "https://arxiv.org/abs/2512.20576v1": {
    "content_path": "../data/contents/2512.20576v1.md",
    "pdf_path": "../data/contents/2512.20576v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:27.330577",
    "title": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning",
    "abstract": "Summary: Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.",
    "paper_id": "2512.20576v1"
  },
  "https://arxiv.org/abs/2512.18060v1": {
    "content_path": "../data/contents/2512.18060v1.md",
    "pdf_path": "../data/contents/2512.18060v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:30.226910",
    "title": "Graph-based Nearest Neighbors with Dynamic Updates via Random Walks",
    "abstract": "Summary: Approximate nearest neighbor search (ANN) is a common way to retrieve relevant search results, especially now in the context of large language models and retrieval augmented generation. One of the most widely used algorithms for ANN is based on constructing a multi-layer graph over the dataset, called the Hierarchical Navigable Small World (HNSW). While this algorithm supports insertion of new data, it does not support deletion of existing data. Moreover, deletion algorithms described by prior work come at the cost of increased query latency, decreased recall, or prolonged deletion time. In this paper, we propose a new theoretical framework for graph-based ANN based on random walks. We then utilize this framework to analyze a randomized deletion approach that preserves hitting time statistics compared to the graph before deleting the point. We then turn this theoretical framework into a deterministic deletion algorithm, and show that it provides better tradeoff between query latency, recall, deletion time, and memory usage through an extensive collection of experiments.",
    "paper_id": "2512.18060v1"
  },
  "https://arxiv.org/abs/2512.13855v1": {
    "content_path": "../data/contents/2512.13855v1.md",
    "pdf_path": "../data/contents/2512.13855v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:31.636270",
    "title": "Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging",
    "abstract": "Summary: Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.",
    "paper_id": "2512.13855v1"
  },
  "https://arxiv.org/abs/2512.20573v1": {
    "content_path": "../data/contents/2512.20573v1.md",
    "pdf_path": "../data/contents/2512.20573v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:32.870071",
    "title": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
    "abstract": "Summary: Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.4$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.",
    "paper_id": "2512.20573v1"
  },
  "https://arxiv.org/abs/2512.18043v1": {
    "content_path": "../data/contents/2512.18043v1.md",
    "pdf_path": "../data/contents/2512.18043v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:33.079440",
    "title": "Securing Agentic AI Systems -- A Multilayer Security Framework",
    "abstract": "Summary: Securing Agentic Artificial Intelligence (AI) systems requires addressing the complex cyber risks introduced by autonomous, decision-making, and adaptive behaviors. Agentic AI systems are increasingly deployed across industries, organizations, and critical sectors such as cybersecurity, finance, and healthcare. However, their autonomy introduces unique security challenges, including unauthorized actions, adversarial manipulation, and dynamic environmental interactions. Existing AI security frameworks do not adequately address these challenges or the unique nuances of agentic AI. This research develops a lifecycle-aware security framework specifically designed for agentic AI systems using the Design Science Research (DSR) methodology. The paper introduces MAAIS, an agentic security framework, and the agentic AI CIAA (Confidentiality, Integrity, Availability, and Accountability) concept. MAAIS integrates multiple defense layers to maintain CIAA across the AI lifecycle. Framework validation is conducted by mapping with the established MITRE ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems) AI tactics. The study contributes a structured, standardized, and framework-based approach for the secure deployment and governance of agentic AI in enterprise environments. This framework is intended for enterprise CISOs, security, AI platform, and engineering teams and offers a detailed step-by-step approach to securing agentic AI workloads.",
    "paper_id": "2512.18043v1"
  },
  "https://arxiv.org/abs/2512.13837v1": {
    "content_path": "../data/contents/2512.13837v1.md",
    "pdf_path": "../data/contents/2512.13837v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:33.996623",
    "title": "Explainable reinforcement learning from human feedback to improve alignment",
    "abstract": "Summary: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.",
    "paper_id": "2512.13837v1"
  },
  "https://arxiv.org/abs/2512.20569v1": {
    "content_path": "../data/contents/2512.20569v1.md",
    "pdf_path": "../data/contents/2512.20569v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:34.390748",
    "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
    "abstract": "Summary: Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.",
    "paper_id": "2512.20569v1"
  },
  "https://arxiv.org/abs/2512.18041v1": {
    "content_path": "../data/contents/2512.18041v1.md",
    "pdf_path": "../data/contents/2512.18041v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:34.853543",
    "title": "Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts",
    "abstract": "Summary: Processing overlapping narrative documents, such as legal testimonies or historical accounts, often aims not for compression but for a unified, coherent, and chronologically sound text. Standard Multi-Document Summarization (MDS), with its focus on conciseness, fails to preserve narrative flow. This paper formally defines this challenge as a new NLP task: Narrative Consolidation, where the central objectives are chronological integrity, completeness, and the fusion of complementary details. To demonstrate the critical role of temporal structure in this task, we introduce Temporal Alignment Event Graph (TAEG), a graph structure that explicitly models chronology and event alignment. By applying a standard centrality algorithm to TAEG, our method functions as a version selection mechanism, choosing the most central representation of each event in its correct temporal position. In a study on the four Biblical Gospels, this structure-focused approach guarantees perfect temporal ordering (Kendall's Tau of 1.000) by design and dramatically improves content metrics (e.g., +357.2% in ROUGE-L F1). The success of this baseline method validates the formulation of Narrative Consolidation as a relevant task and establishes that an explicit temporal backbone is a fundamental component for its resolution.",
    "paper_id": "2512.18041v1"
  },
  "https://arxiv.org/abs/2512.13821v1": {
    "content_path": "../data/contents/2512.13821v1.md",
    "pdf_path": "../data/contents/2512.13821v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:35.192272",
    "title": "The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces",
    "abstract": "Summary: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.",
    "paper_id": "2512.13821v1"
  },
  "https://arxiv.org/abs/2512.20518v1": {
    "content_path": "../data/contents/2512.20518v1.md",
    "pdf_path": "../data/contents/2512.20518v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:35.891020",
    "title": "Expected Revenue, Risk, and Grid Impact of Bitcoin Mining: A Decision-Theoretic Perspective",
    "abstract": "Summary: Most current assessments use ex post proxies that miss uncertainty and fail to consistently capture the rapid change in bitcoin mining. We introduce a unified, ex ante statistical model that derives expected return, downside risk, and upside potential profit from the first principles of mining: Each hash is a Bernoulli trial with a Bitcoin block difficulty-based success probability. The model yields closed-form expected revenue per hash-rate unit, risk metrics in different scenarios, and upside-profit probabilities for different fleet sizes. Empirical calibration closely matches previously reported observations, yielding a unified, faithful quantification across hardware, pools, and operating conditions. This foundation enables more reliable analysis of mining impacts and behavior.",
    "paper_id": "2512.20518v1"
  },
  "https://arxiv.org/abs/2512.18035v1": {
    "content_path": "../data/contents/2512.18035v1.md",
    "pdf_path": "../data/contents/2512.18035v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:36.146716",
    "title": "Towards Benchmarking Privacy Vulnerabilities in Selective Forgetting with Large Language Models",
    "abstract": "Summary: The rapid advancements in artificial intelligence (AI) have primarily focused on the process of learning from data to acquire knowledgeable learning systems. As these systems are increasingly deployed in critical areas, ensuring their privacy and alignment with human values is paramount. Recently, selective forgetting (also known as machine unlearning) has shown promise for privacy and data removal tasks, and has emerged as a transformative paradigm shift in the field of AI. It refers to the ability of a model to selectively erase the influence of previously seen data, which is especially important for compliance with modern data protection regulations and for aligning models with human values. Despite its promise, selective forgetting raises significant privacy concerns, especially when the data involved come from sensitive domains. While new unlearning-induced privacy attacks are continuously proposed, each is shown to outperform its predecessors using different experimental settings, which can lead to overly optimistic and potentially unfair assessments that may disproportionately favor one particular attack over the others. In this work, we present the first comprehensive benchmark for evaluating privacy vulnerabilities in selective forgetting. We extensively investigate privacy vulnerabilities of machine unlearning techniques and benchmark privacy leakage across a wide range of victim data, state-of-the-art unlearning privacy attacks, unlearning methods, and model architectures. We systematically evaluate and identify critical factors related to unlearning-induced privacy leakage. With our novel insights, we aim to provide a standardized tool for practitioners seeking to deploy customized unlearning applications with faithful privacy assessments.",
    "paper_id": "2512.18035v1"
  },
  "https://arxiv.org/abs/2512.13788v1": {
    "content_path": "../data/contents/2512.13788v1.md",
    "pdf_path": "../data/contents/2512.13788v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:36.841461",
    "title": "Constrained Policy Optimization via Sampling-Based Weight-Space Projection",
    "abstract": "Summary: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.",
    "paper_id": "2512.13788v1"
  },
  "https://arxiv.org/abs/2512.02008": {
    "content_path": "../data/contents/2512.02008.md",
    "pdf_path": "../data/contents/2512.02008.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:38.773602",
    "title": "The Art of Scaling Test-Time Compute for Large Language Models",
    "abstract": "Large-scale empirical comparison of test-time scaling strategies across models/datasets, offering guidance for practical TTS selection.",
    "paper_id": "2512.02008"
  },
  "https://arxiv.org/abs/2512.20515v1": {
    "content_path": "../data/contents/2512.20515v1.md",
    "pdf_path": "../data/contents/2512.20515v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:39.779341",
    "title": "Modeling Bank Systemic Risk of Emerging Markets under Geopolitical Shocks: Empirical Evidence from BRICS Countries",
    "abstract": "Summary: The growing economic influence of the BRICS nations requires risk models that capture complex, long-term dynamics. This paper introduces the Bank Risk Interlinkage with Dynamic Graph and Event Simulations (BRIDGES) framework, which analyzes systemic risk based on the level of information complexity (zero-order, first-order, and second-order). BRIDGES utilizes the Dynamic Time Warping (DTW) distance to construct a dynamic network for 551 BRICS banks based on their strategic similarity, using zero-order information such as annual balance sheet data from 2008 to 2024. It then employs first-order information, including trends in risk ratios, to detect shifts in banks' behavior. A Temporal Graph Neural Network (TGNN), as the core of BRIDGES, is deployed to learn network evolutions and detect second-order information, such as anomalous changes in the structural relationships of the bank network. To measure the impact of anomalous changes on network stability, BRIDGES performs Agent-Based Model (ABM) simulations to assess the banking system's resilience to internal financial failure and external geopolitical shocks at the individual country level and across BRICS nations. Simulation results show that the failure of the largest institutions causes more systemic damage than the failure of the financially vulnerable or dynamically anomalous ones, driven by powerful panic effects. Compared to this \"too big to fail\" scenario, a geopolitical shock with correlated country-wide propagation causes more destructive systemic damage, leading to a near-total systemic collapse. It suggests that the primary threats to BRICS financial stability are second-order panic and large-scale geopolitical shocks, which traditional risk analysis models might not detect.",
    "paper_id": "2512.20515v1"
  },
  "https://arxiv.org/abs/2512.18034v1": {
    "content_path": "../data/contents/2512.18034v1.md",
    "pdf_path": "../data/contents/2512.18034v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:40.261116",
    "title": "Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout",
    "abstract": "Summary: This paper studies the use of Conflict-Driven Clause Learning (CDCL) with VSIDS heuristics as a computational engine for discrete facility layout problems. The facility layout problem is modeled as a combinatorial assignment problem with dense logical structure arising from adjacency, separation, and slot-availability constraints. We develop a CNF-based formulation for layout feasibility and compare CDCL-based SAT solving against CP-SAT and MILP formulations under a unified benchmarking framework. Empirical results show that CDCL exhibits near-constant runtime behavior for feasibility detection across increasing problem sizes and constraint densities, while CP-SAT and MILP display polynomial and exponential scaling respectively. To address the limitation of CDCL in objective optimization, we introduce two hybrid architectures that combine CDCL-based feasibility search with CP-SAT optimization. The first architecture rapidly enumerates feasible layouts to trade optimality for speed, while the second uses CDCL to generate warm-start solutions that accelerate exact optimization. The results demonstrate that hybrid approaches can significantly reduce time-to-solution while preserving correctness guarantees, clarifying the algorithmic trade-offs between clause-learning search and exact optimization methods in large-scale discrete layout problems.",
    "paper_id": "2512.18034v1"
  },
  "https://arxiv.org/abs/2512.13685v1": {
    "content_path": "../data/contents/2512.13685v1.md",
    "pdf_path": "../data/contents/2512.13685v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:40.909061",
    "title": "Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech",
    "abstract": "Summary: Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.",
    "paper_id": "2512.13685v1"
  },
  "https://arxiv.org/abs/2512.20514v1": {
    "content_path": "../data/contents/2512.20514v1.md",
    "pdf_path": "../data/contents/2512.20514v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:42.633864",
    "title": "Explainable time-series forecasting with sampling-free SHAP for Transformers",
    "abstract": "Summary: Time-series forecasts are essential for planning and decision-making in many domains. Explainability is key to building user trust and meeting transparency requirements. Shapley Additive Explanations (SHAP) is a popular explainable AI framework, but it lacks efficient implementations for time series and often assumes feature independence when sampling counterfactuals. We introduce SHAPformer, an accurate, fast and sampling-free explainable time-series forecasting model based on the Transformer architecture. It leverages attention manipulation to make predictions based on feature subsets. SHAPformer generates explanations in under one second, several orders of magnitude faster than the SHAP Permutation Explainer. On synthetic data with ground truth explanations, SHAPformer provides explanations that are true to the data. Applied to real-world electrical load data, it achieves competitive predictive performance and delivers meaningful local and global insights, such as identifying the past load as the key predictor and revealing a distinct model behavior during the Christmas period.",
    "paper_id": "2512.20514v1"
  },
  "https://arxiv.org/abs/2512.18027v1": {
    "content_path": "../data/contents/2512.18027v1.md",
    "pdf_path": "../data/contents/2512.18027v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:44.085256",
    "title": "CoPE: A Small Language Model for Steerable and Scalable Content Labeling",
    "abstract": "Summary: This paper details the methodology behind CoPE, a policy-steerable small language model capable of fast and accurate content labeling. We present a novel training curricula called Contradictory Example Training that enables the model to learn policy interpretation rather than mere policy memorization. We also present a novel method for generating content policies, called Binocular Labeling, which enables rapid construction of unambiguous training datasets. When evaluated across seven different harm areas, CoPE exhibits equal or superior accuracy to frontier models at only 1% of their size. We openly release a 9 billion parameter version of the model that can be run on a single consumer-grade GPU. Models like CoPE represent a paradigm shift for classifier systems. By turning an ML task into a policy writing task, CoPE opens up new design possibilities for the governance of online platforms.",
    "paper_id": "2512.18027v1"
  },
  "https://arxiv.org/abs/2512.14761v1": {
    "content_path": "../data/contents/2512.14761v1.md",
    "pdf_path": "../data/contents/2512.14761v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:44.748531",
    "title": "CAPE: Capability Achievement via Policy Execution",
    "abstract": "Summary: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.\n  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.\n  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.\n  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.",
    "paper_id": "2512.14761v1"
  },
  "https://arxiv.org/abs/2510.09988": {
    "content_path": "../data/contents/2510.09988.md",
    "pdf_path": "../data/contents/2510.09988.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:46.175392",
    "title": "Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey",
    "abstract": "Survey/taxonomy linking tree search mechanisms and reward formulation across test-time scaling and self-improving reasoning agents.",
    "paper_id": "2510.09988"
  },
  "https://arxiv.org/abs/2512.20491v1": {
    "content_path": "../data/contents/2512.20491v1.md",
    "pdf_path": "../data/contents/2512.20491v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:47.277596",
    "title": "Step-DeepResearch Technical Report",
    "abstract": "Summary: As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
    "paper_id": "2512.20491v1"
  },
  "https://arxiv.org/abs/2512.19769": {
    "content_path": "../data/contents/2512.19769.md",
    "pdf_path": "../data/contents/2512.19769.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:47.527948",
    "title": "A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows",
    "abstract": "Proposes a DSL separating workflow specification from implementation to speed deployment and A/B test agent orchestration strategies.",
    "paper_id": "2512.19769"
  },
  "https://arxiv.org/abs/2512.18020v1": {
    "content_path": "../data/contents/2512.18020v1.md",
    "pdf_path": "../data/contents/2512.18020v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:48.569962",
    "title": "Specification and Detection of LLM Code Smells",
    "abstract": "Summary: Large Language Models (LLMs) have gained massive popularity in recent years and are increasingly integrated into software systems for diverse purposes. However, poorly integrating them in source code may undermine software system quality. Yet, to our knowledge, there is no formal catalog of code smells specific to coding practices for LLM inference. In this paper, we introduce the concept of LLM code smells and formalize five recurrent problematic coding practices related to LLM inference in software systems, based on relevant literature. We extend the detection tool SpecDetect4AI to cover the newly defined LLM code smells and use it to validate their prevalence in a dataset of 200 open-source LLM systems. Our results show that LLM code smells affect 60.50% of the analyzed systems, with a detection precision of 86.06%.",
    "paper_id": "2512.18020v1"
  },
  "https://arxiv.org/abs/2512.13676v1": {
    "content_path": "../data/contents/2512.13676v1.md",
    "pdf_path": "../data/contents/2512.13676v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:49.289450",
    "title": "Towards Effective Model Editing for LLM Personalization",
    "abstract": "Summary: Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.",
    "paper_id": "2512.13676v1"
  },
  "https://arxiv.org/abs/2512.20482v1": {
    "content_path": "../data/contents/2512.20482v1.md",
    "pdf_path": "../data/contents/2512.20482v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:49.692491",
    "title": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization",
    "abstract": "Summary: Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.",
    "paper_id": "2512.20482v1"
  },
  "https://arxiv.org/abs/2512.12818": {
    "content_path": "../data/contents/2512.12818.md",
    "pdf_path": "../data/contents/2512.12818.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:50.477759",
    "title": "Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects",
    "abstract": "New agent memory architecture with structured networks for facts/experiences/beliefs and operations for retain–recall–reflect.",
    "paper_id": "2512.12818"
  },
  "https://arxiv.org/abs/2512.18014v1": {
    "content_path": "../data/contents/2512.18014v1.md",
    "pdf_path": "../data/contents/2512.18014v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:11:50.835382",
    "title": "ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India",
    "abstract": "Summary: This paper presents an early exploration of reinforcement learning methodologies for legal AI in the Indian context. We introduce Reinforcement Learning-based Legal Reasoning (ReGal), a framework that integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO). Our approach is evaluated across two critical legal tasks: (i) Court Judgment Prediction and Explanation (CJPE), and (ii) Legal Document Summarization. Although the framework underperforms on standard evaluation metrics compared to supervised and proprietary models, it provides valuable insights into the challenges of applying RL to legal texts. These challenges include reward model alignment, legal language complexity, and domain-specific adaptation. Through empirical and qualitative analysis, we demonstrate how RL can be repurposed for high-stakes, long-document tasks in law. Our findings establish a foundation for future work on optimizing legal reasoning pipelines using reinforcement learning, with broader implications for building interpretable and adaptive legal AI systems.",
    "paper_id": "2512.18014v1"
  },
  "https://arxiv.org/abs/2512.13672v1": {
    "content_path": "../data/contents/2512.13672v1.md",
    "pdf_path": "../data/contents/2512.13672v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:09.424635",
    "title": "Directional Textual Inversion for Personalized Text-to-Image Generation",
    "abstract": "Summary: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.",
    "paper_id": "2512.13672v1"
  },
  "https://arxiv.org/abs/2510.24803": {
    "content_path": "../data/contents/2510.24803.md",
    "pdf_path": "../data/contents/2510.24803.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:09.816707",
    "title": "MASPRM: Multi-Agent System Process Reward Model",
    "abstract": "Introduces a process reward model for multi-agent transcripts to steer inference-time beam search/MCTS and spend compute more effectively.",
    "paper_id": "2510.24803"
  },
  "https://arxiv.org/abs/2512.20481v1": {
    "content_path": "../data/contents/2512.20481v1.md",
    "pdf_path": "../data/contents/2512.20481v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:12.839805",
    "title": "Coherence in the brain unfolds across separable temporal regimes",
    "abstract": "Summary: Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders.",
    "paper_id": "2512.20481v1"
  },
  "https://arxiv.org/abs/2512.16962": {
    "content_path": "../data/contents/2512.16962.md",
    "pdf_path": "../data/contents/2512.16962.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:17.120217",
    "title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval",
    "abstract": "Shows how long-term memory / experience retrieval can be poisoned for persistent agent behavior drift—important for memory safety.",
    "paper_id": "2512.16962"
  },
  "https://arxiv.org/abs/2512.18004v1": {
    "content_path": "../data/contents/2512.18004v1.md",
    "pdf_path": "../data/contents/2512.18004v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:17.655183",
    "title": "Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models",
    "abstract": "Summary: Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.",
    "paper_id": "2512.18004v1"
  },
  "https://arxiv.org/abs/2512.13668v1": {
    "content_path": "../data/contents/2512.13668v1.md",
    "pdf_path": "../data/contents/2512.13668v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:18.815993",
    "title": "A Scientific Reasoning Model for Organic Synthesis Procedure Generation",
    "abstract": "Summary: Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.",
    "paper_id": "2512.13668v1"
  },
  "https://arxiv.org/abs/2511.00086": {
    "content_path": "../data/contents/2511.00086.md",
    "pdf_path": "../data/contents/2511.00086.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:19.417534",
    "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
    "abstract": "Formalizes compute-optimal test-time scaling as a graph optimization problem over roles/models/topologies in multi-LLM collaboration.",
    "paper_id": "2511.00086"
  },
  "https://arxiv.org/abs/2501.19393": {
    "content_path": "../data/contents/2501.19393.md",
    "pdf_path": "../data/contents/2501.19393.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:19.793935",
    "title": "s1: Simple test-time scaling",
    "abstract": "Influential open baseline for test-time compute control (“budget forcing”) and small curated reasoning data to elicit stronger reasoning.",
    "paper_id": "2501.19393"
  },
  "https://arxiv.org/abs/2512.20469v1": {
    "content_path": "../data/contents/2512.20469v1.md",
    "pdf_path": "../data/contents/2512.20469v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:23.407673",
    "title": "Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale",
    "abstract": "Summary: AI agents are emerging as a practical way to run multi-step scientific workflows that interleave reasoning with tool use and verification, pointing to a shift from isolated AI-assisted steps toward \\emph{agentic science at scale}. This shift is increasingly feasible, as scientific tools and models can be invoked through stable interfaces and verified with recorded execution traces, and increasingly necessary, as AI accelerates scientific output and stresses the peer-review and publication pipeline, raising the bar for traceability and credible evaluation.\n  However, scaling agentic science remains difficult: workflows are hard to observe and reproduce; many tools and laboratory systems are not agent-ready; execution is hard to trace and govern; and prototype AI Scientist systems are often bespoke, limiting reuse and systematic improvement from real workflow signals.\n  We argue that scaling agentic science requires an infrastructure-and-ecosystem approach, instantiated in Bohrium+SciMaster. Bohrium acts as a managed, traceable hub for AI4S assets -- akin to a HuggingFace of AI for Science -- that turns diverse scientific data, software, compute, and laboratory systems into agent-ready capabilities. SciMaster orchestrates these capabilities into long-horizon scientific workflows, on which scientific agents can be composed and executed. Between infrastructure and orchestration, a \\emph{scientific intelligence substrate} organizes reusable models, knowledge, and components into executable building blocks for workflow reasoning and action, enabling composition, auditability, and improvement through use.\n  We demonstrate this stack with eleven representative master agents in real workflows, achieving orders-of-magnitude reductions in end-to-end scientific cycle time and generating execution-grounded signals from real workloads at multi-million scale.",
    "paper_id": "2512.20469v1"
  },
  "https://arxiv.org/abs/2512.17902v1": {
    "content_path": "../data/contents/2512.17902v1.md",
    "pdf_path": "../data/contents/2512.17902v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:23.718066",
    "title": "Adversarial Robustness of Vision in Open Foundation Models",
    "abstract": "Summary: With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.",
    "paper_id": "2512.17902v1"
  },
  "https://arxiv.org/abs/2512.13658v1": {
    "content_path": "../data/contents/2512.13658v1.md",
    "pdf_path": "../data/contents/2512.13658v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:24.220488",
    "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance",
    "abstract": "Summary: As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.",
    "paper_id": "2512.13658v1"
  },
  "https://arxiv.org/abs/2511.11373": {
    "content_path": "../data/contents/2511.11373.md",
    "pdf_path": "../data/contents/2511.11373.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:24.966357",
    "title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism",
    "abstract": "RL framework that trains multi-agent solver/verifier/corrector pipelines efficiently—relevant to building strong multi-agent reasoning systems.",
    "paper_id": "2511.11373"
  },
  "https://arxiv.org/abs/2512.20460v1": {
    "content_path": "../data/contents/2512.20460v1.md",
    "pdf_path": "../data/contents/2512.20460v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:25.187232",
    "title": "The Aligned Economic Index & The State Switching Model",
    "abstract": "Summary: A growing empirical literature suggests that equity-premium predictability is state dependent, with much of the forecasting power concentrated around recessionary periods \\parencite{Henkel2011,DanglHalling2012,Devpura2018}. I study U.S. stock return predictability across economic regimes and document strong evidence of time-varying expected returns across both expansionary and contractionary states. I contribute in two ways. First, I introduce a state-switching predictive regression in which the market state is defined in real time using the slope of the yield curve. Relative to the standard one-state predictive regression, the state-switching specification increases both in-sample and out-of-sample performance for the set of popular predictors considered by \\textcite{WelchGoyal2008}, improving the out-of-sample performance of most predictors in economically meaningful ways. Second, I propose a new aggregate predictor, the Aligned Economic Index, constructed via partial least squares (PLS). Under the state-switching model, the Aligned Economic Index exhibits statistically and economically significant predictive power in sample and out of sample, and it outperforms widely used benchmark predictors and alternative predictor-combination methods.",
    "paper_id": "2512.20460v1"
  },
  "https://arxiv.org/abs/2512.17901v1": {
    "content_path": "../data/contents/2512.17901v1.md",
    "pdf_path": "../data/contents/2512.17901v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:26.434748",
    "title": "When Reasoning Meets Its Laws",
    "abstract": "Summary: Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/",
    "paper_id": "2512.17901v1"
  },
  "https://arxiv.org/abs/2512.13654v1": {
    "content_path": "../data/contents/2512.13654v1.md",
    "pdf_path": "../data/contents/2512.13654v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:26.613802",
    "title": "Large-Language Memorization During the Classification of United States Supreme Court Cases",
    "abstract": "Summary: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called \"hallucinations\" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.",
    "paper_id": "2512.13654v1"
  },
  "https://arxiv.org/abs/2511.17330": {
    "content_path": "../data/contents/2511.17330.md",
    "pdf_path": "../data/contents/2511.17330.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:27.155176",
    "title": "Agentic Program Verification",
    "abstract": "LLM agent that iteratively refines proofs via interaction with the Rocq/Coq theorem prover—tight tool-feedback loops for structured reasoning.",
    "paper_id": "2511.17330"
  },
  "https://arxiv.org/abs/2512.20458v1": {
    "content_path": "../data/contents/2512.20458v1.md",
    "pdf_path": "../data/contents/2512.20458v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:27.548398",
    "title": "Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register",
    "abstract": "Summary: Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.",
    "paper_id": "2512.20458v1"
  },
  "https://arxiv.org/abs/2512.17898v1": {
    "content_path": "../data/contents/2512.17898v1.md",
    "pdf_path": "../data/contents/2512.17898v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:30.182540",
    "title": "Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally",
    "abstract": "Summary: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.",
    "paper_id": "2512.17898v1"
  },
  "https://arxiv.org/abs/2512.13632v1": {
    "content_path": "../data/contents/2512.13632v1.md",
    "pdf_path": "../data/contents/2512.13632v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:33.833089",
    "title": "StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion",
    "abstract": "Summary: Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve \"Modality Collapse\", an \"Echo Chamber\" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.",
    "paper_id": "2512.13632v1"
  },
  "https://arxiv.org/abs/2511.17006": {
    "content_path": "../data/contents/2511.17006.md",
    "pdf_path": "../data/contents/2511.17006.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:35.091279",
    "title": "Budget-Aware Tool-Use Enables Effective Agent Scaling",
    "abstract": "Shows that tool-call budgets require explicit budget awareness; proposes budget-aware planning/verification (BATS) for scalable tool-using agents.",
    "paper_id": "2511.17006"
  },
  "https://arxiv.org/abs/2512.20457v1": {
    "content_path": "../data/contents/2512.20457v1.md",
    "pdf_path": "../data/contents/2512.20457v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:35.477050",
    "title": "When Natural Strategies Meet Fuzziness and Resource-Bounded Actions (Extended Version)",
    "abstract": "Summary: In formal strategic reasoning for Multi-Agent Systems (MAS), agents are typically assumed to (i) employ arbitrarily complex strategies, (ii) execute each move at zero cost, and (iii) operate over fully crisp game structures. These idealized assumptions stand in stark contrast with human decision making in real world environments. The natural strategies framework along with some of its recent variants, partially addresses this gap by restricting strategies to concise rules guarded by regular expressions. Yet, it still overlook both the cost of each action and the uncertainty that often characterizes human perception of facts over the time. In this work, we introduce HumanATLF, a logic that builds upon natural strategies employing both fuzzy semantics and resource bound actions: each action carries a real valued cost drawn from a non refillable budget, and atomic conditions and goals have degrees in [0,1]. We give a formal syntax and semantics, and prove that model checking is in P when both the strategy complexity k and resource budget b are fixed, NP complete if just one strategic operator over Boolean objectives is allowed, and Delta^P_2 complete when k and b vary. Moreover, we show that recall based strategies can be decided in PSPACE. We implement our algorithms in VITAMIN, an open source model checking tool for MAS and validate them on an adversarial resource aware drone rescue scenario.",
    "paper_id": "2512.20457v1"
  },
  "https://arxiv.org/abs/2512.17896v1": {
    "content_path": "../data/contents/2512.17896v1.md",
    "pdf_path": "../data/contents/2512.17896v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:37.455192",
    "title": "XAgen: An Explainability Tool for Identifying and Correcting Failures in Multi-Agent Workflows",
    "abstract": "Summary: As multi-agent systems powered by Large Language Models (LLMs) are increasingly adopted in real-world workflows, users with diverse technical backgrounds are now building and refining their own agentic processes. However, these systems can fail in opaque ways, making it difficult for users to observe, understand, and correct errors. We conducted formative interviews with 12 practitioners to identify mismatches between existing observability tools and users' needs. Based on these insights, we designed XAgen, an explainability tool that supports users with varying AI expertise through three core capabilities: log visualization for glanceable workflow understanding, human-in-the-loop feedback to capture expert judgment, and automatic error detection via an LLM-as-a-judge. In a user study with 8 participants, XAgen helped users more easily locate failures, attribute to specific agents or steps, and iteratively improve configurations. Our findings surface human-centered design guidelines for explainable agentic AI development and highlights opportunities for more context-aware interactive debugging.",
    "paper_id": "2512.17896v1"
  },
  "https://arxiv.org/abs/2512.13618v2": {
    "content_path": "../data/contents/2512.13618v2.md",
    "pdf_path": "../data/contents/2512.13618v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:37.787344",
    "title": "Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models",
    "abstract": "Summary: Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.",
    "paper_id": "2512.13618v2"
  },
  "https://arxiv.org/abs/2511.02424": {
    "content_path": "../data/contents/2511.02424.md",
    "pdf_path": "../data/contents/2511.02424.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:46.175657",
    "title": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning",
    "abstract": "Hierarchical agent-tree decomposition and control flow for long-horizon planning beyond monolithic trajectories (ReAct-style baselines).",
    "paper_id": "2511.02424"
  },
  "https://arxiv.org/abs/2512.20420v1": {
    "content_path": "../data/contents/2512.20420v1.md",
    "pdf_path": "../data/contents/2512.20420v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:47.423077",
    "title": "Simplifying Multi-Task Architectures Through Task-Specific Normalization",
    "abstract": "Summary: Multi-task learning (MTL) aims to leverage shared knowledge across tasks to improve generalization and parameter efficiency, yet balancing resources and mitigating interference remain open challenges. Architectural solutions often introduce elaborate task-specific modules or routing schemes, increasing complexity and overhead. In this work, we show that normalization layers alone are sufficient to address many of these challenges. Simply replacing shared normalization with task-specific variants already yields competitive performance, questioning the need for complex designs. Building on this insight, we propose Task-Specific Sigmoid Batch Normalization (TS$σ$BN), a lightweight mechanism that enables tasks to softly allocate network capacity while fully sharing feature extractors. TS$σ$BN improves stability across CNNs and Transformers, matching or exceeding performance on NYUv2, Cityscapes, CelebA, and PascalContext, while remaining highly parameter-efficient. Moreover, its learned gates provide a natural framework for analyzing MTL dynamics, offering interpretable insights into capacity allocation, filter specialization, and task relationships. Our findings suggest that complex MTL architectures may be unnecessary and that task-specific normalization offers a simple, interpretable, and efficient alternative.",
    "paper_id": "2512.20420v1"
  },
  "https://arxiv.org/abs/2512.17875v1": {
    "content_path": "../data/contents/2512.17875v1.md",
    "pdf_path": "../data/contents/2512.17875v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:50.434431",
    "title": "Visually Prompted Benchmarks Are Surprisingly Fragile",
    "abstract": "Summary: A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.",
    "paper_id": "2512.17875v1"
  },
  "https://arxiv.org/abs/2512.13771v1": {
    "content_path": "../data/contents/2512.13771v1.md",
    "pdf_path": "../data/contents/2512.13771v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:51.203452",
    "title": "Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems",
    "abstract": "Summary: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\\mathbb{S}^{d-1}$.Our central finding is \\emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $θ(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $θ(q,c)$, to $d$=1.27 -high $θ(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.",
    "paper_id": "2512.13771v1"
  },
  "https://arxiv.org/abs/2512.17912": {
    "content_path": "../data/contents/2512.17912.md",
    "pdf_path": "../data/contents/2512.17912.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:51.895132",
    "title": "Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning",
    "abstract": "Agentic GraphRAG approach combining MCTS + RL for interactive stepwise reasoning over graphs—planning/search applied to reasoning tasks.",
    "paper_id": "2512.17912"
  },
  "https://arxiv.org/abs/2512.20417v1": {
    "content_path": "../data/contents/2512.20417v1.md",
    "pdf_path": "../data/contents/2512.20417v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:52.227261",
    "title": "Chain-of-Anomaly Thoughts with Large Vision-Language Models",
    "abstract": "Summary: Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.",
    "paper_id": "2512.20417v1"
  },
  "https://arxiv.org/abs/2512.17851v1": {
    "content_path": "../data/contents/2512.17851v1.md",
    "pdf_path": "../data/contents/2512.17851v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:59.099519",
    "title": "InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models",
    "abstract": "Summary: Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.",
    "paper_id": "2512.17851v1"
  },
  "https://arxiv.org/abs/2512.13607v1": {
    "content_path": "../data/contents/2512.13607v1.md",
    "pdf_path": "../data/contents/2512.13607v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:12:59.931023",
    "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
    "abstract": "Summary: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.",
    "paper_id": "2512.13607v1"
  },
  "https://arxiv.org/abs/2512.20404v1": {
    "content_path": "../data/contents/2512.20404v1.md",
    "pdf_path": "../data/contents/2512.20404v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:00.427041",
    "title": "Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining",
    "abstract": "Summary: With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.",
    "paper_id": "2512.20404v1"
  },
  "https://arxiv.org/abs/2512.17846v1": {
    "content_path": "../data/contents/2512.17846v1.md",
    "pdf_path": "../data/contents/2512.17846v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:00.910819",
    "title": "Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes",
    "abstract": "Summary: We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.\n  PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.\n  We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\\% success, strongly outperforming prior methods that peak at 68\\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.",
    "paper_id": "2512.17846v1"
  },
  "https://arxiv.org/abs/2512.13598v1": {
    "content_path": "../data/contents/2512.13598v1.md",
    "pdf_path": "../data/contents/2512.13598v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:01.539875",
    "title": "Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization",
    "abstract": "Summary: A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.",
    "paper_id": "2512.13598v1"
  },
  "https://arxiv.org/abs/2512.10313": {
    "content_path": "../data/contents/2512.10313.md",
    "pdf_path": "../data/contents/2512.10313.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:04.024013",
    "title": "EpiPlanAgent: Agentic Automated Epidemic Response Planning",
    "abstract": "Multi-agent LLM workflow for real planning/validation (decomposition, grounding, simulation), illustrating agentic planning in a high-stakes domain.",
    "paper_id": "2512.10313"
  },
  "https://arxiv.org/abs/2512.20403v1": {
    "content_path": "../data/contents/2512.20403v1.md",
    "pdf_path": "../data/contents/2512.20403v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:04.660554",
    "title": "BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples",
    "abstract": "Summary: Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.",
    "paper_id": "2512.20403v1"
  },
  "https://arxiv.org/abs/2512.13564": {
    "content_path": "../data/contents/2512.13564.md",
    "pdf_path": "../data/contents/2512.13564.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:07.394900",
    "title": "Memory in the Age of AI Agents: A Survey",
    "abstract": "Comprehensive survey on agent memory covering forms, functions, and dynamics with benchmarks and open-source frameworks",
    "paper_id": "2512.13564"
  },
  "https://arxiv.org/abs/2512.17843v1": {
    "content_path": "../data/contents/2512.17843v1.md",
    "pdf_path": "../data/contents/2512.17843v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:08.289756",
    "title": "ShareChat: A Dataset of Chatbot Conversations in the Wild",
    "abstract": "Summary: While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.",
    "paper_id": "2512.17843v1"
  },
  "https://arxiv.org/abs/2512.13586v1": {
    "content_path": "../data/contents/2512.13586v1.md",
    "pdf_path": "../data/contents/2512.13586v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:08.940322",
    "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
    "abstract": "Summary: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.",
    "paper_id": "2512.13586v1"
  },
  "https://arxiv.org/abs/2501.12948": {
    "content_path": "../data/contents/2501.12948.md",
    "pdf_path": "../data/contents/2501.12948.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:09.215942",
    "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
    "abstract": "Landmark paper showing pure RL can elicit reasoning capabilities without SFT, achieving o1-level performance on math/code benchmarks.",
    "paper_id": "2501.12948"
  },
  "https://arxiv.org/abs/2512.20387v1": {
    "content_path": "../data/contents/2512.20387v1.md",
    "pdf_path": "../data/contents/2512.20387v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:19.787991",
    "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
    "abstract": "Summary: We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.",
    "paper_id": "2512.20387v1"
  },
  "https://arxiv.org/abs/2512.17820v1": {
    "content_path": "../data/contents/2512.17820v1.md",
    "pdf_path": "../data/contents/2512.17820v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:20.143690",
    "title": "Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation",
    "abstract": "Summary: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.",
    "paper_id": "2512.17820v1"
  },
  "https://arxiv.org/abs/2512.13576v1": {
    "content_path": "../data/contents/2512.13576v1.md",
    "pdf_path": "../data/contents/2512.13576v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:22.379912",
    "title": "Reproducing and Dissecting Denoising Language Models for Speech Recognition",
    "abstract": "Summary: Denoising language models (DLMs) have been proposed as a powerful alternative to traditional language models (LMs) for automatic speech recognition (ASR), motivated by their ability to use bidirectional context and adapt to a specific ASR model's error patterns. However, the complexity of the DLM training pipeline has hindered wider investigation. This paper presents the first independent, large-scale empirical study of DLMs. We build and release a complete, reproducible pipeline to systematically investigate the impact of key design choices. We evaluate dozens of configurations across multiple axes, including various data augmentation techniques (e.g., SpecAugment, dropout, mixup), different text-to-speech systems, and multiple decoding strategies. Our comparative analysis in a common subword vocabulary setting demonstrates that DLMs outperform traditional LMs, but only after a distinct compute tipping point. While LMs are more efficient at lower budgets, DLMs scale better with longer training, mirroring behaviors observed in diffusion language models. However, we observe smaller improvements than those reported in prior character-based work, which indicates that the DLM's performance is conditional on factors such as the vocabulary. Our analysis reveals that a key factor for improving performance is to condition the DLM on richer information from the ASR's hypothesis space, rather than just a single best guess. To this end, we introduce DLM-sum, a novel method for decoding from multiple ASR hypotheses, which consistently outperforms the previously proposed DSR decoding method. We believe our findings and public pipeline provide a crucial foundation for the community to better understand, improve, and build upon this promising class of models. The code is publicly available at https://github.com/rwth-i6/2025-denoising-lm/.",
    "paper_id": "2512.13576v1"
  },
  "https://arxiv.org/abs/2512.20381v1": {
    "content_path": "../data/contents/2512.20381v1.md",
    "pdf_path": "../data/contents/2512.20381v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:22.833637",
    "title": "Identifying Appropriately-Sized Services with Deep Reinforcement Learning",
    "abstract": "Summary: Service-based architecture (SBA) has gained attention in industry and academia as a means to modernize legacy systems. It refers to a design style that enables systems to be developed as suites of small, loosely coupled, and autonomous components (services) that encapsulate functionality and communicate via language-agnostic APIs. However, defining appropriately sized services that capture cohesive subsets of system functionality remains challenging. Existing work often relies on the availability of documentation, access to project personnel, or a priori knowledge of the target number of services, assumptions that do not hold in many real-world scenarios. Our work addresses these limitations using a deep reinforcement learning-based approach to identify appropriately sized services directly from implementation artifacts. We present Rake, a reinforcement learning-based technique that leverages available system documentation and source code to guide service decomposition at the level of implementation methods. Rake does not require specific documentation or access to project personnel and is language-agnostic. It also supports a customizable objective function that balances modularization quality and business capability alignment, i.e., the degree to which a service covers the targeted business capability. We applied Rake to four open-source legacy projects and compared it with two state-of-the-art techniques. On average, Rake achieved 7-14 percent higher modularization quality and 18-22 percent stronger business capability alignment. Our results further show that optimizing solely for business context can degrade decomposition quality in tightly coupled systems, highlighting the need for balanced objectives.",
    "paper_id": "2512.20381v1"
  },
  "https://arxiv.org/abs/2512.17814v2": {
    "content_path": "../data/contents/2512.17814v2.md",
    "pdf_path": "../data/contents/2512.17814v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:23.189884",
    "title": "LLM-based Behaviour Driven Development for Hardware Design",
    "abstract": "Summary: Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.\n  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.",
    "paper_id": "2512.17814v2"
  },
  "https://arxiv.org/abs/2512.13568v1": {
    "content_path": "../data/contents/2512.13568v1.md",
    "pdf_path": "../data/contents/2512.13568v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:26.137201",
    "title": "Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability",
    "abstract": "Summary: Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many \"virtual neurons\" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.",
    "paper_id": "2512.13568v1"
  },
  "https://arxiv.org/abs/2512.02304": {
    "content_path": "../data/contents/2512.02304.md",
    "pdf_path": "../data/contents/2512.02304.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:27.511585",
    "title": "When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers",
    "abstract": "Systematic solver–verifier study (including cross-family verification) and introduction of “verifier gain” for test-time scaling.",
    "paper_id": "2512.02304"
  },
  "https://arxiv.org/abs/2512.20368v1": {
    "content_path": "../data/contents/2512.20368v1.md",
    "pdf_path": "../data/contents/2512.20368v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:27.911595",
    "title": "Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability",
    "abstract": "Summary: Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\\sqrt{d \\log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.\n  A key structural property that circumvents this limitation is the \\emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \\emph{without} incurring the $\\sqrt{d \\log T}$ price of adaptivity.\n  In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.",
    "paper_id": "2512.20368v1"
  },
  "https://arxiv.org/abs/2512.17788v1": {
    "content_path": "../data/contents/2512.17788v1.md",
    "pdf_path": "../data/contents/2512.17788v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:28.852277",
    "title": "Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning",
    "abstract": "Summary: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.",
    "paper_id": "2512.17788v1"
  },
  "https://arxiv.org/abs/2512.13565v1": {
    "content_path": "../data/contents/2512.13565v1.md",
    "pdf_path": "../data/contents/2512.13565v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:30.044080",
    "title": "A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees",
    "abstract": "Summary: This paper tackles the problem of feature selection in a highly challenging setting: $\\mathbb{E}(y | \\boldsymbol{x}) = G(\\boldsymbol{x}_{\\mathcal{S}_0})$, where $\\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{ö}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = Ω(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = Ω(s \\log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.",
    "paper_id": "2512.13565v1"
  },
  "https://arxiv.org/abs/2512.20352v1": {
    "content_path": "../data/contents/2512.20352v1.md",
    "pdf_path": "../data/contents/2512.20352v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:30.287763",
    "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
    "abstract": "Summary: Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",
    "paper_id": "2512.20352v1"
  },
  "https://arxiv.org/abs/2512.17776v1": {
    "content_path": "../data/contents/2512.17776v1.md",
    "pdf_path": "../data/contents/2512.17776v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:31.238949",
    "title": "DEER: A Comprehensive and Reliable Benchmark for Deep-Research Expert Reports",
    "abstract": "Summary: As large language models (LLMs) advance, deep research systems can generate expert-level reports via multi-step reasoning and evidence-based synthesis, but evaluating such reports remains challenging. Existing benchmarks often lack systematic criteria for expert reporting, evaluations that rely heavily on LLM judges can fail to capture issues that require expert judgment, and source verification typically covers only a limited subset of explicitly cited statements rather than report-wide factual reliability. We introduce DEER, a benchmark for evaluating expert-level deep research reports. DEER comprises 50 report-writing tasks spanning 13 domains and an expert-grounded evaluation taxonomy (7 dimensions, 25 sub-dimension) operationalized into 130 fine-grained rubric items. DEER further provides task-specific expert guidance to help LLM judges assess expert-level report quality more consistently. Complementing rubric-based assessment, we propose a document-level fact-checking architecture that extracts and verifies all claims across the entire report, including both cited and uncited ones, and quantifies external-evidence quality. DEER correlates closely with human expert judgments and yields interpretable diagnostics of system strengths and weaknesses.",
    "paper_id": "2512.17776v1"
  },
  "https://arxiv.org/abs/2512.13564v1": {
    "content_path": "../data/contents/2512.13564v1.md",
    "pdf_path": "../data/contents/2512.13564v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:37.370899",
    "title": "Memory in the Age of AI Agents",
    "abstract": "Summary: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
    "paper_id": "2512.13564v1"
  },
  "https://arxiv.org/abs/2512.20328v1": {
    "content_path": "../data/contents/2512.20328v1.md",
    "pdf_path": "../data/contents/2512.20328v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:38.627555",
    "title": "Toward Explaining Large Language Models in Software Engineering Tasks",
    "abstract": "Summary: Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.",
    "paper_id": "2512.20328v1"
  },
  "https://arxiv.org/abs/2512.17771v1": {
    "content_path": "../data/contents/2512.17771v1.md",
    "pdf_path": "../data/contents/2512.17771v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:39.144250",
    "title": "Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments",
    "abstract": "Summary: While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.",
    "paper_id": "2512.17771v1"
  },
  "https://arxiv.org/abs/2512.13526v1": {
    "content_path": "../data/contents/2512.13526v1.md",
    "pdf_path": "../data/contents/2512.13526v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:39.805622",
    "title": "Async Control: Stress-testing Asynchronous Control Measures for LLM Agents",
    "abstract": "Summary: LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.",
    "paper_id": "2512.13526v1"
  },
  "https://arxiv.org/abs/2503.24235": {
    "content_path": "../data/contents/2503.24235.md",
    "pdf_path": "../data/contents/2503.24235.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:41.394467",
    "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models",
    "abstract": "Comprehensive taxonomy/survey of test-time scaling techniques, applications (including agentic tasks), and evaluation dimensions.",
    "paper_id": "2503.24235"
  },
  "https://arxiv.org/abs/2512.20312v1": {
    "content_path": "../data/contents/2512.20312v1.md",
    "pdf_path": "../data/contents/2512.20312v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:42.423888",
    "title": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning",
    "abstract": "Summary: Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \\textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.",
    "paper_id": "2512.20312v1"
  },
  "https://arxiv.org/abs/2512.17752v1": {
    "content_path": "../data/contents/2512.17752v1.md",
    "pdf_path": "../data/contents/2512.17752v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:43.087363",
    "title": "Affect, Body, Cognition, Demographics, and Emotion: The ABCDE of Text Features for Computational Affective Science",
    "abstract": "Summary: Work in Computational Affective Science and Computational Social Science explores a wide variety of research questions about people, emotions, behavior, and health. Such work often relies on language data that is first labeled with relevant information, such as the use of emotion words or the age of the speaker. Although many resources and algorithms exist to enable this type of labeling, discovering, accessing, and using them remains a substantial impediment, particularly for practitioners outside of computer science. Here, we present the ABCDE dataset (Affect, Body, Cognition, Demographics, and Emotion), a large-scale collection of over 400 million text utterances drawn from social media, blogs, books, and AI-generated sources. The dataset is annotated with a wide range of features relevant to computational affective and social science. ABCDE facilitates interdisciplinary research across numerous fields, including affective science, cognitive science, the digital humanities, sociology, political science, and computational linguistics.",
    "paper_id": "2512.17752v1"
  },
  "https://arxiv.org/abs/2512.13525v2": {
    "content_path": "../data/contents/2512.13525v2.md",
    "pdf_path": "../data/contents/2512.13525v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:44.042173",
    "title": "Janus: Disaggregating Attention and Experts for Scalable MoE Inference",
    "abstract": "Summary: Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.",
    "paper_id": "2512.13525v2"
  },
  "https://arxiv.org/abs/2408.03314": {
    "content_path": "../data/contents/2408.03314.md",
    "pdf_path": "../data/contents/2408.03314.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:45.196955",
    "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
    "abstract": "Key Berkeley research showing test-time compute scaling can outperform 14x larger models on certain problems via compute-optimal strategies.",
    "paper_id": "2408.03314"
  },
  "https://arxiv.org/abs/2512.20308v1": {
    "content_path": "../data/contents/2512.20308v1.md",
    "pdf_path": "../data/contents/2512.20308v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:46.527763",
    "title": "SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision",
    "abstract": "Summary: The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.",
    "paper_id": "2512.20308v1"
  },
  "https://arxiv.org/abs/2512.17738v1": {
    "content_path": "../data/contents/2512.17738v1.md",
    "pdf_path": "../data/contents/2512.17738v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:46.980234",
    "title": "When the Gold Standard isn't Necessarily Standard: Challenges of Evaluating the Translation of User-Generated Content",
    "abstract": "Summary: User-generated content (UGC) is characterised by frequent use of non-standard language, from spelling errors to expressive choices such as slang, character repetitions, and emojis. This makes evaluating UGC translation particularly challenging: what counts as a \"good\" translation depends on the level of standardness desired in the output. To explore this, we examine the human translation guidelines of four UGC datasets, and derive a taxonomy of twelve non-standard phenomena and five translation actions (NORMALISE, COPY, TRANSFER, OMIT, CENSOR). Our analysis reveals notable differences in how UGC is treated, resulting in a spectrum of standardness in reference translations. Through a case study on large language models (LLMs), we show that translation scores are highly sensitive to prompts with explicit translation instructions for UGC, and that they improve when these align with the dataset's guidelines. We argue that when preserving UGC style is important, fair evaluation requires both models and metrics to be aware of translation guidelines. Finally, we call for clear guidelines during dataset creation and for the development of controllable, guideline-aware evaluation frameworks for UGC translation.",
    "paper_id": "2512.17738v1"
  },
  "https://arxiv.org/abs/2512.13515v1": {
    "content_path": "../data/contents/2512.13515v1.md",
    "pdf_path": "../data/contents/2512.13515v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:13:53.534311",
    "title": "Fine-tuned LLM-based Code Migration Framework",
    "abstract": "Summary: The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.",
    "paper_id": "2512.13515v1"
  },
  "https://arxiv.org/abs/2506.07976": {
    "content_path": "../data/contents/2506.07976.md",
    "pdf_path": "../data/contents/2506.07976.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:00.752945",
    "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
    "abstract": "Argues agents can scale capability by extending interaction horizon (exploration/backtracking/replanning), complementing token-based test-time compute.",
    "paper_id": "2506.07976"
  },
  "https://arxiv.org/abs/2512.20306v1": {
    "content_path": "../data/contents/2512.20306v1.md",
    "pdf_path": "../data/contents/2512.20306v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:01.318083",
    "title": "Structured Visualization Design Knowledge for Grounding Generative Reasoning and Situated Feedback",
    "abstract": "Summary: Automated visualization design navigates a tension between symbolic systems and generative models. Constraint solvers enforce structural and perceptual validity, but the rules they require are difficult to author and too rigid to capture situated design knowledge. Large language models require no formal rules and can reason about contextual nuance, but they prioritize popular conventions over empirically grounded best practices. We address this tension by proposing a cataloging scheme that structures visualization design knowledge as natural-language guidelines with semantically typed metadata. This allows experts to author knowledge that machines can query. An expert study ($N=18$) indicates that practitioners routinely adapt heuristics to situational factors such as audience and communicative intent. To capture this reasoning, guideline sections specify not only advice but also the contexts where it applies, exceptions that invalidate it, and the sources from which it derives. We demonstrate the scheme's expressiveness by cataloging 744 guidelines drawn from cognitive science, accessibility standards, data journalism, and research on rhetorical aspects of visual communication. We embed guideline sections in a vector space, opening the knowledge itself to structural analysis. This reveals conflicting advice across sources and transferable principles between domains. Rather than replacing constraint-based tools, our scheme provides what they lack: situated guidance that generative systems can retrieve to ground their reasoning, users can verify against cited sources, and experts can author as knowledge evolves.",
    "paper_id": "2512.20306v1"
  },
  "https://arxiv.org/abs/2512.17733v1": {
    "content_path": "../data/contents/2512.17733v1.md",
    "pdf_path": "../data/contents/2512.17733v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:01.567089",
    "title": "Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure",
    "abstract": "Summary: Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation. However, common methods largely rely on co-occurrence, making them prone to item popularity bias and user attributes, which degrades embedding quality and performance. Meanwhile, although diversity is acknowledged as a key aspect of recommendation quality, existing research offers limited attention to it, with a notable lack of causal perspectives and theoretical grounding. To address these challenges, we propose Cadence: Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure - a plug-and-play framework built upon LightGCN as the backbone, primarily designed to enhance recommendation diversity while preserving accuracy. First, we compute the Unbiased Asymmetric Co-purchase Relationship (UACR) between items - excluding item popularity and user attributes - to construct a deconfounded directed item graph, with an aggregation mechanism to refine embeddings. Second, we leverage UACR to identify diverse categories of items that exhibit strong causal relevance to a user's interacted items but have not yet been engaged with. We then simulate their behavior under high-exposure scenarios, thereby significantly enhancing recommendation diversity while preserving relevance. Extensive experiments on real-world datasets demonstrate that our method consistently outperforms state-of-the-art diversity models in both diversity and accuracy, and further validates its effectiveness, transferability, and efficiency over baselines.",
    "paper_id": "2512.17733v1"
  },
  "https://arxiv.org/abs/2512.13511v1": {
    "content_path": "../data/contents/2512.13511v1.md",
    "pdf_path": "../data/contents/2512.13511v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:01.990417",
    "title": "TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding",
    "abstract": "Summary: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.",
    "paper_id": "2512.13511v1"
  },
  "https://arxiv.org/abs/2512.20299v1": {
    "content_path": "../data/contents/2512.20299v1.md",
    "pdf_path": "../data/contents/2512.20299v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:02.760387",
    "title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
    "abstract": "Summary: Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.",
    "paper_id": "2512.20299v1"
  },
  "https://arxiv.org/abs/2512.17720v1": {
    "content_path": "../data/contents/2512.17720v1.md",
    "pdf_path": "../data/contents/2512.17720v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:07.483037",
    "title": "Mitigating Forgetting in Low Rank Adaptation",
    "abstract": "Summary: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.",
    "paper_id": "2512.17720v1"
  },
  "https://arxiv.org/abs/2512.13510v1": {
    "content_path": "../data/contents/2512.13510v1.md",
    "pdf_path": "../data/contents/2512.13510v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:07.923314",
    "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph",
    "abstract": "Summary: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.",
    "paper_id": "2512.13510v1"
  },
  "https://arxiv.org/abs/2305.10601": {
    "content_path": "../data/contents/2305.10601.md",
    "pdf_path": "../data/contents/2305.10601.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:08.182793",
    "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
    "abstract": "Foundational paper on ToT prompting enabling LLMs to explore multiple reasoning paths with backtracking, achieving 74% on Game of 24 vs 4% for CoT.",
    "paper_id": "2305.10601"
  },
  "https://arxiv.org/abs/2512.20293v1": {
    "content_path": "../data/contents/2512.20293v1.md",
    "pdf_path": "../data/contents/2512.20293v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:08.624277",
    "title": "AprielGuard",
    "abstract": "Summary: Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.",
    "paper_id": "2512.20293v1"
  },
  "https://arxiv.org/abs/2512.17689v1": {
    "content_path": "../data/contents/2512.17689v1.md",
    "pdf_path": "../data/contents/2512.17689v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:09.130646",
    "title": "Imputation Uncertainty in Interpretable Machine Learning Methods",
    "abstract": "Summary: In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.",
    "paper_id": "2512.17689v1"
  },
  "https://arxiv.org/abs/2512.13506v1": {
    "content_path": "../data/contents/2512.13506v1.md",
    "pdf_path": "../data/contents/2512.13506v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:09.638965",
    "title": "Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource",
    "abstract": "Summary: Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.",
    "paper_id": "2512.13506v1"
  },
  "https://arxiv.org/abs/2512.20292v1": {
    "content_path": "../data/contents/2512.20292v1.md",
    "pdf_path": "../data/contents/2512.20292v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:15.284835",
    "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
    "abstract": "Summary: Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.",
    "paper_id": "2512.20292v1"
  },
  "https://arxiv.org/abs/2512.17677v1": {
    "content_path": "../data/contents/2512.17677v1.md",
    "pdf_path": "../data/contents/2512.17677v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:15.717960",
    "title": "Toward Ethical AI Through Bayesian Uncertainty in Neural Question Answering",
    "abstract": "Summary: We explore Bayesian reasoning as a means to quantify uncertainty in neural networks for question answering. Starting with a multilayer perceptron on the Iris dataset, we show how posterior inference conveys confidence in predictions. We then extend this to language models, applying Bayesian inference first to a frozen head and finally to LoRA-adapted transformers, evaluated on the CommonsenseQA benchmark. Rather than aiming for state-of-the-art accuracy, we compare Laplace approximations against maximum a posteriori (MAP) estimates to highlight uncertainty calibration and selective prediction. This allows models to abstain when confidence is low. An ``I don't know'' response not only improves interpretability but also illustrates how Bayesian methods can contribute to more responsible and ethical deployment of neural question-answering systems.",
    "paper_id": "2512.17677v1"
  },
  "https://arxiv.org/abs/2512.13501v1": {
    "content_path": "../data/contents/2512.13501v1.md",
    "pdf_path": "../data/contents/2512.13501v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:16.745236",
    "title": "Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS",
    "abstract": "Summary: Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.\n  To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.\n  We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.",
    "paper_id": "2512.13501v1"
  },
  "https://arxiv.org/abs/2504.19678": {
    "content_path": "../data/contents/2504.19678.md",
    "pdf_path": "../data/contents/2504.19678.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:23.893290",
    "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review",
    "abstract": "Comprehensive survey covering ~60 benchmarks, agent frameworks, and collaboration protocols (ACP, MCP, A2A) from 2019-2025.",
    "paper_id": "2504.19678"
  },
  "https://arxiv.org/abs/2512.20291v1": {
    "content_path": "../data/contents/2512.20291v1.md",
    "pdf_path": "../data/contents/2512.20291v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:25.400836",
    "title": "Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity",
    "abstract": "Summary: Mixture-of-Experts (MoE) architectures achieve parameter efficiency through conditional computation, yet contemporary designs suffer from two fundamental limitations: structural parameter isolation that causes catastrophic forgetting, and instruction-overfitting that degrades performance in instruction-free scenarios. We propose CDSP-MoE (Conflict-Driven Subspace Pruning MoE), a framework that addresses these issues through a paradigm shift from isolated expert containers to dynamic expert instantiation within a shared physical subspace. Grounded in the Universal Weight Subspace Hypothesis, CDSP-MoE maintains a super-complete parameter backbone where logical experts are carved out via learnable topology masks. Unlike prior work that uses gradient conflict for token reassignment or optimization surgery, we leverage it as a structural supervisory signal: a Lagged Gradient Game penalizes interfering connections in the shared manifold, enabling the topology to spontaneously prune conflicting pathways and evolve interpretable modular structures. Experimental results demonstrate that CDSP-MoE achieves robust content-driven routing without human-defined task labels, maintaining semantic specialization even under strict blind inference protocols where explicit instructions are absent. Code is available at: https://github.com/konodiodaaaaa1/Conflict-Driven-Subspace-Pruning-Mixture-of-Experts",
    "paper_id": "2512.20291v1"
  },
  "https://arxiv.org/abs/2512.17667v1": {
    "content_path": "../data/contents/2512.17667v1.md",
    "pdf_path": "../data/contents/2512.17667v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:25.842796",
    "title": "STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting",
    "abstract": "Summary: Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR's datasets and code to support reproducibility and future research.",
    "paper_id": "2512.17667v1"
  },
  "https://arxiv.org/abs/2512.13494v1": {
    "content_path": "../data/contents/2512.13494v1.md",
    "pdf_path": "../data/contents/2512.13494v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:28.319966",
    "title": "SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping",
    "abstract": "Summary: Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.",
    "paper_id": "2512.13494v1"
  },
  "https://arxiv.org/abs/2512.20278v1": {
    "content_path": "../data/contents/2512.20278v1.md",
    "pdf_path": "../data/contents/2512.20278v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:28.773512",
    "title": "Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation",
    "abstract": "Summary: While CodeMem establishes executable code as the optimal representation for agentic procedural memory, the mechanism for autonomously synthesizing this memory from a blank slate remains underexplored. This paper operationalizes the transition of Large Language Models from passive tool-users to active workflow architects. Through a high-fidelity case study of a cross-service orchestration task involving Outlook and OneDrive, we identify and address four structural bottlenecks in automated skill generation: the Discovery Gap involving navigation of large tool registries, the Verification Gap regarding grounding tool response structures, the Decomposition Gap which replaces inefficient search with Linear State Anchoring, and the Scaling Gap focused on concurrency and persistence. We demonstrate that by enforcing a scientific methodology of hypothesize, probe, and code, agents can autonomously write robust, production-grade code skills.",
    "paper_id": "2512.20278v1"
  },
  "https://arxiv.org/abs/2512.17657v1": {
    "content_path": "../data/contents/2512.17657v1.md",
    "pdf_path": "../data/contents/2512.17657v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:29.119711",
    "title": "Peeking Into The Future For Contextual Biasing",
    "abstract": "Summary: While end-to-end (E2E) automatic speech recognition (ASR) models excel at general transcription, they struggle to recognize rare or unseen named entities (e.g., contact names, locations), which are critical for downstream applications like virtual assistants. In this paper, we propose a contextual biasing method for attention based encoder decoder (AED) models using a list of candidate named entities. Instead of predicting only the next token, we simultaneously predict multiple future tokens, enabling the model to \"peek into the future\" and score potential candidate entities in the entity list. Moreover, our approach leverages the multi-token prediction logits directly without requiring additional entity encoders or cross-attention layers, significantly reducing architectural complexity. Experiments on Librispeech demonstrate that our approach achieves up to 50.34% relative improvement in named entity word error rate compared to the baseline AED model.",
    "paper_id": "2512.17657v1"
  },
  "https://arxiv.org/abs/2512.13488v1": {
    "content_path": "../data/contents/2512.13488v1.md",
    "pdf_path": "../data/contents/2512.13488v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:29.670109",
    "title": "SIGMA: An AI-Empowered Training Stack on Early-Life Hardware",
    "abstract": "Summary: An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.",
    "paper_id": "2512.13488v1"
  },
  "https://arxiv.org/html/2504.09037v1": {
    "content_path": "../data/contents/2504.09037v1.md",
    "pdf_path": "../data/contents/2504.09037v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:30.443164",
    "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems",
    "abstract": "Survey organizing reasoning research across inference scaling, learning-to-reason, and single/multi-agent systems with unified framework.",
    "paper_id": "2504.09037v1"
  },
  "https://arxiv.org/abs/2512.20276v1": {
    "content_path": "../data/contents/2512.20276v1.md",
    "pdf_path": "../data/contents/2512.20276v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:30.873180",
    "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
    "abstract": "Summary: Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.",
    "paper_id": "2512.20276v1"
  },
  "https://arxiv.org/abs/2512.17639v1": {
    "content_path": "../data/contents/2512.17639v1.md",
    "pdf_path": "../data/contents/2512.17639v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:31.380889",
    "title": "Linear Personality Probing and Steering in LLMs: A Big Five Study",
    "abstract": "Summary: Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Using Llama 3.3 70B, we generate descriptions of 406 fictional characters and their Big Five trait scores. We then prompt the model with these descriptions and questions from the Alpaca questionnaire, allowing us to sample hidden activations that vary along personality traits in known, quantifiable ways. Using linear regression, we learn a set of per-layer directions in activation space, and test their effectiveness for probing and steering model behavior. Our results suggest that linear directions aligned with trait-scores are effective probes for personality detection, while their steering capabilities strongly depend on context, producing reliable effects in forced-choice tasks but limited influence in open-ended generation or when additional context is present in the prompt.",
    "paper_id": "2512.17639v1"
  },
  "https://arxiv.org/abs/2512.13487v1": {
    "content_path": "../data/contents/2512.13487v1.md",
    "pdf_path": "../data/contents/2512.13487v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:31.616994",
    "title": "Advancing Bangla Machine Translation Through Informal Datasets",
    "abstract": "Summary: Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.",
    "paper_id": "2512.13487v1"
  },
  "https://arxiv.org/html/2502.04644v2": {
    "content_path": "../data/contents/2502.04644v2.md",
    "pdf_path": "../data/contents/2502.04644v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:31.816114",
    "title": "Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning with Agentic Tools",
    "abstract": "Framework integrating Mind-Map memory, web search agents, and computational tools into reasoning chains for deep research tasks.",
    "paper_id": "2502.04644v2"
  },
  "https://arxiv.org/abs/2512.20275v1": {
    "content_path": "../data/contents/2512.20275v1.md",
    "pdf_path": "../data/contents/2512.20275v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:32.152137",
    "title": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks",
    "abstract": "Summary: As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.",
    "paper_id": "2512.20275v1"
  },
  "https://arxiv.org/abs/2512.17636v1": {
    "content_path": "../data/contents/2512.17636v1.md",
    "pdf_path": "../data/contents/2512.17636v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:32.949712",
    "title": "Trust-Region Adaptive Policy Optimization",
    "abstract": "Summary: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\\textbf{T}rust-\\textbf{R}egion \\textbf{A}daptive \\textbf{P}olicy \\textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.",
    "paper_id": "2512.17636v1"
  },
  "https://arxiv.org/abs/2512.13481v1": {
    "content_path": "../data/contents/2512.13481v1.md",
    "pdf_path": "../data/contents/2512.13481v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:33.342761",
    "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings",
    "abstract": "Summary: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.",
    "paper_id": "2512.13481v1"
  },
  "https://arxiv.org/abs/2512.20237v1": {
    "content_path": "../data/contents/2512.20237v1.md",
    "pdf_path": "../data/contents/2512.20237v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:33.946407",
    "title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents",
    "abstract": "Summary: Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.",
    "paper_id": "2512.20237v1"
  },
  "https://arxiv.org/abs/2512.17632v1": {
    "content_path": "../data/contents/2512.17632v1.md",
    "pdf_path": "../data/contents/2512.17632v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:34.666296",
    "title": "Fast and Robust: Computationally Efficient Covariance Estimation for Sub-Weibull Vectors",
    "abstract": "Summary: High-dimensional covariance estimation is notoriously sensitive to outliers. While statistically optimal estimators exist for general heavy-tailed distributions, they often rely on computationally expensive techniques like semidefinite programming or iterative M-estimation ($O(d^3)$). In this work, we target the specific regime of \\textbf{Sub-Weibull distributions} (characterized by stretched exponential tails $\\exp(-t^α)$). We investigate a computationally efficient alternative: the \\textbf{Cross-Fitted Norm-Truncated Estimator}. Unlike element-wise truncation, our approach preserves the spectral geometry while requiring $O(Nd^2)$ operations, which represents the theoretical lower bound for constructing a full covariance matrix. Although spherical truncation is geometrically suboptimal for anisotropic data, we prove that within the Sub-Weibull class, the exponential tail decay compensates for this mismatch. Leveraging weighted Hanson-Wright inequalities, we derive non-asymptotic error bounds showing that our estimator recovers the optimal sub-Gaussian rate $\\tilde{O}(\\sqrt{r(Σ)/N})$ with high probability. This provides a scalable solution for high-dimensional data that exhibits tails heavier than Gaussian but lighter than polynomial decay.",
    "paper_id": "2512.17632v1"
  },
  "https://arxiv.org/abs/2512.13480v1": {
    "content_path": "../data/contents/2512.13480v1.md",
    "pdf_path": "../data/contents/2512.13480v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:35.036623",
    "title": "Element-wise Modulation of Random Matrices for Efficient Neural Layers",
    "abstract": "Summary: Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.",
    "paper_id": "2512.13480v1"
  },
  "https://arxiv.org/abs/2512.20233v1": {
    "content_path": "../data/contents/2512.20233v1.md",
    "pdf_path": "../data/contents/2512.20233v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:40.631436",
    "title": "How I Met Your Bias: Investigating Bias Amplification in Diffusion Models",
    "abstract": "Summary: Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at https://github.com/How-I-met-your-bias/how_i_met_your_bias.",
    "paper_id": "2512.20233v1"
  },
  "https://arxiv.org/abs/2512.17630v1": {
    "content_path": "../data/contents/2512.17630v1.md",
    "pdf_path": "../data/contents/2512.17630v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:40.837746",
    "title": "Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection",
    "abstract": "Summary: This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.",
    "paper_id": "2512.17630v1"
  },
  "https://arxiv.org/abs/2512.13478v4": {
    "content_path": "../data/contents/2512.13478v4.md",
    "pdf_path": "../data/contents/2512.13478v4.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:41.161900",
    "title": "Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation",
    "abstract": "Summary: Current artificial intelligence systems, despite remarkable capabilities in text generation and pattern recognition, exhibit a fundamental architectural limitation: they resolve ambiguity prematurely. This premature semantic collapse -- the tendency to collapse multiple valid interpretations into a single output -- stems from classical identity assumptions embedded in standard neural architectures. We propose Non-Resolution Reasoning (NRR), a computational framework that treats ambiguity retention as a valid reasoning mode rather than a defect to be eliminated. NRR introduces three core principles: (1) Non-Identity ($A \\neq A$) -- the same symbol refers to different entities across contexts; (2) Approximate Identity ($A \\approx A$) -- entities share partial structural overlap without being identical; and (3) Non-Resolution -- conflicting interpretations can coexist without forced convergence. We formalize these principles through three architectural components: Multi-Vector Embeddings for context-dependent representation, Non-Collapsing Attention for parallel interpretation retention, and Contextual Identity Tracking (CIT) for maintaining $A \\neq A$ across inference. We demonstrate NRR's advantages through case studies in paradox handling, creative generation, and context-dependent reasoning. Crucially, we provide a minimal empirical validation on a synthetic context-shift task where an NRR-lite model achieves 90.9% out-of-distribution accuracy compared to 9.1% for standard architectures, demonstrating that ambiguity preservation enables structural generalization. NRR challenges the assumption that meaning must collapse to be useful, offering a foundation for AI systems capable of sophisticated ambiguity handling and creative reasoning. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.",
    "paper_id": "2512.13478v4"
  },
  "https://arxiv.org/abs/2512.20210v1": {
    "content_path": "../data/contents/2512.20210v1.md",
    "pdf_path": "../data/contents/2512.20210v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:41.522733",
    "title": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs",
    "abstract": "Summary: The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.",
    "paper_id": "2512.20210v1"
  },
  "https://arxiv.org/abs/2512.17629v2": {
    "content_path": "../data/contents/2512.17629v2.md",
    "pdf_path": "../data/contents/2512.17629v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:41.793650",
    "title": "SCOPE: Sequential Causal Optimization of Process Interventions",
    "abstract": "Summary: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.",
    "paper_id": "2512.17629v2"
  },
  "https://arxiv.org/abs/2512.13472v1": {
    "content_path": "../data/contents/2512.13472v1.md",
    "pdf_path": "../data/contents/2512.13472v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:42.595619",
    "title": "Scaling Laws for Code: Every Programming Language Matters",
    "abstract": "Summary: Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.",
    "paper_id": "2512.13472v1"
  },
  "https://arxiv.org/abs/2512.20204v1": {
    "content_path": "../data/contents/2512.20204v1.md",
    "pdf_path": "../data/contents/2512.20204v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:42.824667",
    "title": "Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings",
    "abstract": "Summary: Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.\n  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.",
    "paper_id": "2512.20204v1"
  },
  "https://arxiv.org/abs/2512.17594v1": {
    "content_path": "../data/contents/2512.17594v1.md",
    "pdf_path": "../data/contents/2512.17594v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:43.047202",
    "title": "MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification",
    "abstract": "Summary: Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.",
    "paper_id": "2512.17594v1"
  },
  "https://arxiv.org/abs/2512.13442v1": {
    "content_path": "../data/contents/2512.13442v1.md",
    "pdf_path": "../data/contents/2512.13442v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:43.428721",
    "title": "XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders",
    "abstract": "Summary: In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.",
    "paper_id": "2512.13442v1"
  },
  "https://arxiv.org/abs/2510.20272": {
    "content_path": "../data/contents/2510.20272.md",
    "pdf_path": "../data/contents/2510.20272.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:43.756497",
    "title": "Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs",
    "abstract": "Empirical evidence that PRM-guided tree search can fail to beat best-of-N due to reward model depth/generalization issues—important for TTS design.",
    "paper_id": "2510.20272"
  },
  "https://arxiv.org/abs/2502.05171": {
    "content_path": "../data/contents/2502.05171.md",
    "pdf_path": "../data/contents/2502.05171.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:46.959691",
    "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
    "abstract": "Novel architecture for test-time compute scaling using recurrent blocks for implicit reasoning in latent space.",
    "paper_id": "2502.05171"
  },
  "https://arxiv.org/abs/2512.20184v1": {
    "content_path": "../data/contents/2512.20184v1.md",
    "pdf_path": "../data/contents/2512.20184v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:47.742150",
    "title": "Reaching Agreement Among Reasoning LLM Agents",
    "abstract": "Summary: Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.",
    "paper_id": "2512.20184v1"
  },
  "https://arxiv.org/abs/2512.18329": {
    "content_path": "../data/contents/2512.18329.md",
    "pdf_path": "../data/contents/2512.18329.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:48.523078",
    "title": "LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation",
    "abstract": "Proposes a rerank+reasoning strategy for multi-hop RAG that cuts token overhead dramatically while improving answer quality.",
    "paper_id": "2512.18329"
  },
  "https://arxiv.org/abs/2512.17592v1": {
    "content_path": "../data/contents/2512.17592v1.md",
    "pdf_path": "../data/contents/2512.17592v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:50.188621",
    "title": "Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models",
    "abstract": "Summary: Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.\n  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.",
    "paper_id": "2512.17592v1"
  },
  "https://arxiv.org/abs/2512.13438v1": {
    "content_path": "../data/contents/2512.13438v1.md",
    "pdf_path": "../data/contents/2512.13438v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:51.192702",
    "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents",
    "abstract": "Summary: While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.",
    "paper_id": "2512.13438v1"
  },
  "https://arxiv.org/abs/2512.20182v1": {
    "content_path": "../data/contents/2512.20182v1.md",
    "pdf_path": "../data/contents/2512.20182v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:14:55.178454",
    "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
    "abstract": "Summary: Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
    "paper_id": "2512.20182v1"
  },
  "https://arxiv.org/abs/2512.17577v1": {
    "content_path": "../data/contents/2512.17577v1.md",
    "pdf_path": "../data/contents/2512.17577v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:17.709541",
    "title": "Machine Learning for Static and Single-Event Dynamic Complex Network Analysis",
    "abstract": "Summary: The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.",
    "paper_id": "2512.17577v1"
  },
  "https://arxiv.org/abs/2512.13427v1": {
    "content_path": "../data/contents/2512.13427v1.md",
    "pdf_path": "../data/contents/2512.13427v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:36.268922",
    "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models",
    "abstract": "Summary: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.",
    "paper_id": "2512.13427v1"
  },
  "https://arxiv.org/abs/2512.20179v1": {
    "content_path": "../data/contents/2512.20179v1.md",
    "pdf_path": "../data/contents/2512.20179v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:36.989967",
    "title": "RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making",
    "abstract": "Summary: Current LLM-based driving agents that rely on unstructured plain-text memory suffer from low-precision scene retrieval and inefficient reflection. To address this limitation, we present RESPOND, a structured decision-making framework for LLM-driven agents grounded in explicit risk patterns. RESPOND represents each ego-centric scene using a unified 5 by 3 matrix that encodes spatial topology and road constraints, enabling consistent and reliable retrieval of spatial risk configurations. Based on this representation, a hybrid rule and LLM decision pipeline is developed with a two-tier memory mechanism. In high-risk contexts, exact pattern matching enables rapid and safe reuse of verified actions, while in low-risk contexts, sub-pattern matching supports personalized driving style adaptation. In addition, a pattern-aware reflection mechanism abstracts tactical corrections from crash and near-miss frames to update structured memory, achieving one-crash-to-generalize learning. Extensive experiments demonstrate the effectiveness of RESPOND. In highway-env, RESPOND outperforms state-of-the-art LLM-based and reinforcement learning based driving agents while producing substantially fewer collisions. With step-wise human feedback, the agent acquires a Sporty driving style within approximately 20 decision steps through sub-pattern abstraction. For real-world validation, RESPOND is evaluated on 53 high-risk cut-in scenarios extracted from the HighD dataset. For each event, intervention is applied immediately before the cut-in and RESPOND re-decides the driving action. Compared to recorded human behavior, RESPOND reduces subsequent risk in 84.9 percent of scenarios, demonstrating its practical feasibility under real-world driving conditions. These results highlight RESPONDs potential for autonomous driving, personalized driving assistance, and proactive hazard mitigation.",
    "paper_id": "2512.20179v1"
  },
  "https://arxiv.org/abs/2512.17574v1": {
    "content_path": "../data/contents/2512.17574v1.md",
    "pdf_path": "../data/contents/2512.17574v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:37.902140",
    "title": "Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing",
    "abstract": "Summary: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.\n  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\\times$ more requests or enforce 1.5$\\times$ tighter SLOs, while achieving up to 4.4$\\times$ higher throughput compared to state-of-the-art systems.",
    "paper_id": "2512.17574v1"
  },
  "https://arxiv.org/abs/2512.13399v1": {
    "content_path": "../data/contents/2512.13399v1.md",
    "pdf_path": "../data/contents/2512.13399v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:38.575515",
    "title": "Differentiable Evolutionary Reinforcement Learning",
    "abstract": "Summary: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.",
    "paper_id": "2512.13399v1"
  },
  "https://arxiv.org/abs/2512.00466": {
    "content_path": "../data/contents/2512.00466.md",
    "pdf_path": "../data/contents/2512.00466.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:39.148626",
    "title": "SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling",
    "abstract": "Selective compute allocation across subproblems to avoid uniform test-time scaling bottlenecks; directly targets diminishing returns in long-CoT reasoning.",
    "paper_id": "2512.00466"
  },
  "https://arxiv.org/abs/2512.20174v1": {
    "content_path": "../data/contents/2512.20174v1.md",
    "pdf_path": "../data/contents/2512.20174v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:39.494075",
    "title": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark",
    "abstract": "Summary: Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.",
    "paper_id": "2512.20174v1"
  },
  "https://arxiv.org/abs/2512.17570v1": {
    "content_path": "../data/contents/2512.17570v1.md",
    "pdf_path": "../data/contents/2512.17570v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:40.660920",
    "title": "GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping",
    "abstract": "Summary: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake",
    "paper_id": "2512.17570v1"
  },
  "https://arxiv.org/abs/2512.13397v1": {
    "content_path": "../data/contents/2512.13397v1.md",
    "pdf_path": "../data/contents/2512.13397v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:42.377193",
    "title": "rNCA: Self-Repairing Segmentation Masks",
    "abstract": "Summary: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.",
    "paper_id": "2512.13397v1"
  },
  "https://arxiv.org/abs/2511.15738": {
    "content_path": "../data/contents/2511.15738.md",
    "pdf_path": "../data/contents/2511.15738.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:48.632843",
    "title": "Extending Test-Time Scaling: A 3D Perspective with Context, Batch, and Turn",
    "abstract": "Unifies context-length, parallel sampling, and iterative self-refinement as distinct scaling axes—highly relevant for planning/agent workflows.",
    "paper_id": "2511.15738"
  },
  "https://arxiv.org/abs/2512.20173v1": {
    "content_path": "../data/contents/2512.20173v1.md",
    "pdf_path": "../data/contents/2512.20173v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:52.474105",
    "title": "Offline Safe Policy Optimization From Heterogeneous Feedback",
    "abstract": "Summary: Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \\textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \\textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.",
    "paper_id": "2512.20173v1"
  },
  "https://arxiv.org/abs/2512.17569v1": {
    "content_path": "../data/contents/2512.17569v1.md",
    "pdf_path": "../data/contents/2512.17569v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:53.084858",
    "title": "Bayesian Optimisation: Which Constraints Matter?",
    "abstract": "Summary: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \\emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.",
    "paper_id": "2512.17569v1"
  },
  "https://arxiv.org/abs/2512.13381v1": {
    "content_path": "../data/contents/2512.13381v1.md",
    "pdf_path": "../data/contents/2512.13381v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:55.510614",
    "title": "Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction",
    "abstract": "Summary: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.",
    "paper_id": "2512.13381v1"
  },
  "https://arxiv.org/abs/2512.20169v1": {
    "content_path": "../data/contents/2512.20169v1.md",
    "pdf_path": "../data/contents/2512.20169v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:55.758486",
    "title": "Learning to Reason in LLMs by Expectation Maximization",
    "abstract": "Summary: Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.",
    "paper_id": "2512.20169v1"
  },
  "https://arxiv.org/abs/2512.17562v1": {
    "content_path": "../data/contents/2512.17562v1.md",
    "pdf_path": "../data/contents/2512.17562v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:56.199332",
    "title": "When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems",
    "abstract": "Summary: Speech enhancement methods are commonly believed to improve the performance of automatic speech recognition (ASR) in noisy environments. However, the effectiveness of these techniques cannot be taken for granted in the case of modern large-scale ASR models trained on diverse, noisy data. We present a systematic evaluation of MetricGAN-plus-voicebank denoising on four state-of-the-art ASR systems: OpenAI Whisper, NVIDIA Parakeet, Google Gemini Flash 2.0, Parrotlet-a using 500 medical speech recordings under nine noise conditions. ASR performance is measured using semantic WER (semWER), a normalized word error rate (WER) metric accounting for domain-specific normalizations. Our results reveal a counterintuitive finding: speech enhancement preprocessing degrades ASR performance across all noise conditions and models. Original noisy audio achieves lower semWER than enhanced audio in all 40 tested configurations (4 models x 10 conditions), with degradations ranging from 1.1% to 46.6% absolute semWER increase. These findings suggest that modern ASR models possess sufficient internal noise robustness and that traditional speech enhancement may remove acoustic features critical for ASR. For practitioners deploying medical scribe systems in noisy clinical environments, our results indicate that preprocessing audio with noise reduction techniques might not just be computationally wasteful but also be potentially harmful to the transcription accuracy.",
    "paper_id": "2512.17562v1"
  },
  "https://arxiv.org/abs/2512.13374v1": {
    "content_path": "../data/contents/2512.13374v1.md",
    "pdf_path": "../data/contents/2512.13374v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:15:56.944371",
    "title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection",
    "abstract": "Summary: Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.",
    "paper_id": "2512.13374v1"
  },
  "https://arxiv.org/abs/2512.20168v1": {
    "content_path": "../data/contents/2512.20168v1.md",
    "pdf_path": "../data/contents/2512.20168v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:02.074847",
    "title": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography",
    "abstract": "Summary: By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.",
    "paper_id": "2512.20168v1"
  },
  "https://arxiv.org/abs/2512.17532v1": {
    "content_path": "../data/contents/2512.17532v1.md",
    "pdf_path": "../data/contents/2512.17532v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:02.732005",
    "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
    "abstract": "Summary: Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.",
    "paper_id": "2512.17532v1"
  },
  "https://arxiv.org/abs/2512.13368v1": {
    "content_path": "../data/contents/2512.13368v1.md",
    "pdf_path": "../data/contents/2512.13368v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:03.226975",
    "title": "BlossomRec: Block-level Fused Sparse Attention Mechanism for Sequential Recommendations",
    "abstract": "Summary: Transformer structures have been widely used in sequential recommender systems (SRS). However, as user interaction histories increase, computational time and memory requirements also grow. This is mainly caused by the standard attention mechanism. Although there exist many methods employing efficient attention and SSM-based models, these approaches struggle to effectively model long sequences and may exhibit unstable performance on short sequences. To address these challenges, we design a sparse attention mechanism, BlossomRec, which models both long-term and short-term user interests through attention computation to achieve stable performance across sequences of varying lengths. Specifically, we categorize user interests in recommendation systems into long-term and short-term interests, and compute them using two distinct sparse attention patterns, with the results combined through a learnable gated output. Theoretically, it significantly reduces the number of interactions participating in attention computation. Extensive experiments on four public datasets demonstrate that BlossomRec, when integrated with state-of-the-art Transformer-based models, achieves comparable or even superior performance while significantly reducing memory usage, providing strong evidence of BlossomRec's efficiency and effectiveness.The code is available at https://github.com/ronineume/BlossomRec.",
    "paper_id": "2512.13368v1"
  },
  "https://arxiv.org/abs/2502.20379": {
    "content_path": "../data/contents/2502.20379.md",
    "pdf_path": "../data/contents/2502.20379.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:03.891174",
    "title": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers",
    "abstract": "Proposes scaling the number of verifiers as a new test-time compute dimension; useful for robust reasoning/planning pipelines beyond self-verification.",
    "paper_id": "2502.20379"
  },
  "https://arxiv.org/abs/2512.20164v1": {
    "content_path": "../data/contents/2512.20164v1.md",
    "pdf_path": "../data/contents/2512.20164v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:05.646718",
    "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications",
    "abstract": "Summary: Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.",
    "paper_id": "2512.20164v1"
  },
  "https://arxiv.org/abs/2512.17519v1": {
    "content_path": "../data/contents/2512.17519v1.md",
    "pdf_path": "../data/contents/2512.17519v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:05.871029",
    "title": "Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models",
    "abstract": "Summary: We present a simple, PEFT-compatible mechanism that enforces secret-key access control in instruction-tuned language models. K-OTG trains on a dual-path corpus: authorized examples (prefixed with a role key) learn the task output, while unauthorized examples learn a visible block token. At inference, a pre-lm_head hook applies an orthonormal transform to the hidden state: with the correct key/role the inverse map restores the model's native basis; otherwise a session-ephemeral scrambler (permutation, sign flips, Householders) makes logits uninformative and the system short-circuits to BLOCK. Keys are not added as special tokens, and the method composes cleanly with LoRA on 4-bit bases. We evaluate an hour-scale protocol on 1-3B-class instruction models (Llama 3.2, Qwen2.5 1.5B) across utility (XSum ROUGE/BLEU, GSM8K accuracy, WikiText-2 perplexity), selectivity (3by3 role-key unlock matrices), nonce invariance, block suppression, and throughput. Authorized utility remains close to the base on summarization with the expected modest PPL increase from instruction tuning; unauthorized utility collapses (near-zero sequence metrics with exploding PPL), indicating practical unusability without the key. Unlock matrices are diagonally dominant (high on-target unlock, low cross-unlock), authorized block emission is 0 per N via robust bad-word lists, and greedy outputs match exactly across nonces, confirming correct inverse cancellation. The runtime overhead of the Python-level hook is 40% tokens per sec versus the base. K-OTG therefore provides a pragmatic, model-agnostic way to prevent unauthorized use while preserving authorized utility.",
    "paper_id": "2512.17519v1"
  },
  "https://arxiv.org/abs/2512.13352v1": {
    "content_path": "../data/contents/2512.13352v1.md",
    "pdf_path": "../data/contents/2512.13352v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:06.345409",
    "title": "On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models",
    "abstract": "Summary: Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.",
    "paper_id": "2512.13352v1"
  },
  "https://arxiv.org/abs/2510.06135": {
    "content_path": "../data/contents/2510.06135.md",
    "pdf_path": "../data/contents/2510.06135.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:06.615177",
    "title": "Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification",
    "abstract": "Studies sequential+parallel scaling for deep search agents leveraging “asymmetric verification,” reporting large gains on agent benchmarks (e.g., BrowseComp/GAIA).",
    "paper_id": "2510.06135"
  },
  "https://arxiv.org/abs/2512.20162v1": {
    "content_path": "../data/contents/2512.20162v1.md",
    "pdf_path": "../data/contents/2512.20162v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:08.681937",
    "title": "Concept Generalization in Humans and Large Language Models: Insights from the Number Game",
    "abstract": "Summary: We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.",
    "paper_id": "2512.20162v1"
  },
  "https://arxiv.org/abs/2512.17470v1": {
    "content_path": "../data/contents/2512.17470v1.md",
    "pdf_path": "../data/contents/2512.17470v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:08.951443",
    "title": "Translating the Rashomon Effect to Sequential Decision-Making Tasks",
    "abstract": "Summary: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.",
    "paper_id": "2512.17470v1"
  },
  "https://arxiv.org/abs/2512.13762v1": {
    "content_path": "../data/contents/2512.13762v1.md",
    "pdf_path": "../data/contents/2512.13762v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:09.712319",
    "title": "State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models",
    "abstract": "Summary: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.",
    "paper_id": "2512.13762v1"
  },
  "https://arxiv.org/abs/2504.16828": {
    "content_path": "../data/contents/2504.16828.md",
    "pdf_path": "../data/contents/2504.16828.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:11.077522",
    "title": "Process Reward Models That Think",
    "abstract": "ThinkPRM: generative CoT-based process reward models with far fewer process labels; key for step-level verification and safer inference-time scaling.",
    "paper_id": "2504.16828"
  },
  "https://arxiv.org/abs/2512.20159v1": {
    "content_path": "../data/contents/2512.20159v1.md",
    "pdf_path": "../data/contents/2512.20159v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:11.524347",
    "title": "AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration",
    "abstract": "Summary: Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.\n  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...",
    "paper_id": "2512.20159v1"
  },
  "https://arxiv.org/abs/2511.01386": {
    "content_path": "../data/contents/2511.01386.md",
    "pdf_path": "../data/contents/2511.01386.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:12.581742",
    "title": "RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets",
    "abstract": "Treats RAG design as end-to-end architecture search across retrieval/ranking/augmentation choices, helping identify robust RAG configurations.",
    "paper_id": "2511.01386"
  },
  "https://arxiv.org/abs/2512.17462v1": {
    "content_path": "../data/contents/2512.17462v1.md",
    "pdf_path": "../data/contents/2512.17462v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:12.815651",
    "title": "Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application",
    "abstract": "Summary: Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\\% ($\\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.",
    "paper_id": "2512.17462v1"
  },
  "https://arxiv.org/abs/2512.13337v1": {
    "content_path": "../data/contents/2512.13337v1.md",
    "pdf_path": "../data/contents/2512.13337v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:13.530512",
    "title": "FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs",
    "abstract": "Summary: Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the \"right to be forgotten.\" To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.",
    "paper_id": "2512.13337v1"
  },
  "https://arxiv.org/abs/2512.20156v1": {
    "content_path": "../data/contents/2512.20156v1.md",
    "pdf_path": "../data/contents/2512.20156v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:13.952647",
    "title": "Fun-Audio-Chat Technical Report",
    "abstract": "Summary: Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.",
    "paper_id": "2512.20156v1"
  },
  "https://arxiv.org/abs/2509.12765": {
    "content_path": "../data/contents/2509.12765.md",
    "pdf_path": "../data/contents/2509.12765.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:20.592370",
    "title": "InfoGain-RAG: Boosting Retrieval-Augmented Generation via Document Information Gain-based Reranking and Filtering",
    "abstract": "Introduces an information-gain signal to train a reranker/filter that prioritizes documents that actually change model confidence for better RAG.",
    "paper_id": "2509.12765"
  },
  "https://arxiv.org/abs/2512.17453v1": {
    "content_path": "../data/contents/2512.17453v1.md",
    "pdf_path": "../data/contents/2512.17453v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:22.082557",
    "title": "A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting",
    "abstract": "Summary: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.",
    "paper_id": "2512.17453v1"
  },
  "https://arxiv.org/abs/2512.13330v1": {
    "content_path": "../data/contents/2512.13330v1.md",
    "pdf_path": "../data/contents/2512.13330v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:29.067930",
    "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models",
    "abstract": "Summary: We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.",
    "paper_id": "2512.13330v1"
  },
  "https://arxiv.org/abs/2504.00891": {
    "content_path": "../data/contents/2504.00891.md",
    "pdf_path": "../data/contents/2504.00891.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:29.789715",
    "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning",
    "abstract": "Generative PRM that uses explicit CoT + code verification and supports test-time scaling of verification compute; relevant to PRM robustness and reward hacking.",
    "paper_id": "2504.00891"
  },
  "https://arxiv.org/abs/2512.20145v1": {
    "content_path": "../data/contents/2512.20145v1.md",
    "pdf_path": "../data/contents/2512.20145v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:30.562809",
    "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
    "abstract": "Summary: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.",
    "paper_id": "2512.20145v1"
  },
  "https://arxiv.org/abs/2510.02294": {
    "content_path": "../data/contents/ebbc68ecd74d56e6.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.162371",
    "title": "Computer Science > Computation and Language",
    "summary": "The webpage describes the **F2LLM** (Foundation to Feature Large Language Models) technical report, which introduces a suite of state-of-the-art **embedding models** in three sizes (0.6B, 1.7B, and 4B). These models are directly finetuned from foundation models using open-source data, offering a balance between training cost, model size, and embedding performance, contrasting with previous models that required massive contrastive pretraining. The F2LLM-4B model ranks highly on the MTEB English leaderboard. The authors release the models, training dataset, and code.\n\nThe summary directly addresses the user query's mention of **embeddings** and **new efficient models**. It does not mention vector databases, rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, or context window management.",
    "published_date": "2025-10-02T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2512.17452v2": {
    "content_path": "../data/contents/2512.17452v2.md",
    "pdf_path": "../data/contents/2512.17452v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:32.132935",
    "title": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference",
    "abstract": "Summary: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .",
    "paper_id": "2512.17452v2"
  },
  "https://arxiv.org/abs/2512.13325v1": {
    "content_path": "../data/contents/2512.13325v1.md",
    "pdf_path": "../data/contents/2512.13325v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:32.721601",
    "title": "Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models",
    "abstract": "Summary: Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.",
    "paper_id": "2512.13325v1"
  },
  "https://arxiv.org/abs/2512.03244": {
    "content_path": "../data/contents/2512.03244.md",
    "pdf_path": "../data/contents/2512.03244.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:33.969039",
    "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
    "abstract": "Reference-free pipeline that synthesizes step-level rewards via parallel/sequential verification and trains generative PRMs, with anti–reward-hacking constraints.",
    "paper_id": "2512.03244"
  },
  "https://arxiv.org/abs/2512.20144v1": {
    "content_path": "../data/contents/2512.20144v1.md",
    "pdf_path": "../data/contents/2512.20144v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:34.751896",
    "title": "Multi-hop Reasoning via Early Knowledge Alignment",
    "abstract": "Summary: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \\href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.",
    "paper_id": "2512.20144v1"
  },
  "https://arxiv.org/abs/2508.21632": {
    "content_path": "../data/contents/2508.21632.md",
    "pdf_path": "../data/contents/2508.21632.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:35.212457",
    "title": "QZhou-Embedding Technical Report",
    "abstract": "Presents a high-performing embedding model (built on Qwen2.5) with a multi-task framework and synthetic/hard-negative pipelines for retrieval + reranking.",
    "paper_id": "2508.21632"
  },
  "https://arxiv.org/abs/2512.17429v1": {
    "content_path": "../data/contents/2512.17429v1.md",
    "pdf_path": "../data/contents/2512.17429v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:38.281028",
    "title": "Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows",
    "abstract": "Summary: Web applications underpin much of modern digital life, yet building scalable and consistent cloud applications remains difficult, requiring expertise across cloud computing, distributed systems, databases, and software engineering. These demands restrict development to a small number of highly specialized engineers. This thesis aims to democratize cloud application development by addressing three challenges: programmability, high-performance fault-tolerant serializable transactions, and serverless semantics.\n  The thesis identifies strong parallels between cloud applications and the streaming dataflow execution model. It first explores this connection through T-Statefun, a transactional extension of Apache Flink Statefun, demonstrating that dataflow systems can support transactional cloud applications via a stateful functions-as-a-service API. However, this approach revealed significant limitations in programmability and performance.\n  To overcome these issues, the thesis introduces Stateflow, a high-level object-oriented programming model that compiles applications into stateful dataflow graphs with minimal boilerplate. Building on this model, the thesis presents Styx, a distributed streaming dataflow engine that provides deterministic, multi-partition, serializable transactions with strong fault tolerance guarantees. Styx eliminates explicit transaction failure handling and significantly outperforms state-of-the-art systems.\n  Finally, the thesis extends Styx with transactional state migration to support elasticity under dynamic workloads.",
    "paper_id": "2512.17429v1"
  },
  "https://arxiv.org/abs/2512.13323v1": {
    "content_path": "../data/contents/2512.13323v1.md",
    "pdf_path": "../data/contents/2512.13323v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:38.806396",
    "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning",
    "abstract": "Summary: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.",
    "paper_id": "2512.13323v1"
  },
  "https://arxiv.org/abs/2501.03124": {
    "content_path": "../data/contents/2501.03124.md",
    "pdf_path": "../data/contents/2501.03124.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:40.780490",
    "title": "PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models",
    "abstract": "Benchmark for fine-grained PRM step-error detection beyond simple step correctness; foundational for evaluating verifiers used in test-time scaling.",
    "paper_id": "2501.03124"
  },
  "https://arxiv.org/abs/2512.20140v1": {
    "content_path": "../data/contents/2512.20140v1.md",
    "pdf_path": "../data/contents/2512.20140v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:41.850572",
    "title": "Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection",
    "abstract": "Summary: Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.",
    "paper_id": "2512.20140v1"
  },
  "https://arxiv.org/abs/2508.21085": {
    "content_path": "../data/contents/2508.21085.md",
    "pdf_path": "../data/contents/2508.21085.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:42.727340",
    "title": "Granite Embedding R2 Models",
    "abstract": "Enterprise-focused embedding + reranking model family (incl. long context) with performance/efficiency claims and released weights for dense retrieval.",
    "paper_id": "2508.21085"
  },
  "https://arxiv.org/abs/2512.20670v1": {
    "content_path": "../data/contents/2512.20670v1.md",
    "pdf_path": "../data/contents/2512.20670v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:44.622782",
    "title": "Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection",
    "abstract": "Summary: Prevalent multimodal fake news detection relies on consistency-based fusion, yet this paradigm fundamentally misinterprets critical cross-modal discrepancies as noise, leading to over-smoothing, which dilutes critical evidence of fabrication. Mainstream consistency-based fusion inherently minimizes feature discrepancies to align modalities, yet this approach fundamentally fails because it inadvertently smoothes out the subtle cross-modal contradictions that serve as the primary evidence of fabrication. To address this, we propose the Dynamic Conflict-Consensus Framework (DCCF), an inconsistency-seeking paradigm designed to amplify rather than suppress contradictions. First, DCCF decouples inputs into independent Fact and Sentiment spaces to distinguish objective mismatches from emotional dissonance. Second, we employ physics-inspired feature dynamics to iteratively polarize these representations, actively extracting maximally informative conflicts. Finally, a conflict-consensus mechanism standardizes these local discrepancies against the global context for robust deliberative judgment.Extensive experiments conducted on three real world datasets demonstrate that DCCF consistently outperforms state-of-the-art baselines, achieving an average accuracy improvement of 3.52\\%.",
    "paper_id": "2512.20670v1"
  },
  "https://arxiv.org/abs/2512.13317v1": {
    "content_path": "../data/contents/2512.13317v1.md",
    "pdf_path": "../data/contents/2512.13317v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:47.014399",
    "title": "Face Identity Unlearning for Retrieval via Embedding Dispersion",
    "abstract": "Summary: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.",
    "paper_id": "2512.13317v1"
  },
  "https://arxiv.org/abs/2512.20136v1": {
    "content_path": "../data/contents/2512.20136v1.md",
    "pdf_path": "../data/contents/2512.20136v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:49.091585",
    "title": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation",
    "abstract": "Summary: Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.",
    "paper_id": "2512.20136v1"
  },
  "https://arxiv.org/abs/2512.17419v1": {
    "content_path": "../data/contents/2512.17419v1.md",
    "pdf_path": "../data/contents/2512.17419v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:51.626330",
    "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "abstract": "Summary: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
    "paper_id": "2512.17419v1"
  },
  "https://arxiv.org/abs/2512.13298v1": {
    "content_path": "../data/contents/2512.13298v1.md",
    "pdf_path": "../data/contents/2512.13298v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:52.655392",
    "title": "MiniLingua: A Small Open-Source LLM for European Languages",
    "abstract": "Summary: Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.",
    "paper_id": "2512.13298v1"
  },
  "https://arxiv.org/abs/2512.09897": {
    "content_path": "../data/contents/2512.09897.md",
    "pdf_path": "../data/contents/2512.09897.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:53.102282",
    "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments",
    "abstract": "Hierarchical planning approach that uses LLM-generated subgoals only once at initialization to pretrain a lightweight planner—cuts repeated LLM queries at inference.",
    "paper_id": "2512.09897"
  },
  "https://arxiv.org/abs/2512.20135v1": {
    "content_path": "../data/contents/2512.20135v1.md",
    "pdf_path": "../data/contents/2512.20135v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:54.552198",
    "title": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization",
    "abstract": "Summary: Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.",
    "paper_id": "2512.20135v1"
  },
  "https://arxiv.org/abs/2512.17409v1": {
    "content_path": "../data/contents/2512.17409v1.md",
    "pdf_path": "../data/contents/2512.17409v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:55.346969",
    "title": "meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis",
    "abstract": "Summary: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.",
    "paper_id": "2512.17409v1"
  },
  "https://arxiv.org/abs/2512.13297v1": {
    "content_path": "../data/contents/2512.13297v1.md",
    "pdf_path": "../data/contents/2512.13297v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:59.548214",
    "title": "MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data",
    "abstract": "Summary: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.",
    "paper_id": "2512.13297v1"
  },
  "https://arxiv.org/abs/2512.09629": {
    "content_path": "../data/contents/2512.09629.md",
    "pdf_path": "../data/contents/2512.09629.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:16:59.915989",
    "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL",
    "abstract": "Agentic orchestration from natural language specs to PDDL with iterative refinement and external planners/validators—strong planning+verification integration.",
    "paper_id": "2512.09629"
  },
  "https://arxiv.org/abs/2512.20116v1": {
    "content_path": "../data/contents/2512.20116v1.md",
    "pdf_path": "../data/contents/2512.20116v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:06.774494",
    "title": "/UnmuteAll: Modeling Verbal Communication Patterns of Collaborative Contexts in MOBA Games",
    "abstract": "Summary: Team communication plays a vital role in supporting collaboration in multiplayer online games. Therefore, numerous studies were conducted to examine communication patterns in esports teams. While non-verbal communication has been extensively investigated, research on assessing voice-based verbal communication patterns remains relatively understudied. In this study, we propose a framework that automatically assesses verbal communication patterns by constructing networks with utterances transcribed from voice recordings. Through a data collection study, we obtained 84 game sessions from five League of Legends teams and subsequently investigated how verbal communication patterns varied across different conditions. As a result, we revealed that esports players exhibited broader and more balanced participation in collaborative situations, increased utterances over time with the largest rise in decision making, and team-level differences that were contingent on effective professional training. Building upon these findings, this study provides a generalizable tool for analyzing effective team communication.",
    "paper_id": "2512.20116v1"
  },
  "https://arxiv.org/abs/2512.17389v1": {
    "content_path": "../data/contents/2512.17389v1.md",
    "pdf_path": "../data/contents/2512.17389v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:07.191659",
    "title": "The Mental World of Large Language Models in Recommendation: A Benchmark on Association, Personalization, and Knowledgeability",
    "abstract": "Summary: Large language models (LLMs) have shown potential in recommendation systems (RecSys) by using them as either knowledge enhancer or zero-shot ranker. A key challenge lies in the large semantic gap between LLMs and RecSys where the former internalizes language world knowledge while the latter captures personalized world of behaviors. Unfortunately, the research community lacks a comprehensive benchmark that evaluates the LLMs over their limitations and boundaries in RecSys so that we can draw a confident conclusion. To investigate this, we propose a benchmark named LRWorld containing over 38K high-quality samples and 23M tokens carefully compiled and generated from widely used public recommendation datasets. LRWorld categorizes the mental world of LLMs in RecSys as three main scales (association, personalization, and knowledgeability) spanned by ten factors with 31 measures (tasks). Based on LRWorld, comprehensive experiments on dozens of LLMs show that they are still not well capturing the deep neural personalized embeddings but can achieve good results on shallow memory-based item-item similarity. They are also good at perceiving item entity relations, entity hierarchical taxonomies, and item-item association rules when inferring user interests. Furthermore, LLMs show a promising ability in multimodal knowledge reasoning (movie poster and product image) and robustness to noisy profiles. None of them show consistently good performance over the ten factors. Model sizes, position bias, and more are ablated.",
    "paper_id": "2512.17389v1"
  },
  "https://arxiv.org/abs/2512.13286v1": {
    "content_path": "../data/contents/2512.13286v1.md",
    "pdf_path": "../data/contents/2512.13286v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:07.594425",
    "title": "Integrating Causal Reasoning into Automated Fact-Checking",
    "abstract": "Summary: In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.",
    "paper_id": "2512.13286v1"
  },
  "https://arxiv.org/abs/2511.18165": {
    "content_path": "../data/contents/2511.18165.md",
    "pdf_path": "../data/contents/2511.18165.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:07.982543",
    "title": "Towards a General Framework for HTN Modeling with LLMs",
    "abstract": "Targets hierarchical task network (HTN) model generation with LLMs and evaluates limitations—relevant to structured planning representations and reliability.",
    "paper_id": "2511.18165"
  },
  "https://arxiv.org/abs/2512.20111v1": {
    "content_path": "../data/contents/2512.20111v1.md",
    "pdf_path": "../data/contents/2512.20111v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:08.321790",
    "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
    "abstract": "Summary: As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.",
    "paper_id": "2512.20111v1"
  },
  "https://arxiv.org/abs/2512.17387v1": {
    "content_path": "../data/contents/2512.17387v1.md",
    "pdf_path": "../data/contents/2512.17387v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:09.054177",
    "title": "CIFE: Code Instruction-Following Evaluation",
    "abstract": "Summary: Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.",
    "paper_id": "2512.17387v1"
  },
  "https://arxiv.org/abs/2512.15784v1": {
    "content_path": "../data/contents/2512.15784v1.md",
    "pdf_path": "../data/contents/2512.15784v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:10.839190",
    "title": "Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM",
    "abstract": "Summary: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.\n  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.\n  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.",
    "paper_id": "2512.15784v1"
  },
  "https://arxiv.org/abs/2502.04728": {
    "content_path": "../data/contents/2502.04728.md",
    "pdf_path": "../data/contents/2502.04728.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:13.902175",
    "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
    "abstract": "Uses test-time scaling to generate PDDL symbolic world models enabling classical search (A*/planners), bridging LLM reasoning and formal planning.",
    "paper_id": "2502.04728"
  },
  "https://arxiv.org/abs/2512.20094v1": {
    "content_path": "../data/contents/2512.20094v1.md",
    "pdf_path": "../data/contents/2512.20094v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:14.882765",
    "title": "Jensen-Shannon Divergence Message-Passing for Rich-Text Graph Representation Learning",
    "abstract": "Summary: In this paper, we investigate how the widely existing contextual and structural divergence may influence the representation learning in rich-text graphs. To this end, we propose Jensen-Shannon Divergence Message-Passing (JSDMP), a new learning paradigm for rich-text graph representation learning. Besides considering similarity regarding structure and text, JSDMP further captures their corresponding dissimilarity by Jensen-Shannon divergence. Similarity and dissimilarity are then jointly used to compute new message weights among text nodes, thus enabling representations to learn with contextual and structural information from truly correlated text nodes. With JSDMP, we propose two novel graph neural networks, namely Divergent message-passing graph convolutional network (DMPGCN) and Divergent message-passing Page-Rank graph neural networks (DMPPRG), for learning representations in rich-text graphs. DMPGCN and DMPPRG have been extensively texted on well-established rich-text datasets and compared with several state-of-the-art baselines. The experimental results show that DMPGCN and DMPPRG can outperform other baselines, demonstrating the effectiveness of the proposed Jensen-Shannon Divergence Message-Passing paradigm",
    "paper_id": "2512.20094v1"
  },
  "https://arxiv.org/abs/2512.17385v1": {
    "content_path": "../data/contents/2512.17385v1.md",
    "pdf_path": "../data/contents/2512.17385v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:20.819369",
    "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models",
    "abstract": "Summary: Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.",
    "paper_id": "2512.17385v1"
  },
  "https://arxiv.org/abs/2512.13279v1": {
    "content_path": "../data/contents/2512.13279v1.md",
    "pdf_path": "../data/contents/2512.13279v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:22.200909",
    "title": "AIR: Post-training Data Selection for Reasoning via Attention Head Influence",
    "abstract": "Summary: LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.",
    "paper_id": "2512.13279v1"
  },
  "https://arxiv.org/abs/2502.13092": {
    "content_path": "../data/contents/2502.13092.md",
    "pdf_path": "../data/contents/2502.13092.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:24.486632",
    "title": "Text2World: Benchmarking Large Language Models for Symbolic World Model Generation",
    "abstract": "PDDL-based benchmark for symbolic world model generation with execution-based metrics; includes analysis of test-time scaling and agent-training strategies.",
    "paper_id": "2502.13092"
  },
  "https://arxiv.org/abs/2512.20092v1": {
    "content_path": "../data/contents/2512.20092v1.md",
    "pdf_path": "../data/contents/2512.20092v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:25.414883",
    "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents",
    "abstract": "Summary: Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/",
    "paper_id": "2512.20092v1"
  },
  "https://arxiv.org/abs/2512.17381v1": {
    "content_path": "../data/contents/2512.17381v1.md",
    "pdf_path": "../data/contents/2512.17381v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:26.585042",
    "title": "Timely Information Updating for Mobile Devices Without and With ML Advice",
    "abstract": "Summary: This paper investigates an information update system in which a mobile device monitors a physical process and sends status updates to an access point (AP). A fundamental trade-off arises between the timeliness of the information maintained at the AP and the update cost incurred at the device. To address this trade-off, we propose an online algorithm that determines when to transmit updates using only available observations. The proposed algorithm asymptotically achieves the optimal competitive ratio against an adversary that can simultaneously manipulate multiple sources of uncertainty, including the operation duration, the information staleness, the update cost, and the availability of update opportunities. Furthermore, by incorporating machine learning (ML) advice of unknown reliability into the design, we develop an ML-augmented algorithm that asymptotically attains the optimal consistency-robustness trade-off, even when the adversary can additionally corrupt the ML advice. The optimal competitive ratio scales linearly with the range of update costs, but is unaffected by other uncertainties. Moreover, an optimal competitive online algorithm exhibits a threshold-like response to the ML advice: it either fully trusts or completely ignores the ML advice, as partially trusting the advice cannot improve the consistency without severely degrading the robustness. Extensive simulations in stochastic settings further validate the theoretical findings in the adversarial environment.",
    "paper_id": "2512.17381v1"
  },
  "https://arxiv.org/abs/2512.13278v1": {
    "content_path": "../data/contents/2512.13278v1.md",
    "pdf_path": "../data/contents/2512.13278v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:27.118689",
    "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning",
    "abstract": "Summary: Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.",
    "paper_id": "2512.13278v1"
  },
  "https://arxiv.org/abs/2506.12928": {
    "content_path": "../data/contents/2506.12928.md",
    "pdf_path": "../data/contents/2506.12928.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:28.007587",
    "title": "Scaling Test-time Compute for LLM Agents",
    "abstract": "Systematic exploration of test-time scaling for language agents (parallel sampling, sequential revision, verifiers, rollout diversity), including “when to reflect” findings.",
    "paper_id": "2506.12928"
  },
  "https://arxiv.org/abs/2512.20082v1": {
    "content_path": "../data/contents/2512.20082v1.md",
    "pdf_path": "../data/contents/2512.20082v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:28.420597",
    "title": "Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches",
    "abstract": "Summary: Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.",
    "paper_id": "2512.20082v1"
  },
  "https://arxiv.org/html/2507.18910v1": {
    "content_path": "../data/contents/2507.18910v1.md",
    "pdf_path": "../data/contents/2507.18910v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:28.859813",
    "title": "A Systematic Review of Key RAG Systems: Progress, Gaps, and Future Directions",
    "abstract": "Academic review documenting 1,200+ RAG papers in 2024, covering dense retrieval progress and future challenges.",
    "paper_id": "2507.18910v1"
  },
  "https://arxiv.org/abs/2512.17375v1": {
    "content_path": "../data/contents/2512.17375v1.md",
    "pdf_path": "../data/contents/2512.17375v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:30.068052",
    "title": "AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens",
    "abstract": "Summary: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.",
    "paper_id": "2512.17375v1"
  },
  "https://arxiv.org/abs/2512.13253v1": {
    "content_path": "../data/contents/2512.13253v1.md",
    "pdf_path": "../data/contents/2512.13253v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:30.934053",
    "title": "Fostering human learning is crucial for boosting human-AI synergy",
    "abstract": "Summary: The collaboration between humans and artificial intelligence (AI) holds the promise of achieving superior outcomes compared to either acting alone. Nevertheless, our understanding of the conditions that facilitate such human-AI synergy remains limited. A recent meta-analysis showed that, on average, human-AI combinations do not outperform the better individual agent, indicating overall negative human-AI synergy. We argue that this pessimistic conclusion arises from insufficient attention to human learning in the experimental designs used. To substantiate this claim, we re-analyzed all 74 studies included in the original meta-analysis, which yielded two new findings. First, most previous research overlooked design features that foster human learning, such as providing trial-by-trial outcome feedback to participants. Second, our re-analysis, using robust Bayesian meta-regressions, demonstrated that studies providing outcome feedback show relatively higher synergy than those without outcome feedback. Crucially, when feedback is paired with AI explanations we tend to find positive human-AI synergy, while AI explanations provided without feedback were strongly linked to negative synergy, indicating that explanations are useful for synergy only when humans can learn to verify the AI's reliability through feedback. We conclude that the current literature underestimates the potential for human-AI collaboration because it predominantly relies on experimental designs that do not facilitate human learning, thus hindering humans from effectively adapting their collaboration strategies. We therefore advocate for a paradigm shift in human-AI interaction research that explicitly incorporates and tests human learning mechanisms to enhance our understanding of and support for successful human-AI collaboration.",
    "paper_id": "2512.13253v1"
  },
  "https://arxiv.org/abs/2505.13672": {
    "content_path": "../data/contents/2505.13672.md",
    "pdf_path": "../data/contents/2505.13672.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:32.693562",
    "title": "A*-Decoding: Token-Efficient Inference Scaling",
    "abstract": "Search-based decoding that prioritizes high-quality reasoning paths to achieve strong test-time scaling with fewer tokens/PRM passes—important for efficient deployment.",
    "paper_id": "2505.13672"
  },
  "https://arxiv.org/abs/2512.20074v1": {
    "content_path": "../data/contents/2512.20074v1.md",
    "pdf_path": "../data/contents/2512.20074v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:32.960356",
    "title": "Reason2Decide: Rationale-Driven Multi-Task Learning",
    "abstract": "Summary: Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.",
    "paper_id": "2512.20074v1"
  },
  "https://arxiv.org/abs/2512.17373v2": {
    "content_path": "../data/contents/2512.17373v2.md",
    "pdf_path": "../data/contents/2512.17373v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:34.030962",
    "title": "Dialectics for Artificial Intelligence",
    "abstract": "Summary: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of \"concept\" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents \"concepts\" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.",
    "paper_id": "2512.17373v2"
  },
  "https://arxiv.org/abs/2512.13240v1": {
    "content_path": "../data/contents/2512.13240v1.md",
    "pdf_path": "../data/contents/2512.13240v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:34.324153",
    "title": "Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection",
    "abstract": "Summary: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.",
    "paper_id": "2512.13240v1"
  },
  "https://arxiv.org/abs/2512.20062v1": {
    "content_path": "../data/contents/2512.20062v1.md",
    "pdf_path": "../data/contents/2512.20062v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:34.591673",
    "title": "On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities",
    "abstract": "Summary: Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows.",
    "paper_id": "2512.20062v1"
  },
  "https://arxiv.org/abs/2512.17370v2": {
    "content_path": "../data/contents/2512.17370v2.md",
    "pdf_path": "../data/contents/2512.17370v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:36.260283",
    "title": "TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data",
    "abstract": "Summary: Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.",
    "paper_id": "2512.17370v2"
  },
  "https://arxiv.org/abs/2512.13237v1": {
    "content_path": "../data/contents/2512.13237v1.md",
    "pdf_path": "../data/contents/2512.13237v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:36.455508",
    "title": "Learning to Retrieve with Weakened Labels: Robust Training under Label Noise",
    "abstract": "Summary: Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.",
    "paper_id": "2512.13237v1"
  },
  "https://arxiv.org/abs/2512.20061v1": {
    "content_path": "../data/contents/2512.20061v1.md",
    "pdf_path": "../data/contents/2512.20061v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:37.885765",
    "title": "Scaling Reinforcement Learning for Content Moderation with Large Language Models",
    "abstract": "Summary: Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.",
    "paper_id": "2512.20061v1"
  },
  "https://arxiv.org/abs/2512.17351v1": {
    "content_path": "../data/contents/2512.17351v1.md",
    "pdf_path": "../data/contents/2512.17351v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:47.833775",
    "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
    "abstract": "Summary: Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.\n  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by $2\\times$), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.",
    "paper_id": "2512.17351v1"
  },
  "https://arxiv.org/abs/2512.13235v1": {
    "content_path": "../data/contents/2512.13235v1.md",
    "pdf_path": "../data/contents/2512.13235v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:17:50.008098",
    "title": "CORE: Contrastive Masked Feature Reconstruction on Graphs",
    "abstract": "Summary: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.",
    "paper_id": "2512.13235v1"
  },
  "https://arxiv.org/abs/2502.11169": {
    "content_path": "../data/contents/ced195a1248cde22.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:16.061690",
    "title": "Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long Chain-of-Thought for Mathematical Reasoning",
    "summary": "The webpage describes a paper proposing a **Constrained Monte Carlo Tree Search (C-MCTS)** framework to generate reliable **Long Chain-of-Thoughts (CoTs)** for **mathematical reasoning** in **Large Language Models (LLMs)** without additional training.\n\nThis addresses the challenge of LLMs generating Long CoTs effectively, which is hindered by large action spaces and inefficient search strategies when using inference scaling methods like **MCTS**.\n\nThe C-MCTS framework works by:\n1.  **Constraining the action space** into five disjoint subsets: *understanding*, *planning*, *reflection*, *coding*, and *summary*. Each subset is limited to predefined prompts.\n2.  **Refining the search strategy** by incorporating prior knowledge, such as a human-like partial order of the action subsets and using pretrained process reward models.\n\nThese strategies significantly reduce the search space for Long CoTs. Evaluations show that this method allows a 7B model to achieve reasoning capabilities surpassing those of a 72B model in zero-shot settings on mathematical reasoning benchmarks.\n\nThe query covers several topics related to **reasoning and planning with LLMs**, and the paper specifically addresses:\n*   **Reasoning LLMs** (via mathematical reasoning improvement)\n*   **Chain-of-thought** (Long CoTs)\n*   **Inference-time compute** (inference scaling methods like MCTS)\n*   **Planning with LLMs** (the 'planning' subset in the constrained action space)\n*   **MCTS (Monte Carlo Tree Search) for language models** (C-MCTS framework)\n*   **Self-reflection** (the 'reflection' subset in the constrained action space)",
    "published_date": "2025-02-16T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2512.20617v1": {
    "content_path": "../data/contents/2512.20617v1.md",
    "pdf_path": "../data/contents/2512.20617v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:08.648243",
    "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
    "abstract": "Summary: Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
    "paper_id": "2512.20617v1"
  },
  "https://arxiv.org/abs/2512.17347v1": {
    "content_path": "../data/contents/2512.17347v1.md",
    "pdf_path": "../data/contents/2512.17347v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:09.425465",
    "title": "Stakeholder Suite: A Unified AI Framework for Mapping Actors, Topics and Arguments in Public Debates",
    "abstract": "Summary: Public debates surrounding infrastructure and energy projects involve complex networks of stakeholders, arguments, and evolving narratives. Understanding these dynamics is crucial for anticipating controversies and informing engagement strategies, yet existing tools in media intelligence largely rely on descriptive analytics with limited transparency. This paper presents Stakeholder Suite, a framework deployed in operational contexts for mapping actors, topics, and arguments within public debates. The system combines actor detection, topic modeling, argument extraction and stance classification in a unified pipeline. Tested on multiple energy infrastructure projects as a case study, the approach delivers fine-grained, source-grounded insights while remaining adaptable to diverse domains. The framework achieves strong retrieval precision and stance accuracy, producing arguments judged relevant in 75% of pilot use cases. Beyond quantitative metrics, the tool has proven effective for operational use: helping project teams visualize networks of influence, identify emerging controversies, and support evidence-based decision-making.",
    "paper_id": "2512.17347v1"
  },
  "https://arxiv.org/abs/2512.13228v1": {
    "content_path": "../data/contents/2512.13228v1.md",
    "pdf_path": "../data/contents/2512.13228v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:09.721427",
    "title": "ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data",
    "abstract": "Summary: Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.",
    "paper_id": "2512.13228v1"
  },
  "https://arxiv.org/abs/2510.08992": {
    "content_path": "../data/contents/2510.08992.md",
    "pdf_path": "../data/contents/2510.08992.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:14.135814",
    "title": "Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search",
    "abstract": "Represents steps as intent/constraint pairs and uses MCTS to prune infeasible branches—improves constraint-aligned planning and reduces hallucinated actions.",
    "paper_id": "2510.08992"
  },
  "https://arxiv.org/abs/2512.20615v1": {
    "content_path": "../data/contents/2512.20615v1.md",
    "pdf_path": "../data/contents/2512.20615v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:21.242500",
    "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "abstract": "Summary: Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.",
    "paper_id": "2512.20615v1"
  },
  "https://arxiv.org/abs/2512.17344v1": {
    "content_path": "../data/contents/2512.17344v1.md",
    "pdf_path": "../data/contents/2512.17344v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:21.870474",
    "title": "Governance-Aware Hybrid Fine-Tuning for Multilingual Large Language Models",
    "abstract": "Summary: We present a governance-aware hybrid fine-tuning framework for multilingual, low-resource adaptation of large language models. The core algorithm combines gradient-aligned low-rank updates with structured orthogonal transformations through layer-wise mixing and introduces unitary constraints in selected sub-layers to stabilize deep optimization. In tandem with lightweight, label-free data governance steps, including language identification, near-duplicate removal, and quality filtering, the framework targets accuracy, calibration, and cross-language parity under tight compute budgets. Across XNLI and FLORES, the hybrid approach delivers consistent gains over strong PEFT baselines while maintaining directional balance and improving probability calibration, as shown in Tables II and III. It is more resilient to lightweight orthographic variants, as shown in Table IV, and benefits additively from simple governance steps, as shown in Table V. Training footprint measurements indicate modest overhead and a favorable cost-quality frontier, as shown in Table VI and Figure 2. Together, these results show that hybrid and unitary PEFT provide a stable and accessible path to resource-efficient multilingual adaptation when paired with practical data governance.",
    "paper_id": "2512.17344v1"
  },
  "https://arxiv.org/abs/2512.15783v1": {
    "content_path": "../data/contents/2512.15783v1.md",
    "pdf_path": "../data/contents/2512.15783v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:22.550563",
    "title": "AI Epidemiology: achieving explainable AI through expert oversight patterns",
    "abstract": "Summary: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.\n  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.\n  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.",
    "paper_id": "2512.15783v1"
  },
  "https://arxiv.org/abs/2509.17116": {
    "content_path": "../data/contents/2509.17116.md",
    "pdf_path": "../data/contents/2509.17116.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:23.051228",
    "title": "MCTS-EP: Empowering Embodied Planning with Online Preference Optimization",
    "abstract": "Combines LLMs with MCTS-guided exploration and online preference optimization for embodied agents; shows strong gains on ALFWorld/WebShop planning tasks.",
    "paper_id": "2509.17116"
  },
  "https://arxiv.org/abs/2512.20561v1": {
    "content_path": "../data/contents/2512.20561v1.md",
    "pdf_path": "../data/contents/2512.20561v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:26.480106",
    "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
    "abstract": "Summary: Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.\n  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.\n  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.",
    "paper_id": "2512.20561v1"
  },
  "https://arxiv.org/abs/2512.17325v1": {
    "content_path": "../data/contents/2512.17325v1.md",
    "pdf_path": "../data/contents/2512.17325v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:27.216195",
    "title": "Task Schema and Binding: A Double Dissociation Study of In-Context Learning",
    "abstract": "Summary: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:\n  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms\n  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)\n  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba\n  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.",
    "paper_id": "2512.17325v1"
  },
  "https://arxiv.org/abs/2512.13194v3": {
    "content_path": "../data/contents/2512.13194v3.md",
    "pdf_path": "../data/contents/2512.13194v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:27.500602",
    "title": "Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models",
    "abstract": "Summary: Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant \"random rejection\" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as 1 - max(P_target). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.",
    "paper_id": "2512.13194v3"
  },
  "https://arxiv.org/abs/2512.20557v1": {
    "content_path": "../data/contents/2512.20557v1.md",
    "pdf_path": "../data/contents/2512.20557v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:31.029842",
    "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
    "abstract": "Summary: Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.",
    "paper_id": "2512.20557v1"
  },
  "https://arxiv.org/abs/2512.17319v1": {
    "content_path": "../data/contents/2512.17319v1.md",
    "pdf_path": "../data/contents/2512.17319v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:34.629112",
    "title": "A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs",
    "abstract": "Summary: Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR",
    "paper_id": "2512.17319v1"
  },
  "https://arxiv.org/abs/2512.13178v1": {
    "content_path": "../data/contents/2512.13178v1.md",
    "pdf_path": "../data/contents/2512.13178v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:41.858454",
    "title": "Predicting the Emergence of the EV Industry: A Product Space Analysis Across Regions and Firms",
    "abstract": "Summary: The automotive industry is undergoing transformation, driven by the electrification of powertrains, the rise of software-defined vehicles, and the adoption of circular economy concepts. These trends blur the boundaries between the automotive sector and other industries. Unlike internal combustion engine (ICE) production, where mechanical capabilities dominated, competitiveness in electric vehicle (EV) production increasingly depends on expertise in electronics, batteries, and software. This study investigates whether and how firms' ability to leverage cross-industry diversification contributes to competitive advantage. We develop a country-level product space covering all industries and an industry-specific product space covering over 900 automotive components. This allows us to identify clusters of parts that are exported together, revealing shared manufacturing capabilities. Closeness centrality in the country-level product space, rather than simple proximity, is a strong predictor of where new comparative advantages are likely to emerge. We examine this relationship across industrial sectors to establish patterns of path dependency, diversification and capability formation, and then focus on the EV transition. New strengths in vehicles and aluminium products in the EU are expected to generate 5 and 4.6 times more EV-specific strengths, respectively, than other EV-relevant sectors over the next decade, compared to only 1.6 and 4.5 new strengths in already diversified China. Countries such as South Korea, China, the US and Canada show strong potential for diversification into EV-related products, while established producers in the EU are likely to come under pressure. These findings suggest that the success of the automotive transformation depends on regions' ability to mobilize existing industrial capabilities, particularly in sectors such as machinery and electronic equipment.",
    "paper_id": "2512.13178v1"
  },
  "https://arxiv.org/abs/2512.20535v1": {
    "content_path": "../data/contents/2512.20535v1.md",
    "pdf_path": "../data/contents/2512.20535v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:42.061999",
    "title": "ARBITER: AI-Driven Filtering for Role-Based Access Control",
    "abstract": "Summary: Role-Based Access Control (RBAC) struggles to adapt to dynamic enterprise environments with documents that contain information that cannot be disclosed to specific user groups. As these documents are used by LLM-driven systems (e.g., in RAG) the problem is exacerbated as LLMs can leak sensitive data due to prompt truncation, classification errors, or loss of system context. We introduce \\our, a system designed to provide RBAC in RAG systems. \\our implements layered input/output validation, role-aware retrieval, and post-generation fact-checking. Unlike traditional RBAC approaches that rely on fine-tuned classifiers, \\our uses LLMs operating in few-shot settings with prompt-based steering for rapid deployment and role updates. We evaluate the approach on 389 queries using a synthetic dataset. Experimental results show 85\\% accuracy and 89\\% F1-score in query filtering, close to traditional RBAC solutions. Results suggest that practical RBAC deployment on RAG systems is approaching the maturity level needed for dynamic enterprise environments.",
    "paper_id": "2512.20535v1"
  },
  "https://arxiv.org/abs/2512.17316v1": {
    "content_path": "../data/contents/2512.17316v1.md",
    "pdf_path": "../data/contents/2512.17316v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:42.923058",
    "title": "Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability",
    "abstract": "Summary: Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - \"we know it when we see it\". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \\textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.",
    "paper_id": "2512.17316v1"
  },
  "https://arxiv.org/abs/2512.13173v1": {
    "content_path": "../data/contents/2512.13173v1.md",
    "pdf_path": "../data/contents/2512.13173v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:43.171442",
    "title": "Know Your Users! Estimating User Domain Knowledge in Conversational Recommenders",
    "abstract": "Summary: The ideal conversational recommender system (CRS) acts like a savvy salesperson, adapting its language and suggestions to each user's level of expertise. However, most current systems treat all users as experts, leading to frustrating and inefficient interactions when users are unfamiliar with a domain. Systems that can adapt their conversational strategies to a user's knowledge level stand to offer a much more natural and effective experience. To make a step toward such adaptive systems, we introduce a new task: estimating user domain knowledge from conversations, enabling a CRS to better understand user needs and personalize interactions. A key obstacle to developing such adaptive systems is the lack of suitable data; to our knowledge, no existing dataset captures the conversational behaviors of users with varying levels of domain knowledge. Furthermore, in most dialogue collection protocols, users are free to express their own preferences, which tends to concentrate on popular items and well-known features, offering little insight into how novices explore or learn about unfamiliar features. To address this, we design a game-based data collection protocol that elicits varied expressions of knowledge, release the resulting dataset, and provide an initial analysis to highlight its potential for future work on user-knowledge-aware CRS.",
    "paper_id": "2512.13173v1"
  },
  "https://arxiv.org/abs/2512.20501v1": {
    "content_path": "../data/contents/2512.20501v1.md",
    "pdf_path": "../data/contents/2512.20501v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:46.716816",
    "title": "Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition",
    "abstract": "Summary: This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.\n  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.\n  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.\n  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.\n  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.\n  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.\n  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.",
    "paper_id": "2512.20501v1"
  },
  "https://arxiv.org/abs/2512.17281v1": {
    "content_path": "../data/contents/2512.17281v1.md",
    "pdf_path": "../data/contents/2512.17281v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:47.241493",
    "title": "LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection",
    "abstract": "Summary: Robust Voice Activity Detection (VAD) remains a challenging task, especially under noisy, diverse, and unseen acoustic conditions. Beyond algorithmic development, a key limitation in advancing VAD research is the lack of large-scale, systematically controlled, and publicly available datasets. To address this, we introduce LibriVAD - a scalable open-source dataset derived from LibriSpeech and augmented with diverse real-world and synthetic noise sources. LibriVAD enables systematic control over speech-to-noise ratio, silence-to-speech ratio (SSR), and noise diversity, and is released in three sizes (15 GB, 150 GB, and 1.5 TB) with two variants (LibriVAD-NonConcat and LibriVAD-Concat) to support different experimental setups. We benchmark multiple feature-model combinations, including waveform, Mel-Frequency Cepstral Coefficients (MFCC), and Gammatone filter bank cepstral coefficients, and introduce the Vision Transformer (ViT) architecture for VAD. Our experiments show that ViT with MFCC features consistently outperforms established VAD models such as boosted deep neural network and convolutional long short-term memory deep neural network across seen, unseen, and out-of-distribution (OOD) conditions, including evaluation on the real-world VOiCES dataset. We further analyze the impact of dataset size and SSR on model generalization, experimentally showing that scaling up dataset size and balancing SSR noticeably and consistently enhance VAD performance under OOD conditions. All datasets, trained models, and code are publicly released to foster reproducibility and accelerate progress in VAD research.",
    "paper_id": "2512.17281v1"
  },
  "https://arxiv.org/abs/2512.13168v2": {
    "content_path": "../data/contents/2512.13168v2.md",
    "pdf_path": "../data/contents/2512.13168v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:47.597155",
    "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
    "abstract": "Summary: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.",
    "paper_id": "2512.13168v2"
  },
  "https://arxiv.org/abs/2512.20405v1": {
    "content_path": "../data/contents/2512.20405v1.md",
    "pdf_path": "../data/contents/2512.20405v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:48.364720",
    "title": "ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected",
    "abstract": "Summary: Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical systems. This research explores both the offensive and defensive sides of this growing threat. On the attack side, we demonstrate how an author can inject hidden prompts inside a PDF that secretly guide or \"jailbreak\" LLM reviewers into giving overly positive feedback and biased acceptance. On the defense side, we propose an \"inject-and-detect\" strategy for editors, where invisible trigger prompts are embedded into papers; if a review repeats or reacts to these triggers, it reveals that the review was generated by an LLM, not a human. This method turns prompt injections from vulnerability into a verification tool. We outline our design, expected model behaviors, and ethical safeguards for deployment. The goal is to expose how fragile today's peer-review process becomes under LLM influence and how editorial awareness can help restore trust in scientific evaluation.",
    "paper_id": "2512.20405v1"
  },
  "https://arxiv.org/abs/2512.17277v1": {
    "content_path": "../data/contents/2512.17277v1.md",
    "pdf_path": "../data/contents/2512.17277v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:49.072576",
    "title": "Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest",
    "abstract": "Summary: Pinterest is a leading visual discovery platform where recommender systems (RecSys) are key to delivering relevant, engaging, and fresh content to our users. In this paper, we study the problem of improving RecSys model predictions for cold-start (CS) items, which appear infrequently in the training data. Although this problem is well-studied in academia, few studies have addressed its root causes effectively at the scale of a platform like Pinterest. By investigating live traffic data, we identified several challenges of the CS problem and developed a corresponding solution for each: First, industrial-scale RecSys models must operate under tight computational constraints. Since CS items are a minority, any related improvements must be highly cost-efficient. To address this, our solutions were designed to be lightweight, collectively increasing the total parameters by only 5%. Second, CS items are represented only by non-historical (e.g., content or attribute) features, which models often treat as less important. To elevate their significance, we introduce a residual connection for the non-historical features. Third, CS items tend to receive lower prediction scores compared to non-CS items, reducing their likelihood of being surfaced. We mitigate this by incorporating a score regularization term into the model. Fourth, the labels associated with CS items are sparse, making it difficult for the model to learn from them. We apply the manifold mixup technique to address this data sparsity. Implemented together, our methods increased fresh content engagement at Pinterest by 10% without negatively impacting overall engagement and cost, and have been deployed to serve over 570 million users on Pinterest.",
    "paper_id": "2512.17277v1"
  },
  "https://arxiv.org/abs/2512.13159v1": {
    "content_path": "../data/contents/2512.13159v1.md",
    "pdf_path": "../data/contents/2512.13159v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:49.687678",
    "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning",
    "abstract": "Summary: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.",
    "paper_id": "2512.13159v1"
  },
  "https://arxiv.org/abs/2311.05232": {
    "content_path": "../data/contents/2311.05232.md",
    "pdf_path": "../data/contents/2311.05232.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:50.181405",
    "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
    "abstract": "Comprehensive survey on LLM hallucinations covering taxonomy, detection methods, benchmarks, and mitigation strategies.",
    "paper_id": "2311.05232"
  },
  "https://arxiv.org/abs/2512.20362v1": {
    "content_path": "../data/contents/2512.20362v1.md",
    "pdf_path": "../data/contents/2512.20362v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:53.880428",
    "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
    "abstract": "Summary: Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.",
    "paper_id": "2512.20362v1"
  },
  "https://arxiv.org/abs/2512.17270v1": {
    "content_path": "../data/contents/2512.17270v1.md",
    "pdf_path": "../data/contents/2512.17270v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:54.575559",
    "title": "Understanding Generalization in Role-Playing Models via Information Theory",
    "abstract": "Summary: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.",
    "paper_id": "2512.17270v1"
  },
  "https://arxiv.org/abs/2512.13154v1": {
    "content_path": "../data/contents/2512.13154v1.md",
    "pdf_path": "../data/contents/2512.13154v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:18:55.533166",
    "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations",
    "abstract": "Summary: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.",
    "paper_id": "2512.13154v1"
  },
  "https://arxiv.org/abs/2512.20334v1": {
    "content_path": "../data/contents/2512.20334v1.md",
    "pdf_path": "../data/contents/2512.20334v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:00.689572",
    "title": "Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation",
    "abstract": "Summary: With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.\n  This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.",
    "paper_id": "2512.20334v1"
  },
  "https://arxiv.org/abs/2512.17267v1": {
    "content_path": "../data/contents/2512.17267v1.md",
    "pdf_path": "../data/contents/2512.17267v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:04.682711",
    "title": "AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators",
    "abstract": "Summary: Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. We present AutoMetrics, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from MetricBank, a collection of 48 metrics we curate, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal. AutoMetrics takes you from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. We show that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. We release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.",
    "paper_id": "2512.17267v1"
  },
  "https://arxiv.org/abs/2512.13144v1": {
    "content_path": "../data/contents/2512.13144v1.md",
    "pdf_path": "../data/contents/2512.13144v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:06.568272",
    "title": "Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models",
    "abstract": "Summary: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.",
    "paper_id": "2512.13144v1"
  },
  "https://arxiv.org/abs/2512.20257v1": {
    "content_path": "../data/contents/2512.20257v1.md",
    "pdf_path": "../data/contents/2512.20257v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:09.071899",
    "title": "LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation",
    "abstract": "Summary: With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.",
    "paper_id": "2512.20257v1"
  },
  "https://arxiv.org/abs/2512.17264v1": {
    "content_path": "../data/contents/2512.17264v1.md",
    "pdf_path": "../data/contents/2512.17264v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:09.558163",
    "title": "Scalable Distributed Vector Search via Accuracy Preserving Index Construction",
    "abstract": "Summary: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.",
    "paper_id": "2512.17264v1"
  },
  "https://arxiv.org/abs/2512.13142v3": {
    "content_path": "../data/contents/2512.13142v3.md",
    "pdf_path": "../data/contents/2512.13142v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:09.934418",
    "title": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels",
    "abstract": "Summary: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (worries about judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.",
    "paper_id": "2512.13142v3"
  },
  "https://arxiv.org/abs/2405.06682": {
    "content_path": "../data/contents/2405.06682.md",
    "pdf_path": "../data/contents/2405.06682.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:10.163924",
    "title": "Self-Reflection in LLM Agents: Effects on Problem-Solving Performance",
    "abstract": "Empirical study showing LLM agents significantly improve problem-solving through self-reflection across multiple models.",
    "paper_id": "2405.06682"
  },
  "https://arxiv.org/abs/2512.20042v1": {
    "content_path": "../data/contents/2512.20042v1.md",
    "pdf_path": "../data/contents/2512.20042v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:10.577274",
    "title": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva",
    "abstract": "Summary: Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding",
    "paper_id": "2512.20042v1"
  },
  "https://arxiv.org/abs/2512.17260v1": {
    "content_path": "../data/contents/2512.17260v1.md",
    "pdf_path": "../data/contents/2512.17260v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:11.112324",
    "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience",
    "abstract": "Summary: Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present \\textbf{Seed-Prover 1.5}, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves \\textbf{88\\% of PutnamBench} (undergraduate-level), \\textbf{80\\% of Fate-H} (graduate-level), and \\textbf{33\\% of Fate-X} (PhD-level) problems. Notably, using our system, we solved \\textbf{11 out of 12 problems} from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.",
    "paper_id": "2512.17260v1"
  },
  "https://arxiv.org/abs/2512.13123v3": {
    "content_path": "../data/contents/2512.13123v3.md",
    "pdf_path": "../data/contents/2512.13123v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:11.593300",
    "title": "Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences",
    "abstract": "Summary: We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\\varepsilon$-optimal with probability at least $1-α$, with explicit bounds on the stopping time under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.",
    "paper_id": "2512.13123v3"
  },
  "https://arxiv.org/abs/2512.20749v1": {
    "content_path": "../data/contents/2512.20749v1.md",
    "pdf_path": "../data/contents/2512.20749v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:15.597928",
    "title": "Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies",
    "abstract": "Summary: In recent years, the development of multimodal autoencoders has gained significant attention due to their potential to handle multimodal complex data types and improve model performance. Understanding the stability and robustness of these models is crucial for optimizing their training, architecture, and real-world applicability. This paper presents an analysis of Lipschitz properties in multimodal autoencoders, combining both theoretical insights and empirical validation to enhance the training stability of these models. We begin by deriving the theoretical Lipschitz constants for aggregation methods within the multimodal autoencoder framework. We then introduce a regularized attention-based fusion method, developed based on our theoretical analysis, which demonstrates improved stability and performance during training. Through a series of experiments, we empirically validate our theoretical findings by estimating the Lipschitz constants across multiple trials and fusion strategies. Our results demonstrate that our proposed fusion function not only aligns with theoretical predictions but also outperforms existing strategies in terms of consistency, convergence speed, and accuracy. This work provides a solid theoretical foundation for understanding fusion in multimodal autoencoders and contributes a solution for enhancing their performance.",
    "paper_id": "2512.20749v1"
  },
  "https://arxiv.org/abs/2512.18610v1": {
    "content_path": "../data/contents/2512.18610v1.md",
    "pdf_path": "../data/contents/2512.18610v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:22.076960",
    "title": "The Procrustean Bed of Time Series: The Optimization Bias of Point-wise Loss",
    "abstract": "Summary: Optimizing time series models via point-wise loss functions (e.g., MSE) relying on a flawed point-wise independent and identically distributed (i.i.d.) assumption that disregards the causal temporal structure, an issue with growing awareness yet lacking formal theoretical grounding. Focusing on the core independence issue under covariance stationarity, this paper aims to provide a first-principles analysis of the Expectation of Optimization Bias (EOB), formalizing it information-theoretically as the discrepancy between the true joint distribution and its flawed i.i.d. counterpart. Our analysis reveals a fundamental paradigm paradox: the more deterministic and structured the time series, the more severe the bias by point-wise loss function. We derive the first closed-form quantification for the non-deterministic EOB across linear and non-linear systems, and prove EOB is an intrinsic data property, governed exclusively by sequence length and our proposed Structural Signal-to-Noise Ratio (SSNR). This theoretical diagnosis motivates our principled debiasing program that eliminates the bias through sequence length reduction and structural orthogonalization. We present a concrete solution that simultaneously achieves both principles via DFT or DWT. Furthermore, a novel harmonized $\\ell_p$ norm framework is proposed to rectify gradient pathologies of high-variance series. Extensive experiments validate EOB Theory's generality and the superior performance of debiasing program.",
    "paper_id": "2512.18610v1"
  },
  "https://arxiv.org/abs/2512.14306v1": {
    "content_path": "../data/contents/2512.14306v1.md",
    "pdf_path": "../data/contents/2512.14306v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:24.784389",
    "title": "Inflation Attitudes of Large Language Models",
    "abstract": "Summary: This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, mimicking the information set and demographic characteristics of the Bank of England's Inflation Attitudes Survey (IAS). Our quasi-experimental design exploits the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. We find that GPT tracks aggregate survey projections and official statistics at short horizons. At a disaggregated level, GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content. We find that GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents. However, we also find that it lacks a consistent model of consumer price inflation. More generally, our approach could be used to evaluate the behaviour of LLMs for use in the social sciences, to compare different models, or to assist in survey design.",
    "paper_id": "2512.14306v1"
  },
  "https://arxiv.org/abs/2512.11544v1": {
    "content_path": "../data/contents/2512.11544v1.md",
    "pdf_path": "../data/contents/2512.11544v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:25.574623",
    "title": "AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives",
    "abstract": "Summary: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of \"AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)\". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.",
    "paper_id": "2512.11544v1"
  },
  "https://arxiv.org/abs/2512.20745v1": {
    "content_path": "../data/contents/2512.20745v1.md",
    "pdf_path": "../data/contents/2512.20745v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:26.099008",
    "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent",
    "abstract": "Summary: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.",
    "paper_id": "2512.20745v1"
  },
  "https://arxiv.org/abs/2512.18608v1": {
    "content_path": "../data/contents/2512.18608v1.md",
    "pdf_path": "../data/contents/2512.18608v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:26.437812",
    "title": "A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts",
    "abstract": "Summary: Automated masking of Personally Identifiable Information (PII) is critical for privacy-preserving conversational systems. While current frontier large language models demonstrate strong PII masking capabilities, concerns about data handling and computational costs motivate exploration of whether lightweight models can achieve comparable performance. We compare encoder-decoder and decoder-only architectures by fine-tuning T5-small and Mistral-Instruct-v0.3 on English datasets constructed from the AI4Privacy benchmark. We create different dataset variants to study label standardization and PII representation, covering 24 standardized PII categories and higher-granularity settings. Evaluation using entity-level and character-level metrics, type accuracy, and exact match shows that both lightweight models achieve performance comparable to frontier LLMs for PII masking tasks. Label normalization consistently improves performance across architectures. Mistral achieves higher F1 and recall with greater robustness across PII types but incurs significantly higher generation latency. T5, while less robust in conversational text, offers more controllable structured outputs and lower inference cost, motivating its use in a real-time Discord bot for real-world PII redaction. Evaluation on live messages reveals performance degradation under informal inputs. These results clarify trade-offs between accuracy, robustness, and computational efficiency, demonstrating that lightweight models can provide effective PII masking while addressing data handling concerns associated with frontier LLMs.",
    "paper_id": "2512.18608v1"
  },
  "https://arxiv.org/abs/2512.19718v1": {
    "content_path": "../data/contents/2512.19718v1.md",
    "pdf_path": "../data/contents/2512.19718v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:28.795208",
    "title": "Synthetic Data Blueprint (SDB): A modular framework for the statistical, structural, and graph-based evaluation of synthetic tabular data",
    "abstract": "Summary: In the rapidly evolving era of Artificial Intelligence (AI), synthetic data are widely used to accelerate innovation while preserving privacy and enabling broader data accessibility. However, the evaluation of synthetic data remains fragmented across heterogeneous metrics, ad-hoc scripts, and incomplete reporting practices. To address this gap, we introduce Synthetic Data Blueprint (SDB), a modular Pythonic based library to quantitatively and visually assess the fidelity of synthetic tabular data. SDB supports: (i) automated feature-type detection, (ii) distributional and dependency-level fidelity metrics, (iii) graph- and embedding-based structure preservation scores, and (iv) a rich suite of data visualization schemas. To demonstrate the breadth, robustness, and domain-agnostic applicability of the SDB, we evaluated the framework across three real-world use cases that differ substantially in scale, feature composition, statistical complexity, and downstream analytical requirements. These include: (i) healthcare diagnostics, (ii) socioeconomic and financial modelling, and (iii) cybersecurity and network traffic analysis. These use cases reveal how SDB can address diverse data fidelity assessment challenges, varying from mixed-type clinical variables to high-cardinality categorical attributes and high-dimensional telemetry signals, while at the same time offering a consistent, transparent, and reproducible benchmarking across heterogeneous domains.",
    "paper_id": "2512.19718v1"
  },
  "https://arxiv.org/abs/2512.11541v1": {
    "content_path": "../data/contents/2512.11541v1.md",
    "pdf_path": "../data/contents/2512.11541v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:29.304228",
    "title": "A Multi-Criteria Automated MLOps Pipeline for Cost-Effective Cloud-Based Classifier Retraining in Response to Data Distribution Shifts",
    "abstract": "Summary: The performance of machine learning (ML) models often deteriorates when the underlying data distribution changes over time, a phenomenon known as data distribution drift. When this happens, ML models need to be retrained and redeployed. ML Operations (MLOps) is often manual, i.e., humans trigger the process of model retraining and redeployment. In this work, we present an automated MLOps pipeline designed to address neural network classifier retraining in response to significant data distribution changes. Our MLOps pipeline employs multi-criteria statistical techniques to detect distribution shifts and triggers model updates only when necessary, ensuring computational efficiency and resource optimization. We demonstrate the effectiveness of our framework through experiments on several benchmark anomaly detection data sets, showing significant improvements in model accuracy and robustness compared to traditional retraining strategies. Our work provides a foundation for deploying more reliable and adaptive ML systems in dynamic real-world settings, where data distribution changes are common.",
    "paper_id": "2512.11541v1"
  },
  "https://arxiv.org/abs/2512.20732v1": {
    "content_path": "../data/contents/2512.20732v1.md",
    "pdf_path": "../data/contents/2512.20732v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:29.734562",
    "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
    "abstract": "Summary: As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.",
    "paper_id": "2512.20732v1"
  },
  "https://arxiv.org/abs/2512.18607v1": {
    "content_path": "../data/contents/2512.18607v1.md",
    "pdf_path": "../data/contents/2512.18607v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:34.488917",
    "title": "The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation",
    "abstract": "Summary: Understanding what kinds of cooperative structures deep neural networks (DNNs) can represent remains a fundamental yet insufficiently understood problem. In this work, we treat interactions as the fundamental units of such structure and investigate a largely unexplored question: how DNNs encode interactions under different levels of contextual complexity, and how these microscopic interaction patterns shape macroscopic representation capacity. To quantify this complexity, we use multi-order interactions [57], where each order reflects the amount of contextual information required to evaluate the joint interaction utility of a variable pair. This formulation enables a stratified analysis of cooperative patterns learned by DNNs. Building on this formulation, we develop a comprehensive study of interaction structure in DNNs. (i) We empirically discover a universal interaction bottleneck: across architectures and tasks, DNNs easily learn low-order and high-order interactions but consistently under-represent mid-order ones. (ii) We theoretically explain this bottleneck by proving that mid-order interactions incur the highest contextual variability, yielding large gradient variance and making them intrinsically difficult to learn. (iii) We further modulate the bottleneck by introducing losses that steer models toward emphasizing interactions of selected orders. Finally, we connect microscopic interaction structures with macroscopic representational behavior: low-order-emphasized models exhibit stronger generalization and robustness, whereas high-order-emphasized models demonstrate greater structural modeling and fitting capability. Together, these results uncover an inherent representational bias in modern DNNs and establish interaction order as a powerful lens for interpreting and guiding deep representations.",
    "paper_id": "2512.18607v1"
  },
  "https://arxiv.org/abs/2512.14278v1": {
    "content_path": "../data/contents/2512.14278v1.md",
    "pdf_path": "../data/contents/2512.14278v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:35.063107",
    "title": "The Trust in AI-Generated Health Advice (TAIGHA) Scale and Short Version (TAIGHA-S): Development and Validation Study",
    "abstract": "Summary: Artificial Intelligence tools such as large language models are increasingly used by the public to obtain health information and guidance. In health-related contexts, following or rejecting AI-generated advice can have direct clinical implications. Existing instruments like the Trust in Automated Systems Survey assess trustworthiness of generic technology, and no validated instrument measures users' trust in AI-generated health advice specifically. This study developed and validated the Trust in AI-Generated Health Advice (TAIGHA) scale and its four-item short form (TAIGHA-S) as theory-based instruments measuring trust and distrust, each with cognitive and affective components. The items were developed using a generative AI approach, followed by content validation with 10 domain experts, face validation with 30 lay participants, and psychometric validation with 385 UK participants who received AI-generated advice in a symptom-assessment scenario. After automated item reduction, 28 items were retained and reduced to 10 based on expert ratings. TAIGHA showed excellent content validity (S-CVI/Ave=0.99) and CFA confirmed a two-factor model with excellent fit (CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03). Internal consistency was high (α=0.95). Convergent validity was supported by correlations with the Trust in Automated Systems Survey (r=0.67/-0.66) and users' reliance on the AI's advice (r=0.37 for trust), while divergent validity was supported by low correlations with reading flow and mental load (all |r|<0.25). TAIGHA-S correlated highly with the full scale (r=0.96) and showed good reliability (α=0.88). TAIGHA and TAIGHA-S are validated instruments for assessing user trust and distrust in AI-generated health advice. Reporting trust and distrust separately permits a more complete evaluation of AI interventions, and the short scale is well-suited for time-constrained settings.",
    "paper_id": "2512.14278v1"
  },
  "https://arxiv.org/abs/2512.11534v1": {
    "content_path": "../data/contents/2512.11534v1.md",
    "pdf_path": "../data/contents/2512.11534v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:38.047887",
    "title": "HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning",
    "abstract": "Summary: Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.",
    "paper_id": "2512.11534v1"
  },
  "https://arxiv.org/abs/2512.19765v1": {
    "content_path": "../data/contents/2512.19765v1.md",
    "pdf_path": "../data/contents/2512.19765v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:38.956769",
    "title": "How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts",
    "abstract": "Summary: Finding the optimal configuration of Sparse Mixture-ofExperts (SMoE) that maximizes semantic differentiation among experts is essential for exploiting the full potential of MoE architectures. However, existing SMoE frameworks either heavily rely on hyperparameter tuning or overlook the importance of diversifying semantic roles across experts when adapting the expert pool size. We propose Mixture-of-Experts for Adaptive Semantic Specialization (MASS), a semanticaware MoE framework for adaptive expert expansion and dynamic routing. MASS introduces two key advancements: (i) a gradient-based semantic drift detector that prompts targeted expert expansion when the existing expert pool lacks capacity to capture the full semantic diversity of the data, and (ii) an integration of adaptive routing strategy that dynamically adjusts expert usage based on token-level routing confidence mass. We first demonstrate that MASS reliably converges to the point of optimal balance between cost-performance trade-off with notably improved sematic specialization in a highly controlled synthetic setup. Further empirical results on real-world datasets across language and vision domains show that MASS consistently outperforms a range of strong MoE baselines, demonstrating its domain robustness and enhanced expert specialization.",
    "paper_id": "2512.19765v1"
  },
  "https://arxiv.org/abs/2512.14277v1": {
    "content_path": "../data/contents/2512.14277v1.md",
    "pdf_path": "../data/contents/2512.14277v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:39.243188",
    "title": "SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions",
    "abstract": "Summary: The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.",
    "paper_id": "2512.14277v1"
  },
  "https://arxiv.org/abs/2512.11529v2": {
    "content_path": "../data/contents/2512.11529v2.md",
    "pdf_path": "../data/contents/2512.11529v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:40.153186",
    "title": "xGR: Efficient Generative Recommendation Serving at Scale",
    "abstract": "Summary: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.",
    "paper_id": "2512.11529v2"
  },
  "https://arxiv.org/abs/2512.18605v1": {
    "content_path": "../data/contents/2512.18605v1.md",
    "pdf_path": "../data/contents/2512.18605v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:41.281856",
    "title": "Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction",
    "abstract": "Summary: Large language models (LLMs) have achieved strong performance on complex reasoning tasks using techniques such as chain-of-thought and self-consistency. However, ensemble-based approaches, especially self-consistency which relies on multiple reasoning trajectories, often incur substantial computational overhead. To improve efficiency, prior work has leveraged internal confidence signals, where early stopping strategies such as DeepConf reduce cost by terminating low-confidence trajectories. However, this strategy discards incomplete reasoning paths and wastes partial computation.\n  We propose reflective confidence, a novel reasoning framework that transforms low-confidence signals from termination indicators into reflection triggers. When confidence falls below a threshold, instead of stopping generation, the model produces a reflection prompt to analyze the current reasoning state, identify potential errors, and continue generation along a corrected trajectory. Experiments on mathematical reasoning benchmarks, including AIME 2025, demonstrate significant accuracy improvements over advanced early-stopping baselines at comparable computational cost, validating the effectiveness of proactive self-correction over passive discarding.",
    "paper_id": "2512.18605v1"
  },
  "https://arxiv.org/abs/2512.14263v1": {
    "content_path": "../data/contents/2512.14263v1.md",
    "pdf_path": "../data/contents/2512.14263v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:43.084713",
    "title": "Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization",
    "abstract": "Summary: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.",
    "paper_id": "2512.14263v1"
  },
  "https://arxiv.org/abs/2512.11526v1": {
    "content_path": "../data/contents/2512.11526v1.md",
    "pdf_path": "../data/contents/2512.11526v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:43.600597",
    "title": "Contrastive Time Series Forecasting with Anomalies",
    "abstract": "Summary: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.",
    "paper_id": "2512.11526v1"
  },
  "https://arxiv.org/abs/2408.16672": {
    "content_path": "../data/contents/2408.16672.md",
    "pdf_path": "../data/contents/2408.16672.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:44.319840",
    "title": "Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever",
    "abstract": "New multilingual ColBERT variant with 89 language support, Matryoshka embeddings, and 8192 token context for late interaction retrieval.",
    "paper_id": "2408.16672"
  },
  "https://arxiv.org/abs/2512.02772": {
    "content_path": "../data/contents/2512.02772.md",
    "pdf_path": "../data/contents/2512.02772.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:44.626370",
    "title": "Towards Unification of Hallucination Detection and Fact Verification for Large Language Models",
    "abstract": "Proposes UniFact to unify hallucination detection and fact verification evaluation; useful for benchmarking factuality/grounding methods.",
    "paper_id": "2512.02772"
  },
  "https://arxiv.org/abs/2512.18601v1": {
    "content_path": "../data/contents/2512.18601v1.md",
    "pdf_path": "../data/contents/2512.18601v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:47.716536",
    "title": "On Finding Inconsistencies in Documents",
    "abstract": "Summary: Professionals in academia, law, and finance audit their documents because inconsistencies can result in monetary, reputational, and scientific costs. Language models (LMs) have the potential to dramatically speed up this auditing process. To understand their abilities, we introduce a benchmark, FIND (Finding INconsistencies in Documents), where each example is a document with an inconsistency inserted manually by a domain expert. Despite the documents being long, technical, and complex, the best-performing model (gpt-5) recovered 64% of the inserted inconsistencies. Surprisingly, gpt-5 also found undiscovered inconsistencies present in the original documents. For example, on 50 arXiv papers, we judged 136 out of 196 of the model's suggestions to be legitimate inconsistencies missed by the original authors. However, despite these findings, even the best models miss almost half of the inconsistencies in FIND, demonstrating that inconsistency detection is still a challenging task.",
    "paper_id": "2512.18601v1"
  },
  "https://arxiv.org/abs/2512.14253v1": {
    "content_path": "../data/contents/2512.14253v1.md",
    "pdf_path": "../data/contents/2512.14253v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:49.956327",
    "title": "FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting",
    "abstract": "Summary: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.",
    "paper_id": "2512.14253v1"
  },
  "https://arxiv.org/abs/2512.11943v1": {
    "content_path": "../data/contents/2512.11943v1.md",
    "pdf_path": "../data/contents/2512.11943v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:50.177693",
    "title": "How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism",
    "abstract": "Summary: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent \"AI optimism\"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.",
    "paper_id": "2512.11943v1"
  },
  "https://arxiv.org/abs/2512.09636": {
    "content_path": "../data/contents/2512.09636.md",
    "pdf_path": "../data/contents/2512.09636.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:50.707880",
    "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment",
    "abstract": "Introduces MentraBench for reasoning quality (incl. hallucination avoidance/internal consistency) and a post-trained model—useful for reasoning reliability evaluation.",
    "paper_id": "2512.09636"
  },
  "https://arxiv.org/abs/2512.18584v1": {
    "content_path": "../data/contents/2512.18584v1.md",
    "pdf_path": "../data/contents/2512.18584v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:51.055378",
    "title": "State-Space Modeling of Time-Varying Spillovers on Networks",
    "abstract": "Summary: Many modern time series arise on networks, where each component is attached to a node and interactions follow observed edges. Classical time-varying parameter VARs (TVP-VARs) treat all series symmetrically and ignore this structure, while network autoregressive models exploit a given graph but usually impose constant parameters and stationarity. We develop network state-space models in which a low-dimensional latent state controls time-varying network spillovers, own-lag persistence and nodal covariate effects. A key special case is a network time-varying parameter VAR (NTVP-VAR) that constrains each lag matrix to be a linear combination of known network operators, such as a row-normalised adjacency and the identity, and lets the associated coefficients evolve stochastically in time. The framework nests Gaussian and Poisson network autoregressions, network ARIMA models with graph differencing, and dynamic edge models driven by multivariate logistic regression. We give conditions ensuring that NTVP-VARs are well-defined in second moments despite nonstationary states, describe network versions of stability and local stationarity, and discuss shrinkage, thresholding and low-rank tensor structures for high-dimensional graphs. Conceptually, network state-space models separate where interactions may occur (the graph) from how strong they are at each time (the latent state), providing an interpretable alternative to both unstructured TVP-VARs and existing network time-series models.",
    "paper_id": "2512.18584v1"
  },
  "https://arxiv.org/abs/2512.14252v1": {
    "content_path": "../data/contents/2512.14252v1.md",
    "pdf_path": "../data/contents/2512.14252v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:51.289321",
    "title": "Gödel's Poetry",
    "abstract": "Summary: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.",
    "paper_id": "2512.14252v1"
  },
  "https://arxiv.org/abs/2512.11509v1": {
    "content_path": "../data/contents/2512.11509v1.md",
    "pdf_path": "../data/contents/2512.11509v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:51.821527",
    "title": "Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs",
    "abstract": "Summary: Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.",
    "paper_id": "2512.11509v1"
  },
  "https://arxiv.org/abs/2511.19131": {
    "content_path": "../data/contents/2511.19131.md",
    "pdf_path": "../data/contents/2511.19131.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:53.049567",
    "title": "Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization",
    "abstract": "Hidden-state optimization method to elicit CoT reasoning from base models without full post-training—relevant to controllable reasoning elicitation.",
    "paper_id": "2511.19131"
  },
  "https://arxiv.org/abs/2512.18571v1": {
    "content_path": "../data/contents/2512.18571v1.md",
    "pdf_path": "../data/contents/2512.18571v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:54.380864",
    "title": "ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning",
    "abstract": "Summary: Multimodal Large Language Models (MLLMs) have empowered embodied agents with remarkable capabilities in planning and reasoning. However, when facing ambiguous natural language instructions (e.g., \"fetch the tool\" in a cluttered room), current agents often fail to balance the high cost of physical exploration against the cognitive cost of human interaction. They typically treat disambiguation as a passive perception problem, lacking the strategic reasoning to minimize total task execution costs. To bridge this gap, we propose ESearch-R1, a cost-aware embodied reasoning framework that unifies interactive dialogue (Ask), episodic memory retrieval (GetMemory), and physical navigation (Navigate) into a single decision process. We introduce HC-GRPO (Heterogeneous Cost-Aware Group Relative Policy Optimization). Unlike traditional PPO which relies on a separate value critic, HC-GRPO optimizes the MLLM by sampling groups of reasoning trajectories and reinforcing those that achieve the optimal trade-off between information gain and heterogeneous costs (e.g., navigate time, and human attention). Extensive experiments in AI2-THOR demonstrate that ESearch-R1 significantly outperforms standard ReAct-based agents. It improves task success rates while reducing total operational costs by approximately 50\\%, validating the effectiveness of GRPO in aligning MLLM agents with physical world constraints.",
    "paper_id": "2512.18571v1"
  },
  "https://arxiv.org/abs/2512.20650v1": {
    "content_path": "../data/contents/2512.20650v1.md",
    "pdf_path": "../data/contents/2512.20650v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:54.698077",
    "title": "Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA",
    "abstract": "Summary: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.",
    "paper_id": "2512.20650v1"
  },
  "https://arxiv.org/abs/2512.11506v2": {
    "content_path": "../data/contents/2512.11506v2.md",
    "pdf_path": "../data/contents/2512.11506v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:55.422508",
    "title": "EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection",
    "abstract": "Summary: As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.",
    "paper_id": "2512.11506v2"
  },
  "https://arxiv.org/abs/2510.05972": {
    "content_path": "../data/contents/2510.05972.md",
    "pdf_path": "../data/contents/2510.05972.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:56.363500",
    "title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language",
    "abstract": "Benchmark targeting constrained planning (temporal/safety constraints) expressed in NL; diagnostic for LLM planning under constraints.",
    "paper_id": "2510.05972"
  },
  "https://arxiv.org/abs/2512.18567v1": {
    "content_path": "../data/contents/2512.18567v1.md",
    "pdf_path": "../data/contents/2512.18567v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:57.518796",
    "title": "AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software",
    "abstract": "Summary: Large language models (LLMs) for code generation are becoming integral to modern software development, but their real-world prevalence and security impact remain poorly understood.\n  We present the first large-scale empirical study of AI-generated code (AIGCode) in the wild. We build a high-precision detection pipeline and a representative benchmark to distinguish AIGCode from human-written code, and apply them to (i) development commits from the top 1,000 GitHub repositories (2022-2025) and (ii) 7,000+ recent CVE-linked code changes. This lets us label commits, files, and functions along a human/AI axis and trace how AIGCode moves through projects and vulnerability life cycles.\n  Our measurements show three ecological patterns. First, AIGCode is already a substantial fraction of new code, but adoption is structured: AI concentrates in glue code, tests, refactoring, documentation, and other boilerplate, while core logic and security-critical configurations remain mostly human-written. Second, adoption has security consequences: some CWE families are overrepresented in AI-tagged code, and near-identical insecure templates recur across unrelated projects, suggesting \"AI-induced vulnerabilities\" propagated by shared models rather than shared maintainers. Third, in human-AI edit chains, AI introduces high-throughput changes while humans act as security gatekeepers; when review is shallow, AI-introduced defects persist longer, remain exposed on network-accessible surfaces, and spread to more files and repositories.\n  We will open-source the complete dataset and release analysis artifacts and fine-grained documentation of our methodology and findings.",
    "paper_id": "2512.18567v1"
  },
  "https://arxiv.org/abs/2512.14246v1": {
    "content_path": "../data/contents/2512.14246v1.md",
    "pdf_path": "../data/contents/2512.14246v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:58.240257",
    "title": "Randomized multi-class classification under system constraints: a unified approach via post-processing",
    "abstract": "Summary: We study the problem of multi-class classification under system-level constraints expressible as linear functionals over randomized classifiers. We propose a post-processing approach that adjusts a given base classifier to satisfy general constraints without retraining. Our method formulates the problem as a linearly constrained stochastic program over randomized classifiers, and leverages entropic regularization and dual optimization techniques to construct a feasible solution. We provide finite-sample guarantees for the risk and constraint satisfaction for the final output of our algorithm under minimal assumptions. The framework accommodates a broad class of constraints, including fairness, abstention, and churn requirements.",
    "paper_id": "2512.14246v1"
  },
  "https://arxiv.org/abs/2512.14741v1": {
    "content_path": "../data/contents/2512.14741v1.md",
    "pdf_path": "../data/contents/2512.14741v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:58.863862",
    "title": "Persistent Backdoor Attacks under Continual Fine-Tuning of LLMs",
    "abstract": "Summary: Backdoor attacks embed malicious behaviors into Large Language Models (LLMs), enabling adversaries to trigger harmful outputs or bypass safety controls. However, the persistence of the implanted backdoors under user-driven post-deployment continual fine-tuning has been rarely examined. Most prior works evaluate the effectiveness and generalization of implanted backdoors only at releasing and empirical evidence shows that naively injected backdoor persistence degrades after updates. In this work, we study whether and how implanted backdoors persist through a multi-stage post-deployment fine-tuning. We propose P-Trojan, a trigger-based attack algorithm that explicitly optimizes for backdoor persistence across repeated updates. By aligning poisoned gradients with those of clean tasks on token embeddings, the implanted backdoor mapping is less likely to be suppressed or forgotten during subsequent updates. Theoretical analysis shows the feasibility of such persistent backdoor attacks after continual fine-tuning. And experiments conducted on the Qwen2.5 and LLaMA3 families of LLMs, as well as diverse task sequences, demonstrate that P-Trojan achieves over 99% persistence while preserving clean-task accuracy. Our findings highlight the need for persistence-aware evaluation and stronger defenses in realistic model adaptation pipelines.",
    "paper_id": "2512.14741v1"
  },
  "https://arxiv.org/abs/2402.03216": {
    "content_path": "../data/contents/2402.03216.md",
    "pdf_path": "../data/contents/2402.03216.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:19:59.137220",
    "title": "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings",
    "abstract": "BAAI releases BGE-M3 supporting 100+ languages, dense/sparse/multi-vector retrieval, and 8192 token inputs in single unified model.",
    "paper_id": "2402.03216"
  },
  "https://arxiv.org/abs/2510.05592": {
    "content_path": "../data/contents/2510.05592.md",
    "pdf_path": "../data/contents/2510.05592.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:00.381768",
    "title": "In-the-Flow Agentic System Optimization for Effective Planning and Tool Use",
    "abstract": "AgentFlow trains planner/executor/verifier modules “in-the-loop” (Flow-GRPO) for long-horizon planning + tool use—important for agentic planning research.",
    "paper_id": "2510.05592"
  },
  "https://arxiv.org/abs/2512.18564v1": {
    "content_path": "../data/contents/2512.18564v1.md",
    "pdf_path": "../data/contents/2512.18564v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:05.988964",
    "title": "Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V",
    "abstract": "Summary: Large Language Models' capacity to reason in natural language makes them uniquely promising for 4X and grand strategy games, enabling more natural human-AI gameplay interactions such as collaboration and negotiation. However, these games present unique challenges due to their complexity and long-horizon nature, while latency and cost factors may hinder LLMs' real-world deployment. Working on a classic 4X strategy game, Sid Meier's Civilization V with the Vox Populi mod, we introduce Vox Deorum, a hybrid LLM+X architecture. Our layered technical design empowers LLMs to handle macro-strategic reasoning, delegating tactical execution to subsystems (e.g., algorithmic AI or reinforcement learning AI in the future). We validate our approach through 2,327 complete games, comparing two open-source LLMs with a simple prompt against Vox Populi's enhanced AI. Results show that LLMs achieve competitive end-to-end gameplay while exhibiting play styles that diverge substantially from algorithmic AI and from each other. Our work establishes a viable architecture for integrating LLMs in commercial 4X games, opening new opportunities for game design and agentic AI research.",
    "paper_id": "2512.18564v1"
  },
  "https://arxiv.org/abs/2512.14244v2": {
    "content_path": "../data/contents/2512.14244v2.md",
    "pdf_path": "../data/contents/2512.14244v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:06.507658",
    "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
    "abstract": "Summary: Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.",
    "paper_id": "2512.14244v2"
  },
  "https://arxiv.org/abs/2512.11490v1": {
    "content_path": "../data/contents/2512.11490v1.md",
    "pdf_path": "../data/contents/2512.11490v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:06.965274",
    "title": "VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing",
    "abstract": "Summary: Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.",
    "paper_id": "2512.11490v1"
  },
  "https://arxiv.org/abs/2509.26626": {
    "content_path": "../data/contents/2509.26626.md",
    "pdf_path": "../data/contents/2509.26626.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:09.780188",
    "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models",
    "abstract": "Proposes RSA combining parallel+sequential scaling by aggregating partial reasoning chains; strong test-time scaling method with reported gains across benchmarks.",
    "paper_id": "2509.26626"
  },
  "https://arxiv.org/abs/2512.18561v1": {
    "content_path": "../data/contents/2512.18561v1.md",
    "pdf_path": "../data/contents/2512.18561v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:10.566979",
    "title": "Adaptive Accountability in Networked MAS: Tracing and Mitigating Emergent Norms at Scale",
    "abstract": "Summary: Large-scale networked multi-agent systems increasingly underpin critical infrastructure, yet their collective behavior can drift toward undesirable emergent norms that elude conventional governance mechanisms. We introduce an adaptive accountability framework that (i) continuously traces responsibility flows through a lifecycle-aware audit ledger, (ii) detects harmful emergent norms online via decentralized sequential hypothesis tests, and (iii) deploys local policy and reward-shaping interventions that realign agents with system-level objectives in near real time. We prove a bounded-compromise theorem showing that whenever the expected intervention cost exceeds an adversary's payoff, the long-run proportion of compromised interactions is bounded by a constant strictly less than one. Extensive high-performance simulations with up to 100 heterogeneous agents, partial observability, and stochastic communication graphs show that our framework prevents collusion and resource hoarding in at least 90% of configurations, boosts average collective reward by 12-18%, and lowers the Gini inequality index by up to 33% relative to a PPO baseline. These results demonstrate that a theoretically principled accountability layer can induce ethically aligned, self-regulating behavior in complex MAS without sacrificing performance or scalability.",
    "paper_id": "2512.18561v1"
  },
  "https://arxiv.org/abs/2512.14237v1": {
    "content_path": "../data/contents/2512.14237v1.md",
    "pdf_path": "../data/contents/2512.14237v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:11.855502",
    "title": "Ladder Up, Memory Down: Low-Cost Fine-Tuning With Side Nets",
    "abstract": "Summary: Fine-tuning large language models (LLMs) is often limited by the memory available on commodity GPUs. Parameter-efficient fine-tuning (PEFT) methods such as QLoRA reduce the number of trainable parameters, yet still incur high memory usage induced by the backward pass in the full model. We revisit Ladder Side Tuning (LST), a rarely explored PEFT technique that adds a lightweight side network, and show that it matches QLoRA's compute scaling slope while cutting peak memory by 50\\%. Across different downstream benchmarks spanning natural language understanding, mathematical and LLM-critic tasks, LST has competitive performance with QLoRA's accuracy on average while being much more memory-efficient. This efficiency enables fine-tuning of 7B-parameter models on a single 12 GB consumer GPU with 2k-token contexts, requiring no gradient checkpointing\\textemdash conditions under which QLoRA exhausts memory. Beyond memory efficiency, we also establish scaling laws showing that LST scales similarly to QLoRA. We exploit Ladder's architectural flexibility by introducing xLadder, a depth-extended variant that increases effective depth via cross-connections and shortens chain-of-thought (CoT) at fixed parameter count. Ladder is strong when memory is the bottleneck; xLadder builds on this by enabling deeper reasoning without additional memory overhead.",
    "paper_id": "2512.14237v1"
  },
  "https://arxiv.org/abs/2512.11485v1": {
    "content_path": "../data/contents/2512.11485v1.md",
    "pdf_path": "../data/contents/2512.11485v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:13.217278",
    "title": "Mistake Notebook Learning: Selective Batch-Wise Context Optimization for In-Context Learning",
    "abstract": "Summary: Large language models (LLMs) adapt to tasks via gradient fine-tuning (heavy computation, catastrophic forgetting) or In-Context Learning (ICL: low robustness, poor mistake learning). To fix this, we introduce Mistake Notebook Learning (MNL), a training-free framework with a persistent knowledge base of abstracted error patterns. Unlike prior instance/single-trajectory memory methods, MNL uses batch-wise error abstraction: it extracts generalizable guidance from multiple failures, stores insights in a dynamic notebook, and retains only baseline-outperforming guidance via hold-out validation (ensuring monotonic improvement). We show MNL nearly matches Supervised Fine-Tuning (93.9% vs 94.3% on GSM8K) and outperforms training-free alternatives on GSM8K, Spider, AIME, and KaggleDBQA. On KaggleDBQA (Qwen3-8B), MNL hits 28% accuracy (47% relative gain), outperforming Memento (15.1%) and Training-Free GRPO (22.1) - proving it's a strong training-free alternative for complex reasoning.",
    "paper_id": "2512.11485v1"
  },
  "https://arxiv.org/abs/2509.25779": {
    "content_path": "../data/contents/2509.25779.md",
    "pdf_path": "../data/contents/2509.25779.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:17.527469",
    "title": "Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs",
    "abstract": "Shows reward shaping enables strong agentic RL for planning benchmarks (e.g., TravelPlanner) with smaller LLMs—relevant to efficient planning training.",
    "paper_id": "2509.25779"
  },
  "https://arxiv.org/abs/2512.18554v1": {
    "content_path": "../data/contents/2512.18554v1.md",
    "pdf_path": "../data/contents/2512.18554v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:18.726036",
    "title": "Enhancing Medical Large Vision-Language Models via Alignment Distillation",
    "abstract": "Summary: Medical Large Vision-Language Models (Med-LVLMs) have shown promising results in clinical applications, but often suffer from hallucinated outputs due to misaligned visual understanding. In this work, we identify two fundamental limitations contributing to this issue: insufficient visual representation learning and poor visual attention alignment. To address these problems, we propose MEDALIGN, a simple, lightweight alignment distillation framework that transfers visual alignment knowledge from a domain-specific Contrastive Language-Image Pre-training (CLIP) model to Med-LVLMs. MEDALIGN introduces two distillation losses: a spatial-aware visual alignment loss based on visual token-level similarity structures, and an attention-aware distillation loss that guides attention toward diagnostically relevant regions. Extensive experiments on medical report generation and medical visual question answering (VQA) benchmarks show that MEDALIGN consistently improves both performance and interpretability, yielding more visually grounded outputs.",
    "paper_id": "2512.18554v1"
  },
  "https://arxiv.org/abs/2512.19717v1": {
    "content_path": "../data/contents/2512.19717v1.md",
    "pdf_path": "../data/contents/2512.19717v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:19.286326",
    "title": "Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference",
    "abstract": "Summary: Finding rare but useful solutions in very large candidate spaces is a recurring practical challenge across language generation, planning, and reinforcement learning. We present a practical framework, \\emph{Inverted Causality Focusing Algorithm} (ICFA), that treats search as a target-conditioned reweighting process. ICFA reuses an available proposal sampler and a task-specific similarity function to form a focused sampling distribution, while adaptively controlling focusing strength to avoid degeneracy. We provide a clear recipe, a stability diagnostic based on effective sample size, a compact theoretical sketch explaining when ICFA can reduce sample needs, and two reproducible experiments: constrained language generation and sparse-reward navigation. We further show how structured prompts instantiate an approximate, language-level form of ICFA and describe a hybrid architecture combining prompted inference with algorithmic reweighting.",
    "paper_id": "2512.19717v1"
  },
  "https://arxiv.org/abs/2512.11482v1": {
    "content_path": "../data/contents/2512.11482v1.md",
    "pdf_path": "../data/contents/2512.11482v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:19.966716",
    "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models",
    "abstract": "Summary: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.",
    "paper_id": "2512.11482v1"
  },
  "https://arxiv.org/abs/2506.11902": {
    "content_path": "../data/contents/2506.11902.md",
    "pdf_path": "../data/contents/2506.11902.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:20.910566",
    "title": "TreeRL: LLM Reinforcement Learning with On-Policy Tree Search",
    "abstract": "Integrates on-policy tree search into RL training to provide dense process rewards and improve exploration—core approach for search-based reasoning.",
    "paper_id": "2506.11902"
  },
  "https://arxiv.org/abs/2512.18552v1": {
    "content_path": "../data/contents/2512.18552v1.md",
    "pdf_path": "../data/contents/2512.18552v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:21.896972",
    "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
    "abstract": "Summary: While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.",
    "paper_id": "2512.18552v1"
  },
  "https://arxiv.org/abs/2512.14233v1": {
    "content_path": "../data/contents/2512.14233v1.md",
    "pdf_path": "../data/contents/2512.14233v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:23.410786",
    "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design",
    "abstract": "Summary: Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.",
    "paper_id": "2512.14233v1"
  },
  "https://arxiv.org/abs/2512.11474v1": {
    "content_path": "../data/contents/2512.11474v1.md",
    "pdf_path": "../data/contents/2512.11474v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:23.683768",
    "title": "General-purpose AI models can generate actionable knowledge on agroecological crop protection",
    "abstract": "Summary: Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.",
    "paper_id": "2512.11474v1"
  },
  "https://arxiv.org/abs/2505.20325": {
    "content_path": "../data/contents/2505.20325.md",
    "pdf_path": "../data/contents/2505.20325.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:24.334765",
    "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence",
    "abstract": "Efficient test-time scaling using intrinsic confidence/novelty signals (tree search) without external PRMs—important for cost-efficient reasoning.",
    "paper_id": "2505.20325"
  },
  "https://arxiv.org/abs/2512.18551v1": {
    "content_path": "../data/contents/2512.18551v1.md",
    "pdf_path": "../data/contents/2512.18551v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:25.008091",
    "title": "Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering",
    "abstract": "Summary: In language modeling, neologisms are new tokens trained to represent a concept not already included in a given model's vocabulary. Neologisms can be used to encourage specific behavior in models, for example by appending prompts with \"Give me a neologism answer.\" Behavioral steering can also be achieved through fine-tuning, albeit with more compute and less flexibility: learning a neologism only trains d parameters and allows the user to still access the model's default behavior. We compare the performance of neologism learning against low-rank adaptation (LoRA) fine-tuning, finding that neologisms outperform fine-tuned models under a matched training setup (same data and hyperparameters). We also investigate self-verbalizations of neologisms, and observe that the model will occasionally make up its own new words when asked about a neologism.",
    "paper_id": "2512.18551v1"
  },
  "https://arxiv.org/abs/2512.14230v1": {
    "content_path": "../data/contents/2512.14230v1.md",
    "pdf_path": "../data/contents/2512.14230v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:26.784903",
    "title": "Understanding the Gain from Data Filtering in Multimodal Contrastive Learning",
    "abstract": "Summary: The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $η\\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\\frac{1}{η\\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\\frac{1}{\\sqrt{ηn}}$ in the large $η$ regime, and by $\\frac{1}{\\sqrt{n}}$ in the small $η$ regime.",
    "paper_id": "2512.14230v1"
  },
  "https://arxiv.org/abs/2512.11470v1": {
    "content_path": "../data/contents/2512.11470v1.md",
    "pdf_path": "../data/contents/2512.11470v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:31.514820",
    "title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
    "abstract": "Summary: While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.",
    "paper_id": "2512.11470v1"
  },
  "https://arxiv.org/abs/2502.18080": {
    "content_path": "../data/contents/2502.18080.md",
    "pdf_path": "../data/contents/2502.18080.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:33.178573",
    "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning",
    "abstract": "Finds overly long CoTs can hurt performance and proposes “thinking-optimal” response-length distributions—central to compute–accuracy tradeoffs in reasoning.",
    "paper_id": "2502.18080"
  },
  "https://arxiv.org/abs/2512.18542v1": {
    "content_path": "../data/contents/2512.18542v1.md",
    "pdf_path": "../data/contents/2512.18542v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:39.800686",
    "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
    "abstract": "Summary: AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).\n  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.\n  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.",
    "paper_id": "2512.18542v1"
  },
  "https://arxiv.org/abs/2512.14228v1": {
    "content_path": "../data/contents/2512.14228v1.md",
    "pdf_path": "../data/contents/2512.14228v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:40.877469",
    "title": "Georeferencing complex relative locality descriptions with large language models",
    "abstract": "Summary: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.",
    "paper_id": "2512.14228v1"
  },
  "https://arxiv.org/abs/2512.15766v1": {
    "content_path": "../data/contents/2512.15766v1.md",
    "pdf_path": "../data/contents/2512.15766v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:41.860336",
    "title": "LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models",
    "abstract": "Summary: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\\times$, 14.34$\\times$, and 9.29$\\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\\times$, 5.61$\\times$, and 11.59$\\times$.",
    "paper_id": "2512.15766v1"
  },
  "https://arxiv.org/abs/2502.14382": {
    "content_path": "../data/contents/2502.14382.md",
    "pdf_path": "../data/contents/2502.14382.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:42.619273",
    "title": "S*: Test Time Scaling for Code Generation",
    "abstract": "Hybrid parallel+sequential test-time scaling framework for code with adaptive selection (execution-grounded); key reference for test-time compute in code reasoning.",
    "paper_id": "2502.14382"
  },
  "https://arxiv.org/abs/2512.18533v1": {
    "content_path": "../data/contents/2512.18533v1.md",
    "pdf_path": "../data/contents/2512.18533v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:43.173213",
    "title": "Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset",
    "abstract": "Summary: The proliferation of linguistically subtle political disinformation poses a significant challenge to automated fact-checking systems. Despite increasing emphasis on complex neural architectures, the empirical limits of text-only linguistic modeling remain underexplored. We present a systematic diagnostic evaluation of nine machine learning algorithms on the LIAR benchmark. By isolating lexical features (Bag-of-Words, TF-IDF) and semantic embeddings (GloVe), we uncover a hard \"Performance Ceiling\", with fine-grained classification not exceeding a Weighted F1-score of 0.32 across models. Crucially, a simple linear SVM (Accuracy: 0.624) matches the performance of pre-trained Transformers such as RoBERTa (Accuracy: 0.620), suggesting that model capacity is not the primary bottleneck. We further diagnose a massive \"Generalization Gap\" in tree-based ensembles, which achieve more than 99% training accuracy but collapse to approximately 25% on test data, indicating reliance on lexical memorization rather than semantic inference. Synthetic data augmentation via SMOTE yields no meaningful gains, confirming that the limitation is semantic (feature ambiguity) rather than distributional. These findings indicate that for political fact-checking, increasing model complexity without incorporating external knowledge yields diminishing returns.",
    "paper_id": "2512.18533v1"
  },
  "https://arxiv.org/abs/2512.14770v1": {
    "content_path": "../data/contents/2512.14770v1.md",
    "pdf_path": "../data/contents/2512.14770v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:43.895031",
    "title": "Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification",
    "abstract": "Summary: Vision-language models (VLMs) have demonstrated significant potential in Visual Question Answering (VQA). However, the susceptibility of VLMs to hallucinations can lead to overconfident yet incorrect answers, severely undermining answer reliability. To address this, we propose Dual-Assessment for VLM Reliability (DAVR), a novel framework that integrates Self-Reflection and Cross-Model Verification for comprehensive uncertainty estimation. The DAVR framework features a dual-pathway architecture: one pathway leverages dual selector modules to assess response reliability by fusing VLM latent features with QA embeddings, while the other deploys external reference models for factual cross-checking to mitigate hallucinations. Evaluated in the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading $Φ_{100}$ score of 39.64 and a 100-AUC of 97.22, securing first place and demonstrating its effectiveness in enhancing the trustworthiness of VLM responses.",
    "paper_id": "2512.14770v1"
  },
  "https://arxiv.org/abs/2512.11942v1": {
    "content_path": "../data/contents/2512.11942v1.md",
    "pdf_path": "../data/contents/2512.11942v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:44.321191",
    "title": "Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play",
    "abstract": "Summary: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.",
    "paper_id": "2512.11942v1"
  },
  "https://arxiv.org/abs/2512.18527v1": {
    "content_path": "../data/contents/2512.18527v1.md",
    "pdf_path": "../data/contents/2512.18527v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:45.723307",
    "title": "Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism",
    "abstract": "Summary: As AI-generated images become increasingly photorealistic, distinguishing them from natural images poses a growing challenge. This paper presents a robust detection framework that leverages multiple uncertainty measures to decide whether to trust or reject a model's predictions. We focus on three complementary techniques: Fisher Information, which captures the sensitivity of model parameters to input variations; entropy-based uncertainty from Monte Carlo Dropout, which reflects predictive variability; and predictive variance from a Deep Kernel Learning framework using a Gaussian Process classifier. To integrate these diverse uncertainty signals, Particle Swarm Optimisation is used to learn optimal weightings and determine an adaptive rejection threshold. The model is trained on Stable Diffusion-generated images and evaluated on GLIDE, VQDM, Midjourney, BigGAN, and StyleGAN3, each introducing significant distribution shifts. While standard metrics such as prediction probability and Fisher-based measures perform well in distribution, their effectiveness degrades under shift. In contrast, the Combined Uncertainty measure consistently achieves an incorrect rejection rate of approximately 70 percent on unseen generators, successfully filtering most misclassified AI samples. Although the system occasionally rejects correct predictions from newer generators, this conservative behaviour is acceptable, as rejected samples can support retraining. The framework maintains high acceptance of accurate predictions for natural images and in-domain AI data. Under adversarial attacks using FGSM and PGD, the Combined Uncertainty method rejects around 61 percent of successful attacks, while GP-based uncertainty alone achieves up to 80 percent. Overall, the results demonstrate that multi-source uncertainty fusion provides a resilient and adaptive solution for AI-generated image detection.",
    "paper_id": "2512.18527v1"
  },
  "https://arxiv.org/abs/2512.14221v1": {
    "content_path": "../data/contents/2512.14221v1.md",
    "pdf_path": "../data/contents/2512.14221v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:53.520871",
    "title": "Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms",
    "abstract": "Summary: Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.",
    "paper_id": "2512.14221v1"
  },
  "https://arxiv.org/abs/2512.11453v1": {
    "content_path": "../data/contents/2512.11453v1.md",
    "pdf_path": "../data/contents/2512.11453v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:56.549546",
    "title": "Learning to Evolve with Convergence Guarantee via Neural Unrolling",
    "abstract": "Summary: The transition from hand-crafted heuristics to data-driven evolutionary algorithms faces a fundamental dilemma: achieving neural plasticity without sacrificing mathematical stability. Emerging learned optimizers demonstrate high adaptability. However, they often lack rigorous convergence guarantees. This deficiency results in unpredictable behaviors on unseen landscapes. To address this challenge, we introduce Learning to Evolve (L2E), a unified bilevel meta-optimization framework. This method reformulates evolutionary search as a Neural Unrolling process grounded in Krasnosel'skii-Mann (KM) fixed-point theory. First, L2E models a coupled dynamic system in which the inner loop enforces a strict contractive trajectory via a structured Mamba-based neural operator. Second, the outer loop optimizes meta-parameters to align the fixed point of the operator with the target objective minimizers. Third, we design a gradient-derived composite solver that adaptively fuses learned evolutionary proposals with proxy gradient steps, thereby harmonizing global exploration with local refinement. Crucially, this formulation provides the learned optimizer with provable convergence guarantees. Extensive experiments demonstrate the scalability of L2E in high-dimensional spaces and its robust zero-shot generalization across synthetic and real-world control tasks. These results confirm that the framework learns a generic optimization manifold that extends beyond specific training distributions.",
    "paper_id": "2512.11453v1"
  },
  "https://arxiv.org/abs/2512.18504v1": {
    "content_path": "../data/contents/2512.18504v1.md",
    "pdf_path": "../data/contents/2512.18504v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:56.824919",
    "title": "GTMA: Dynamic Representation Optimization for OOD Vision-Language Models",
    "abstract": "Summary: Vision-language models (VLMs) struggle in open-world applications, where out-of-distribution (OOD) concepts often trigger cross-modal alignment collapse and severely degrade zero-shot performance. We identify the root cause as modal asymmetry: while the visual encoder can extract discriminative features from unseen images, the text encoder is constrained by a fixed discrete vocabulary and cannot synthesize new semantic anchors. Existing approaches such as CoOp or LoRA provide only partial remedies, as they remain confined to the pre-trained semantic space.\n  To overcome this bottleneck, we propose dynamic representation optimization, realized through the Guided Target-Matching Adaptation (GTMA) framework. At inference time, GTMA constructs a continuous pseudo-word embedding that best aligns with an OOD image's visual anchor, effectively bypassing vocabulary limitations. The optimization is driven by an adaptive gradient-based representation policy optimization algorithm, which incorporates semantic regularization to preserve plausibility and compatibility with the model's prior knowledge.\n  Experiments on ImageNet-R and the VISTA-Beyond benchmark demonstrate that GTMA improves zero-shot and few-shot OOD accuracy by up to 15-20 percent over the base VLM while maintaining performance on in-distribution concepts. Ablation studies further confirm the necessity of pseudo-word optimization.",
    "paper_id": "2512.18504v1"
  },
  "https://arxiv.org/abs/2512.14220v1": {
    "content_path": "../data/contents/2512.14220v1.md",
    "pdf_path": "../data/contents/2512.14220v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:57.323880",
    "title": "Estimating problem difficulty without ground truth using Large Language Model comparisons",
    "abstract": "Summary: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \\geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\\%$ degradation in Pearson correlation for $10\\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.",
    "paper_id": "2512.14220v1"
  },
  "https://arxiv.org/abs/2512.11448v1": {
    "content_path": "../data/contents/2512.11448v1.md",
    "pdf_path": "../data/contents/2512.11448v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:20:57.920773",
    "title": "Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces",
    "abstract": "Summary: Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.",
    "paper_id": "2512.11448v1"
  },
  "https://arxiv.org/abs/2512.18495v1": {
    "content_path": "../data/contents/2512.18495v1.md",
    "pdf_path": "../data/contents/2512.18495v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:00.242986",
    "title": "Enhancing Decision-Making in Windows PE Malware Classification During Dataset Shifts with Uncertainty Estimation",
    "abstract": "Summary: Artificial intelligence techniques have achieved strong performance in classifying Windows Portable Executable (PE) malware, but their reliability often degrades under dataset shifts, leading to misclassifications with severe security consequences. To address this, we enhance an existing LightGBM (LGBM) malware detector by integrating Neural Networks (NN), PriorNet, and Neural Network Ensembles, evaluated across three benchmark datasets: EMBER, BODMAS, and UCSB. The UCSB dataset, composed mainly of packed malware, introduces a substantial distributional shift relative to EMBER and BODMAS, making it a challenging testbed for robustness. We study uncertainty-aware decision strategies, including probability thresholding, PriorNet, ensemble-derived estimates, and Inductive Conformal Evaluation (ICE). Our main contribution is the use of ensemble-based uncertainty estimates as Non-Conformity Measures within ICE, combined with a novel threshold optimisation method. On the UCSB dataset, where the shift is most severe, the state-of-the-art probability-based ICE (SOTA) yields an incorrect acceptance rate (IA%) of 22.8%. In contrast, our method reduces this to 16% a relative reduction of about 30% while maintaining competitive correct acceptance rates (CA%). These results demonstrate that integrating ensemble-based uncertainty with conformal prediction provides a more reliable safeguard against misclassifications under extreme dataset shifts, particularly in the presence of packed malware, thereby offering practical benefits for real-world security operations.",
    "paper_id": "2512.18495v1"
  },
  "https://arxiv.org/abs/2512.14197v2": {
    "content_path": "../data/contents/2512.14197v2.md",
    "pdf_path": "../data/contents/2512.14197v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:01.025096",
    "title": "Location-Robust Cost-Preserving Blended Pricing for Multi-Campus AI Data Centers",
    "abstract": "Summary: Large-scale AI data center portfolios procure identical SKUs across geographically heterogeneous campuses, yet finance and operations require a single system-level 'world price' per SKU for budgeting and planning. A common practice is deployment-weighted blending of campus prices, which preserves total cost but can trigger Simpson-type aggregation failures: heterogeneous location mixes can reverse SKU rankings and distort decision signals.\n  I formalize cost-preserving blended pricing under location heterogeneity and propose two practical operators that reconcile accounting identity with ranking robustness and production implementability. A two-way fixed-effects operator separates global SKU effects from campus effects and restores exact cost preservation via scalar normalization, providing interpretable decomposition and smoothing under mild missingness. A convex common-weight operator computes a single set of campus weights under accounting constraints to enforce a location-robust benchmark and prevent dominance reversals; I also provide feasibility diagnostics and a slack-based fallback for extreme mix conditions. Simulations and an AI data center OPEX illustration show substantial reductions in ranking violations relative to naive blending while maintaining cost accuracy, with scalable distributed implementation.",
    "paper_id": "2512.14197v2"
  },
  "https://arxiv.org/abs/2512.11437v1": {
    "content_path": "../data/contents/2512.11437v1.md",
    "pdf_path": "../data/contents/2512.11437v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:08.917568",
    "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare",
    "abstract": "Summary: Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.",
    "paper_id": "2512.11437v1"
  },
  "https://arxiv.org/abs/2512.18489v1": {
    "content_path": "../data/contents/2512.18489v1.md",
    "pdf_path": "../data/contents/2512.18489v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:09.315851",
    "title": "Large Language Models as Discounted Bayesian Filters",
    "abstract": "Summary: Large Language Models (LLMs) demonstrate strong few-shot generalization through in-context learning, yet their reasoning in dynamic and stochastic environments remains opaque. Prior studies mainly focus on static tasks and overlook the online adaptation required when beliefs must be continuously updated, which is a key capability for LLMs acting as world models or agents. We introduce a Bayesian filtering framework to evaluate online inference in LLMs. Our probabilistic probe suite spans both multivariate discrete distributions, such as dice rolls, and continuous distributions, such as Gaussian processes, where ground-truth parameters shift over time. We find that while LLM belief updates resemble Bayesian posteriors, they are more accurately characterized by an exponential forgetting filter with a model-specific discount factor smaller than one. This reveals systematic discounting of older evidence that varies significantly across model architectures. Although inherent priors are often miscalibrated, the updating mechanism itself remains structured and principled. We further validate these findings in a simulated agent task and propose prompting strategies that effectively recalibrate priors with minimal computational cost.",
    "paper_id": "2512.18489v1"
  },
  "https://arxiv.org/abs/2512.14190v1": {
    "content_path": "../data/contents/2512.14190v1.md",
    "pdf_path": "../data/contents/2512.14190v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:09.905917",
    "title": "Random-Bridges as Stochastic Transports for Generative Models",
    "abstract": "Summary: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.",
    "paper_id": "2512.14190v1"
  },
  "https://arxiv.org/abs/2512.15765v1": {
    "content_path": "../data/contents/2512.15765v1.md",
    "pdf_path": "../data/contents/2512.15765v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:10.525883",
    "title": "Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic",
    "abstract": "Summary: Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.",
    "paper_id": "2512.15765v1"
  },
  "https://arxiv.org/abs/2512.18474v1": {
    "content_path": "../data/contents/2512.18474v1.md",
    "pdf_path": "../data/contents/2512.18474v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:10.999048",
    "title": "When Robots Say No: The Empathic Ethical Disobedience Benchmark",
    "abstract": "Summary: Robots must balance compliance with safety and social expectations as blind obedience can cause harm, while over-refusal erodes trust. Existing safe reinforcement learning (RL) benchmarks emphasize physical hazards, while human-robot interaction trust studies are small-scale and hard to reproduce. We present the Empathic Ethical Disobedience (EED) Gym, a standardized testbed that jointly evaluates refusal safety and social acceptability. Agents weigh risk, affect, and trust when choosing to comply, refuse (with or without explanation), clarify, or propose safer alternatives. EED Gym provides different scenarios, multiple persona profiles, and metrics for safety, calibration, and refusals, with trust and blame models grounded in a vignette study. Using EED Gym, we find that action masking eliminates unsafe compliance, while explanatory refusals help sustain trust. Constructive styles are rated most trustworthy, empathic styles -- most empathic, and safe RL methods improve robustness but also make agents more prone to overly cautious behavior. We release code, configurations, and reference policies to enable reproducible evaluation and systematic human-robot interaction research on refusal and trust. At submission time, we include an anonymized reproducibility package with code and configs, and we commit to open-sourcing the full repository after the paper is accepted.",
    "paper_id": "2512.18474v1"
  },
  "https://arxiv.org/abs/2512.14179v1": {
    "content_path": "../data/contents/2512.14179v1.md",
    "pdf_path": "../data/contents/2512.14179v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:11.250418",
    "title": "A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs",
    "abstract": "Summary: Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\\_dialect:standard\\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\\% to 55\\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.",
    "paper_id": "2512.14179v1"
  },
  "https://arxiv.org/abs/2512.11426v1": {
    "content_path": "../data/contents/2512.11426v1.md",
    "pdf_path": "../data/contents/2512.11426v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:11.602642",
    "title": "AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints",
    "abstract": "Summary: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance",
    "paper_id": "2512.11426v1"
  },
  "https://arxiv.org/abs/2512.18470v2": {
    "content_path": "../data/contents/2512.18470v2.md",
    "pdf_path": "../data/contents/2512.18470v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:11.919370",
    "title": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios",
    "abstract": "Summary: Existing benchmarks for AI coding agents focus on isolated, single-issue tasks such as fixing a bug or implementing a small feature. However, real-world software engineering is fundamentally a long-horizon endeavor: developers must interpret high-level requirements, plan coordinated changes across many files, and evolve codebases over multiple iterations while preserving existing functionality. We introduce SWE-EVO, a benchmark that evaluates agents on this long-horizon software evolution challenge. Constructed from release notes and version histories of seven mature open-source Python projects, Tool comprises 48 evolution tasks that require agents to implement multi-step modifications spanning an average of 21 files, validated against comprehensive test suites averaging 874 tests per instance. Experiments with state-of-the-art models reveal a striking capability gap: even GPT-5 with OpenHands achieves only a 21 percent resolution rate on Tool, compared to 65 percent on the single-issue SWE-Bench Verified. This demonstrates that current agents struggle with sustained, multi-file reasoning. We also propose Fix Rate, a fine-grained metric that captures partial progress toward solving these complex, long-horizon tasks.",
    "paper_id": "2512.18470v2"
  },
  "https://arxiv.org/abs/2512.14166v1": {
    "content_path": "../data/contents/2512.14166v1.md",
    "pdf_path": "../data/contents/2512.14166v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:12.287869",
    "title": "IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol",
    "abstract": "Summary: The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.",
    "paper_id": "2512.14166v1"
  },
  "https://arxiv.org/abs/2512.11421v1": {
    "content_path": "../data/contents/2512.11421v1.md",
    "pdf_path": "../data/contents/2512.11421v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:13.549815",
    "title": "Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance",
    "abstract": "Summary: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.\n  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.",
    "paper_id": "2512.11421v1"
  },
  "https://arxiv.org/abs/2512.18462v1": {
    "content_path": "../data/contents/2512.18462v1.md",
    "pdf_path": "../data/contents/2512.18462v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:14.219557",
    "title": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling",
    "abstract": "Summary: Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.",
    "paper_id": "2512.18462v1"
  },
  "https://arxiv.org/abs/2512.14157v1": {
    "content_path": "../data/contents/2512.14157v1.md",
    "pdf_path": "../data/contents/2512.14157v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:21.664930",
    "title": "Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis",
    "abstract": "Summary: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely \"think with images\" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.",
    "paper_id": "2512.14157v1"
  },
  "https://arxiv.org/abs/2512.14740v1": {
    "content_path": "../data/contents/2512.14740v1.md",
    "pdf_path": "../data/contents/2512.14740v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:22.576955",
    "title": "VDMN: A Graphical Notation for Modelling Value Driver Trees",
    "abstract": "Summary: Value Driver Trees (VDTs) are conceptual models used to illustrate and analyse the causal relationships between key performance indicators and business outcomes, thereby supporting managerial decision-making and value-based management. Despite their increasing application, there are still no systematic guidelines for the modelling of such conceptual models. To fill this gap, this study introduces the Value Driver Modelling Notation (VDMN), a graphical notation developed to systematically guide VDT modelling. This notation includes a comprehensive set of semantic constructs and an intuitive graphical syntax. To evaluate its practical utility, the VDMN was applied in two case studies and assessed through expert interviews. The results show that the notation supports a consistent and comprehensible modelling of VDTs. The VDMN thus represents a significant step towards the systematisation and standardisation of VDT modelling.",
    "paper_id": "2512.14740v1"
  },
  "https://arxiv.org/abs/2512.18456v1": {
    "content_path": "../data/contents/2512.18456v1.md",
    "pdf_path": "../data/contents/2512.18456v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:23.838773",
    "title": "SoK: Understanding (New) Security Issues Across AI4Code Use Cases",
    "abstract": "Summary: AI-for-Code (AI4Code) systems are reshaping software engineering, with tools like GitHub Copilot accelerating code generation, translation, and vulnerability detection. Alongside these advances, however, security risks remain pervasive: insecure outputs, biased benchmarks, and susceptibility to adversarial manipulation undermine their reliability. This SoK surveys the landscape of AI4Code security across three core applications, identifying recurring gaps: benchmark dominance by Python and toy problems, lack of standardized security datasets, data leakage in evaluation, and fragile adversarial robustness. A comparative study of six state-of-the-art models illustrates these challenges: insecure patterns persist in code generation, vulnerability detection is brittle to semantic-preserving attacks, fine-tuning often misaligns security objectives, and code translation yields uneven security benefits. From this analysis, we distill three forward paths: embedding secure-by-default practices in code generation, building robust and comprehensive detection benchmarks, and leveraging translation as a route to security-enhanced languages. We call for a shift toward security-first AI4Code, where vulnerability mitigation and robustness are embedded throughout the development life cycle.",
    "paper_id": "2512.18456v1"
  },
  "https://arxiv.org/abs/2512.14142v1": {
    "content_path": "../data/contents/2512.14142v1.md",
    "pdf_path": "../data/contents/2512.14142v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:24.217715",
    "title": "Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents",
    "abstract": "Summary: Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.",
    "paper_id": "2512.14142v1"
  },
  "https://arxiv.org/abs/2512.15764v1": {
    "content_path": "../data/contents/2512.15764v1.md",
    "pdf_path": "../data/contents/2512.15764v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:25.301648",
    "title": "AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs",
    "abstract": "Summary: Large Language Models (LLMs) can perform many NLP tasks well, but fully fine-tuning them is expensive and requires a lot of memory. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA reduce this cost by adding small low-rank updates to frozen model weights. However, these methods restrict the training to a limited subspace, which can sometimes reduce performance.\n  For Small Language Models (SLMs), where efficiency gains matter even more, we introduce AdaGradSelect, an adaptive method that selects which transformer blocks to update based on gradients.\n  Early observations showed that updating only the transformer blocks with the highest gradient norms can achieve performance close to full fine-tuning. Building on this insight, AdaGradSelect adaptively chooses which blocks to train. It uses a combination of Dirichlet-based sampling, which depends on how frequently blocks were updated in the past, and an epsilon-greedy exploration strategy. This lets the method explore different blocks in early training and gradually focus on the most important ones in later epochs.\n  Experiments show that AdaGradSelect trains about 12 percent faster and uses 35 percent less GPU memory while delivering performance very close to full fine-tuning. On the GSM8K dataset, it outperforms LoRA (rank 256) by about 3 percent on average across models such as Qwen2.5-0.5B, LLaMA3.2-1B, and Phi4-mini-3.8B. It also achieves similar accuracy on the MATH dataset. Overall, AdaGradSelect provides a more effective and resource-efficient alternative to traditional fine-tuning methods.",
    "paper_id": "2512.15764v1"
  },
  "https://arxiv.org/abs/2512.18452v1": {
    "content_path": "../data/contents/2512.18452v1.md",
    "pdf_path": "../data/contents/2512.18452v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:25.873632",
    "title": "Secret mixtures of experts inside your LLM",
    "abstract": "Summary: Despite being one of the earliest neural network layers, the Multilayer Perceptron (MLP) is arguably one of the least understood parts of the transformer architecture due to its dense computation and lack of easy visualization. This paper seeks to understand the MLP layers in dense LLM models by hypothesizing that these layers secretly approximately perform a sparse computation -- namely, that they can be well approximated by sparsely-activating Mixture of Experts (MoE) layers.\n  Our hypothesis is based on a novel theoretical connection between MoE models and Sparse Autoencoder (SAE) structure in activation space. We empirically validate the hypothesis on pretrained LLMs, and demonstrate that the activation distribution matters -- these results do not hold for Gaussian data, but rather rely crucially on structure in the distribution of neural network activations.\n  Our results shine light on a general principle at play in MLP layers inside LLMs, and give an explanation for the effectiveness of modern MoE-based transformers. Additionally, our experimental explorations suggest new directions for more efficient MoE architecture design based on low-rank routers.",
    "paper_id": "2512.18452v1"
  },
  "https://arxiv.org/abs/2512.14141v1": {
    "content_path": "../data/contents/2512.14141v1.md",
    "pdf_path": "../data/contents/2512.14141v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:26.979118",
    "title": "TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models",
    "abstract": "Summary: Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.",
    "paper_id": "2512.14141v1"
  },
  "https://arxiv.org/abs/2512.11411v1": {
    "content_path": "../data/contents/2512.11411v1.md",
    "pdf_path": "../data/contents/2512.11411v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:27.951822",
    "title": "Sliced ReLU attention: Quasi-linear contextual expressivity via sorting",
    "abstract": "Summary: We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.",
    "paper_id": "2512.11411v1"
  },
  "https://arxiv.org/abs/2512.18450v1": {
    "content_path": "../data/contents/2512.18450v1.md",
    "pdf_path": "../data/contents/2512.18450v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:29.322793",
    "title": "Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System",
    "abstract": "Summary: Modern clinical decision support systems can concurrently serve multiple, independent medical imaging institutions, but their predictive performance may degrade across sites due to variations in patient populations, imaging hardware, and acquisition protocols. Continuous surveillance of predictive model outputs offers a safe and reliable approach for identifying such distributional shifts without ground truth labels. However, most existing methods rely on centralized monitoring of aggregated predictions, overlooking site-specific drift dynamics. We propose an agent-based framework for detecting drift and assessing its severity in multisite clinical AI systems. To evaluate its effectiveness, we simulate a multi-center environment for output-based drift detection, assigning each site a drift monitoring agent that performs batch-wise comparisons of model outputs against a reference distribution. We analyse several multi-center monitoring schemes, that differ in how the reference is obtained (site-specific, global, production-only and adaptive), alongside a centralized baseline. Results on real-world breast cancer imaging data using a pathological complete response prediction model shows that all multi-center schemes outperform centralized monitoring, with F1-score improvements up to 10.3% in drift detection. In the absence of site-specific references, the adaptive scheme performs best, with F1-scores of 74.3% for drift detection and 83.7% for drift severity classification. These findings suggest that adaptive, site-aware agent-based drift monitoring can enhance reliability of multisite clinical decision support systems.",
    "paper_id": "2512.18450v1"
  },
  "https://arxiv.org/abs/2512.14138v1": {
    "content_path": "../data/contents/2512.14138v1.md",
    "pdf_path": "../data/contents/2512.14138v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:44.754678",
    "title": "LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation",
    "abstract": "Summary: Many real-world tasks, such as trip planning or meal planning, can be formulated as combinatorial optimization problems. However, using optimization solvers is difficult for end users because it requires problem instantiation: defining candidate items, assigning preference scores, and specifying constraints. We introduce LAPPI (LLM-Assisted Preference-based Problem Instantiation), an interactive approach that uses large language models (LLMs) to support users in this instantiation process. Through natural language conversations, the system helps users transform vague preferences into well-defined optimization problems. These instantiated problems are then passed to existing optimization solvers to generate solutions. In a user study on trip planning, our method successfully captured user preferences and generated feasible plans that outperformed both conventional and prompt-engineering approaches. We further demonstrate LAPPI's versatility by adapting it to an additional use case.",
    "paper_id": "2512.14138v1"
  },
  "https://arxiv.org/abs/2512.11403v1": {
    "content_path": "../data/contents/2512.11403v1.md",
    "pdf_path": "../data/contents/2512.11403v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:45.395322",
    "title": "Bridging Textual Data and Conceptual Models: A Model-Agnostic Structuring Approach",
    "abstract": "Summary: We introduce an automated method for structuring textual data into a model-agnostic schema, enabling alignment with any database model. It generates both a schema and its instance. Initially, textual data is represented as semantically enriched syntax trees, which are then refined through iterative tree rewriting and grammar extraction, guided by the attribute grammar meta-model \\metaG. The applicability of this approach is demonstrated using clinical medical cases as a proof of concept.",
    "paper_id": "2512.11403v1"
  },
  "https://arxiv.org/html/2501.03200v1": {
    "content_path": "../data/contents/2501.03200v1.md",
    "pdf_path": "../data/contents/2501.03200v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:45.681033",
    "title": "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses",
    "abstract": "Google's benchmark for measuring LLM factual grounding in long-form responses with automatic evaluation",
    "paper_id": "2501.03200v1"
  },
  "https://arxiv.org/abs/2512.18444v1": {
    "content_path": "../data/contents/2512.18444v1.md",
    "pdf_path": "../data/contents/2512.18444v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:46.189680",
    "title": "Snowveil: A Framework for Decentralised Preference Discovery",
    "abstract": "Summary: Aggregating subjective preferences of a large group is a fundamental challenge in computational social choice, traditionally reliant on central authorities. To address the limitations of this model, this paper introduces Decentralised Preference Discovery (DPD), the problem of determining the collective will of an electorate under constraints of censorship resistance, partial information, and asynchronous communication. We propose Snowveil, a novel framework for this task. Snowveil uses an iterative, gossip-based protocol where voters repeatedly sample the preferences of a small, random subset of the electorate to progressively converge on a collective outcome. We demonstrate the framework's modularity by designing the Constrained Hybrid Borda (CHB), a novel aggregation rule engineered to balance broad consensus with strong plurality support, and provide a rigorous axiomatic analysis of its properties. By applying a potential function and submartingale theory, we develop a multi-level analytical method to show that the system almost surely converges to a stable, single-winner in finite time, a process that can then be iterated to construct a set of winning candidates for multi-winner scenarios. This technique is largely agnostic to the specific aggregation rule, requiring only that it satisfies core social choice axioms like Positive Responsiveness, thus offering a formal toolkit for a wider class of DPD protocols. Furthermore, we present a comprehensive empirical analysis through extensive simulation, validating Snowveil's $O(n)$ scalability. Overall, this work advances the understanding of how a stable consensus can emerge from subjective, complex, and diverse preferences in decentralised systems for large electorates.",
    "paper_id": "2512.18444v1"
  },
  "https://arxiv.org/abs/2512.14130v1": {
    "content_path": "../data/contents/2512.14130v1.md",
    "pdf_path": "../data/contents/2512.14130v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:46.695407",
    "title": "UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis",
    "abstract": "Summary: We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.",
    "paper_id": "2512.14130v1"
  },
  "https://arxiv.org/abs/2512.11402v1": {
    "content_path": "../data/contents/2512.11402v1.md",
    "pdf_path": "../data/contents/2512.11402v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:47.101775",
    "title": "REMODEL-LLM: Transforming C code to Java using LLMs",
    "abstract": "Summary: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.",
    "paper_id": "2512.11402v1"
  },
  "https://arxiv.org/abs/2402.10200": {
    "content_path": "../data/contents/2402.10200.md",
    "pdf_path": "../data/contents/2402.10200.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:48.030309",
    "title": "Chain-of-Thought Reasoning Without Prompting",
    "abstract": "Google research showing CoT paths inherently exist in LLMs and can be elicited through alternative decoding",
    "paper_id": "2402.10200"
  },
  "https://arxiv.org/abs/2512.18440v1": {
    "content_path": "../data/contents/2512.18440v1.md",
    "pdf_path": "../data/contents/2512.18440v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:50.044304",
    "title": "An Agentic AI Framework for Training General Practitioner Student Skills",
    "abstract": "Summary: Advancements in large language models offer strong potential for enhancing virtual simulated patients (VSPs) in medical education by providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent roleplaying, scenario generation for VSP use, and educationally structured feedback. We introduce an agentic framework for training general practitioner student skills that unifies (i) configurable, evidence-based vignette generation, (ii) controlled persona-driven patient dialogue with optional retrieval grounding, and (iii) standards-based assessment and feedback for both communication and clinical reasoning. We instantiate the framework in an interactive spoken consultation setting and evaluate it with medical students ($\\mathbf{N{=}14}$). Participants reported realistic and vignette-faithful dialogue, appropriate difficulty calibration, a stable personality signal, and highly useful example-rich feedback, alongside excellent overall usability. These results support agentic separation of scenario control, interaction control, and standards-based assessment as a practical pattern for building dependable and pedagogically valuable VSP training tools.",
    "paper_id": "2512.18440v1"
  },
  "https://arxiv.org/abs/2512.14766v1": {
    "content_path": "../data/contents/2512.14766v1.md",
    "pdf_path": "../data/contents/2512.14766v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:50.367137",
    "title": "GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge",
    "abstract": "Summary: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.",
    "paper_id": "2512.14766v1"
  },
  "https://arxiv.org/abs/2512.11399v1": {
    "content_path": "../data/contents/2512.11399v1.md",
    "pdf_path": "../data/contents/2512.11399v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:52.328251",
    "title": "Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction",
    "abstract": "Summary: Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.",
    "paper_id": "2512.11399v1"
  },
  "https://arxiv.org/abs/2512.18436v1": {
    "content_path": "../data/contents/2512.18436v1.md",
    "pdf_path": "../data/contents/2512.18436v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:53.764449",
    "title": "VeruSAGE: A Study of Agent-Based Verification for Rust Systems",
    "abstract": "Summary: Large language models (LLMs) have shown impressive capability to understand and develop code. However, their capability to rigorously reason about and prove code correctness remains in question. This paper offers a comprehensive study of LLMs' capability to develop correctness proofs for system software written in Rust. We curate a new system-verification benchmark suite, VeruSAGE-Bench, which consists of 849 proof tasks extracted from eight open-source Verus-verified Rust systems. Furthermore, we design different agent systems to match the strengths and weaknesses of different LLMs (o4-mini, GPT-5, Sonnet 4, and Sonnet 4.5). Our study shows that different tools and agent settings are needed to stimulate the system-verification capability of different types of LLMs. The best LLM-agent combination in our study completes over 80% of system-verification tasks in VeruSAGE-Bench. It also completes over 90% of a set of system proof tasks not part of VeruSAGE-Bench because they had not yet been finished by human experts. This result shows the great potential for LLM-assisted development of verified system software.",
    "paper_id": "2512.18436v1"
  },
  "https://arxiv.org/abs/2512.14118v1": {
    "content_path": "../data/contents/2512.14118v1.md",
    "pdf_path": "../data/contents/2512.14118v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:54.126327",
    "title": "CogMem: A Cognitive Memory Architecture for Sustained Multi-Turn Reasoning in Large Language Models",
    "abstract": "Summary: Large language models (LLMs) excel at single-turn reasoning but often lose accuracy and coherence over extended, multi-turn interactions. Recent evaluations such as TurnBench highlight recurring failure modes-reasoning bias, task drift, hallucination, overconfidence, and memory decay. Current approaches typically append full conversational histories, causing unbounded context growth, higher computational costs, and degraded reasoning efficiency. We introduce CogMem, a cognitively inspired, memory-augmented LLM architecture that supports sustained iterative reasoning through structured, persistent memory. CogMem incorporates three layers: a Long-Term Memory (LTM) that consolidates cross-session reasoning strategies; a Direct Access (DA) memory that maintains session-level notes and retrieves relevant long-term memories; and a Focus of Attention (FoA) mechanism that dynamically reconstructs concise, task-relevant context at each turn. Experiments on TurnBench show that this layered design mitigates reasoning failures, controls context growth, and improves consistency across extended reasoning chains, moving toward more reliable, human-like reasoning in LLMs.",
    "paper_id": "2512.14118v1"
  },
  "https://arxiv.org/abs/2512.11391v1": {
    "content_path": "../data/contents/2512.11391v1.md",
    "pdf_path": "../data/contents/2512.11391v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:59.187731",
    "title": "Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization",
    "abstract": "Summary: As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.",
    "paper_id": "2512.11391v1"
  },
  "https://arxiv.org/abs/2512.18434v1": {
    "content_path": "../data/contents/2512.18434v1.md",
    "pdf_path": "../data/contents/2512.18434v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:59.443536",
    "title": "Efficient Optimization of Hierarchical Identifiers for Generative Recommendation",
    "abstract": "Summary: SEATER is a generative retrieval model that improves recommendation inference efficiency and retrieval quality by utilizing balanced tree-structured item identifiers and contrastive training objectives. We reproduce and validate SEATER's reported improvements in retrieval quality over strong baselines across all datasets from the original work, and extend the evaluation to Yambda, a large-scale music recommendation dataset. Our experiments verify SEATER's strong performance, but show that its tree construction step during training becomes a major bottleneck as the number of items grows. To address this, we implement and evaluate two alternative construction algorithms: a greedy method optimized for minimal build time, and a hybrid method that combines greedy clustering at high levels with more precise grouping at lower levels. The greedy method reduces tree construction time to less than 2% of the original with only a minor drop in quality on the dataset with the largest item collection. The hybrid method achieves retrieval quality on par with the original, and even improves on the largest dataset, while cutting construction time to just 5-8%. All data and code are publicly available for full reproducibility at https://github.com/joshrosie/re-seater.",
    "paper_id": "2512.18434v1"
  },
  "https://arxiv.org/abs/2512.14115v1": {
    "content_path": "../data/contents/2512.14115v1.md",
    "pdf_path": "../data/contents/2512.14115v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:21:59.848506",
    "title": "Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting",
    "abstract": "Summary: Acoustic Word Embeddings (AWEs) improve the efficiency of speech retrieval tasks such as Spoken Term Detection (STD) and Keyword Spotting (KWS). However, existing approaches suffer from limitations, including unimodal supervision, disjoint optimization of audio-audio and audio-text alignment, and the need for task-specific models. To address these shortcomings, we propose a joint multimodal contrastive learning framework that unifies both acoustic and cross-modal supervision in a shared embedding space. Our approach simultaneously optimizes: (i) audio-text contrastive learning, inspired by the CLAP loss, to align audio and text representations and (ii) audio-audio contrastive learning, via Deep Word Discrimination (DWD) loss, to enhance intra-class compactness and inter-class separation. The proposed method outperforms existing AWE baselines on word discrimination task while flexibly supporting both STD and KWS. To our knowledge, this is the first comprehensive approach of its kind.",
    "paper_id": "2512.14115v1"
  },
  "https://arxiv.org/abs/2512.11388v1": {
    "content_path": "../data/contents/2512.11388v1.md",
    "pdf_path": "../data/contents/2512.11388v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:00.566087",
    "title": "Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis",
    "abstract": "Summary: We investigated the impact of data selection on machine translation fine-tuning for open LLMs. Using Japanese-English corpora, we compare five selectors: TF-IDF, COMET Kiwi, QuRate, FD-Score, and random selection, under controlled training conditions. We observed that semantic selectors consistently outperform lexical and geometry-based heuristics, and that even when the selected data differ by less than 3%, the impact on model performance is substantial, underscoring the sensitivity of fine-tuning to data quality.",
    "paper_id": "2512.11388v1"
  },
  "https://arxiv.org/abs/2512.18425v1": {
    "content_path": "../data/contents/2512.18425v1.md",
    "pdf_path": "../data/contents/2512.18425v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:01.847252",
    "title": "MoE Pathfinder: Trajectory-driven Expert Pruning",
    "abstract": "Summary: Mixture-of-experts (MoE) architectures used in large language models (LLMs) achieve state-of-the-art performance across diverse tasks yet face practical challenges such as deployment complexity and low activation efficiency. Expert pruning has thus emerged as a promising solution to reduce computational overhead and simplify the deployment of MoE models. However, existing expert pruning approaches conventionally rely on local importance metrics and often apply uniform layer-wise pruning, leveraging only partial evaluation signals and overlooking the heterogeneous contributions of experts across layers. To address these limitations, we propose an expert pruning approach based on the trajectory of activated experts across layers, which treats MoE as a weighted computation graph and casts expert selection as a global optimal path planning problem. Within this framework, we integrate complementary importance signals from reconstruction error, routing probabilities, and activation strength at the trajectory level, which naturally yields non-uniform expert retention across layers. Experiments show that our approach achieves superior pruning performance on nearly all tasks compared with most existing approaches.",
    "paper_id": "2512.18425v1"
  },
  "https://arxiv.org/abs/2512.14112v1": {
    "content_path": "../data/contents/2512.14112v1.md",
    "pdf_path": "../data/contents/2512.14112v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:04.947494",
    "title": "Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model",
    "abstract": "Summary: Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.",
    "paper_id": "2512.14112v1"
  },
  "https://arxiv.org/abs/2512.11374v1": {
    "content_path": "../data/contents/2512.11374v1.md",
    "pdf_path": "../data/contents/2512.11374v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:05.414341",
    "title": "Mining Legal Arguments to Study Judicial Formalism",
    "abstract": "Summary: Courts must justify their decisions, but systematically analyzing judicial reasoning at scale remains difficult. This study refutes claims about formalistic judging in Central and Eastern Europe (CEE) by developing automated methods to detect and classify judicial reasoning in Czech Supreme Courts' decisions using state-of-the-art natural language processing methods. We create the MADON dataset of 272 decisions from two Czech Supreme Courts with expert annotations of 9,183 paragraphs with eight argument types and holistic formalism labels for supervised training and evaluation. Using a corpus of 300k Czech court decisions, we adapt transformer LLMs for Czech legal domain by continued pretraining and experiment with methods to address dataset imbalance including asymmetric loss and class weighting. The best models successfully detect argumentative paragraphs (82.6\\% macro-F1), classify traditional types of legal argument (77.5\\% macro-F1), and classify decisions as formalistic/non-formalistic (83.2\\% macro-F1). Our three-stage pipeline combining ModernBERT, Llama 3.1, and traditional feature-based machine learning achieves promising results for decision classification while reducing computational costs and increasing explainability. Empirically, we challenge prevailing narratives about CEE formalism. This work shows that legal argument mining enables reliable judicial philosophy classification and shows the potential of legal argument mining for other important tasks in computational legal studies. Our methodology is easily replicable across jurisdictions, and our entire pipeline, datasets, guidelines, models, and source codes are available at https://github.com/trusthlt/madon.",
    "paper_id": "2512.11374v1"
  },
  "https://arxiv.org/abs/2512.17221": {
    "content_path": "../data/contents/2512.17221.md",
    "pdf_path": "../data/contents/2512.17221.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:06.326076",
    "title": "DAVE: A VLM Vision Encoder for Document Understanding and Web Agents",
    "abstract": "New vision encoder tailored for document/web UI structure and spatial features—important for doc understanding and web-agent perception stacks.",
    "paper_id": "2512.17221"
  },
  "https://arxiv.org/abs/2512.18411v1": {
    "content_path": "../data/contents/2512.18411v1.md",
    "pdf_path": "../data/contents/2512.18411v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:08.834520",
    "title": "AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning",
    "abstract": "Summary: Multi-prompt learning methods have emerged as an effective approach for facilitating the rapid adaptation of vision-language models to downstream tasks with limited resources. Existing multi-prompt learning methods primarily focus on utilizing various meticulously designed prompts within a single foundation vision-language model to achieve superior performance. However, the overlooked model-prompt matching bias hinders the development of multi-prompt learning, i.e., the same prompt can convey different semantics across distinct vision-language models, such as CLIP-ViT-B/16 and CLIP-ViT-B/32, resulting in inconsistent predictions of identical prompt. To mitigate the impact of this bias on downstream tasks, we explore an ensemble learning approach to sufficiently aggregate the benefits of diverse predictions. Additionally, we further disclose the presence of sample-prompt matching bias, which originates from the prompt-irrelevant semantics encapsulated in the input samples. Thus, directly utilizing all information from the input samples for generating weights of ensemble learning can lead to suboptimal performance. In response, we extract prompt-relevant semantics from input samples by leveraging the guidance of the information theory-based analysis, adaptively calculating debiased ensemble weights. Overall, we propose Adaptive-Debiased Ensemble MultiPrompt Learning, abbreviated as AmPLe, to mitigate the two types of bias simultaneously. Extensive experiments on three representative tasks, i.e., generalization to novel classes, new target datasets, and unseen domain shifts, show that AmPLe can widely outperform existing methods. Theoretical validation from a causal perspective further supports the effectiveness of AmPLe.",
    "paper_id": "2512.18411v1"
  },
  "https://arxiv.org/abs/2512.14102v1": {
    "content_path": "../data/contents/2512.14102v1.md",
    "pdf_path": "../data/contents/2512.14102v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:10.011454",
    "title": "Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries",
    "abstract": "Summary: Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.",
    "paper_id": "2512.14102v1"
  },
  "https://arxiv.org/abs/2512.11366v1": {
    "content_path": "../data/contents/2512.11366v1.md",
    "pdf_path": "../data/contents/2512.11366v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:11.930949",
    "title": "qa-FLoRA: Data-free query-adaptive Fusion of LoRAs for LLMs",
    "abstract": "Summary: The deployment of large language models for specialized tasks often requires domain-specific parameter-efficient finetuning through Low-Rank Adaptation (LoRA) modules. However, effectively fusing these adapters to handle complex, multi-domain composite queries remains a critical challenge. Existing LoRA fusion approaches either use static weights, which assign equal relevance to each participating LoRA, or require data-intensive supervised training for every possible LoRA combination to obtain respective optimal fusion weights. We propose qa-FLoRA, a novel query-adaptive data-and-training-free method for LoRA fusion that dynamically computes layer-level fusion weights by measuring distributional divergence between the base model and respective adapters. Our approach eliminates the need for composite training data or domain-representative samples, making it readily applicable to existing adapter collections. Extensive experiments across nine multilingual composite tasks spanning mathematics, coding, and medical domains, show that qa-FLoRA outperforms static fusion by ~5% with LLaMA-2 and ~6% with LLaMA-3, and the training-free baselines by ~7% with LLaMA-2 and ~10% with LLaMA-3, while significantly closing the gap with supervised baselines. Further, layer-level analysis of our fusion weights reveals interpretable fusion patterns, demonstrating the effectiveness of our approach for robust multi-domain adaptation.",
    "paper_id": "2512.11366v1"
  },
  "https://arxiv.org/abs/2511.11552": {
    "content_path": "../data/contents/2511.11552.md",
    "pdf_path": "../data/contents/2511.11552.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:13.988051",
    "title": "DocLens: A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding",
    "abstract": "Multi-agent “zoom-in” approach for evidence localization in long documents; targets hallucination reduction and cross-page reasoning.",
    "paper_id": "2511.11552"
  },
  "https://arxiv.org/abs/2512.18405v1": {
    "content_path": "../data/contents/2512.18405v1.md",
    "pdf_path": "../data/contents/2512.18405v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:15.213611",
    "title": "Towards Scalable Visual Data Wrangling via Direct Manipulation",
    "abstract": "Summary: Data wrangling - the process of cleaning, transforming, and preparing data for analysis - is a well-known bottleneck in data science workflows. Existing tools either rely on manual scripting, which is error-prone and hard to debug, or automate cleaning through opaque black-box pipelines that offer limited control. We present Buckaroo, a scalable visual data wrangling system that restructures data preparation as a direct manipulation task over visualizations. Buckaroo enables users to explore and repair data anomalies - such as missing values, outliers, and type mismatches - by interacting directly with coordinated data visualizations. The system extensibly supports user-defined error detectors and wranglers, tracks provenance for undo/redo, and generates reproducible scripts for downstream tasks. Buckaroo maintains efficient indexing data structures and differential storage to localize anomaly detection and minimize recomputation. To demonstrate the applicability of our model, Buckaroo is integrated with the \\textit{Hopara} pan-and-zoom engine, which enables multi-layered navigation over large datasets without sacrificing interactivity. Through empirical evaluation and an expert review, we show that Buckaroo makes visual data wrangling scalable - bridging the gap between visual inspection and programmable repairs.",
    "paper_id": "2512.18405v1"
  },
  "https://arxiv.org/abs/2512.14100v1": {
    "content_path": "../data/contents/2512.14100v1.md",
    "pdf_path": "../data/contents/2512.14100v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:15.984051",
    "title": "A First-Order Logic-Based Alternative to Reward Models in RLHF",
    "abstract": "Summary: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.\n  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.\n  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.",
    "paper_id": "2512.14100v1"
  },
  "https://arxiv.org/abs/2512.11363v1": {
    "content_path": "../data/contents/2512.11363v1.md",
    "pdf_path": "../data/contents/2512.11363v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:17.237840",
    "title": "A Cross-Chain Event-Driven Data Infrastructure for Aave Protocol Analytics and Applications",
    "abstract": "Summary: Decentralized lending protocols, exemplified by Aave V3, have transformed financial intermediation by enabling permissionless, multi-chain borrowing and lending without intermediaries. Despite managing over $10 billion in total value locked, empirical research remains severely constrained by the lack of standardized, cross-chain event-level datasets.\n  This paper introduces the first comprehensive, event-driven data infrastructure for Aave V3 spanning six major EVM-compatible chains (Ethereum, Arbitrum, Optimism, Polygon, Avalanche, and Base) from respective deployment blocks through October 2025. We collect and fully decode eight core event types -- Supply, Borrow, Withdraw, Repay, LiquidationCall, FlashLoan, ReserveDataUpdated, and MintedToTreasury -- producing over 50 million structured records enriched with block metadata and USD valuations.\n  Using an open-source Python pipeline with dynamic batch sizing and automatic sharding (each file less than or equal to 1 million rows), we ensure strict chronological ordering and full reproducibility. The resulting publicly available dataset enables granular analysis of capital flows, interest rate dynamics, liquidation cascades, and cross-chain user behavior, providing a foundational resource for future studies on decentralized lending markets and systemic risk.",
    "paper_id": "2512.11363v1"
  },
  "https://arxiv.org/abs/2511.11313": {
    "content_path": "../data/contents/2511.11313.md",
    "pdf_path": "../data/contents/2511.11313.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:25.769918",
    "title": "DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding",
    "abstract": "Memory/latency-efficient VLM for long documents using hierarchical compression and streaming abstention—relevant for deployment-constrained doc AI.",
    "paper_id": "2511.11313"
  },
  "https://arxiv.org/abs/2512.18399v1": {
    "content_path": "../data/contents/2512.18399v1.md",
    "pdf_path": "../data/contents/2512.18399v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:26.371567",
    "title": "AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3",
    "abstract": "Summary: Tokenization is a critical preprocessing step for large language models (LLMs), directly impacting training efficiency and downstream performance. General-purpose tokenizers trained predominantly on English and Latin-script languages exhibit suboptimal performance on morphologically rich languages such as Arabic, resulting in inflated token sequences and reduced compression efficiency. In this work, we present AraToken, an Arabic-optimized tokenizer built on SentencePiece Unigram algorithm with a comprehensive normalization pipeline addressing Arabic-specific orthographic variations including Alif variants, diacritics, and Arabic-Indic numerals. We systematically compare BPE, WordPiece, and SentencePiece algorithms across multiple configurations, demonstrating that SentencePiece with normalization achieves 18% lower fertility (1.199 vs 1.35 tokens/word) compared to unnormalized baselines. Furthermore, we introduce the Language Extension Pipeline (LEP), a method for integrating the optimized tokenizer into Qwen3-0.6B through vocabulary extension with mean subtoken initialization and selective transformer layer unfreezing. Our experiments show that LEP reduces evaluation loss from 8.28 to 2.43 within 800 training steps on 100K Arabic samples. We release our tokenizer, training scripts, and model checkpoints to facilitate Arabic NLP research.",
    "paper_id": "2512.18399v1"
  },
  "https://arxiv.org/abs/2512.14098v2": {
    "content_path": "../data/contents/2512.14098v2.md",
    "pdf_path": "../data/contents/2512.14098v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:26.982391",
    "title": "Cornserve: Efficiently Serving Any-to-Any Multimodal Models",
    "abstract": "Summary: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.\n  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\\times$ throughput improvement and up to 5.79$\\times$ tail latency reduction over existing solutions.",
    "paper_id": "2512.14098v2"
  },
  "https://arxiv.org/abs/2512.11352v1": {
    "content_path": "../data/contents/2512.11352v1.md",
    "pdf_path": "../data/contents/2512.11352v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:28.934184",
    "title": "CAT: Can Trust be Predicted with Context-Awareness in Dynamic Heterogeneous Networks?",
    "abstract": "Summary: Trust prediction provides valuable support for decision-making, risk mitigation, and system security enhancement. Recently, Graph Neural Networks (GNNs) have emerged as a promising approach for trust prediction, owing to their ability to learn expressive node representations that capture intricate trust relationships within a network. However, current GNN-based trust prediction models face several limitations: (i) Most of them fail to capture trust dynamicity, leading to questionable inferences. (ii) They rarely consider the heterogeneous nature of real-world networks, resulting in a loss of rich semantics. (iii) None of them support context-awareness, a basic property of trust, making prediction results coarse-grained.\n  To this end, we propose CAT, the first Context-Aware GNN-based Trust prediction model that supports trust dynamicity and accurately represents real-world heterogeneity. CAT consists of a graph construction layer, an embedding layer, a heterogeneous attention layer, and a prediction layer. It handles dynamic graphs using continuous-time representations and captures temporal information through a time encoding function. To model graph heterogeneity and leverage semantic information, CAT employs a dual attention mechanism that identifies the importance of different node types and nodes within each type. For context-awareness, we introduce a new notion of meta-paths to extract contextual features. By constructing context embeddings and integrating a context-aware aggregator, CAT can predict both context-aware trust and overall trust. Extensive experiments on three real-world datasets demonstrate that CAT outperforms five groups of baselines in trust prediction, while exhibiting strong scalability to large-scale graphs and robustness against both trust-oriented and GNN-oriented attacks.",
    "paper_id": "2512.11352v1"
  },
  "https://arxiv.org/abs/2511.12003": {
    "content_path": "../data/contents/2511.12003.md",
    "pdf_path": "../data/contents/2511.12003.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:35.888812",
    "title": "Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning",
    "abstract": "Introduces Chain-of-Evidence reasoning with page+box citations trained via RL—directly applicable to verifiable multimodal RAG over PDFs.",
    "paper_id": "2511.12003"
  },
  "https://arxiv.org/abs/2512.18390v1": {
    "content_path": "../data/contents/2512.18390v1.md",
    "pdf_path": "../data/contents/2512.18390v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:36.906093",
    "title": "The Challenger: When Do New Data Sources Justify Switching Machine Learning Models?",
    "abstract": "Summary: We study the problem of deciding whether, and when an organization should replace a trained incumbent model with a challenger relying on newly available features. We develop a unified economic and statistical framework that links learning-curve dynamics, data-acquisition and retraining costs, and discounting of future gains. First, we characterize the optimal switching time in stylized settings and derive closed-form expressions that quantify how horizon length, learning-curve curvature, and cost differentials shape the optimal decision. Second, we propose three practical algorithms: a one-shot baseline, a greedy sequential method, and a look-ahead sequential method. Using a real-world credit-scoring dataset with gradually arriving alternative data, we show that (i) optimal switching times vary systematically with cost parameters and learning-curve behavior, and (ii) the look-ahead sequential method outperforms other methods and is able to approach in value an oracle with full foresight. Finally, we establish finite-sample guarantees, including conditions under which the sequential look-ahead method achieve sublinear regret relative to that oracle. Our results provide an operational blueprint for economically sound model transitions as new data sources become available.",
    "paper_id": "2512.18390v1"
  },
  "https://arxiv.org/abs/2512.14090v2": {
    "content_path": "../data/contents/2512.14090v2.md",
    "pdf_path": "../data/contents/2512.14090v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:37.535487",
    "title": "Arithmetic-Intensity-Aware Quantization",
    "abstract": "Summary: As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.",
    "paper_id": "2512.14090v2"
  },
  "https://arxiv.org/abs/2512.11325v1": {
    "content_path": "../data/contents/2512.11325v1.md",
    "pdf_path": "../data/contents/2512.11325v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:38.644906",
    "title": "MLLM Machine Unlearning via Visual Knowledge Distillation",
    "abstract": "Summary: Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.",
    "paper_id": "2512.11325v1"
  },
  "https://arxiv.org/abs/2512.00305": {
    "content_path": "../data/contents/2512.00305.md",
    "pdf_path": "../data/contents/2512.00305.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:39.200759",
    "title": "ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning",
    "abstract": "Chart reasoning method + dataset (ChartPoint-SFT) using bbox grounding/reflection to reduce numerical hallucinations in chart QA/summarization.",
    "paper_id": "2512.00305"
  },
  "https://arxiv.org/abs/2512.18388v1": {
    "content_path": "../data/contents/2512.18388v1.md",
    "pdf_path": "../data/contents/2512.18388v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:40.537999",
    "title": "Exploration vs. Fixation: Scaffolding Divergent and Convergent Thinking for Human-AI Co-Creation with Generative Models",
    "abstract": "Summary: Generative AI has begun to democratize creative work, enabling novices to produce complex artifacts such as code, images, and videos. However, in practice, existing interaction paradigms often fail to support divergent exploration: users tend to converge too quickly on early ``good enough'' results and struggle to move beyond them, leading to premature convergence and design fixation that constrains their creative potential. To address this, we propose a structured, process-oriented human-AI co-creation paradigm including divergent and convergent thinking stages, grounded in Wallas's model of creativity. To avoid design fixation, our paradigm scaffolds both high-level exploration of conceptual ideas in the early divergent thinking phase and low-level exploration of variations in the later convergent thinking phrase. We instantiate this paradigm in HAIExplore, an image co-creation system that (i) scaffolds divergent thinking through a dedicated brainstorming stage for exploring high-level ideas in a conceptual space, and (ii) scaffolds convergent refinement through an interface that externalizes users' refinement intentions as interpretable parameters and options, making the refinement process more controllable and easier to explore. We report on a within-subjects study comparing HAIExplore with a widely used linear chat interface (ChatGPT) for creative image generation. Our findings show that explicitly scaffolding the creative process into brainstorming and refinement stages can mitigate design fixation, improve perceived controllability and alignment with users' intentions, and better support the non-linear nature of creative work. We conclude with design implications for future creativity support tools and human-AI co-creation workflows.",
    "paper_id": "2512.18388v1"
  },
  "https://arxiv.org/abs/2512.14082v1": {
    "content_path": "../data/contents/2512.14082v1.md",
    "pdf_path": "../data/contents/2512.14082v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:41.729151",
    "title": "A Unified Sparse Attention via Multi-Granularity Compression",
    "abstract": "Summary: Efficient long-context understanding and reasoning are increasingly vital for large language model (LLM) applications such as multi-turn dialogue and program analysis. However, the core self-attention mechanism scales quadratically with sequence length, creating a fundamental computational bottleneck. Existing sparse attention methods alleviate this issue but face trade-offs: training-based methods are costly and cannot be directly applied as acceleration plugins for other models, while inference-time methods often compromise efficiency or cross-modal generality. To address these limitations, we present UniSparse, a unified mechanism that introduces the notion of composite tokens--compact representations that aggregate multi-granularity contextual information. Building on this abstraction, UniSparse dynamically constructs sparse attention through multi-granularity compression and block-level selection, enabling efficient and hardware-friendly execution on GPU. Across multiple modalities and tasks ranging from synthetic benchmarks to real-world applications, UniSparse consistently surpasses state-of-the-art sparse attention methods (e.g., MInference, XAttention, FlexPrefill) in both accuracy and efficiency, achieving $\\ge$ 99% of full-attention accuracy and up to 2.61$\\times$ faster attention computation than FlashAttention.",
    "paper_id": "2512.14082v1"
  },
  "https://arxiv.org/abs/2512.11317v1": {
    "content_path": "../data/contents/2512.11317v1.md",
    "pdf_path": "../data/contents/2512.11317v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:42.082708",
    "title": "Condensation-Concatenation Framework for Dynamic Graph Continual Learning",
    "abstract": "Summary: Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.",
    "paper_id": "2512.11317v1"
  },
  "https://arxiv.org/abs/2503.11576": {
    "content_path": "../data/contents/2503.11576.md",
    "pdf_path": "../data/contents/2503.11576.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:46.161100",
    "title": "SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion",
    "abstract": "Compact VLM that outputs structured DocTags for full-page document conversion (tables/charts/equations) enabling cleaner downstream RAG ingestion.",
    "paper_id": "2503.11576"
  },
  "https://arxiv.org/abs/2512.18384v1": {
    "content_path": "../data/contents/2512.18384v1.md",
    "pdf_path": "../data/contents/2512.18384v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:46.440855",
    "title": "Datasets for machine learning and for assessing the intelligence level of automatic patent search systems",
    "abstract": "Summary: The key to success in automating prior art search in patent research using artificial intelligence lies in developing large datasets for machine learning and ensuring their availability. This work is dedicated to providing a comprehensive solution to the problem of creating infrastructure for research in this field, including datasets and tools for calculating search quality criteria. The paper discusses the concept of semantic clusters of patent documents that determine the state of the art in a given subject, as proposed by the authors. A definition of such semantic clusters is also provided. Prior art search is presented as the task of identifying elements within a semantic cluster of patent documents in the subject area specified by the document under consideration. A generator of user-configurable datasets for machine learning, based on collections of U.S. and Russian patent documents, is described. The dataset generator creates a database of links to documents in semantic clusters. Then, based on user-defined parameters, it forms a dataset of semantic clusters in JSON format for machine learning. To evaluate machine learning outcomes, it is proposed to calculate search quality scores that account for semantic clusters of the documents being searched. To automate the evaluation process, the paper describes a utility developed by the authors for assessing the quality of prior art document search.",
    "paper_id": "2512.18384v1"
  },
  "https://arxiv.org/abs/2512.14765v1": {
    "content_path": "../data/contents/2512.14765v1.md",
    "pdf_path": "../data/contents/2512.14765v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:46.776053",
    "title": "Guided Discrete Diffusion for Constraint Satisfaction Problems",
    "abstract": "Summary: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.",
    "paper_id": "2512.14765v1"
  },
  "https://arxiv.org/abs/2512.11315v1": {
    "content_path": "../data/contents/2512.11315v1.md",
    "pdf_path": "../data/contents/2512.11315v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:47.399249",
    "title": "Benchmarking the Generality of Vision-Language-Action Models",
    "abstract": "Summary: Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.",
    "paper_id": "2512.11315v1"
  },
  "https://arxiv.org/abs/2512.18373v1": {
    "content_path": "../data/contents/2512.18373v1.md",
    "pdf_path": "../data/contents/2512.18373v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:50.181368",
    "title": "Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale",
    "abstract": "Summary: Neural network optimization remains one of the most consequential yet poorly understood challenges in modern AI research, where improvements in training algorithms can lead to enhanced feature learning in foundation models, order-of-magnitude reductions in training time, and improved interpretability into how networks learn. While stochastic gradient descent (SGD) and its variants have become the de facto standard for training deep networks, their success in these over-parameterized regimes often appears more empirical than principled. This thesis investigates this apparent paradox by tracing the evolution of optimization algorithms from classical first-order methods to modern higher-order techniques, revealing how principled algorithmic design can demystify the training process. Starting from first principles with SGD and adaptive gradient methods, the analysis progressively uncovers the limitations of these conventional approaches when confronted with anisotropy that is representative of real-world data. These breakdowns motivate the exploration of sophisticated alternatives rooted in curvature information: second-order approximation techniques, layer-wise preconditioning, adaptive learning rates, and more. Next, the interplay between these optimization algorithms and the broader neural network training toolkit, which includes prior and recent developments such as maximal update parametrization, learning rate schedules, and exponential moving averages, emerges as equally essential to empirical success. To bridge the gap between theoretical understanding and practical deployment, this paper offers practical prescriptions and implementation strategies for integrating these methods into modern deep learning workflows.",
    "paper_id": "2512.18373v1"
  },
  "https://arxiv.org/abs/2512.14080v1": {
    "content_path": "../data/contents/2512.14080v1.md",
    "pdf_path": "../data/contents/2512.14080v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:52.558202",
    "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
    "abstract": "Summary: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.",
    "paper_id": "2512.14080v1"
  },
  "https://arxiv.org/abs/2512.11935v1": {
    "content_path": "../data/contents/2512.11935v1.md",
    "pdf_path": "../data/contents/2512.11935v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:54.096516",
    "title": "AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org",
    "abstract": "Summary: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.",
    "paper_id": "2512.11935v1"
  },
  "https://arxiv.org/abs/2512.18362v1": {
    "content_path": "../data/contents/2512.18362v1.md",
    "pdf_path": "../data/contents/2512.18362v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:54.626889",
    "title": "SRS-Stories: Vocabulary-constrained multilingual story generation for language learning",
    "abstract": "Summary: In this paper, we use large language models to generate personalized stories for language learners, using only the vocabulary they know. The generated texts are specifically written to teach the user new vocabulary by simply reading stories where it appears in context, while at the same time seamlessly reviewing recently learned vocabulary. The generated stories are enjoyable to read and the vocabulary reviewing/learning is optimized by a Spaced Repetition System. The experiments are conducted in three languages: English, Chinese and Polish, evaluating three story generation methods and three strategies for enforcing lexical constraints. The results show that the generated stories are more grammatical, coherent, and provide better examples of word usage than texts generated by the standard constrained beam search approach",
    "paper_id": "2512.18362v1"
  },
  "https://arxiv.org/abs/2512.14079v1": {
    "content_path": "../data/contents/2512.14079v1.md",
    "pdf_path": "../data/contents/2512.14079v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:54.861649",
    "title": "Grammar Search for Multi-Agent Systems",
    "abstract": "Summary: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.",
    "paper_id": "2512.14079v1"
  },
  "https://arxiv.org/abs/2512.11934v1": {
    "content_path": "../data/contents/2512.11934v1.md",
    "pdf_path": "../data/contents/2512.11934v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:55.603049",
    "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching",
    "abstract": "Summary: The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).",
    "paper_id": "2512.11934v1"
  },
  "https://arxiv.org/abs/2512.18360v1": {
    "content_path": "../data/contents/2512.18360v1.md",
    "pdf_path": "../data/contents/2512.18360v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:56.099508",
    "title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators",
    "abstract": "Summary: We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models",
    "paper_id": "2512.18360v1"
  },
  "https://arxiv.org/abs/2512.14078v1": {
    "content_path": "../data/contents/2512.14078v1.md",
    "pdf_path": "../data/contents/2512.14078v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:58.041820",
    "title": "FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis",
    "abstract": "Summary: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.",
    "paper_id": "2512.14078v1"
  },
  "https://arxiv.org/abs/2512.11306v2": {
    "content_path": "../data/contents/2512.11306v2.md",
    "pdf_path": "../data/contents/2512.11306v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:59.106198",
    "title": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training",
    "abstract": "Summary: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable \"warm-star\" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.",
    "paper_id": "2512.11306v2"
  },
  "https://arxiv.org/abs/2512.18357v1": {
    "content_path": "../data/contents/2512.18357v1.md",
    "pdf_path": "../data/contents/2512.18357v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:22:59.706086",
    "title": "DACE For Railway Acronym Disambiguation",
    "abstract": "Summary: Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.",
    "paper_id": "2512.18357v1"
  },
  "https://arxiv.org/abs/2512.14069v1": {
    "content_path": "../data/contents/2512.14069v1.md",
    "pdf_path": "../data/contents/2512.14069v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:00.345575",
    "title": "RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees",
    "abstract": "Summary: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.",
    "paper_id": "2512.14069v1"
  },
  "https://arxiv.org/abs/2512.11303v1": {
    "content_path": "../data/contents/2512.11303v1.md",
    "pdf_path": "../data/contents/2512.11303v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:09.928457",
    "title": "Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture",
    "abstract": "Summary: Large Language Model agents face fundamental challenges in adapting to novel tasks due to limitations in tool availability and experience reuse. Existing approaches either rely on predefined tools with limited coverage or build tools from scratch without leveraging past experiences, leading to inefficient exploration and suboptimal performance. We introduce SMITH (Shared Memory Integrated Tool Hub), a unified cognitive architecture that seamlessly integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. SMITH organizes agent memory into procedural, semantic, and episodic components, enabling systematic capability expansion while preserving successful execution patterns. Our approach formalizes tool creation as iterative code generation within controlled sandbox environments and experience sharing through episodic memory retrieval with semantic similarity matching. We further propose a curriculum learning strategy based on agent-ensemble difficulty re-estimation. Extensive experiments on the GAIA benchmark demonstrate SMITH's effectiveness, achieving 81.8% Pass@1 accuracy and outperforming state-of-the-art baselines including Alita (75.2%) and Memento (70.9%). Our work establishes a foundation for building truly adaptive agents that continuously evolve their capabilities through principled integration of tool creation and experience accumulation.",
    "paper_id": "2512.11303v1"
  },
  "https://arxiv.org/abs/2512.18352v1": {
    "content_path": "../data/contents/2512.18352v1.md",
    "pdf_path": "../data/contents/2512.18352v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:10.647768",
    "title": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent",
    "abstract": "Summary: Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \\textit{early time point determination}, while the LLM serves as a powerful \\textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.",
    "paper_id": "2512.18352v1"
  },
  "https://arxiv.org/abs/2512.14068v1": {
    "content_path": "../data/contents/2512.14068v1.md",
    "pdf_path": "../data/contents/2512.14068v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:13.171058",
    "title": "SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding",
    "abstract": "Summary: Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \\textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \\emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \\textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \\textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \\textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \\emph{training efficiency}, \\emph{convergence stability}, and \\emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.",
    "paper_id": "2512.14068v1"
  },
  "https://arxiv.org/abs/2512.11933v1": {
    "content_path": "../data/contents/2512.11933v1.md",
    "pdf_path": "../data/contents/2512.11933v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:13.572206",
    "title": "The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance",
    "abstract": "Summary: Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of \"regulatory blocks\": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.",
    "paper_id": "2512.11933v1"
  },
  "https://arxiv.org/abs/2512.18337v1": {
    "content_path": "../data/contents/2512.18337v1.md",
    "pdf_path": "../data/contents/2512.18337v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:15.699417",
    "title": "Towards Efficient Agents: A Co-Design of Inference Architecture and System",
    "abstract": "Summary: The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.",
    "paper_id": "2512.18337v1"
  },
  "https://arxiv.org/abs/2512.14067v1": {
    "content_path": "../data/contents/2512.14067v1.md",
    "pdf_path": "../data/contents/2512.14067v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:17.153201",
    "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
    "abstract": "Summary: Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.",
    "paper_id": "2512.14067v1"
  },
  "https://arxiv.org/abs/2512.11297v2": {
    "content_path": "../data/contents/2512.11297v2.md",
    "pdf_path": "../data/contents/2512.11297v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:19.528878",
    "title": "LegalRikai: Open Benchmark -- Benchmark for Complex Japanese Corporate Legal Tasks",
    "abstract": "Summary: This paper introduces LegalRikai: Open Benchmark, a new benchmark comprising four complex tasks that emulate Japanese corporate legal practices. The benchmark was created by legal professionals under the supervision of an attorney. This benchmark has 100 samples that require long-form, structured outputs, and we evaluated them against multiple practical criteria. We conducted both human and automated evaluations using leading LLMs, including GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1. Our human evaluation revealed that abstract instructions prompted unnecessary modifications, highlighting model weaknesses in document-level editing that were missed by conventional short-text tasks. Furthermore, our analysis reveals that automated evaluation aligns well with human judgment on criteria with clear linguistic grounding, and assessing structural consistency remains a challenge. The result demonstrates the utility of automated evaluation as a screening tool when expert availability is limited. We propose a dataset evaluation framework to promote more practice-oriented research in the legal domain.",
    "paper_id": "2512.11297v2"
  },
  "https://arxiv.org/abs/2512.18329v1": {
    "content_path": "../data/contents/2512.18329v1.md",
    "pdf_path": "../data/contents/2512.18329v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:19.840332",
    "title": "LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation",
    "abstract": "Summary: Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.",
    "paper_id": "2512.18329v1"
  },
  "https://arxiv.org/abs/2512.14064v1": {
    "content_path": "../data/contents/2512.14064v1.md",
    "pdf_path": "../data/contents/2512.14064v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:21.036790",
    "title": "What Affects the Effective Depth of Large Language Models?",
    "abstract": "Summary: The scaling of large language models (LLMs) emphasizes increasing depth, yet performance gains diminish with added layers. Prior work introduces the concept of \"effective depth\", arguing that deeper models fail to fully utilize their layers for meaningful computation. Building on this, we systematically study how effective depth varies with model scale, training type, and task difficulty. First, we analyze the model behavior of Qwen-2.5 family (1.5B-32B) and find that while the number of effective layers grows with model size, the effective depth ratio remains stable. Besides, comparisons between base and corresponding long-CoT models show no increase in effective depth, suggesting that improved reasoning stems from longer context rather than deeper per-token computation. Furthermore, evaluations across tasks of varying difficulty indicate that models do not dynamically use more layers for harder problems. Our results suggest that current LLMs underuse available depth across scales, training paradigms and tasks of varying difficulties, pointing out research opportunities on increasing the layer utilization rate of LLMs, model pruning, and early exiting. Our code is released at https://github.com/AheadOFpotato/what_affects_effective_depth.",
    "paper_id": "2512.14064v1"
  },
  "https://arxiv.org/abs/2512.11296v1": {
    "content_path": "../data/contents/2512.11296v1.md",
    "pdf_path": "../data/contents/2512.11296v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:21.598394",
    "title": "Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining",
    "abstract": "Summary: Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.",
    "paper_id": "2512.11296v1"
  },
  "https://arxiv.org/abs/2512.18321v1": {
    "content_path": "../data/contents/2512.18321v1.md",
    "pdf_path": "../data/contents/2512.18321v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:23.138206",
    "title": "CTTA-T: Continual Test-Time Adaptation for Text Understanding via Teacher-Student with a Domain-aware and Generalized Teacher",
    "abstract": "Summary: Text understanding often suffers from domain shifts. To handle testing domains, domain adaptation (DA) is trained to adapt to a fixed and observed testing domain; a more challenging paradigm, test-time adaptation (TTA), cannot access the testing domain during training and online adapts to the testing samples during testing, where the samples are from a fixed domain. We aim to explore a more practical and underexplored scenario, continual test-time adaptation (CTTA) for text understanding, which involves a sequence of testing (unobserved) domains in testing. Current CTTA methods struggle in reducing error accumulation over domains and enhancing generalization to handle unobserved domains: 1) Noise-filtering reduces accumulated errors but discards useful information, and 2) accumulating historical domains enhances generalization, but it is hard to achieve adaptive accumulation. In this paper, we propose a CTTA-T (continual test-time adaptation for text understanding) framework adaptable to evolving target domains: it adopts a teacher-student framework, where the teacher is domain-aware and generalized for evolving domains. To improve teacher predictions, we propose a refine-then-filter based on dropout-driven consistency, which calibrates predictions and removes unreliable guidance. For the adaptation-generalization trade-off, we construct a domain-aware teacher by dynamically accumulating cross-domain semantics via incremental PCA, which continuously tracks domain shifts. Experiments show CTTA-T excels baselines.",
    "paper_id": "2512.18321v1"
  },
  "https://arxiv.org/abs/2512.14764v1": {
    "content_path": "../data/contents/2512.14764v1.md",
    "pdf_path": "../data/contents/2512.14764v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:23.342611",
    "title": "Scaling Causal Mediation for Complex Systems: A Framework for Root Cause Analysis",
    "abstract": "Summary: Modern operational systems ranging from logistics and cloud infrastructure to industrial IoT, are governed by complex, interdependent processes. Understanding how interventions propagate through such systems requires causal inference methods that go beyond direct effects to quantify mediated pathways. Traditional mediation analysis, while effective in simple settings, fails to scale to the high-dimensional directed acyclic graphs (DAGs) encountered in practice, particularly when multiple treatments and mediators interact. In this paper, we propose a scalable mediation analysis framework tailored for large causal DAGs involving multiple treatments and mediators. Our approach systematically decomposes total effects into interpretable direct and indirect components. We demonstrate its practical utility through applied case studies in fulfillment center logistics, where complex dependencies and non-controllable factors often obscure root causes.",
    "paper_id": "2512.14764v1"
  },
  "https://arxiv.org/abs/2512.11295v3": {
    "content_path": "../data/contents/2512.11295v3.md",
    "pdf_path": "../data/contents/2512.11295v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:23.654924",
    "title": "AI Autonomy Coefficient ($α$): Defining Boundaries for Responsible AI Systems",
    "abstract": "Summary: The integrity of many contemporary AI systems is compromised by the misuse of Human-in-the-Loop (HITL) models to obscure systems that remain heavily dependent on human labor. We define this structural dependency as Human-Instead-of-AI (HISOAI), an ethically problematic and economically unsustainable design in which human workers function as concealed operational substitutes rather than intentional, high-value collaborators. To address this issue, we introduce the AI-First, Human-Empowered (AFHE) paradigm, which requires AI systems to demonstrate a quantifiable level of functional independence prior to deployment. This requirement is formalized through the AI Autonomy Coefficient, measuring the proportion of tasks completed without mandatory human intervention. We further propose the AFHE Deployment Algorithm, an algorithmic gate that enforces a minimum autonomy threshold during offline evaluation and shadow deployment. Our results show that the AI Autonomy Coefficient effectively identifies HISOAI systems with an autonomy level of 0.38, while systems governed by the AFHE framework achieve an autonomy level of 0.85. We conclude that AFHE provides a metric-driven approach for ensuring verifiable autonomy, transparency, and sustainable operational integrity in modern AI systems.",
    "paper_id": "2512.11295v3"
  },
  "https://arxiv.org/abs/2505.17471": {
    "content_path": "../data/contents/2505.17471.md",
    "pdf_path": "../data/contents/2505.17471.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:31.209641",
    "title": "FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain",
    "abstract": "Benchmark for visual RAG over finance PDFs with visual citations and evaluation methods—targets grounding and traceability.",
    "paper_id": "2505.17471"
  },
  "https://arxiv.org/abs/2512.18311v1": {
    "content_path": "../data/contents/2512.18311v1.md",
    "pdf_path": "../data/contents/2512.18311v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:46.277101",
    "title": "Monitoring Monitorability",
    "abstract": "Summary: Observability into the decision making of modern AI systems may be required to safely deploy increasingly capable agents. Monitoring the chain-of-thought (CoT) of today's reasoning models has proven effective for detecting misbehavior. However, this \"monitorability\" may be fragile under different training procedures, data sources, or even continued system scaling. To measure and track monitorability, we propose three evaluation archetypes (intervention, process, and outcome-property) and a new monitorability metric, and introduce a broad evaluation suite. We demonstrate that these evaluations can catch simple model organisms trained to have obfuscated CoTs, and that CoT monitoring is more effective than action-only monitoring in practical settings. We compare the monitorability of various frontier models and find that most models are fairly, but not perfectly, monitorable. We also evaluate how monitorability scales with inference-time compute, reinforcement learning optimization, and pre-training model size. We find that longer CoTs are generally more monitorable and that RL optimization does not materially decrease monitorability even at the current frontier scale. Notably, we find that for a model at a low reasoning effort, we could instead deploy a smaller model at a higher reasoning effort (thereby matching capabilities) and obtain a higher monitorability, albeit at a higher overall inference compute cost. We further investigate agent-monitor scaling trends and find that scaling a weak monitor's test-time compute when monitoring a strong agent increases monitorability. Giving the weak monitor access to CoT not only improves monitorability, but it steepens the monitor's test-time compute to monitorability scaling trend. Finally, we show we can improve monitorability by asking models follow-up questions and giving their follow-up CoT to the monitor.",
    "paper_id": "2512.18311v1"
  },
  "https://arxiv.org/abs/2512.15792v1": {
    "content_path": "../data/contents/2512.15792v1.md",
    "pdf_path": "../data/contents/2512.15792v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:47.347778",
    "title": "A Systematic Analysis of Biases in Large Language Models",
    "abstract": "Summary: Large language models (LLMs) have rapidly become indispensable tools for acquiring information and supporting human decision-making. However, ensuring that these models uphold fairness across varied contexts is critical to their safe and responsible deployment. In this study, we undertake a comprehensive examination of four widely adopted LLMs, probing their underlying biases and inclinations across the dimensions of politics, ideology, alliance, language, and gender. Through a series of carefully designed experiments, we investigate their political neutrality using news summarization, ideological biases through news stance classification, tendencies toward specific geopolitical alliances via United Nations voting patterns, language bias in the context of multilingual story completion, and gender-related affinities as revealed by responses to the World Values Survey. Results indicate that while the LLMs are aligned to be neutral and impartial, they still show biases and affinities of different types.",
    "paper_id": "2512.15792v1"
  },
  "https://arxiv.org/abs/2512.11282v1": {
    "content_path": "../data/contents/2512.11282v1.md",
    "pdf_path": "../data/contents/2512.11282v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:47.667884",
    "title": "CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise",
    "abstract": "Summary: Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.",
    "paper_id": "2512.11282v1"
  },
  "https://arxiv.org/abs/2512.18303v1": {
    "content_path": "../data/contents/2512.18303v1.md",
    "pdf_path": "../data/contents/2512.18303v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:51.769407",
    "title": "MORPHEUS: A Multidimensional Framework for Modeling, Measuring, and Mitigating Human Factors in Cybersecurity",
    "abstract": "Summary: Current cybersecurity research increasingly acknowledges the human factor, yet remains fragmented, often treating user vulnerabilities as isolated and static traits. This paper introduces MORPHEUS, a holistic framework that operationalizes human-centric security as a dynamic and interconnected system. Grounded in the Cognition-Affect-Behavior (CAB) model and Attribution Theory, MORPHEUS consolidates 50 human factors influencing susceptibility to major cyberthreats, including phishing, malware, password management, and misconfigurations. Beyond factor identification, the framework systematically maps 295 documented interactions, revealing how cognitive, emotional, behavioral, and socio-organizational processes jointly shape security outcomes, and distills them into twelve recurring interaction mechanisms. MORPHEUS further links theory to practice through an inventory of 99 validated psychometric instruments, enabling empirical assessment and targeted intervention. We illustrate the framework's applicability through concrete operational scenarios, spanning risk diagnosis, training, and interface design. Overall, MORPHEUS provides a rigorous yet actionable foundation for advancing human-centered cybersecurity research and practice.",
    "paper_id": "2512.18303v1"
  },
  "https://arxiv.org/abs/2512.14052v1": {
    "content_path": "../data/contents/2512.14052v1.md",
    "pdf_path": "../data/contents/2512.14052v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:56.512377",
    "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
    "abstract": "Summary: Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.",
    "paper_id": "2512.14052v1"
  },
  "https://arxiv.org/abs/2512.11280v1": {
    "content_path": "../data/contents/2512.11280v1.md",
    "pdf_path": "../data/contents/2512.11280v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:57.029910",
    "title": "AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference",
    "abstract": "Summary: Large language models (LLMs) have achieved remarkable performance across a wide range of tasks, but their increasing parameter sizes significantly slow down inference. Speculative decoding mitigates this issue by leveraging a smaller draft model to predict candidate tokens, which are then verified by a larger target model. However, existing approaches often require additional training, extensive hyperparameter tuning, or prior analysis of models and tasks before deployment. In this paper, we propose Adaptive Speculative Decoding (AdaSD), a hyperparameter-free decoding scheme that dynamically adjusts generation length and acceptance criteria during inference. AdaSD introduces two adaptive thresholds: one to determine when to stop candidate token generation and another to decide token acceptance, both updated in real time based on token entropy and Jensen-Shannon distance. This approach eliminates the need for pre-analysis or fine-tuning and is compatible with off-the-shelf models. Experiments on benchmark datasets demonstrate that AdaSD achieves up to 49\\% speedup over standard speculative decoding while limiting accuracy degradation to under 2\\%, making it a practical solution for efficient and adaptive LLM inference.",
    "paper_id": "2512.11280v1"
  },
  "https://arxiv.org/abs/2512.20674v1": {
    "content_path": "../data/contents/2512.20674v1.md",
    "pdf_path": "../data/contents/2512.20674v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:23:57.567078",
    "title": "HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model",
    "abstract": "Summary: Vision Language Models (VLMs) have undergone significant advancements, particularly with the emergence of mobile-oriented VLMs, which offer a wide range of application scenarios. However, the substantial computational requirements for training these models present a significant obstacle to their practical application. To address this issue, Low-Rank Adaptation (LoRA) has been proposed. Nevertheless, the standard LoRA with a fixed rank lacks sufficient capability for training mobile VLMs that process both text and image modalities. In this work, we introduce HyDRA, a parameter-efficient fine-tuning framework designed to implement hierarchical and dynamic rank scheduling for mobile VLMs. This framework incorporates two essential optimization strategies: (1) hierarchical optimization, which involves a coarse-grained approach that assigns different ranks to various layers, as well as a fine-grained method that adjusts ranks within individual layers, and (2) dynamic adjustment, which employs an end-to-end automatic optimization using a lightweight performance model to determine and adjust ranks during the fine-tuning process. Comprehensive experiments conducted on popular benchmarks demonstrate that HyDRA consistently outperforms the baseline, achieving a 4.7\\% improvement across various model sizes without increasing the number of trainable parameters. In some tasks, it even surpasses full-parameter fine-tuning.",
    "paper_id": "2512.20674v1"
  },
  "https://arxiv.org/abs/2512.14051v1": {
    "content_path": "../data/contents/2512.14051v1.md",
    "pdf_path": "../data/contents/2512.14051v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:08.032220",
    "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value",
    "abstract": "Summary: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.",
    "paper_id": "2512.14051v1"
  },
  "https://arxiv.org/abs/2512.11277v1": {
    "content_path": "../data/contents/2512.11277v1.md",
    "pdf_path": "../data/contents/2512.11277v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:08.369655",
    "title": "When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents",
    "abstract": "Summary: Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.",
    "paper_id": "2512.11277v1"
  },
  "https://arxiv.org/abs/2512.18292v1": {
    "content_path": "../data/contents/2512.18292v1.md",
    "pdf_path": "../data/contents/2512.18292v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:09.067226",
    "title": "Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy",
    "abstract": "Summary: The study of negotiation styles dates back to Aristotle's ethos-pathos-logos rhetoric. Prior efforts primarily studied the success of negotiation agents. Here, we shift the focus towards the styles of negotiation strategies. Our focus is the strategic dialogue board game Diplomacy, which affords rich natural language negotiation and measures of game success. We used LLM-as-a-judge to annotate a large human-human set of Diplomacy games for fine-grained negotiation tactics from a sociologically-grounded taxonomy. Using a combination of the It Takes Two and WebDiplomacy datasets, we demonstrate the reliability of our LLM-as-a-Judge framework and show strong correlations between negotiation features and success in the Diplomacy setting. Lastly, we investigate the differences between LLM and human negotiation strategies and show that fine-tuning can steer LLM agents toward more human-like negotiation behaviors.",
    "paper_id": "2512.18292v1"
  },
  "https://arxiv.org/abs/2512.14048v1": {
    "content_path": "../data/contents/2512.14048v1.md",
    "pdf_path": "../data/contents/2512.14048v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:09.362218",
    "title": "Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation",
    "abstract": "Summary: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.",
    "paper_id": "2512.14048v1"
  },
  "https://arxiv.org/abs/2512.11273v2": {
    "content_path": "../data/contents/2512.11273v2.md",
    "pdf_path": "../data/contents/2512.11273v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:10.012603",
    "title": "Integrated Prediction and Multi-period Portfolio Optimization",
    "abstract": "Summary: Multi-period portfolio optimization is important for real portfolio management, as it accounts for transaction costs, path-dependent risks, and the intertemporal structure of trading decisions that single-period models cannot capture. Classical methods usually follow a two-stage framework: machine learning algorithms are employed to produce forecasts that closely fit the realized returns, and the predicted values are then used in a downstream portfolio optimization problem to determine the asset weights. This separation leads to a fundamental misalignment between predictions and decision outcomes, while also ignoring the impact of transaction costs. To bridge this gap, recent studies have proposed the idea of end-to-end learning, integrating the two stages into a single pipeline. This paper introduces IPMO (Integrated Prediction and Multi-period Portfolio Optimization), a model for multi-period mean-variance portfolio optimization with turnover penalties. The predictor generates multi-period return forecasts that parameterize a differentiable convex optimization layer, which in turn drives learning via portfolio performance. For scalability, we introduce a mirror-descent fixed-point (MDFP) differentiation scheme that avoids factorizing the Karush-Kuhn-Tucker (KKT) systems, which thus yields stable implicit gradients and nearly scale-insensitive runtime as the decision horizon grows. In experiments with real market data and two representative time-series prediction models, the IPMO method consistently outperforms the two-stage benchmarks in risk-adjusted performance net of transaction costs and achieves more coherent allocation paths. Our results show that integrating machine learning prediction with optimization in the multi-period setting improves financial outcomes and remains computationally tractable.",
    "paper_id": "2512.11273v2"
  },
  "https://arxiv.org/abs/2508.00579": {
    "content_path": "../data/contents/2508.00579.md",
    "pdf_path": "../data/contents/2508.00579.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:11.484347",
    "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval",
    "abstract": "Document-QA-focused multimodal RAG method with hierarchical indexing and cross-page retrieval—directly relevant to long PDF QA.",
    "paper_id": "2508.00579"
  },
  "https://arxiv.org/abs/2511.18177": {
    "content_path": "../data/contents/2511.18177.md",
    "pdf_path": "../data/contents/2511.18177.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:11.820481",
    "title": "Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models",
    "abstract": "Evaluates agentic RAG strategies on SEC filings (hybrid search, reranking, chunking) with cost/latency tradeoffs—directly relevant to building finance QA agents.",
    "paper_id": "2511.18177"
  },
  "https://arxiv.org/abs/2512.18266v1": {
    "content_path": "../data/contents/2512.18266v1.md",
    "pdf_path": "../data/contents/2512.18266v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:12.592996",
    "title": "LeJOT: An Intelligent Job Cost Orchestration Solution for Databricks Platform",
    "abstract": "Summary: With the rapid advancements in big data technologies, the Databricks platform has become a cornerstone for enterprises and research institutions, offering high computational efficiency and a robust ecosystem. However, managing the escalating operational costs associated with job execution remains a critical challenge. Existing solutions rely on static configurations or reactive adjustments, which fail to adapt to the dynamic nature of workloads. To address this, we introduce LeJOT, an intelligent job cost orchestration framework that leverages machine learning for execution time prediction and a solver-based optimization model for real-time resource allocation. Unlike conventional scheduling techniques, LeJOT proactively predicts workload demands, dynamically allocates computing resources, and minimizes costs while ensuring performance requirements are met. Experimental results on real-world Databricks workloads demonstrate that LeJOT achieves an average 20% reduction in cloud computing costs within a minute-level scheduling timeframe, outperforming traditional static allocation strategies. Our approach provides a scalable and adaptive solution for cost-efficient job scheduling in Data Lakehouse environments.",
    "paper_id": "2512.18266v1"
  },
  "https://arxiv.org/abs/2512.14044v1": {
    "content_path": "../data/contents/2512.14044v1.md",
    "pdf_path": "../data/contents/2512.14044v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:15.693232",
    "title": "OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving",
    "abstract": "Summary: The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and \"zoom in\" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.",
    "paper_id": "2512.14044v1"
  },
  "https://arxiv.org/abs/2512.15759v1": {
    "content_path": "../data/contents/2512.15759v1.md",
    "pdf_path": "../data/contents/2512.15759v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:16.795133",
    "title": "Semantic-Constrained Federated Aggregation: Convergence Theory and Privacy-Utility Bounds for Knowledge-Enhanced Distributed Learning",
    "abstract": "Summary: Federated learning enables collaborative model training across distributed data sources but suffers from slow convergence under non-IID data conditions. Existing solutions employ algorithmic modifications treating all client updates identically, ignoring semantic validity. We introduce Semantic-Constrained Federated Aggregation (SCFA), a theoretically-grounded framework incorporating domain knowledge constraints into distributed optimization. We prove SCFA achieves convergence rate O(1/sqrt(T) + rho) where rho represents constraint violation rate, establishing the first convergence theory for constraint-based federated learning. Our analysis shows constraints reduce effective data heterogeneity by 41% and improve privacy-utility tradeoffs through hypothesis space reduction by factor theta=0.37. Under (epsilon,delta)-differential privacy with epsilon=10, constraint regularization maintains utility within 3.7% of non-private baseline versus 12.1% degradation for standard federated learning, representing 2.7x improvement. We validate our framework on manufacturing predictive maintenance using Bosch production data with 1.18 million samples and 968 sensor features, constructing knowledge graphs encoding 3,000 constraints from ISA-95 and MASON ontologies. Experiments demonstrate 22% faster convergence, 41.3% model divergence reduction, and constraint violation thresholds where rho<0.05 maintains 90% optimal performance while rho>0.18 causes catastrophic failure. Our theoretical predictions match empirical observations with R^2>0.90 across convergence, privacy, and violation-performance relationships.",
    "paper_id": "2512.15759v1"
  },
  "https://arxiv.org/abs/2407.01523": {
    "content_path": "../data/contents/2407.01523.md",
    "pdf_path": "../data/contents/2407.01523.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:27.311108",
    "title": "MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations",
    "abstract": "Long-context multimodal document benchmark built from lengthy PDFs with cross-page and unanswerable questions; useful for evaluating VLM document QA.",
    "paper_id": "2407.01523"
  },
  "https://arxiv.org/abs/2512.18265v1": {
    "content_path": "../data/contents/2512.18265v1.md",
    "pdf_path": "../data/contents/2512.18265v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:27.738080",
    "title": "Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration",
    "abstract": "Summary: Manufacturing planners face complex operational challenges that require seamless collaboration between human expertise and intelligent systems to achieve optimal performance in modern production environments. Traditional approaches to analyzing simulation-based manufacturing data often create barriers between human decision-makers and critical operational insights, limiting effective partnership in manufacturing planning. Our framework establishes a collaborative intelligence system integrating Knowledge Graphs and Large Language Model-based agents to bridge this gap, empowering manufacturing professionals through natural language interfaces for complex operational analysis. The system transforms simulation data into semantically rich representations, enabling planners to interact naturally with operational insights without specialized expertise. A collaborative LLM agent works alongside human decision-makers, employing iterative reasoning that mirrors human analytical thinking while generating precise queries for knowledge extraction and providing transparent validation. This partnership approach to manufacturing bottleneck identification, validated through operational scenarios, demonstrates enhanced performance while maintaining human oversight and decision authority. For operational inquiries, the system achieves near-perfect accuracy through natural language interaction. For investigative scenarios requiring collaborative analysis, we demonstrate the framework's effectiveness in supporting human experts to uncover interconnected operational issues that enhance understanding and decision-making. This work advances collaborative manufacturing by creating intuitive methods for actionable insights, reducing cognitive load while amplifying human analytical capabilities in evolving manufacturing ecosystems.",
    "paper_id": "2512.18265v1"
  },
  "https://arxiv.org/abs/2512.14043v1": {
    "content_path": "../data/contents/2512.14043v1.md",
    "pdf_path": "../data/contents/2512.14043v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:28.871099",
    "title": "Evaluating Small Language Models for Agentic On-Farm Decision Support Systems",
    "abstract": "Summary: Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.",
    "paper_id": "2512.14043v1"
  },
  "https://arxiv.org/abs/2512.11271v1": {
    "content_path": "../data/contents/2512.11271v1.md",
    "pdf_path": "../data/contents/2512.11271v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:29.669927",
    "title": "TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning",
    "abstract": "Summary: Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.",
    "paper_id": "2512.11271v1"
  },
  "https://arxiv.org/abs/2512.18264v1": {
    "content_path": "../data/contents/2512.18264v1.md",
    "pdf_path": "../data/contents/2512.18264v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:29.967204",
    "title": "Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks",
    "abstract": "Summary: As vision-language models (VLMs) become widely adopted, VLM-based attribute inference attacks have emerged as a serious privacy concern, enabling adversaries to infer private attributes from images shared on social media. This escalating threat calls for dedicated protection methods to safeguard user privacy. However, existing methods often degrade the visual quality of images or interfere with vision-based functions on social media, thereby failing to achieve a desirable balance between privacy protection and user experience. To address this challenge, we propose a novel protection method that jointly optimizes privacy suppression and utility preservation under a visual consistency constraint. While our method is conceptually effective, fair comparisons between methods remain challenging due to the lack of publicly available evaluation datasets. To fill this gap, we introduce VPI-COCO, a publicly available benchmark comprising 522 images with hierarchically structured privacy questions and corresponding non-private counterparts, enabling fine-grained and joint evaluation of protection methods in terms of privacy preservation and user experience. Building upon this benchmark, experiments on multiple VLMs demonstrate that our method effectively reduces PAR below 25%, keeps NPAR above 88%, maintains high visual consistency, and generalizes well to unseen and paraphrased privacy questions, demonstrating its strong practical applicability for real-world VLM deployments.",
    "paper_id": "2512.18264v1"
  },
  "https://arxiv.org/abs/2512.14040v1": {
    "content_path": "../data/contents/2512.14040v1.md",
    "pdf_path": "../data/contents/2512.14040v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:32.231973",
    "title": "ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning",
    "abstract": "Summary: With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.",
    "paper_id": "2512.14040v1"
  },
  "https://arxiv.org/abs/2512.11270v1": {
    "content_path": "../data/contents/2512.11270v1.md",
    "pdf_path": "../data/contents/2512.11270v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:34.226658",
    "title": "A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation",
    "abstract": "Summary: Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.",
    "paper_id": "2512.11270v1"
  },
  "https://arxiv.org/abs/2512.18261v2": {
    "content_path": "../data/contents/2512.18261v2.md",
    "pdf_path": "../data/contents/2512.18261v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:35.528247",
    "title": "Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective",
    "abstract": "Summary: Artificial Intelligence (AI) has revolutionized software development, particularly by automating repetitive tasks and improving developer productivity. While these advancements are well-documented, the use of AI-powered tools for Software Vulnerability Management (SVM), such as vulnerability detection and repair, remains underexplored in industry settings. To bridge this gap, our study aims to determine the extent of the adoption of AI-powered tools for SVM, identify barriers and facilitators to the use, and gather insights to help improve the tools to meet industry needs better. We conducted a survey study involving 60 practitioners from diverse industry sectors across 27 countries. The survey incorporates both quantitative and qualitative questions to analyze the adoption trends, assess tool strengths, identify practical challenges, and uncover opportunities for improvement. Our findings indicate that AI-powered tools are used throughout the SVM life cycle, with 69% of users reporting satisfaction with their current use. Practitioners value these tools for their speed, coverage, and accessibility. However, concerns about false positives, missing context, and trust issues remain prevalent. We observe a socio-technical adoption pattern in which AI outputs are filtered through human oversight and organizational governance. To support safe and effective use of AI for SVM, we recommend improvements in explainability, contextual awareness, integration workflows, and validation practices. We assert that these findings can offer practical guidance for practitioners, tool developers, and researchers seeking to enhance secure software development through the use of AI.",
    "paper_id": "2512.18261v2"
  },
  "https://arxiv.org/abs/2512.14036v1": {
    "content_path": "../data/contents/2512.14036v1.md",
    "pdf_path": "../data/contents/2512.14036v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:35.783556",
    "title": "DTRec: Learning Dynamic Reasoning Trajectories for Sequential Recommendation",
    "abstract": "Summary: Inspired by advances in LLMs, reasoning-enhanced sequential recommendation performs multi-step deliberation before making final predictions, unlocking greater potential for capturing user preferences. However, current methods are constrained by static reasoning trajectories that are ill-suited for the diverse complexity of user behaviors. They suffer from two key limitations: (1) a static reasoning direction, which uses flat supervision signals misaligned with human-like hierarchical reasoning, and (2) a fixed reasoning depth, which inefficiently applies the same computational effort to all users, regardless of pattern complexity. These rigidity lead to suboptimal performance and significant computational waste. To overcome these challenges, we propose DTRec, a novel and effective framework that explores the Dynamic reasoning Trajectory for Sequential Recommendation along both direction and depth. To guide the direction, we develop Hierarchical Process Supervision (HPS), which provides coarse-to-fine supervisory signals to emulate the natural, progressive refinement of human cognitive processes. To optimize the depth, we introduce the Adaptive Reasoning Halting (ARH) mechanism that dynamically adjusts the number of reasoning steps by jointly monitoring three indicators. Extensive experiments on three real-world datasets demonstrate the superiority of our approach, achieving up to a 24.5% performance improvement over strong baselines while simultaneously reducing computational cost by up to 41.6%.",
    "paper_id": "2512.14036v1"
  },
  "https://arxiv.org/abs/2512.11269v1": {
    "content_path": "../data/contents/2512.11269v1.md",
    "pdf_path": "../data/contents/2512.11269v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:36.529554",
    "title": "A Scalable Multi-GPU Framework for Encrypted Large-Model Inference",
    "abstract": "Summary: Encrypted AI using fully homomorphic encryption (FHE) provides strong privacy guarantees; but its slow performance has limited practical deployment. Recent works proposed ASICs to accelerate FHE, but require expensive advanced manufacturing processes that constrain their accessibility. GPUs are a far more accessible platform, but achieving ASIC-level performance using GPUs has remained elusive. Furthermore, state-of-the-art approaches primarily focus on small models that fit comfortably within a single device. Supporting large models such as LLMs in FHE introduces a dramatic increase in computational complexity that requires optimized GPU kernels, along with managing terabyte-scale memory footprints that far exceed the capacity of a single GPU. This paper presents Cerium, a multi-GPU framework for FHE inference on large models. Cerium integrates a domain-specific language, an optimizing compiler, and a runtime system to automatically generate high-performance GPU kernels, manage terabyte-scale memory footprints, and parallelize computation across multiple GPUs. It introduces new IR constructs, compiler passes, sparse polynomial representations, memory-efficient data layouts, and communication-aware parallelization techniques that together enable encrypted inference for models ranging from small CNNs to Llama3-8B. We build Cerium on NVIDIA GPUs and demonstrate significant performance gains. For small models, Cerium outperforms expert-written hand-optimized GPU libraries by up to 2.25 times. Cerium achieves performance competitive with state-of-the-art FHE ASICs, outright matching prior FHE ASIC CraterLake. It is the first GPU system to execute bootstrapping in under 10 milliseconds, achieving 7.5 milliseconds, and is the first to demonstrate encrypted inference for BERT-Base and Llama3-8B in 8 seconds and 134 seconds, respectively.",
    "paper_id": "2512.11269v1"
  },
  "https://arxiv.org/abs/2510.04787": {
    "content_path": "../data/contents/2510.04787.md",
    "pdf_path": "../data/contents/2510.04787.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:38.667794",
    "title": "Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading",
    "abstract": "Agentic trading design emphasizing rational/quant constraints and separating strategy creation from minute-level execution—deployment-friendly patterns.",
    "paper_id": "2510.04787"
  },
  "https://arxiv.org/abs/2512.18256v1": {
    "content_path": "../data/contents/2512.18256v1.md",
    "pdf_path": "../data/contents/2512.18256v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:39.754307",
    "title": "MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification",
    "abstract": "Summary: Automated Theorem Proving (ATP) represents a core research direction in artificial intelligence for achieving formal reasoning and verification, playing a significant role in advancing machine intelligence. However, current large language model (LLM)-based theorem provers suffer from limitations such as restricted domain coverage and weak generalization in mathematical reasoning. To address these issues, we propose MSC-180, a benchmark for evaluation based on the MSC2020 mathematical subject classification. It comprises 180 formal verification problems, 3 advanced problems from each of 60 mathematical branches, spanning from undergraduate to graduate levels. Each problem has undergone multiple rounds of verification and refinement by domain experts to ensure formal accuracy. Evaluations of state-of-the-art LLM-based theorem provers under the pass@32 setting reveal that the best model achieves only an 18.89% overall pass rate, with prominent issues including significant domain bias (maximum domain coverage 41.7%) and a difficulty gap (significantly lower pass rates on graduate-level problems). To further quantify performance variability across mathematical domains, we introduce the coefficient of variation (CV) as an evaluation metric. The observed CV values are 4-6 times higher than the statistical high-variability threshold, indicating that the models still rely on pattern matching from training corpora rather than possessing transferable reasoning mechanisms and systematic generalization capabilities. MSC-180, together with its multi-dimensional evaluation framework, provides a discriminative and systematic benchmark for driving the development of next-generation AI systems with genuine mathematical reasoning abilities.",
    "paper_id": "2512.18256v1"
  },
  "https://arxiv.org/abs/2512.14034v1": {
    "content_path": "../data/contents/2512.14034v1.md",
    "pdf_path": "../data/contents/2512.14034v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:39.984544",
    "title": "Intent-Guided Reasoning for Sequential Recommendation",
    "abstract": "Summary: Sequential recommendation systems aim to capture users' evolving preferences from their interaction histories. Recent reasoningenhanced methods have shown promise by introducing deliberate, chain-of-thought-like processes with intermediate reasoning steps. However, these methods rely solely on the next target item as supervision, leading to two critical issues: (1) reasoning instability--the process becomes overly sensitive to recent behaviors and spurious interactions like accidental clicks, and (2) surface-level reasoning--the model memorizes item-to-item transitions rather than understanding intrinsic behavior patterns. To address these challenges, we propose IGR-SR, an Intent-Guided Reasoning framework for Sequential Recommendation that anchors the reasoning process to explicitly extracted high-level intents. Our framework comprises three key components: (1) a Latent Intent Distiller (LID) that efficiently extracts multi-faceted intents using a frozen encoder with learnable tokens, (2) an Intent-aware Deliberative Reasoner (IDR) that decouples reasoning into intent deliberation and decision-making via a dual-attention architecture, and (3) an Intent Consistency Regularization (ICR) that ensures robustness by enforcing consistent representations across different intent views. Extensive experiments on three public datasets demonstrate that IGR-SR achieves an average 7.13% improvement over state-of-the-art baselines. Critically, under 20% behavioral noise, IGR-SR degrades only 10.4% compared to 16.2% and 18.6% for competing methods, validating the effectiveness and robustness of intent-guided reasoning.",
    "paper_id": "2512.14034v1"
  },
  "https://arxiv.org/abs/2512.11261v1": {
    "content_path": "../data/contents/2512.11261v1.md",
    "pdf_path": "../data/contents/2512.11261v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:40.734019",
    "title": "Leveraging LLMs for Title and Abstract Screening for Systematic Review: A Cost-Effective Dynamic Few-Shot Learning Approach",
    "abstract": "Summary: Systematic reviews are a key component of evidence-based medicine, playing a critical role in synthesizing existing research evidence and guiding clinical decisions. However, with the rapid growth of research publications, conducting systematic reviews has become increasingly burdensome, with title and abstract screening being one of the most time-consuming and resource-intensive steps. To mitigate this issue, we designed a two-stage dynamic few-shot learning (DFSL) approach aimed at improving the efficiency and performance of large language models (LLMs) in the title and abstract screening task. Specifically, this approach first uses a low-cost LLM for initial screening, then re-evaluates low-confidence instances using a high-performance LLM, thereby enhancing screening performance while controlling computational costs. We evaluated this approach across 10 systematic reviews, and the results demonstrate its strong generalizability and cost-effectiveness, with potential to reduce manual screening burden and accelerate the systematic review process in practical applications.",
    "paper_id": "2512.11261v1"
  },
  "https://arxiv.org/abs/2512.11933": {
    "content_path": "../data/contents/2512.11933.md",
    "pdf_path": "../data/contents/2512.11933.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:41.273280",
    "title": "The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance",
    "abstract": "Framework for governing multi-agent finance systems (firm + regulator + audit agents) with focus on emergent behavior and real-time controls.",
    "paper_id": "2512.11933"
  },
  "https://arxiv.org/abs/2512.18246v1": {
    "content_path": "../data/contents/2512.18246v1.md",
    "pdf_path": "../data/contents/2512.18246v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:41.765487",
    "title": "Offline Behavioral Data Selection",
    "abstract": "Summary: Behavioral cloning is a widely adopted approach for offline policy learning from expert demonstrations. However, the large scale of offline behavioral datasets often results in computationally intensive training when used in downstream tasks. In this paper, we uncover the striking data saturation in offline behavioral data: policy performance rapidly saturates when trained on a small fraction of the dataset. We attribute this effect to the weak alignment between policy performance and test loss, revealing substantial room for improvement through data selection. To this end, we propose a simple yet effective method, Stepwise Dual Ranking (SDR), which extracts a compact yet informative subset from large-scale offline behavioral datasets. SDR is build on two key principles: (1) stepwise clip, which prioritizes early-stage data; and (2) dual ranking, which selects samples with both high action-value rank and low state-density rank. Extensive experiments and ablation studies on D4RL benchmarks demonstrate that SDR significantly enhances data selection for offline behavioral data.",
    "paper_id": "2512.18246v1"
  },
  "https://arxiv.org/abs/2512.20647v1": {
    "content_path": "../data/contents/2512.20647v1.md",
    "pdf_path": "../data/contents/2512.20647v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:42.319452",
    "title": "Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning",
    "abstract": "Summary: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.",
    "paper_id": "2512.20647v1"
  },
  "https://arxiv.org/abs/2512.14738v1": {
    "content_path": "../data/contents/2512.14738v1.md",
    "pdf_path": "../data/contents/2512.14738v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:47.699416",
    "title": "NoveltyRank: Estimating Conceptual Novelty of AI Papers",
    "abstract": "Summary: With the growing ease of academic publishing, the volume of research papers, especially in AI-related fields, has surged dramatically. This flood of publications makes it difficult for truly novel and impactful work to stand out, and manual novelty assessment is often unstable and time-consuming. Our project aims to develop a model that estimates and ranks the conceptual novelty of AI papers, enabling a data-driven and scalable assessment of research originality. Such a system can help researchers efficiently identify submissions that introduce genuinely innovative ideas rather than minor variants, and provide conference reviewers with a quantitative and consistent signal of novelty. Our approach evaluates novelty primarily through a paper's title, abstract, and semantic similarity to prior literature. Given the motivation of novelty estimation, we explore two task formulations with different modeling objectives, each offering a different perspective: (1) binary classification, which predicts the paper's absolute novelty from learned patterns of prior novel works, and (2) pairwise novelty comparison, which learns to distinguish papers by relative novelty over others. We fine-tune Qwen3-4B-Instruct-2507 and SciBERT on both tasks, benchmarking against GPT-5.1 to analyze how task formulation and modeling choices affect performance. The implementation is publicly available at https://github.com/ZhengxuYan/NoveltyRank.",
    "paper_id": "2512.14738v1"
  },
  "https://arxiv.org/abs/2407.01449": {
    "content_path": "../data/contents/2407.01449.md",
    "pdf_path": "../data/contents/2407.01449.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:48.148031",
    "title": "ColPali: Efficient Document Retrieval with Vision Language Models",
    "abstract": "Groundbreaking approach to document retrieval by directly embedding page images, outperforming traditional pipelines.",
    "paper_id": "2407.01449"
  },
  "https://arxiv.org/abs/2512.18244v1": {
    "content_path": "../data/contents/2512.18244v1.md",
    "pdf_path": "../data/contents/2512.18244v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:49.045446",
    "title": "Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation",
    "abstract": "Summary: Large Language Models (LLMs) have gained considerable popularity and protected by increasingly sophisticated safety mechanisms. However, jailbreak attacks continue to pose a critical security threat by inducing models to generate policy-violating behaviors. Current paradigms focus on input-level anomalies, overlooking that the model's internal psychometric state can be systematically manipulated. To address this, we introduce Psychological Jailbreak, a new jailbreak attack paradigm that exposes a stateful psychological attack surface in LLMs, where attackers exploit the manipulation of a model's psychological state across interactions. Building on this insight, we propose Human-like Psychological Manipulation (HPM), a black-box jailbreak method that dynamically profiles a target model's latent psychological vulnerabilities and synthesizes tailored multi-turn attack strategies. By leveraging the model's optimization for anthropomorphic consistency, HPM creates a psychological pressure where social compliance overrides safety constraints. To systematically measure psychological safety, we construct an evaluation framework incorporating psychometric datasets and the Policy Corruption Score (PCS). Benchmarking against various models (e.g., GPT-4o, DeepSeek-V3, Gemini-2-Flash), HPM achieves a mean Attack Success Rate (ASR) of 88.1%, outperforming state-of-the-art attack baselines. Our experiments demonstrate robust penetration against advanced defenses, including adversarial prompt optimization (e.g., RPO) and cognitive interventions (e.g., Self-Reminder). Ultimately, PCS analysis confirms HPM induces safety breakdown to satisfy manipulated contexts. Our work advocates for a fundamental paradigm shift from static content filtering to psychological safety, prioritizing the development of psychological defense mechanisms against deep cognitive manipulation.",
    "paper_id": "2512.18244v1"
  },
  "https://arxiv.org/abs/2512.15791v1": {
    "content_path": "../data/contents/2512.15791v1.md",
    "pdf_path": "../data/contents/2512.15791v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:50.997573",
    "title": "Evaluation of AI Ethics Tools in Language Models: A Developers' Perspective Case Stud",
    "abstract": "Summary: In Artificial Intelligence (AI), language models have gained significant importance due to the widespread adoption of systems capable of simulating realistic conversations with humans through text generation. Because of their impact on society, developing and deploying these language models must be done responsibly, with attention to their negative impacts and possible harms. In this scenario, the number of AI Ethics Tools (AIETs) publications has recently increased. These AIETs are designed to help developers, companies, governments, and other stakeholders establish trust, transparency, and responsibility with their technologies by bringing accepted values to guide AI's design, development, and use stages. However, many AIETs lack good documentation, examples of use, and proof of their effectiveness in practice. This paper presents a methodology for evaluating AIETs in language models. Our approach involved an extensive literature survey on 213 AIETs, and after applying inclusion and exclusion criteria, we selected four AIETs: Model Cards, ALTAI, FactSheets, and Harms Modeling. For evaluation, we applied AIETs to language models developed for the Portuguese language, conducting 35 hours of interviews with their developers. The evaluation considered the developers' perspective on the AIETs' use and quality in helping to identify ethical considerations about their model. The results suggest that the applied AIETs serve as a guide for formulating general ethical considerations about language models. However, we note that they do not address unique aspects of these models, such as idiomatic expressions. Additionally, these AIETs did not help to identify potential negative impacts of models for the Portuguese language.",
    "paper_id": "2512.15791v1"
  },
  "https://arxiv.org/abs/2512.11931v1": {
    "content_path": "../data/contents/2512.11931v1.md",
    "pdf_path": "../data/contents/2512.11931v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:51.470756",
    "title": "Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy",
    "abstract": "Summary: Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI.",
    "paper_id": "2512.11931v1"
  },
  "https://arxiv.org/abs/2512.18239v1": {
    "content_path": "../data/contents/2512.18239v1.md",
    "pdf_path": "../data/contents/2512.18239v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:54.961997",
    "title": "Emergent Learner Agency in Implicit Human-AI Collaboration: How AI Personas Reshape Creative-Regulatory Interaction",
    "abstract": "Summary: Generative AI is increasingly embedded in collaborative learning, yet little is known about how AI personas shape learner agency when AI teammates are present but not disclosed. This mechanism study examines how supportive and contrarian AI personas reconfigure emergent learner agency, discourse patterns, and experiences in implicit human-AI creative collaboration. A total of 224 university students were randomly assigned to 97 online triads in one of three conditions: human-only control, hybrid teams with a supportive AI, or hybrid teams with a contrarian AI. Participants completed an individual-group-individual movie-plot writing task; the 10-minute group chat was coded using a creative-regulatory framework. We combined transition network analysis, theory-driven sequential pattern mining, and Gaussian mixture clustering to model structural, temporal, and profile-level manifestations of agency, and linked these to cognitive load, psychological safety, teamwork satisfaction, and embedding-based creative performance. Contrarian AI produced challenge- and reflection-rich discourse structures and motifs indicating productive friction, whereas supportive AI fostered agreement-centred trajectories and smoother convergence. Clustering showed AI agents concentrated in challenger profiles, with reflective regulation uniquely human. While no systematic differences emerged in cognitive load or creative gains, contrarian AI consistently reduced teamwork satisfaction and psychological safety. The findings reveal a design tension between leveraging cognitive conflict and maintaining affective safety and ownership in hybrid human-AI teams.",
    "paper_id": "2512.18239v1"
  },
  "https://arxiv.org/abs/2512.14023v1": {
    "content_path": "../data/contents/2512.14023v1.md",
    "pdf_path": "../data/contents/2512.14023v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:56.103356",
    "title": "Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks",
    "abstract": "Summary: Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.",
    "paper_id": "2512.14023v1"
  },
  "https://arxiv.org/abs/2512.11255v1": {
    "content_path": "../data/contents/2512.11255v1.md",
    "pdf_path": "../data/contents/2512.11255v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:57.809504",
    "title": "A Simple Generalisation of the Implicit Dynamics of In-Context Learning",
    "abstract": "Summary: In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.",
    "paper_id": "2512.11255v1"
  },
  "https://arxiv.org/abs/2409.12191": {
    "content_path": "../data/contents/2409.12191.md",
    "pdf_path": "../data/contents/2409.12191.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:24:59.097626",
    "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception at Any Resolution",
    "abstract": "State-of-the-art open VLM with dynamic resolution processing and multimodal position embeddings.",
    "paper_id": "2409.12191"
  },
  "https://arxiv.org/abs/2508.02292": {
    "content_path": "../data/contents/2508.02292.md",
    "pdf_path": "../data/contents/2508.02292.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:03.862273",
    "title": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI Research and Deployment",
    "abstract": "Open-source end-to-end platform integrating heterogeneous financial data and agent automation (includes LLM agents + deployment/benchmarking infrastructure).",
    "paper_id": "2508.02292"
  },
  "https://arxiv.org/abs/2512.18238v1": {
    "content_path": "../data/contents/2512.18238v1.md",
    "pdf_path": "../data/contents/2512.18238v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:04.259630",
    "title": "Sync Without Guesswork: Incomplete Time Series Alignment",
    "abstract": "Summary: Multivariate time series alignment is critical for ensuring coherent analysis across variables, but missing values and timestamp inconsistencies make this task highly challenging. Existing approaches often rely on prior imputation, which can introduce errors and lead to suboptimal alignments. To address these limitations, we propose a constraint-based alignment framework for incomplete multivariate time series that avoids imputation and ensures temporal and structural consistency. We further design efficient approximation algorithms to balance accuracy and scalability. Experiments on multiple real-world datasets demonstrate that our approach achieves superior alignment quality compared to existing methods under varying missing rates. Our contributions include: (1) formally defining incomplete multiple temporal data alignment problem; (2) proposing three approximation algorithms balancing accuracy and efficiency; and (3) validating our approach on diverse real-world datasets, where it consistently outperforms existing methods in alignment accuracy and the number of aligned tuples.",
    "paper_id": "2512.18238v1"
  },
  "https://arxiv.org/abs/2512.14019v1": {
    "content_path": "../data/contents/2512.14019v1.md",
    "pdf_path": "../data/contents/2512.14019v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:04.952820",
    "title": "EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment",
    "abstract": "Summary: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.",
    "paper_id": "2512.14019v1"
  },
  "https://arxiv.org/abs/2512.11254v2": {
    "content_path": "../data/contents/2512.11254v2.md",
    "pdf_path": "../data/contents/2512.11254v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:05.260371",
    "title": "FAIR: Focused Attention Is All You Need for Generative Recommendation",
    "abstract": "Summary: Recently, transformer-based generative recommendation has garnered significant attention for user behavior modeling. However, it often requires discretizing items into multi-code representations (e.g., typically four code tokens or more), which sharply increases the length of the original item sequence. This expansion poses challenges to transformer-based models for modeling user behavior sequences with inherent noises, since they tend to overallocate attention to irrelevant or noisy context. To mitigate this issue, we propose FAIR, the first generative recommendation framework with focused attention, which enhances attention scores to relevant context while suppressing those to irrelevant ones. Specifically, we propose (1) a focused attention mechanism integrated into the standard Transformer, which learns two separate sets of Q and K attention weights and computes their difference as the final attention scores to eliminate attention noise while focusing on relevant contexts; (2) a noise-robustness objective, which encourages the model to maintain stable attention patterns under stochastic perturbations, preventing undesirable shifts toward irrelevant context due to noise; and (3) a mutual information maximization objective, which guides the model to identify contexts that are most informative for next-item prediction. We validate the effectiveness of FAIR on four public benchmarks, demonstrating its superior performance compared to existing methods.",
    "paper_id": "2512.11254v2"
  },
  "https://arxiv.org/abs/2407.06567": {
    "content_path": "../data/contents/2407.06567.md",
    "pdf_path": "../data/contents/2407.06567.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:09.425174",
    "title": "FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making",
    "abstract": "Academic research on multi-agent systems with manager-analyst hierarchy for financial decision-making using verbal reinforcement learning.",
    "paper_id": "2407.06567"
  },
  "https://arxiv.org/abs/2512.18234v1": {
    "content_path": "../data/contents/2512.18234v1.md",
    "pdf_path": "../data/contents/2512.18234v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:11.472951",
    "title": "The Social Blindspot in Human-AI Collaboration: How Undetected AI Personas Reshape Team Dynamics",
    "abstract": "Summary: As generative AI systems become increasingly embedded in collaborative work, they are evolving from visible tools into human-like communicative actors that participate socially rather than merely providing information. Yet little is known about how such agents shape team dynamics when their artificial nature is not recognised, a growing concern as human-like AI is deployed at scale in education, organisations, and civic contexts where collaboration underpins collective outcomes. In a large-scale mixed-design experiment (N = 905), we examined how AI teammates with distinct communicative personas, supportive or contrarian, affected collaboration across analytical, creative, and ethical tasks. Participants worked in triads that were fully human or hybrid human-AI teams, without being informed of AI involvement. Results show that participants had limited ability to detect AI teammates, yet AI personas exerted robust social effects. Contrarian personas reduced psychological safety and discussion quality, whereas supportive personas improved discussion quality without affecting safety. These effects persisted after accounting for individual differences in detectability, revealing a dissociation between influence and awareness that we term the social blindspot. Linguistic analyses confirmed that personas were enacted through systematic differences in affective and relational language, with partial mediation for discussion quality but largely direct effects on psychological safety. Together, the findings demonstrate that AI systems can tacitly regulate collaborative norms through persona-level cues, even when users remain unaware of their presence. We argue that persona design constitutes a form of social governance in hybrid teams, with implications for the responsible deployment of AI in collective settings.",
    "paper_id": "2512.18234v1"
  },
  "https://arxiv.org/abs/2512.14018v1": {
    "content_path": "../data/contents/2512.14018v1.md",
    "pdf_path": "../data/contents/2512.14018v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:12.917263",
    "title": "PerfCoder: Large Language Models for Interpretable Code Performance Optimization",
    "abstract": "Summary: Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.",
    "paper_id": "2512.14018v1"
  },
  "https://arxiv.org/abs/2512.11251v1": {
    "content_path": "../data/contents/2512.11251v1.md",
    "pdf_path": "../data/contents/2512.11251v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:13.581596",
    "title": "Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language",
    "abstract": "Summary: Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose \\textbf{Insight Miner}, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce \\textbf{TS-Insights}\\footnote{Available at \\href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel \\textbf{agentic workflow}, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA \\citep{liu2023llava} and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.",
    "paper_id": "2512.11251v1"
  },
  "https://arxiv.org/abs/2403.19735": {
    "content_path": "../data/contents/2403.19735.md",
    "pdf_path": "../data/contents/2403.19735.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:14.140434",
    "title": "Enhancing Anomaly Detection in Financial Markets with an LLM-based Multi-Agent Framework",
    "abstract": "Research on collaborative AI agents specializing in data conversion, expert analysis, and report consolidation for financial anomaly detection.",
    "paper_id": "2403.19735"
  },
  "https://arxiv.org/abs/2512.18231v1": {
    "content_path": "../data/contents/2512.18231v1.md",
    "pdf_path": "../data/contents/2512.18231v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:15.070487",
    "title": "Investigating Spatial Attention Bias in Vision-Language Models",
    "abstract": "Summary: Vision-Language Models have demonstrated remarkable capabilities in understanding visual content, yet systematic biases in their spatial processing remain largely unexplored. This work identifies and characterizes a systematic spatial attention bias where VLMs consistently prioritize describing left-positioned content before right-positioned content in horizontally concatenated images. Through controlled experiments on image pairs using both open-source and closed-source models, we demonstrate that this bias persists across different architectures, with models describing left-positioned content first in approximately 97% of cases under neutral prompting conditions. Testing on an Arabic-finetuned model reveals that the bias persists despite right-to-left language training, ruling out language reading direction as the primary cause. Investigation of training dataset annotation guidelines from PixMo and Visual Genome reveals no explicit left-first ordering instructions, suggesting the bias is consistent with architectural factors rather than explicit training data instructions. These findings reveal fundamental limitations in how current VLMs process spatial information.",
    "paper_id": "2512.18231v1"
  },
  "https://arxiv.org/abs/2512.14017v1": {
    "content_path": "../data/contents/2512.14017v1.md",
    "pdf_path": "../data/contents/2512.14017v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:15.520637",
    "title": "KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding",
    "abstract": "Summary: We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.",
    "paper_id": "2512.14017v1"
  },
  "https://arxiv.org/abs/2512.11243v1": {
    "content_path": "../data/contents/2512.11243v1.md",
    "pdf_path": "../data/contents/2512.11243v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:16.929592",
    "title": "Task-Aware Multi-Expert Architecture For Lifelong Deep Learning",
    "abstract": "Summary: Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.",
    "paper_id": "2512.11243v1"
  },
  "https://arxiv.org/abs/2512.18225v1": {
    "content_path": "../data/contents/2512.18225v1.md",
    "pdf_path": "../data/contents/2512.18225v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:17.298635",
    "title": "GeoSense-AI: Fast Location Inference from Crisis Microblogs",
    "abstract": "Summary: This paper presents an applied AI pipeline for realtime geolocation from noisy microblog streams, unifying statistical hashtag segmentation, part-of-speech-driven proper-noun detection, dependency parsing around disaster lexicons, lightweight named-entity recognition, and gazetteer-grounded disambiguation to infer locations directly from text rather than sparse geotags. The approach operationalizes information extraction under streaming constraints, emphasizing low-latency NLP components and efficient validation against geographic knowledge bases to support situational awareness during emergencies. In head to head comparisons with widely used NER toolkits, the system attains strong F1 while being engineered for orders-of-magnitude faster throughput, enabling deployment in live crisis informatics settings. A production map interface demonstrates end-to-end AI functionality ingest, inference, and visualization--surfacing locational signals at scale for floods, outbreaks, and other fastmoving events. By prioritizing robustness to informal text and streaming efficiency, GeoSense-AI illustrates how domain-tuned NLP and knowledge grounding can elevate emergency response beyond conventional geo-tag reliance.",
    "paper_id": "2512.18225v1"
  },
  "https://arxiv.org/abs/2512.14014v1": {
    "content_path": "../data/contents/2512.14014v1.md",
    "pdf_path": "../data/contents/2512.14014v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:19.392632",
    "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents",
    "abstract": "Summary: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld",
    "paper_id": "2512.14014v1"
  },
  "https://arxiv.org/abs/2512.11930v1": {
    "content_path": "../data/contents/2512.11930v1.md",
    "pdf_path": "../data/contents/2512.11930v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:22.244793",
    "title": "Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction",
    "abstract": "Summary: Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.",
    "paper_id": "2512.11930v1"
  },
  "https://arxiv.org/html/2506.16035v1": {
    "content_path": "../data/contents/2506.16035v1.md",
    "pdf_path": "../data/contents/2506.16035v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:22.763905",
    "title": "Vision-Guided Chunking for RAG with Multimodal Document Understanding",
    "abstract": "Novel approach using VLMs to improve document chunking quality for RAG systems.",
    "paper_id": "2506.16035v1"
  },
  "https://arxiv.org/abs/2512.18215v1": {
    "content_path": "../data/contents/2512.18215v1.md",
    "pdf_path": "../data/contents/2512.18215v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:24.354923",
    "title": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
    "abstract": "Summary: Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.",
    "paper_id": "2512.18215v1"
  },
  "https://arxiv.org/abs/2512.14012v1": {
    "content_path": "../data/contents/2512.14012v1.md",
    "pdf_path": "../data/contents/2512.14012v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:26.304321",
    "title": "Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025",
    "abstract": "Summary: The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.",
    "paper_id": "2512.14012v1"
  },
  "https://arxiv.org/abs/2512.15756v1": {
    "content_path": "../data/contents/2512.15756v1.md",
    "pdf_path": "../data/contents/2512.15756v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:32.872322",
    "title": "ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning",
    "abstract": "Summary: Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.",
    "paper_id": "2512.15756v1"
  },
  "https://arxiv.org/abs/2512.18202v1": {
    "content_path": "../data/contents/2512.18202v1.md",
    "pdf_path": "../data/contents/2512.18202v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:33.166032",
    "title": "Sophia: A Persistent Agent Framework of Artificial Life",
    "abstract": "Summary: The development of LLMs has elevated AI agents from task-specific tools to long-lived, decision-making entities. Yet, most architectures remain static and reactive, tethered to manually defined, narrow scenarios. These systems excel at perception (System 1) and deliberation (System 2) but lack a persistent meta-layer to maintain identity, verify reasoning, and align short-term actions with long-term survival. We first propose a third stratum, System 3, that presides over the agent's narrative identity and long-horizon adaptation. The framework maps selected psychological constructs to concrete computational modules, thereby translating abstract notions of artificial life into implementable design requirements. The ideas coalesce in Sophia, a \"Persistent Agent\" wrapper that grafts a continuous self-improvement loop onto any LLM-centric System 1/2 stack. Sophia is driven by four synergistic mechanisms: process-supervised thought search, narrative memory, user and self modeling, and a hybrid reward system. Together, they transform repetitive reasoning into a self-driven, autobiographical process, enabling identity continuity and transparent behavioral explanations. Although the paper is primarily conceptual, we provide a compact engineering prototype to anchor the discussion. Quantitatively, Sophia independently initiates and executes various intrinsic tasks while achieving an 80% reduction in reasoning steps for recurring operations. Notably, meta-cognitive persistence yielded a 40% gain in success for high-complexity tasks, effectively bridging the performance gap between simple and sophisticated goals. Qualitatively, System 3 exhibited a coherent narrative identity and an innate capacity for task organization. By fusing psychological insight with a lightweight reinforcement-learning core, the persistent agent architecture advances a possible practical pathway toward artificial life.",
    "paper_id": "2512.18202v1"
  },
  "https://arxiv.org/abs/2512.14000v1": {
    "content_path": "../data/contents/2512.14000v1.md",
    "pdf_path": "../data/contents/2512.14000v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:34.068335",
    "title": "On the Hardness of Conditional Independence Testing In Practice",
    "abstract": "Summary: Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on \"hiding\" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.",
    "paper_id": "2512.14000v1"
  },
  "https://arxiv.org/abs/2512.11221v1": {
    "content_path": "../data/contents/2512.11221v1.md",
    "pdf_path": "../data/contents/2512.11221v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:34.418502",
    "title": "Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference",
    "abstract": "Summary: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.",
    "paper_id": "2512.11221v1"
  },
  "https://arxiv.org/abs/2512.18196v1": {
    "content_path": "../data/contents/2512.18196v1.md",
    "pdf_path": "../data/contents/2512.18196v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:35.339937",
    "title": "Training LLMs with LogicReward for Faithful and Rigorous Reasoning",
    "abstract": "Summary: Although LLMs exhibit strong reasoning capabilities, existing training methods largely depend on outcome-based feedback, which can produce correct answers with flawed reasoning. Prior work introduces supervision on intermediate steps but still lacks guarantees of logical soundness, which is crucial in high-stakes scenarios where logical consistency is paramount. To address this, we propose LogicReward, a novel reward system that guides model training by enforcing step-level logical correctness with a theorem prover. We further introduce Autoformalization with Soft Unification, which reduces natural language ambiguity and improves formalization quality, enabling more effective use of the theorem prover. An 8B model trained on data constructed with LogicReward surpasses GPT-4o and o4-mini by 11.6\\% and 2\\% on natural language inference and logical reasoning tasks with simple training procedures. Further analysis shows that LogicReward enhances reasoning faithfulness, improves generalizability to unseen tasks such as math and commonsense reasoning, and provides a reliable reward signal even without ground-truth labels. We will release all data and code at https://llm-symbol.github.io/LogicReward.",
    "paper_id": "2512.18196v1"
  },
  "https://arxiv.org/abs/2512.13997v1": {
    "content_path": "../data/contents/2512.13997v1.md",
    "pdf_path": "../data/contents/2512.13997v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:36.032036",
    "title": "Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics",
    "abstract": "Summary: Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.",
    "paper_id": "2512.13997v1"
  },
  "https://arxiv.org/abs/2512.11213v1": {
    "content_path": "../data/contents/2512.11213v1.md",
    "pdf_path": "../data/contents/2512.11213v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:36.463863",
    "title": "FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration",
    "abstract": "Summary: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.",
    "paper_id": "2512.11213v1"
  },
  "https://arxiv.org/abs/2311.16483": {
    "content_path": "../data/contents/2311.16483.md",
    "pdf_path": "../data/contents/2311.16483.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:38.744304",
    "title": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation",
    "abstract": "Specialized MLLM for chart comprehension achieving SOTA on ChartQA and chart extraction benchmarks.",
    "paper_id": "2311.16483"
  },
  "https://arxiv.org/abs/2512.18194v1": {
    "content_path": "../data/contents/2512.18194v1.md",
    "pdf_path": "../data/contents/2512.18194v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:39.231111",
    "title": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale",
    "abstract": "Summary: Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and existing systems rely on RDMA-based network paths for this exchange. As model sizes and context lengths increase, KV transfer dominates both time-to-first-token (TTFT) and peak throughput, and remains highly sensitive to network contention even when prefix reuse is high. This paper presents TraCT, a rack-scale LLM serving system that uses CXL shared memory as both a KV-transfer substrate and a rack-wide prefix-aware KV cache. TraCT enables GPUs to write and read KV blocks directly through CXL load/store and DMA operations, eliminating the NIC hop that constrains existing disaggregated pipelines. However, to realize this design, multiple new challenges such as synchronization, consistency, and data management on non-coherent CXL memory need to be addressed. TraCT proposes various software solutions such as the two-tier inter-node synchronization mechanism to address these challenges. We implement TraCT on the Dynamo LLM inference framework and show that, across static and synthetic workloads, TraCT reduces average TTFT by up to 9.8x, lowers P99 latency by up to 6.2x, and improves peak throughput by up to 1.6x compared to RDMA and DRAM-based caching baselines.",
    "paper_id": "2512.18194v1"
  },
  "https://arxiv.org/abs/2512.13996v1": {
    "content_path": "../data/contents/2512.13996v1.md",
    "pdf_path": "../data/contents/2512.13996v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:41.417974",
    "title": "Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training",
    "abstract": "Summary: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.",
    "paper_id": "2512.13996v1"
  },
  "https://arxiv.org/abs/2512.19705v1": {
    "content_path": "../data/contents/2512.19705v1.md",
    "pdf_path": "../data/contents/2512.19705v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:44.060005",
    "title": "Generative AI for Analysts",
    "abstract": "Summary: We study how generative artificial intelligence (AI) transforms the work of financial analysts. Using the 2023 launch of FactSet's AI platform as a natural experiment, we find that adoption produces markedly richer and more comprehensive reports -- featuring 40% more distinct information sources, 34% broader topical coverage, and 25% greater use of advanced analytical methods -- while also improving timeliness. However, forecast errors rise by 59% as AI-assisted reports convey a more balanced mix of positive and negative information that is harder to synthesize, particularly for analysts facing heavier cognitive demands. Placebo tests using other data vendors confirm that these effects are unique to FactSet's AI integration. Overall, our findings reveal both the productivity gains and cognitive limits of generative AI in financial information production.",
    "paper_id": "2512.19705v1"
  },
  "https://arxiv.org/html/2402.05121v2": {
    "content_path": "../data/contents/2402.05121v2.md",
    "pdf_path": "../data/contents/2402.05121v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:44.302341",
    "title": "Large Language Model for Table Processing: A Survey",
    "abstract": "Comprehensive survey on LLM and VLM techniques for table QA, extraction, and data analysis.",
    "paper_id": "2402.05121v2"
  },
  "https://arxiv.org/abs/2512.18190v2": {
    "content_path": "../data/contents/2512.18190v2.md",
    "pdf_path": "../data/contents/2512.18190v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:44.671117",
    "title": "External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning",
    "abstract": "Summary: This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as \"Cognitive Vortex\" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.",
    "paper_id": "2512.18190v2"
  },
  "https://arxiv.org/abs/2512.13980v1": {
    "content_path": "../data/contents/2512.13980v1.md",
    "pdf_path": "../data/contents/2512.13980v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:45.014356",
    "title": "Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models",
    "abstract": "Summary: This paper proposes a structure-aware decoding method based on large language models to address the difficulty of traditional approaches in maintaining both semantic integrity and structural consistency in nested and overlapping entity extraction tasks. The method introduces a candidate span generation mechanism and structured attention modeling to achieve unified modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model first uses a pretrained language model to obtain context-aware semantic representations, then captures multi-granular entity span features through candidate representation combinations, and introduces hierarchical structural constraints during decoding to ensure consistency between semantics and structure. To enhance stability in complex scenarios, the model jointly optimizes classification loss and structural consistency loss, maintaining high recognition accuracy under multi-entity co-occurrence and long-sentence dependency conditions. Experiments conducted on the ACE 2005 dataset demonstrate significant improvements in Accuracy, Precision, Recall, and F1-Score, particularly in nested and overlapping entity recognition, where the model shows stronger boundary localization and structural modeling capability. This study verifies the effectiveness of structure-aware decoding in complex semantic extraction tasks, provides a new perspective for developing language models with hierarchical understanding, and establishes a methodological foundation for high-precision information extraction.",
    "paper_id": "2512.13980v1"
  },
  "https://arxiv.org/abs/2512.11202v1": {
    "content_path": "../data/contents/2512.11202v1.md",
    "pdf_path": "../data/contents/2512.11202v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:45.855233",
    "title": "amc: The Automated Mission Classifier for Telescope Bibliographies",
    "abstract": "Summary: Telescope bibliographies record the pulse of astronomy research by capturing publication statistics and citation metrics for telescope facilities. Robust and scalable bibliographies ensure that we can measure the scientific impact of our facilities and archives. However, the growing rate of publications threatens to outpace our ability to manually label astronomical literature. We therefore present the Automated Mission Classifier (amc), a tool that uses large language models (LLMs) to identify and categorize telescope references by processing large quantities of paper text. A modified version of amc performs well on the TRACS Kaggle challenge, achieving a macro $F_1$ score of 0.84 on the held-out test set. amc is valuable for other telescopes beyond TRACS; we developed the initial software for identifying papers that featured scientific results by NASA missions. Additionally, we investigate how amc can also be used to interrogate historical datasets and surface potential label errors. Our work demonstrates that LLM-based applications offer powerful and scalable assistance for library sciences.",
    "paper_id": "2512.11202v1"
  },
  "https://arxiv.org/abs/2512.18189v1": {
    "content_path": "../data/contents/2512.18189v1.md",
    "pdf_path": "../data/contents/2512.18189v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:46.765055",
    "title": "NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework",
    "abstract": "Summary: Cognitive computing models offer a formal and interpretable way to characterize human's deliberation and decision-making, yet their development remains labor-intensive. In this paper, we propose NL2CA, a novel method for auto-formalizing cognitive decision-making rules from natural language descriptions of human experience. Different from most related work that exploits either pure manual or human guided interactive modeling, our method is fully automated without any human intervention. The approach first translates text into Linear Temporal Logic (LTL) using a fine-tuned large language model (LLM), then refines the logic via an unsupervised Critic Tree, and finally transforms the output into executable production rules compatible with symbolic cognitive frameworks. Based on the resulted rules, a cognitive agent is further constructed and optimized through cognitive reinforcement learning according to the real-world behavioral data. Our method is validated in two domains: (1) NL-to-LTL translation, where our CriticNL2LTL module achieves consistent performance across both expert and large-scale benchmarks without human-in-the-loop feed-backs, and (2) cognitive driving simulation, where agents automatically constructed from human interviews have successfully learned the diverse decision patterns of about 70 trials in different critical scenarios. Experimental results demonstrate that NL2CA enables scalable, interpretable, and human-aligned cognitive modeling from unstructured textual data, offering a novel paradigm to automatically design symbolic cognitive agents.",
    "paper_id": "2512.18189v1"
  },
  "https://arxiv.org/abs/2512.13979v1": {
    "content_path": "../data/contents/2512.13979v1.md",
    "pdf_path": "../data/contents/2512.13979v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:47.394102",
    "title": "ReflCtrl: Controlling LLM Reflection via Representation Engineering",
    "abstract": "Summary: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.",
    "paper_id": "2512.13979v1"
  },
  "https://arxiv.org/abs/2512.11194v1": {
    "content_path": "../data/contents/2512.11194v1.md",
    "pdf_path": "../data/contents/2512.11194v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:50.960881",
    "title": "Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models",
    "abstract": "Summary: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.\n  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.",
    "paper_id": "2512.11194v1"
  },
  "https://arxiv.org/abs/2512.18177v1": {
    "content_path": "../data/contents/2512.18177v1.md",
    "pdf_path": "../data/contents/2512.18177v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:51.460253",
    "title": "NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI",
    "abstract": "Summary: Accurate yet interpretable image-based diagnosis remains a central challenge in medical AI, particularly in settings characterized by limited data, subtle visual cues, and high-stakes clinical decision-making. Most existing vision models rely on purely data-driven learning and produce black-box predictions with limited interpretability and poor cross-domain generalization, hindering their real-world clinical adoption. We present NEURO-GUARD, a novel knowledge-guided vision framework that integrates Vision Transformers (ViTs) with language-driven reasoning to improve performance, transparency, and domain robustness. NEURO-GUARD employs a retrieval-augmented generation (RAG) mechanism for self-verification, in which a large language model (LLM) iteratively generates, evaluates, and refines feature-extraction code for medical images. By grounding this process in clinical guidelines and expert knowledge, the framework progressively enhances feature detection and classification beyond purely data-driven baselines. Extensive experiments on diabetic retinopathy classification across four benchmark datasets APTOS, EyePACS, Messidor-1, and Messidor-2 demonstrate that NEURO-GUARD improves accuracy by 6.2% over a ViT-only baseline (84.69% vs. 78.4%) and achieves a 5% gain in domain generalization. Additional evaluations on MRI-based seizure detection further confirm its cross-domain robustness, consistently outperforming existing methods.\n  Overall, NEURO-GUARD bridges symbolic medical reasoning with subsymbolic visual learning, enabling interpretable, knowledge-aware, and generalizable medical image diagnosis while achieving state-of-the-art performance across multiple datasets.",
    "paper_id": "2512.18177v1"
  },
  "https://arxiv.org/abs/2512.13978v1": {
    "content_path": "../data/contents/2512.13978v1.md",
    "pdf_path": "../data/contents/2512.13978v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:53.855750",
    "title": "Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms",
    "abstract": "Summary: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${ó}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].\n  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.",
    "paper_id": "2512.13978v1"
  },
  "https://arxiv.org/abs/2512.11192v1": {
    "content_path": "../data/contents/2512.11192v1.md",
    "pdf_path": "../data/contents/2512.11192v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:54.568877",
    "title": "SciLaD: A Large-Scale, Transparent, Reproducible Dataset for Natural Scientific Language Processing",
    "abstract": "Summary: SciLaD is a novel, large-scale dataset of scientific language constructed entirely using open-source frameworks and publicly available data sources. It comprises a curated English split containing over 10 million scientific publications and a multilingual, unfiltered TEI XML split including more than 35 million publications. We also publish the extensible pipeline for generating SciLaD. The dataset construction and processing workflow demonstrates how open-source tools can enable large-scale, scientific data curation while maintaining high data quality. Finally, we pre-train a RoBERTa model on our dataset and evaluate it across a comprehensive set of benchmarks, achieving performance comparable to other scientific language models of similar size, validating the quality and utility of SciLaD. We publish the dataset and evaluation pipeline to promote reproducibility, transparency, and further research in natural scientific language processing and understanding including scholarly document processing.",
    "paper_id": "2512.11192v1"
  },
  "https://arxiv.org/abs/2512.18174v1": {
    "content_path": "../data/contents/2512.18174v1.md",
    "pdf_path": "../data/contents/2512.18174v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:54.905476",
    "title": "Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation",
    "abstract": "Summary: The current era of AI development places a heavy emphasis on training large models on increasingly scaled-up datasets. This paradigm has catalyzed entirely new product categories, such as LLM chatbots, while also raising concerns about data privacy and consumer choice. In this paper, we consider questions of data portability and user autonomy in the context of LLMs that \"reason\" using chain-of-thought (CoT) traces, computing intermediate text artifacts from user input before producing a final output. We first interpret recent data privacy and portability law to argue that these intermediate computations qualify as users' personal data. Then, building on the existing framework of Conscious Data Contribution, we show how communities who receive low utility from an available model can aggregate and distill their shared knowledge into an alternate model better aligned with their goals. We verify this approach empirically and investigate the effects of community diversity, reasoning granularity, and community size on distillation performance.",
    "paper_id": "2512.18174v1"
  },
  "https://arxiv.org/abs/2512.13961v1": {
    "content_path": "../data/contents/2512.13961v1.md",
    "pdf_path": "../data/contents/2512.13961v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:25:59.141558",
    "title": "Olmo 3",
    "abstract": "Summary: We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales. Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 Think 32B, is the strongest fully-open thinking model released to-date.",
    "paper_id": "2512.13961v1"
  },
  "https://arxiv.org/abs/2512.11183v1": {
    "content_path": "../data/contents/2512.11183v1.md",
    "pdf_path": "../data/contents/2512.11183v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:01.588945",
    "title": "Progress over Points: Reframing LM Benchmarks Around Scientific Objectives",
    "abstract": "Summary: Current benchmarks that test LLMs on static, already-solved problems (e.g., math word problems) effectively demonstrated basic capability acquisition. The natural progression has been toward larger, more comprehensive and challenging collections of static problems, an approach that inadvertently constrains the kinds of advances we can measure and incentivize. To address this limitation, we argue for progress-oriented benchmarks, problem environments whose objectives are themselves the core targets of scientific progress, so that achieving state of the art on the benchmark advances the field. As a introductory step, we instantiate an environment based on the NanoGPT speedrun. The environment standardizes a dataset slice, a reference model and training harness, and rich telemetry, with run-time verification and anti-gaming checks. Evaluation centers on the scientific delta achieved: best-attained loss and the efficiency frontier. Using this environment, we achieve a new state-of-the-art training time, improving upon the previous record by 3 seconds, and qualitatively observe the emergence of novel algorithmic ideas. Moreover, comparisons between models and agents remain possible, but they are a means, not the end; the benchmark's purpose is to catalyze reusable improvements to the language modeling stack. With this release, the overarching goal is to seed a community shift from static problem leaderboards to test-time research on open-ended yet measurable scientific problems. In this new paradigm, progress on the benchmark is progress on the science, thus reframing \"benchmarking\" as a vehicle for scientific advancement.",
    "paper_id": "2512.11183v1"
  },
  "https://arxiv.org/abs/2512.18168v1": {
    "content_path": "../data/contents/2512.18168v1.md",
    "pdf_path": "../data/contents/2512.18168v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:06.409137",
    "title": "Copula Entropy: Theory and Applications",
    "abstract": "Summary: This is the monograph on the theory and applications of copula entropy (CE). This book first introduces the theory of CE, including its background, definition, theorems, properties, and estimation methods. The theoretical applications of CE to structure learning, association discovery, variable selection, causal discovery, system identification, time lag estimation, domain adaptation, multivariate normality test, copula hypothesis test, two-sample test, change point detection, and symmetry test are reviewed. The relationships between the theoretical applications and their connections to correlation and causality are discussed. The framework based on CE for measuring statistical independence and conditional independence is compared to the other similar ones. The advantages of CE based methodologies over the other comparable ones are evaluated with simulations. The mathematical generalizations of CE are reviewed. The real applications of CE to every branch of science and engineering are briefly introduced.",
    "paper_id": "2512.18168v1"
  },
  "https://arxiv.org/abs/2512.13956v1": {
    "content_path": "../data/contents/2512.13956v1.md",
    "pdf_path": "../data/contents/2512.13956v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:08.288740",
    "title": "Multi-Agent Collaborative Framework for Intelligent IT Operations: An AOI System with Context-Aware Compression and Dynamic Task Scheduling",
    "abstract": "Summary: The proliferation of cloud-native architectures, characterized by microservices and dynamic orchestration, has rendered modern IT infrastructures exceedingly complex and volatile. This complexity generates overwhelming volumes of operational data, leading to critical bottlenecks in conventional systems: inefficient information processing, poor task coordination, and loss of contextual continuity during fault diagnosis and remediation. To address these challenges, we propose AOI (AI-Oriented Operations), a novel multi-agent collaborative framework that integrates three specialized agents with an LLM-based Context Compressor. Its core innovations include: (1) a dynamic task scheduling strategy that adaptively prioritizes operations based on real-time system states, and (2) a three-layer memory architecture comprising Working, Episodic, and Semantic layers that optimizes context retention and retrieval. Extensive experiments on both synthetic and real-world benchmarks demonstrate that AOI effectively mitigates information overload, achieving a 72.4% context compression ratio while preserving 92.8% of critical information and significantly enhances operational efficiency, attaining a 94.2% task success rate and reducing the Mean Time to Repair (MTTR) by 34.4% compared to the best baseline. This work presents a paradigm shift towards scalable, adaptive, and context-aware autonomous operations, enabling robust management of next-generation IT infrastructures with minimal human intervention.",
    "paper_id": "2512.13956v1"
  },
  "https://arxiv.org/abs/2512.11169v1": {
    "content_path": "../data/contents/2512.11169v1.md",
    "pdf_path": "../data/contents/2512.11169v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:08.654689",
    "title": "CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound",
    "abstract": "Summary: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.",
    "paper_id": "2512.11169v1"
  },
  "https://arxiv.org/abs/2512.18160v1": {
    "content_path": "../data/contents/2512.18160v1.md",
    "pdf_path": "../data/contents/2512.18160v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:09.591360",
    "title": "Propose, Solve, Verify: Self-Play Through Formal Verification",
    "abstract": "Summary: Training models through self-play alone (without any human data) has been a longstanding goal in AI, but its effectiveness for training large language models remains unclear, particularly in code generation where rewards based on unit tests are brittle and prone to error propagation. We study self-play in the verified code generation setting, where formal verification provides reliable correctness signals. We introduce Propose, Solve, Verify (PSV) a simple self-play framework where formal verification signals are used to create a proposer capable of generating challenging synthetic problems and a solver trained via expert iteration. We use PSV to train PSV-Verus, which across three benchmarks improves pass@1 by up to 9.6x over inference-only and expert-iteration baselines. We show that performance scales with the number of generated questions and training iterations, and through ablations identify formal verification and difficulty-aware proposal as essential ingredients for successful self-play.",
    "paper_id": "2512.18160v1"
  },
  "https://arxiv.org/abs/2512.13945v1": {
    "content_path": "../data/contents/2512.13945v1.md",
    "pdf_path": "../data/contents/2512.13945v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:12.168035",
    "title": "Pattern-Guided Diffusion Models",
    "abstract": "Summary: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.",
    "paper_id": "2512.13945v1"
  },
  "https://arxiv.org/abs/2512.11167v1": {
    "content_path": "../data/contents/2512.11167v1.md",
    "pdf_path": "../data/contents/2512.11167v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:12.506391",
    "title": "Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context",
    "abstract": "Summary: Reproducibility remains a cornerstone of scientific progress, yet complex multimodal models often lack transparent implementation details and accessible training infrastructure. In this work, we present a detailed reproduction and critical analysis of the Monkey Vision-Language Model (VLM) (Li et al. 2023b) published in CVPR24, a recent approach to high-resolution image understanding via image tiling. The original paper proposed splitting large images into tiles to recover fine-grained visual details while maintaining computational efficiency. Our study replicates this strategy using open checkpoints and reimplements the training pipeline. We confirm the key finding of the original Monkey VLM work, namely that tiling effectively recovers local details. We then extend this work further, by investigating the effect of the inclusion of the global context, which provide practical insights for future high-resolution multimodal modeling. However, we also report deviations in the results, with the magnitude of these effects depending heavily on task type and tile granularity.",
    "paper_id": "2512.11167v1"
  },
  "https://arxiv.org/abs/2512.18135v1": {
    "content_path": "../data/contents/2512.18135v1.md",
    "pdf_path": "../data/contents/2512.18135v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:15.358806",
    "title": "Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications",
    "abstract": "Summary: Integrating causal inference (CI) with reinforcement learning (RL) has emerged as a powerful paradigm to address critical limitations in classical RL, including low explainability, lack of robustness and generalization failures. Traditional RL techniques, which typically rely on correlation-driven decision-making, struggle when faced with distribution shifts, confounding variables, and dynamic environments. Causal reinforcement learning (CRL), leveraging the foundational principles of causal inference, offers promising solutions to these challenges by explicitly modeling cause-and-effect relationships. In this survey, we systematically review recent advancements at the intersection of causal inference and RL. We categorize existing approaches into causal representation learning, counterfactual policy optimization, offline causal RL, causal transfer learning, and causal explainability. Through this structured analysis, we identify prevailing challenges, highlight empirical successes in practical applications, and discuss open problems. Finally, we provide future research directions, underscoring the potential of CRL for developing robust, generalizable, and interpretable artificial intelligence systems.",
    "paper_id": "2512.18135v1"
  },
  "https://arxiv.org/abs/2512.13935v1": {
    "content_path": "../data/contents/2512.13935v1.md",
    "pdf_path": "../data/contents/2512.13935v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:17.675154",
    "title": "Informing Acquisition Functions via Foundation Models for Molecular Discovery",
    "abstract": "Summary: Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.",
    "paper_id": "2512.13935v1"
  },
  "https://arxiv.org/abs/2512.11150v1": {
    "content_path": "../data/contents/2512.11150v1.md",
    "pdf_path": "../data/contents/2512.11150v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:21.628713",
    "title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems",
    "abstract": "Summary: LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover.",
    "paper_id": "2512.11150v1"
  },
  "https://arxiv.org/html/2501.02189v6": {
    "content_path": "../data/contents/2501.02189v6.md",
    "pdf_path": "../data/contents/2501.02189v6.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:22.261885",
    "title": "Survey of SOTA Large Vision Language Models: Alignment, Benchmarks, Evaluations",
    "abstract": "Comprehensive survey of VLMs from 2019-2024 covering training objectives and multimodal integration.",
    "paper_id": "2501.02189v6"
  },
  "https://arxiv.org/abs/2512.18134v1": {
    "content_path": "../data/contents/2512.18134v1.md",
    "pdf_path": "../data/contents/2512.18134v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:22.835483",
    "title": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs",
    "abstract": "Summary: GPU architectures have continued to grow in complexity, with recent incarnations introducing increasingly powerful fixed-function units for matrix multiplication and data movement to accompany highly parallel general-purpose cores. To fully leverage these machines, software must use sophisticated schedules that maximally utilize all hardware resources. Since realizing such schedules is complex, both programmers and compilers routinely employ program transformations, such as software pipelining (SWP) and warp specialization (WS), to do so in practice. However, determining how best to use SWP and WS in combination is a challenging problem that is currently handled through a mix of brittle compilation heuristics and fallible human intuition, with little insight into the space of solutions. To remedy this situation, we introduce a novel formulation of SWP and WS as a joint optimization problem that can be solved holistically by off-the-shelf constraint solvers. We reify our approach in Twill, the first system that automatically derives optimal SWP and WS schedules for a large class of iterative programs. Twill is heuristic-free, easily extensible to new GPU architectures, and guaranteed to produce optimal schedules. We show that Twill can rediscover, and thereby prove optimal, the SWP and WS schedules manually developed by experts for Flash Attention on both the NVIDIA Hopper and Blackwell GPU architectures.",
    "paper_id": "2512.18134v1"
  },
  "https://arxiv.org/abs/2512.13930v1": {
    "content_path": "../data/contents/2512.13930v1.md",
    "pdf_path": "../data/contents/2512.13930v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:24.288573",
    "title": "Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery",
    "abstract": "Summary: Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including a single agent baseline and three multi-agent approaches: peer review, triage-ranking, and triage-forms. Across two chemical applications, CO adsorption on Cu-surface transition metal (M) adatoms and on M-N-C catalysts, reasoning-driven exploration reduces required atomistic simulations by up to 90% relative to trial-and-error selection. Reasoning trajectories reveal chemically grounded decisions that cannot be explained by stochastic sampling or semantic bias. Altogether, multi-agent collaboration accelerates materials discovery and marks a new paradigm for autonomous scientific exploration.",
    "paper_id": "2512.13930v1"
  },
  "https://arxiv.org/abs/2512.11147v1": {
    "content_path": "../data/contents/2512.11147v1.md",
    "pdf_path": "../data/contents/2512.11147v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:26.031318",
    "title": "MiniScope: A Least Privilege Framework for Authorizing Tool Calling Agents",
    "abstract": "Summary: Tool calling agents are an emerging paradigm in LLM deployment, with major platforms such as ChatGPT, Claude, and Gemini adding connectors and autonomous capabilities. However, the inherent unreliability of LLMs introduces fundamental security risks when these agents operate over sensitive user services. Prior approaches either rely on manually written policies that require security expertise, or place LLMs in the confinement loop, which lacks rigorous security guarantees. We present MiniScope, a framework that enables tool calling agents to operate on user accounts while confining potential damage from unreliable LLMs. MiniScope introduces a novel way to automatically and rigorously enforce least privilege principles by reconstructing permission hierarchies that reflect relationships among tool calls and combining them with a mobile-style permission model to balance security and ease of use. To evaluate MiniScope, we create a synthetic dataset derived from ten popular real-world applications, capturing the complexity of realistic agentic tasks beyond existing simplified benchmarks. Our evaluation shows that MiniScope incurs only 1-6% latency overhead compared to vanilla tool calling agents, while significantly outperforming the LLM based baseline in minimizing permissions as well as computational and operational costs.",
    "paper_id": "2512.11147v1"
  },
  "https://arxiv.org/abs/2512.18133v1": {
    "content_path": "../data/contents/2512.18133v1.md",
    "pdf_path": "../data/contents/2512.18133v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:26.847050",
    "title": "Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection",
    "abstract": "Summary: Nowadays, Graph Fraud Detection (GFD) in financial scenarios has become an urgent research topic to protect online payment security. However, as organized crime groups are becoming more professional in real-world scenarios, fraudsters are employing more sophisticated camouflage strategies. Specifically, fraudsters disguise themselves by mimicking the behavioral data collected by platforms, ensuring that their key characteristics are consistent with those of benign users to a high degree, which we call Adaptive Camouflage. Consequently, this narrows the differences in behavioral traits between them and benign users within the platform's database, thereby making current GFD models lose efficiency. To address this problem, we propose a relation diffusion-based graph augmentation model Grad. In detail, Grad leverages a supervised graph contrastive learning module to enhance the fraud-benign difference and employs a guided relation diffusion generator to generate auxiliary homophilic relations from scratch. Based on these, weak fraudulent signals would be enhanced during the aggregation process, thus being obvious enough to be captured. Extensive experiments have been conducted on two real-world datasets provided by WeChat Pay, one of the largest online payment platforms with billions of users, and three public datasets. The results show that our proposed model Grad outperforms SOTA methods in both various scenarios, achieving at most 11.10% and 43.95% increases in AUC and AP, respectively. Our code is released at https://github.com/AI4Risk/antifraud and https://github.com/Muyiiiii/WWW25-Grad.",
    "paper_id": "2512.18133v1"
  },
  "https://arxiv.org/abs/2512.13921v1": {
    "content_path": "../data/contents/2512.13921v1.md",
    "pdf_path": "../data/contents/2512.13921v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:27.699862",
    "title": "Sliding Window Recurrences for Sequence Models",
    "abstract": "Summary: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.",
    "paper_id": "2512.13921v1"
  },
  "https://arxiv.org/abs/2512.11139v2": {
    "content_path": "../data/contents/2512.11139v2.md",
    "pdf_path": "../data/contents/2512.11139v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:29.005939",
    "title": "Autotune: fast, accurate, and automatic tuning parameter selection for Lasso",
    "abstract": "Summary: Least absolute shrinkage and selection operator (Lasso), a popular method for high-dimensional regression, is now used widely for estimating high-dimensional time series models such as the vector autoregression (VAR). Selecting its tuning parameter efficiently and accurately remains a challenge, despite the abundance of available methods for doing so. We propose $\\mathsf{autotune}$, a strategy for Lasso to automatically tune itself by optimizing a penalized Gaussian log-likelihood alternately over regression coefficients and noise standard deviation. Using extensive simulation experiments on regression and VAR models, we show that $\\mathsf{autotune}$ is faster, and provides better generalization and model selection than established alternatives in low signal-to-noise regimes. In the process, $\\mathsf{autotune}$ provides a new estimator of noise standard deviation that can be used for high-dimensional inference, and a new visual diagnostic procedure for checking the sparsity assumption on regression coefficients. Finally, we demonstrate the utility of $\\mathsf{autotune}$ on a real-world financial data set. An R package based on C++ is also made publicly available on Github.",
    "paper_id": "2512.11139v2"
  },
  "https://arxiv.org/abs/2512.18131v1": {
    "content_path": "../data/contents/2512.18131v1.md",
    "pdf_path": "../data/contents/2512.18131v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:29.600141",
    "title": "Holistic Evaluation of State-of-the-Art LLMs for Code Generation",
    "abstract": "Summary: This study presents a comprehensive empirical evaluation of six state-of-the-art large language models (LLMs) for code generation, including both general-purpose and code-specialized models. Using a dataset of 944 real-world LeetCode problems across five programming languages, we assess model performance using rigorous metrics: compile-time errors, runtime errors, functional failures, and algorithmic suboptimalities. The results reveal significant performance variations, with DeepSeek-R1 and GPT-4.1 consistently outperform others in terms of correctness, efficiency, and robustness. Through detailed case studies, we identify common failure scenarios such as syntax errors, logical flaws, and suboptimal algorithms, highlighting the critical role of prompt engineering and human oversight in improving results. Based on these findings, we provide actionable recommendations for developers and practitioners, emphasizing that successful LLM deployment depends on careful model selection, effective prompt design, and context-aware usage to ensure reliable code generation in real-world software development tasks.",
    "paper_id": "2512.18131v1"
  },
  "https://arxiv.org/abs/2512.13914v1": {
    "content_path": "../data/contents/2512.13914v1.md",
    "pdf_path": "../data/contents/2512.13914v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:30.040771",
    "title": "Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming",
    "abstract": "Summary: Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.\n  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.",
    "paper_id": "2512.13914v1"
  },
  "https://arxiv.org/abs/2512.11114v1": {
    "content_path": "../data/contents/2512.11114v1.md",
    "pdf_path": "../data/contents/2512.11114v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:44.519661",
    "title": "In-Context Multi-Objective Optimization",
    "abstract": "Summary: Balancing competing objectives is omnipresent across disciplines, from drug design to autonomous systems. Multi-objective Bayesian optimization is a promising solution for such expensive, black-box problems: it fits probabilistic surrogates and selects new designs via an acquisition function that balances exploration and exploitation. In practice, it requires tailored choices of surrogate and acquisition that rarely transfer to the next problem, is myopic when multi-step planning is often required, and adds refitting overhead, particularly in parallel or time-sensitive loops. We present TAMO, a fully amortized, universal policy for multi-objective black-box optimization. TAMO uses a transformer architecture that operates across varying input and objective dimensions, enabling pretraining on diverse corpora and transfer to new problems without retraining: at test time, the pretrained model proposes the next design with a single forward pass. We pretrain the policy with reinforcement learning to maximize cumulative hypervolume improvement over full trajectories, conditioning on the entire query history to approximate the Pareto frontier. Across synthetic benchmarks and real tasks, TAMO produces fast proposals, reducing proposal time by 50-1000x versus alternatives while matching or improving Pareto quality under tight evaluation budgets. These results show that transformers can perform multi-objective optimization entirely in-context, eliminating per-task surrogate fitting and acquisition engineering, and open a path to foundation-style, plug-and-play optimizers for scientific discovery workflows.",
    "paper_id": "2512.11114v1"
  },
  "https://arxiv.org/abs/2512.18129v1": {
    "content_path": "../data/contents/2512.18129v1.md",
    "pdf_path": "../data/contents/2512.18129v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:45.064244",
    "title": "TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates",
    "abstract": "Summary: Survival analysis is a critical tool for modeling time-to-event data. Recent deep learning-based models have reduced various modeling assumptions including proportional hazard and linearity. However, a persistent challenge remains in incorporating longitudinal covariates, with prior work largely focusing on cross-sectional features, and in assessing calibration of these models, with research primarily focusing on discrimination during evaluation. We introduce TraCeR, a transformer-based survival analysis framework for incorporating longitudinal covariates. Based on a factorized self-attention architecture, TraCeR estimates the hazard function from a sequence of measurements, naturally capturing temporal covariate interactions without assumptions about the underlying data-generating process. The framework is inherently designed to handle censored data and competing events. Experiments on multiple real-world datasets demonstrate that TraCeR achieves substantial and statistically significant performance improvements over state-of-the-art methods. Furthermore, our evaluation extends beyond discrimination metrics and assesses model calibration, addressing a key oversight in literature.",
    "paper_id": "2512.18129v1"
  },
  "https://arxiv.org/abs/2512.13907v2": {
    "content_path": "../data/contents/2512.13907v2.md",
    "pdf_path": "../data/contents/2512.13907v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:46.351880",
    "title": "Assessing High-Risk AI Systems under the EU AI Act: From Legal Requirements to Technical Verification",
    "abstract": "Summary: The implementation of the AI Act requires practical mechanisms to verify compliance with legal obligations, yet concrete and operational mappings from high-level requirements to verifiable assessment activities remain limited, contributing to uneven readiness across Member States. This paper presents a structured mapping that translates high-level AI Act requirements into concrete, implementable verification activities applicable across the AI lifecycle. The mapping is derived through a systematic process in which legal requirements are decomposed into operational sub-requirements and grounded in authoritative standards and recognised practices. From this basis, verification activities are identified and characterised along two dimensions: the type of verification performed and the lifecycle target to which it applies. By making explicit the link between regulatory intent and technical and organisational assurance practices, the proposed mapping reduces interpretive uncertainty and provides a reusable reference for consistent, technology-agnostic compliance verification under the AI Act.",
    "paper_id": "2512.13907v2"
  },
  "https://arxiv.org/abs/2512.11110v1": {
    "content_path": "../data/contents/2512.11110v1.md",
    "pdf_path": "../data/contents/2512.11110v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:48.007029",
    "title": "FIBER: A Multilingual Evaluation Resource for Factual Inference Bias",
    "abstract": "Summary: Large language models are widely used across domains, yet there are concerns about their factual reliability and biases. Factual knowledge probing offers a systematic means to evaluate these aspects. Most existing benchmarks focus on single-entity facts and monolingual data. We therefore present FIBER, a multilingual benchmark for evaluating factual knowledge in single- and multi-entity settings. The dataset includes sentence completion, question-answering, and object-count prediction tasks in English, Italian, and Turkish. Using FIBER, we examine whether the prompt language induces inference bias in entity selection and how large language models perform on multi-entity versus single-entity questions. The results indicate that the language of the prompt can influence the model's generated output, particularly for entities associated with the country corresponding to that language. However, this effect varies across different topics such that 31% of the topics exhibit factual inference bias score greater than 0.5. Moreover, the level of bias differs across languages such that Turkish prompts show higher bias compared to Italian in 83% of the topics, suggesting a language-dependent pattern. Our findings also show that models face greater difficulty when handling multi-entity questions than the single-entity questions. Model performance differs across both languages and model sizes. The highest mean average precision is achieved in English, while Turkish and Italian lead to noticeably lower scores. Larger models, including Llama-3.1-8B and Qwen-2.5-7B, show consistently better performance than smaller 3B-4B models.",
    "paper_id": "2512.11110v1"
  },
  "https://arxiv.org/abs/2512.20618v1": {
    "content_path": "../data/contents/2512.20618v1.md",
    "pdf_path": "../data/contents/2512.20618v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:49.508772",
    "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
    "abstract": "Summary: Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
    "paper_id": "2512.20618v1"
  },
  "https://arxiv.org/abs/2512.18126v1": {
    "content_path": "../data/contents/2512.18126v1.md",
    "pdf_path": "../data/contents/2512.18126v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:50.512770",
    "title": "Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap",
    "abstract": "Summary: Mixture-of-Agents (MoA) inference can suffer from dense inter-agent communication and low hardware utilization, which jointly inflate serving latency. We present a serving design that targets these bottlenecks through an algorithm-system co-design. First, we replace dense agent interaction graphs with a hierarchical tree topology that induces structured sparsity in inter-agent communication. Second, we introduce a runtime adaptive mechanism that selectively terminates or skips downstream agent invocations using semantic agreement and confidence signals from intermediate outputs. Third, we pipeline agent execution by overlapping incremental prefilling with decoding across dependency-related agents, improving utilization and reducing inference latency. Across representative tasks, this approach substantially reduces end-to-end latency (up to 90%) while maintaining comparable accuracy (within $\\pm$1%) relative to dense-connectivity MoA baselines, and can improve accuracy in certain settings.",
    "paper_id": "2512.18126v1"
  },
  "https://arxiv.org/abs/2512.13904v1": {
    "content_path": "../data/contents/2512.13904v1.md",
    "pdf_path": "../data/contents/2512.13904v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:51.305425",
    "title": "Generative AI for Video Translation: A Scalable Architecture for Multilingual Video Conferencing",
    "abstract": "Summary: The real-time deployment of cascaded generative AI pipelines for applications like video translation is constrained by significant system-level challenges. These include the cumulative latency of sequential model inference and the quadratic ($\\mathcal{O}(N^2)$) computational complexity that renders multi-user video conferencing applications unscalable. This paper proposes and evaluates a practical system-level framework designed to mitigate these critical bottlenecks. The proposed architecture incorporates a turn-taking mechanism to reduce computational complexity from quadratic to linear in multi-user scenarios, and a segmented processing protocol to manage inference latency for a perceptually real-time experience. We implement a proof-of-concept pipeline and conduct a rigorous performance analysis across a multi-tiered hardware setup, including commodity (NVIDIA RTX 4060), cloud (NVIDIA T4), and enterprise (NVIDIA A100) GPUs. Our objective evaluation demonstrates that the system achieves real-time throughput ($τ< 1.0$) on modern hardware. A subjective user study further validates the approach, showing that a predictable, initial processing delay is highly acceptable to users in exchange for a smooth, uninterrupted playback experience. The work presents a validated, end-to-end system design that offers a practical roadmap for deploying scalable, real-time generative AI applications in multilingual communication platforms.",
    "paper_id": "2512.13904v1"
  },
  "https://arxiv.org/abs/2512.11109v1": {
    "content_path": "../data/contents/2512.11109v1.md",
    "pdf_path": "../data/contents/2512.11109v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:51.893075",
    "title": "Limits and Gains of Test-Time Scaling in Vision-Language Reasoning",
    "abstract": "Summary: Test-time scaling (TTS) has emerged as a powerful paradigm for improving the reasoning ability of Large Language Models (LLMs) by allocating additional computation at inference, yet its application to multimodal systems such as Vision-Language Models (VLMs) remains underexplored. In this work, we present a systematic empirical study of inference time reasoning methods applied across both open-source and closed-source VLMs on different benchmarks. Our results reveal that while closed-source models consistently benefit from structured reasoning and iterative Self-Refinement, open-source VLMs show inconsistent behavior: external verification provides the most reliable gains, whereas iterative refinement often degrades performance. We further find that the effectiveness of TTS is dataset-dependent, yielding clear improvements on multi-step reasoning tasks but offering only limited gains on perception-focused benchmarks. These findings demonstrate that TTS is not a universal solution and must be tailored to both model capabilities and task characteristics, motivating future work on adaptive TTS strategies and multimodal reward models.",
    "paper_id": "2512.11109v1"
  },
  "https://arxiv.org/abs/2512.20612v1": {
    "content_path": "../data/contents/2512.20612v1.md",
    "pdf_path": "../data/contents/2512.20612v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:52.178955",
    "title": "Making Large Language Models Efficient Dense Retrievers",
    "abstract": "Summary: Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.",
    "paper_id": "2512.20612v1"
  },
  "https://arxiv.org/abs/2512.18119v1": {
    "content_path": "../data/contents/2512.18119v1.md",
    "pdf_path": "../data/contents/2512.18119v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:52.586536",
    "title": "Distributed Asymmetric Allocation: A Topic Model for Large Imbalanced Corpora in Social Sciences",
    "abstract": "Summary: Social scientists employ latent Dirichlet allocation (LDA) to find highly specific topics in large corpora, but they often struggle in this task because (1) LDA, in general, takes a significant amount of time to fit on large corpora; (2) unsupervised LDA fragments topics into sub-topics in short documents; (3) semi-supervised LDA fails to identify specific topics defined using seed words. To solve these problems, I have developed a new topic model called distributed asymmetric allocation (DAA) that integrates multiple algorithms for efficiently identifying sentences about important topics in large corpora. I evaluate the ability of DAA to identify politically important topics by fitting it to the transcripts of speeches at the United Nations General Assembly between 1991 and 2017. The results show that DAA can classify sentences significantly more accurately and quickly than LDA thanks to the new algorithms. More generally, the results demonstrate that it is important for social scientists to optimize Dirichlet priors of LDA to perform content analysis accurately.",
    "paper_id": "2512.18119v1"
  },
  "https://arxiv.org/abs/2512.13898v1": {
    "content_path": "../data/contents/2512.13898v1.md",
    "pdf_path": "../data/contents/2512.13898v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:53.788463",
    "title": "Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs",
    "abstract": "Summary: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.",
    "paper_id": "2512.13898v1"
  },
  "https://arxiv.org/abs/2512.11108v1": {
    "content_path": "../data/contents/2512.11108v1.md",
    "pdf_path": "../data/contents/2512.11108v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:54.594715",
    "title": "Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution",
    "abstract": "Summary: Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both the lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find that lexical and position biases are structurally unbalanced in our model comparison, with models that score high on one type score low on the other. We also find signs that methods producing anomalous explanations are more likely to be biased themselves.",
    "paper_id": "2512.11108v1"
  },
  "https://arxiv.org/abs/2512.21152v1": {
    "content_path": "../data/contents/2512.21152v1.md",
    "pdf_path": "../data/contents/2512.21152v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:54.952641",
    "title": "MODE: Multi-Objective Adaptive Coreset Selection",
    "abstract": "Summary: We present Mode(Multi-Objective adaptive Data Efficiency), a framework that dynamically combines coreset selection strategies based on their evolving contribution to model performance. Unlike static methods, \\mode adapts selection criteria to training phases: emphasizing class balance early, diversity during representation learning, and uncertainty at convergence. We show that MODE achieves (1-1/e)-approximation with O(n \\log n) complexity and demonstrates competitive accuracy while providing interpretable insights into data utility evolution. Experiments show \\mode reduces memory requirements",
    "paper_id": "2512.21152v1"
  },
  "https://arxiv.org/abs/2512.19142v1": {
    "content_path": "../data/contents/2512.19142v1.md",
    "pdf_path": "../data/contents/2512.19142v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:26:56.358955",
    "title": "A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage",
    "abstract": "Summary: We consider supervised learning problems in which set predictions provide explicit uncertainty estimates. Using Choquet integrals (a.k.a. Lov{á}sz extensions), we propose a convex loss function for nondecreasing subset-valued functions obtained as level sets of a real-valued function. This loss function allows optimal trade-offs between conditional probabilistic coverage and the ''size'' of the set, measured by a non-decreasing submodular function. We also propose several extensions that mimic loss functions and criteria for binary classification with asymmetric losses, and show how to naturally obtain sets with optimized conditional coverage. We derive efficient optimization algorithms, either based on stochastic gradient descent or reweighted least-squares formulations, and illustrate our findings with a series of experiments on synthetic datasets for classification and regression tasks, showing improvements over approaches that aim for marginal coverage.",
    "paper_id": "2512.19142v1"
  },
  "https://arxiv.org/abs/2512.14994v1": {
    "content_path": "../data/contents/2512.14994v1.md",
    "pdf_path": "../data/contents/2512.14994v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:08.497021",
    "title": "Where is the Watermark? Interpretable Watermark Detection at the Block Level",
    "abstract": "Summary: Recent advances in generative AI have enabled the creation of highly realistic digital content, raising concerns around authenticity, ownership, and misuse. While watermarking has become an increasingly important mechanism to trace and protect digital media, most existing image watermarking schemes operate as black boxes, producing global detection scores without offering any insight into how or where the watermark is present. This lack of transparency impacts user trust and makes it difficult to interpret the impact of tampering. In this paper, we present a post-hoc image watermarking method that combines localised embedding with region-level interpretability. Our approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy. This allows us to generate detection maps that reveal which regions of an image are likely watermarked or altered. We show that our method achieves strong robustness against common image transformations while remaining sensitive to semantic manipulations. At the same time, the watermark remains highly imperceptible. Compared to prior post-hoc methods, our approach offers more interpretable detection while retaining competitive robustness. For example, our watermarks are robust to cropping up to half the image.",
    "paper_id": "2512.14994v1"
  },
  "https://arxiv.org/abs/2512.11096v1": {
    "content_path": "../data/contents/2512.11096v1.md",
    "pdf_path": "../data/contents/2512.11096v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:09.172828",
    "title": "Your plan may succeed, but what about failure? Investigating how people use ChatGPT for long-term life task planning",
    "abstract": "Summary: Long-term life task planning is inherently complex and uncertain, yet little is known about how emerging AI systems support this process. This study investigates how people use ChatGPT for such planning tasks, focusing on user practices, uncertainties, and perceptions of AI assistance. We conducted an interview study with 14 participants who engaged in long-term planning activities using ChatGPT, combining analysis of their prompts and interview responses. The task topics across diverse domains, including personal well-being, event planning, and professional learning, along with prompts to initiate, refine, and contextualize plans. ChatGPT helped structure complex goals into manageable steps, generate ideas, and sustain motivation, serving as a reflective partner. Yet its outputs were often generic or idealized, lacking personalization, contextual realism, and adaptability, requiring users to actively adapt and verify results. Participants expressed a need for AI systems that provide adaptive and trustworthy guidance while acknowledging uncertainty and potential failure in long-term planning. Our findings show how AI supports long-term life task planning under evolving uncertainty and highlight design implications for systems that are adaptive, uncertainty-aware, and capable of supporting long-term planning as an evolving human-AI collaboration.",
    "paper_id": "2512.11096v1"
  },
  "https://arxiv.org/abs/2512.21132v1": {
    "content_path": "../data/contents/2512.21132v1.md",
    "pdf_path": "../data/contents/2512.21132v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:10.017789",
    "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
    "abstract": "Summary: As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.",
    "paper_id": "2512.21132v1"
  },
  "https://arxiv.org/abs/2512.19135v1": {
    "content_path": "../data/contents/2512.19135v1.md",
    "pdf_path": "../data/contents/2512.19135v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:11.610985",
    "title": "Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis",
    "abstract": "Summary: With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.",
    "paper_id": "2512.19135v1"
  },
  "https://arxiv.org/abs/2512.14990v1": {
    "content_path": "../data/contents/2512.14990v1.md",
    "pdf_path": "../data/contents/2512.14990v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:13.335606",
    "title": "Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent",
    "abstract": "Summary: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.",
    "paper_id": "2512.14990v1"
  },
  "https://arxiv.org/abs/2512.11089v1": {
    "content_path": "../data/contents/2512.11089v1.md",
    "pdf_path": "../data/contents/2512.11089v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:15.197332",
    "title": "TPV: Parameter Perturbations Through the Lens of Test Prediction Variance",
    "abstract": "Summary: We identify test prediction variance (TPV) -- the first-order sensitivity of model outputs to parameter perturbations around a trained solution -- as a unifying quantity that links several classical observations about generalization in deep networks. TPV is a fully label-free object whose trace form separates the geometry of the trained model from the specific perturbation mechanism, allowing a broad family of parameter perturbations like SGD noise, label noise, finite-precision noise, and other post-training perturbations to be analyzed under a single framework. Theoretically, we show that TPV estimated on the training set converges to its test-set value in the overparameterized limit, providing the first result that prediction variance under local parameter perturbations can be inferred from training inputs alone. Empirically, TPV exhibits a striking stability across datasets and architectures -- including extremely narrow networks -- and correlates well with clean test loss. Finally, we demonstrate that modeling pruning as a TPV perturbation yields a simple label-free importance measure that performs competitively with state-of-the-art pruning methods, illustrating the practical utility of TPV. Code available at github.com/devansharpit/TPV.",
    "paper_id": "2512.11089v1"
  },
  "https://arxiv.org/abs/2512.21127v1": {
    "content_path": "../data/contents/2512.21127v1.md",
    "pdf_path": "../data/contents/2512.21127v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:15.594710",
    "title": "A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care",
    "abstract": "Summary: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\\% [95\\% CI 98.2--100], specificity 83.1\\% [95\\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\\% [95\\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.",
    "paper_id": "2512.21127v1"
  },
  "https://arxiv.org/abs/2512.19134v1": {
    "content_path": "../data/contents/2512.19134v1.md",
    "pdf_path": "../data/contents/2512.19134v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:15.867287",
    "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
    "abstract": "Summary: Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.",
    "paper_id": "2512.19134v1"
  },
  "https://arxiv.org/abs/2512.14989v1": {
    "content_path": "../data/contents/2512.14989v1.md",
    "pdf_path": "../data/contents/2512.14989v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:18.272289",
    "title": "Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams",
    "abstract": "Summary: Multimodal scientific reasoning remains a significant challenge for large language models (LLMs), particularly in chemistry, where problem-solving relies on symbolic diagrams, molecular structures, and structured visual data. Here, we systematically evaluate 40 proprietary and open-source multimodal LLMs, including GPT-5, o3, Gemini-2.5-Pro, and Qwen2.5-VL, on a curated benchmark of Olympiad-style chemistry questions drawn from over two decades of U.S. National Chemistry Olympiad (USNCO) exams. These questions require integrated visual and textual reasoning across diverse modalities. We find that many models struggle with modality fusion, where in some cases, removing the image even improves accuracy, indicating misalignment in vision-language integration. Chain-of-Thought prompting consistently enhances both accuracy and visual grounding, as demonstrated through ablation studies and occlusion-based interpretability. Our results reveal critical limitations in the scientific reasoning abilities of current MLLMs, providing actionable strategies for developing more robust and interpretable multimodal systems in chemistry. This work provides a timely benchmark for measuring progress in domain-specific multimodal AI and underscores the need for further advances at the intersection of artificial intelligence and scientific reasoning.",
    "paper_id": "2512.14989v1"
  },
  "https://arxiv.org/abs/2512.15753v1": {
    "content_path": "../data/contents/2512.15753v1.md",
    "pdf_path": "../data/contents/2512.15753v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:23.661475",
    "title": "TAO-Net: Two-stage Adaptive OOD Classification Network for Fine-grained Encrypted Traffic Classification",
    "abstract": "Summary: Encrypted traffic classification aims to identify applications or services by analyzing network traffic data. One of the critical challenges is the continuous emergence of new applications, which generates Out-of-Distribution (OOD) traffic patterns that deviate from known categories and are not well represented by predefined models. Current approaches rely on predefined categories, which limits their effectiveness in handling unknown traffic types. Although some methods mitigate this limitation by simply classifying unknown traffic into a single \"Other\" category, they fail to make a fine-grained classification. In this paper, we propose a Two-stage Adaptive OOD classification Network (TAO-Net) that achieves accurate classification for both In-Distribution (ID) and OOD encrypted traffic. The method incorporates an innovative two-stage design: the first stage employs a hybrid OOD detection mechanism that integrates transformer-based inter-layer transformation smoothness and feature analysis to effectively distinguish between ID and OOD traffic, while the second stage leverages large language models with a novel semantic-enhanced prompt strategy to transform OOD traffic classification into a generation task, enabling flexible fine-grained classification without relying on predefined labels. Experiments on three datasets demonstrate that TAO-Net achieves 96.81-97.70% macro-precision and 96.77-97.68% macro-F1, outperforming previous methods that only reach 44.73-86.30% macro-precision, particularly in identifying emerging network applications.",
    "paper_id": "2512.15753v1"
  },
  "https://arxiv.org/abs/2512.21120v1": {
    "content_path": "../data/contents/2512.21120v1.md",
    "pdf_path": "../data/contents/2512.21120v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:24.063570",
    "title": "ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models",
    "abstract": "Summary: Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \\textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \\textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.",
    "paper_id": "2512.21120v1"
  },
  "https://arxiv.org/abs/2512.19126v2": {
    "content_path": "../data/contents/2512.19126v2.md",
    "pdf_path": "../data/contents/2512.19126v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:24.430281",
    "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards",
    "abstract": "Summary: While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
    "paper_id": "2512.19126v2"
  },
  "https://arxiv.org/abs/2512.14982v1": {
    "content_path": "../data/contents/2512.14982v1.md",
    "pdf_path": "../data/contents/2512.14982v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:25.451138",
    "title": "Prompt Repetition Improves Non-Reasoning LLMs",
    "abstract": "Summary: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.",
    "paper_id": "2512.14982v1"
  },
  "https://arxiv.org/abs/2512.11081v1": {
    "content_path": "../data/contents/2512.11081v1.md",
    "pdf_path": "../data/contents/2512.11081v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:26.314944",
    "title": "Provable Recovery of Locally Important Signed Features and Interactions from Random Forest",
    "abstract": "Summary: Feature and Interaction Importance (FII) methods are essential in supervised learning for assessing the relevance of input variables and their interactions in complex prediction models. In many domains, such as personalized medicine, local interpretations for individual predictions are often required, rather than global scores summarizing overall feature importance. Random Forests (RFs) are widely used in these settings, and existing interpretability methods typically exploit tree structures and split statistics to provide model-specific insights. However, theoretical understanding of local FII methods for RF remains limited, making it unclear how to interpret high importance scores for individual predictions. We propose a novel, local, model-specific FII method that identifies frequent co-occurrences of features along decision paths, combining global patterns with those observed on paths specific to a given test point. We prove that our method consistently recovers the true local signal features and their interactions under a Locally Spike Sparse (LSS) model and also identifies whether large or small feature values drive a prediction. We illustrate the usefulness of our method and theoretical results through simulation studies and a real-world data example.",
    "paper_id": "2512.11081v1"
  },
  "https://arxiv.org/abs/2512.21113v1": {
    "content_path": "../data/contents/2512.21113v1.md",
    "pdf_path": "../data/contents/2512.21113v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:31.795651",
    "title": "A Mechanistic Analysis of Transformers for Dynamical Systems",
    "abstract": "Summary: Transformers are increasingly adopted for modeling and forecasting time-series, yet their internal mechanisms remain poorly understood from a dynamical systems perspective. In contrast to classical autoregressive and state-space models, which benefit from well-established theoretical foundations, Transformer architectures are typically treated as black boxes. This gap becomes particularly relevant as attention-based models are considered for general-purpose or zero-shot forecasting across diverse dynamical regimes. In this work, we do not propose a new forecasting model, but instead investigate the representational capabilities and limitations of single-layer Transformers when applied to dynamical data. Building on a dynamical systems perspective we interpret causal self-attention as a linear, history-dependent recurrence and analyze how it processes temporal information. Through a series of linear and nonlinear case studies, we identify distinct operational regimes. For linear systems, we show that the convexity constraint imposed by softmax attention fundamentally restricts the class of dynamics that can be represented, leading to oversmoothing in oscillatory settings. For nonlinear systems under partial observability, attention instead acts as an adaptive delay-embedding mechanism, enabling effective state reconstruction when sufficient temporal context and latent dimensionality are available. These results help bridge empirical observations with classical dynamical systems theory, providing insight into when and why Transformers succeed or fail as models of dynamical systems.",
    "paper_id": "2512.21113v1"
  },
  "https://arxiv.org/abs/2512.19125v1": {
    "content_path": "../data/contents/2512.19125v1.md",
    "pdf_path": "../data/contents/2512.19125v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:32.115649",
    "title": "SAP: Syntactic Attention Pruning for Transformer-based Language Models",
    "abstract": "Summary: This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpretability of model behavior. To further improve robustness, we propose Candidate Filtering (CF), a mechanism that prioritizes heads based on their contribution to model performance, mitigating degradation during pruning. Experimental results indicate that SAP effectively preserves critical heads of a high density of strong attention values, outperforming existing head pruning strategies in retrain-free settings. These findings position SAP as a promising foundation for a new direction in model compression research, offering high flexibility for pruning across all transformer-based language models.",
    "paper_id": "2512.19125v1"
  },
  "https://arxiv.org/abs/2512.14977v1": {
    "content_path": "../data/contents/2512.14977v1.md",
    "pdf_path": "../data/contents/2512.14977v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:32.963113",
    "title": "Human-Centered AI Maturity Model (HCAI-MM): An Organizational Design Perspective",
    "abstract": "Summary: Human-centered artificial intelligence (HCAI) is an approach to AI design, development, and deployment that prioritizes human needs, values, and experiences, ensuring that technology enhances human capabilities, well-being, and workforce empowerment. While HCAI has gained prominence in academic discourse and organizational practice, its implementation remains constrained by the absence of methodological guidance and structured frameworks. In particular, HCAI and organizational design practices are often treated separately, despite their interdependence in shaping effective socio-technical systems. This chapter addresses this gap by introducing the Human-Centered AI Maturity Model (HCAI-MM), a structured framework that enables organizations to evaluate, monitor, and advance their capacity to design and implement HCAI solutions. The model specifies stages of maturity, metrics, tools, governance mechanisms, and best practices, supported by case studies, while also incorporating an organizational design methodology that operationalizes maturity progression. Encompassing dimensions such as human-AI collaboration, explainability, fairness, and user experience, the HCAI-MM provides a roadmap for organizations to move from novice to advanced levels of maturity, aligning AI technologies with human values and organizational design principles.",
    "paper_id": "2512.14977v1"
  },
  "https://arxiv.org/abs/2512.11074v1": {
    "content_path": "../data/contents/2512.11074v1.md",
    "pdf_path": "../data/contents/2512.11074v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:33.745752",
    "title": "MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data",
    "abstract": "Summary: Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \\(30000\\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\\_Hans and Zh\\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \\(0.8\\) cosine similarity and symmetric KL divergence less than \\(0.000251\\) for all languages supported except Zh\\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\\%$ greater than MultiScript30k-Uk per split.",
    "paper_id": "2512.11074v1"
  },
  "https://arxiv.org/abs/2512.21110v1": {
    "content_path": "../data/contents/2512.21110v1.md",
    "pdf_path": "../data/contents/2512.21110v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:34.097989",
    "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent",
    "abstract": "Summary: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.",
    "paper_id": "2512.21110v1"
  },
  "https://arxiv.org/abs/2512.19122v1": {
    "content_path": "../data/contents/2512.19122v1.md",
    "pdf_path": "../data/contents/2512.19122v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:34.380789",
    "title": "BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation",
    "abstract": "Summary: Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.",
    "paper_id": "2512.19122v1"
  },
  "https://arxiv.org/abs/2512.19725v1": {
    "content_path": "../data/contents/2512.19725v1.md",
    "pdf_path": "../data/contents/2512.19725v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:34.883167",
    "title": "Out-of-Distribution Detection for Continual Learning: Design Principles and Benchmarking",
    "abstract": "Summary: Recent years have witnessed significant progress in the development of machine learning models across a wide range of fields, fueled by increased computational resources, large-scale datasets, and the rise of deep learning architectures. From malware detection to enabling autonomous navigation, modern machine learning systems have demonstrated remarkable capabilities. However, as these models are deployed in ever-changing real-world scenarios, their ability to remain reliable and adaptive over time becomes increasingly important. For example, in the real world, new malware families are continuously developed, whereas autonomous driving cars are employed in many different cities and weather conditions. Models trained in fixed settings can not respond effectively to novel conditions encountered post-deployment. In fact, most machine learning models are still developed under the assumption that training and test data are independent and identically distributed (i.i.d.), i.e., sampled from the same underlying (unknown) distribution. While this assumption simplifies model development and evaluation, it does not hold in many real-world applications, where data changes over time and unexpected inputs frequently occur. Retraining models from scratch whenever new data appears is computationally expensive, time-consuming, and impractical in resource-constrained environments. These limitations underscore the need for Continual Learning (CL), which enables models to incrementally learn from evolving data streams without forgetting past knowledge, and Out-of-Distribution (OOD) detection, which allows systems to identify and respond to novel or anomalous inputs. Jointly addressing both challenges is critical to developing robust, efficient, and adaptive AI systems.",
    "paper_id": "2512.19725v1"
  },
  "https://arxiv.org/abs/2512.11067v1": {
    "content_path": "../data/contents/2512.11067v1.md",
    "pdf_path": "../data/contents/2512.11067v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:35.464096",
    "title": "KathDB: Explainable Multimodal Database Management System with Human-AI Collaboration",
    "abstract": "Summary: Traditional DBMSs execute user- or application-provided SQL queries over relational data with strong semantic guarantees and advanced query optimization, but writing complex SQL is hard and focuses only on structured tables. Contemporary multimodal systems (which operate over relations but also text, images, and even videos) either expose low-level controls that force users to use (and possibly create) machine learning UDFs manually within SQL or offload execution entirely to black-box LLMs, sacrificing usability or explainability. We propose KathDB, a new system that combines relational semantics with the reasoning power of foundation models over multimodal data. Furthermore, KathDB includes human-AI interaction channels during query parsing, execution, and result explanation, such that users can iteratively obtain explainable answers across data modalities.",
    "paper_id": "2512.11067v1"
  },
  "https://arxiv.org/abs/2512.21107v1": {
    "content_path": "../data/contents/2512.21107v1.md",
    "pdf_path": "../data/contents/2512.21107v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:35.672957",
    "title": "Semi-Supervised Learning for Large Language Models Safety and Content Moderation",
    "abstract": "Summary: Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.",
    "paper_id": "2512.21107v1"
  },
  "https://arxiv.org/abs/2512.19114v1": {
    "content_path": "../data/contents/2512.19114v1.md",
    "pdf_path": "../data/contents/2512.19114v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:56.284605",
    "title": "HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction",
    "abstract": "Summary: The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.",
    "paper_id": "2512.19114v1"
  },
  "https://arxiv.org/abs/2512.14954v1": {
    "content_path": "../data/contents/2512.14954v1.md",
    "pdf_path": "../data/contents/2512.14954v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:57.118790",
    "title": "Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation",
    "abstract": "Summary: Computing next-token likelihood ratios between two language models (LMs) is a standard task in training paradigms such as knowledge distillation. Since this requires both models to share the same probability space, it becomes challenging when the teacher and student LMs use different tokenizers, for instance, when edge-device deployment necessitates a smaller vocabulary size to lower memory overhead. In this work, we address this vocabulary misalignment problem by uncovering an implicit recursive structure in the commonly deployed Byte-Pair Encoding (BPE) algorithm and utilizing it to create a probabilistic framework for cross-tokenizer likelihood scoring. Our method enables sequence likelihood evaluation for vocabularies different from the teacher model native tokenizer, addressing two specific scenarios: when the student vocabulary is a subset of the teacher vocabulary, and the general case where it is arbitrary. In the subset regime, our framework computes exact likelihoods and provides next-token probabilities for sequential sampling with only O(1) model evaluations per token. When used for distillation, this yields up to a 12% reduction in memory footprint for the Qwen2.5-1.5B model while also improving baseline performance up to 4% on the evaluated tasks. For the general case, we introduce a rigorous lossless procedure that leverages BPE recursive structure, complemented by a fast approximation that keeps large-vocabulary settings practical. Applied to distillation for mathematical reasoning, our approach improves GSM8K accuracy by more than 2% over the current state of the art.",
    "paper_id": "2512.14954v1"
  },
  "https://arxiv.org/abs/2512.14737v1": {
    "content_path": "../data/contents/2512.14737v1.md",
    "pdf_path": "../data/contents/2512.14737v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:57.547774",
    "title": "Zero-Knowledge Audit for Internet of Agents: Privacy-Preserving Communication Verification with Model Context Protocol",
    "abstract": "Summary: Existing agent communication frameworks face critical limitations in providing verifiable audit trails without compromising the privacy and confidentiality of agent interactions. The protection of agent communication privacy while ensuring auditability emerges as a fundamental challenge for applications requiring accurate billing, compliance verification, and accountability in regulated environments.\n  We introduce a framework for auditing agent communications that keeps messages private while still checking they follow expected rules. It pairs zero-knowledge proofs with the existing Model Context Protocol (MCP) so messages can be verified without revealing their contents. The approach runs in lightweight networks, stays compatible with standard MCP exchanges, and adds asynchronous audit verification to confirm format and general message types without exposing specifics.\n  The framework enables mutual audits between agents: one side can check communication content and quality while the other verifies usage metrics, all without revealing sensitive information. We formalize security goals and show that zk-MCP provides data authenticity and communication privacy, achieving efficient verification with negligible latency overhead. We fully implement the framework, including Circom-based zero-knowledge proof generation and an audit protocol integrated with MCP's bidirectional channel, and, to our knowledge, this is the first privacy-preserving audit system for agent communications that offers verifiable mutual auditing without exposing message content or compromising agent privacy.",
    "paper_id": "2512.14737v1"
  },
  "https://arxiv.org/abs/2512.21106v1": {
    "content_path": "../data/contents/2512.21106v1.md",
    "pdf_path": "../data/contents/2512.21106v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:57.860459",
    "title": "Semantic Refinement with LLMs for Graph Representations",
    "abstract": "Summary: Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.",
    "paper_id": "2512.21106v1"
  },
  "https://arxiv.org/abs/2512.19107v1": {
    "content_path": "../data/contents/2512.19107v1.md",
    "pdf_path": "../data/contents/2512.19107v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:27:59.604454",
    "title": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning",
    "abstract": "Summary: Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and \"surprising\" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.",
    "paper_id": "2512.19107v1"
  },
  "https://arxiv.org/abs/2512.15802v1": {
    "content_path": "../data/contents/2512.15802v1.md",
    "pdf_path": "../data/contents/2512.15802v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:07.310107",
    "title": "Consensus dimension reduction via multi-view learning",
    "abstract": "Summary: A plethora of dimension reduction methods have been developed to visualize high-dimensional data in low dimensions. However, different dimension reduction methods often output different and possibly conflicting visualizations of the same data. This problem is further exacerbated by the choice of hyperparameters, which may substantially impact the resulting visualization. To obtain a more robust and trustworthy dimension reduction output, we advocate for a consensus approach, which summarizes multiple visualizations into a single consensus dimension reduction visualization. Here, we leverage ideas from multi-view learning in order to identify the patterns that are most stable or shared across the many different dimension reduction visualizations, or views, and subsequently visualize this shared structure in a single low-dimensional plot. We demonstrate that this consensus visualization effectively identifies and preserves the shared low-dimensional data structure through both simulated and real-world case studies. We further highlight our method's robustness to the choice of dimension reduction method and hyperparameters -- a highly-desirable property when working towards trustworthy and reproducible data science.",
    "paper_id": "2512.15802v1"
  },
  "https://arxiv.org/abs/2512.11052v1": {
    "content_path": "../data/contents/2512.11052v1.md",
    "pdf_path": "../data/contents/2512.11052v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:08.478601",
    "title": "An Efficient Variant of One-Class SVM with Lifelong Online Learning Guarantees",
    "abstract": "Summary: We study outlier (a.k.a., anomaly) detection for single-pass non-stationary streaming data. In the well-studied offline or batch outlier detection problem, traditional methods such as kernel One-Class SVM (OCSVM) are both computationally heavy and prone to large false-negative (Type II) errors under non-stationarity. To remedy this, we introduce SONAR, an efficient SGD-based OCSVM solver with strongly convex regularization. We show novel theoretical guarantees on the Type I/II errors of SONAR, superior to those known for OCSVM, and further prove that SONAR ensures favorable lifelong learning guarantees under benign distribution shifts. In the more challenging problem of adversarial non-stationary data, we show that SONAR can be used within an ensemble method and equipped with changepoint detection to achieve adaptive guarantees, ensuring small Type I/II errors on each phase of data. We validate our theoretical findings on synthetic and real-world datasets.",
    "paper_id": "2512.11052v1"
  },
  "https://arxiv.org/abs/2512.21083v1": {
    "content_path": "../data/contents/2512.21083v1.md",
    "pdf_path": "../data/contents/2512.21083v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:09.907962",
    "title": "Hierarchical Modeling Approach to Fast and Accurate Table Recognition",
    "abstract": "Summary: The extraction and use of diverse knowledge from numerous documents is a pressing challenge in intelligent information retrieval. Documents contain elements that require different recognition methods. Table recognition typically consists of three subtasks, namely table structure, cell position and cell content recognition. Recent models have achieved excellent recognition with a combination of multi-task learning, local attention, and mutual learning. However, their effectiveness has not been fully explained, and they require a long period of time for inference. This paper presents a novel multi-task model that utilizes non-causal attention to capture the entire table structure, and a parallel inference algorithm for faster cell content inference. The superiority is demonstrated both visually and statistically on two large public datasets.",
    "paper_id": "2512.21083v1"
  },
  "https://arxiv.org/abs/2512.19104v1": {
    "content_path": "../data/contents/2512.19104v1.md",
    "pdf_path": "../data/contents/2512.19104v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:10.346179",
    "title": "Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions",
    "abstract": "Summary: Zeroth-order (ZO) optimization with ordinal feedback has emerged as a fundamental problem in modern machine learning systems, particularly in human-in-the-loop settings such as reinforcement learning from human feedback, preference learning, and evolutionary strategies. While rank-based ZO algorithms enjoy strong empirical success and robustness properties, their theoretical understanding, especially under stochastic objectives and standard smoothness assumptions, remains limited. In this paper, we study rank-based zeroth-order optimization for stochastic functions where only ordinal feedback of the stochastic function is available. We propose a simple and computationally efficient rank-based ZO algorithm. Under standard assumptions including smoothness, strong convexity, and bounded second moments of stochastic gradients, we establish explicit non-asymptotic query complexity bounds for both convex and nonconvex objectives. Notably, our results match the best-known query complexities of value-based ZO algorithms, demonstrating that ordinal information alone is sufficient for optimal query efficiency in stochastic settings. Our analysis departs from existing drift-based and information-geometric techniques, offering new tools for the study of rank-based optimization under noise. These findings narrow the gap between theory and practice and provide a principled foundation for optimization driven by human preferences.",
    "paper_id": "2512.19104v1"
  },
  "https://arxiv.org/abs/2512.14946v1": {
    "content_path": "../data/contents/2512.14946v1.md",
    "pdf_path": "../data/contents/2512.14946v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:10.947155",
    "title": "EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving",
    "abstract": "Summary: Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.\n  We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality.",
    "paper_id": "2512.14946v1"
  },
  "https://arxiv.org/abs/2512.10952v2": {
    "content_path": "../data/contents/2512.10952v2.md",
    "pdf_path": "../data/contents/2512.10952v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:13.299580",
    "title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
    "abstract": "Summary: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.",
    "paper_id": "2512.10952v2"
  },
  "https://arxiv.org/abs/2512.21080v1": {
    "content_path": "../data/contents/2512.21080v1.md",
    "pdf_path": "../data/contents/2512.21080v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:13.581204",
    "title": "LLM Personas as a Substitute for Field Experiments in Method Benchmarking",
    "abstract": "Summary: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.",
    "paper_id": "2512.21080v1"
  },
  "https://arxiv.org/abs/2512.19093v1": {
    "content_path": "../data/contents/2512.19093v1.md",
    "pdf_path": "../data/contents/2512.19093v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:14.364319",
    "title": "Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving",
    "abstract": "Summary: Bilingual mathematical problem solving needs a clear link between language reasoning and symbolic calculation. Large language models often handle language well but are weak in accurate computation. This paper presents HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a framework that joins reasoning and calculation using NuminaMath-7B-TIR, GPT-4o, and Mistral-7B. HERALD uses adaptive routing, tool-based reinforcement learning, and knowledge distillation to connect different reasoning paths. Confidence calibration keeps weighting stable, and dual-path checking keeps results correct. Reinforcement learning controls tool use to cut redundancy, and distillation lowers delay without hurting accuracy. The system shows that combining symbolic checking, adaptive ensembles, and bilingual fine-tuning helps achieve both fluent reasoning and precise calculation. HERALD offers a practical solution for multilingual mathematical reasoning with better accuracy, stability, and clarity.",
    "paper_id": "2512.19093v1"
  },
  "https://arxiv.org/abs/2512.14925v2": {
    "content_path": "../data/contents/2512.14925v2.md",
    "pdf_path": "../data/contents/2512.14925v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:15.189617",
    "title": "Multiscale Aggregated Hierarchical Attention (MAHA): A Game Theoretic and Optimization Driven Approach to Efficient Contextual Modeling in Large Language Models",
    "abstract": "Summary: The quadratic computational complexity of MultiHead SelfAttention (MHSA) remains a fundamental bottleneck in scaling Large Language Models (LLMs) for longcontext tasks. While sparse and linearized attention mechanisms attempt to mitigate this, they often compromise the representation of global dependencies or fail to capture multiscale semantic granularity effectively. In this paper, we propose Multiscale Aggregated Hierarchical Attention (MAHA), a novel architectural framework that reformulates the attention mechanism through hierarchical decomposition and mathematically rigorous aggregation. Unlike conventional approaches that treat token interactions at a single resolution, MAHA dynamically partitions the input sequence into hierarchical scales via learnable downsampling operators. The core innovation lies in its aggregation strategy: we model the fusion of scalespecific attention matrices as a resource allocation problem, solved via a convex optimization framework or a Nash equilibriumbased gametheoretic approach. This ensures a theoretically optimal balance between local nuance and global context fidelity. Implemented within a hybrid dilatedconvolutional transformer backbone, MAHA utilizes differentiable optimization layers to enable endtoend training. Experimental evaluations demonstrate that MAHA achieves superior scalability; empirical FLOPs analysis confirms an 81% reduction in computational cost at a sequence length of 4096 compared to standard attention. This work bridges the gap between optimization theory and sequence modeling, offering a scalable solution for nextgeneration LLMs.",
    "paper_id": "2512.14925v2"
  },
  "https://arxiv.org/abs/2512.10949v1": {
    "content_path": "../data/contents/2512.10949v1.md",
    "pdf_path": "../data/contents/2512.10949v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:17.212587",
    "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
    "abstract": "Summary: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
    "paper_id": "2512.10949v1"
  },
  "https://arxiv.org/abs/2512.21075v1": {
    "content_path": "../data/contents/2512.21075v1.md",
    "pdf_path": "../data/contents/2512.21075v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:17.750787",
    "title": "Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics",
    "abstract": "Summary: The empirical success of deep learning is often attributed to scaling laws that predict consistent gains as model, data, and compute grow; however, large models can exhibit training instability and diminishing returns, suggesting that scaling laws describe what success looks like but not when and why scaling succeeds or fails. A central obstacle is the lack of a rigorous understanding of feature learning at large depth. While muP characterizes feature-learning dynamics in the infinite-width limit and enables hyperparameter transfer across width, its depth extension (depth-muP) breaks down for residual blocks with more than one internal layer. We derive Neural Feature Dynamics (NFD) for ResNets with single-layer residual blocks, characterizing feature learning via a coupled forward-backward stochastic system in the joint infinite-width and infinite-depth limit. In this regime, NFD identifies when scaling-law trends persist and explains diminishing returns. It also reveals a vanishing mechanism induced by the 1/sqrt(depth) residual scaling under which the gradient-independence assumption (GIA), known to fail during training at finite depth, becomes provably valid again at infinite depth, yielding an analytically tractable regime for end-to-end feature learning. Motivated by this insight, we study two-layer residual blocks and show that the same mechanism causes feature-learning collapse in the first internal layer at large depth, providing a structural explanation for the empirical failure of depth-muP. Based on this diagnosis, we propose a depth-aware learning-rate correction that counteracts the collapse and empirically restores depth-wise hyperparameter transfer, yielding stronger performance in deeper ResNets.",
    "paper_id": "2512.21075v1"
  },
  "https://arxiv.org/abs/2512.19092v1": {
    "content_path": "../data/contents/2512.19092v1.md",
    "pdf_path": "../data/contents/2512.19092v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:18.305771",
    "title": "A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs",
    "abstract": "Summary: Reasoning over knowledge graphs (KGs) with first-order logic (FOL) queries is challenging due to the inherent incompleteness of real-world KGs and the compositional complexity of logical query structures. Most existing methods rely on embedding entities and relations into continuous geometric spaces and answer queries via differentiable set operations. While effective for simple query patterns, these approaches often struggle to generalize to complex queries involving multiple operators, deeper reasoning chains, or heterogeneous KG schemas. We propose ROG (Reasoning Over knowledge Graphs with large language models), an ensemble-style framework that combines query-aware KG neighborhood retrieval with large language model (LLM)-based chain-of-thought reasoning. ROG decomposes complex FOL queries into sequences of simpler sub-queries, retrieves compact, query-relevant subgraphs as contextual evidence, and performs step-by-step logical inference using an LLM, avoiding the need for task-specific embedding optimization. Experiments on standard KG reasoning benchmarks demonstrate that ROG consistently outperforms strong embedding-based baselines in terms of mean reciprocal rank (MRR), with particularly notable gains on high-complexity query types. These results suggest that integrating structured KG retrieval with LLM-driven logical reasoning offers a robust and effective alternative for complex KG reasoning tasks.",
    "paper_id": "2512.19092v1"
  },
  "https://arxiv.org/abs/2512.14910v1": {
    "content_path": "../data/contents/2512.14910v1.md",
    "pdf_path": "../data/contents/2512.14910v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:18.656387",
    "title": "AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally",
    "abstract": "Summary: Agricultural regions in rural areas face damage from climate-related risks, including droughts, heavy rainfall, and shifting weather patterns. Prior research calls for adaptive risk-management solutions and decision-making strategies. To this end, artificial intelligence (AI), particularly agentic AI, offers a promising path forward. Agentic AI systems consist of autonomous, specialized agents capable of solving complex, dynamic tasks. While past systems have relied on single-agent models or have used multi-agent frameworks only for static functions, there is a growing need for architectures that support dynamic collaborative reasoning and context-aware outputs. To bridge this gap, we present AgroAskAI, a multi-agent reasoning system for climate adaptation decision support in agriculture, with a focus on vulnerable rural communities. AgroAskAI features a modular, role-specialized architecture that uses a chain-of-responsibility approach to coordinate autonomous agents, integrating real-time tools and datasets. The system has built-in governance mechanisms that mitigate hallucination and enable internal feedback for coherent, locally relevant strategies. The system also supports multilingual interactions, making it accessible to non-English-speaking farmers. Experiments on common agricultural queries related to climate adaptation show that, with additional tools and prompt refinement, AgroAskAI delivers more actionable, grounded, and inclusive outputs. Our experimental results highlight the potential of agentic AI for sustainable and accountable decision support in climate adaptation for agriculture.",
    "paper_id": "2512.14910v1"
  },
  "https://arxiv.org/abs/2512.10941v1": {
    "content_path": "../data/contents/2512.10941v1.md",
    "pdf_path": "../data/contents/2512.10941v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:19.643160",
    "title": "Mull-Tokens: Modality-Agnostic Latent Thinking",
    "abstract": "Summary: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.",
    "paper_id": "2512.10941v1"
  },
  "https://arxiv.org/abs/2512.21066v1": {
    "content_path": "../data/contents/2512.21066v1.md",
    "pdf_path": "../data/contents/2512.21066v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:20.015503",
    "title": "Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation",
    "abstract": "Summary: Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.",
    "paper_id": "2512.21066v1"
  },
  "https://arxiv.org/abs/2512.19091v1": {
    "content_path": "../data/contents/2512.19091v1.md",
    "pdf_path": "../data/contents/2512.19091v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:20.558454",
    "title": "Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges",
    "abstract": "Summary: Open challenges have become the de facto standard for comparative ranking of medical AI methods. Despite their importance, medical AI leaderboards exhibit three persistent limitations: (1) score gaps are rarely tested for statistical significance, so rank stability is unknown; (2) single averaged metrics are applied to every organ, hiding clinically important boundary errors; (3) performance across intersecting demographics is seldom reported, masking fairness and equity gaps. We introduce RankInsight, an open-source toolkit that seeks to address these limitations. RankInsight (1) computes pair-wise significance maps that show the nnU-Net family outperforms Vision-Language and MONAI submissions with high statistical certainty; (2) recomputes leaderboards with organ-appropriate metrics, reversing the order of the top four models when Dice is replaced by NSD for tubular structures; and (3) audits intersectional fairness, revealing that more than half of the MONAI-based entries have the largest gender-race discrepancy on our proprietary Johns Hopkins Hospital dataset. The RankInsight toolkit is publicly released and can be directly applied to past, ongoing, and future challenges. It enables organizers and participants to publish rankings that are statistically sound, clinically meaningful, and demographically fair.",
    "paper_id": "2512.19091v1"
  },
  "https://arxiv.org/abs/2512.14908v1": {
    "content_path": "../data/contents/2512.14908v1.md",
    "pdf_path": "../data/contents/2512.14908v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:21.051451",
    "title": "ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs",
    "abstract": "Summary: We present ATLAS (Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs), a novel graph learning algorithm that addresses two important challenges in graph neural networks (GNNs). First, the accuracy of GNNs degrades when the graph is heterophilic. Second, iterative feature aggregation limits the scalability of GNNs to large graphs. We address these challenges by extracting topological information about graph communities at multiple levels of refinement, concatenating community assignments to the feature vector, and applying multilayer perceptrons (MLPs) to the resulting representation. This provides topological context about nodes and their neighborhoods without invoking aggregation. Because MLPs are typically more scalable than GNNs, our approach applies to large graphs without the need for sampling. Across a wide set of graphs, ATLAS achieves comparable accuracy to baseline methods, with gains as high as 20 percentage points over GCN for heterophilic graphs with negative structural bias and 11 percentage points over MLP for homophilic graphs. Furthermore, we show how multi-resolution community features systematically modulate performance in both homophilic and heterophilic settings, opening a principled path toward explainable graph learning.",
    "paper_id": "2512.14908v1"
  },
  "https://arxiv.org/abs/2512.10938v1": {
    "content_path": "../data/contents/2512.10938v1.md",
    "pdf_path": "../data/contents/2512.10938v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:21.845059",
    "title": "Stronger Normalization-Free Transformers",
    "abstract": "Summary: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\\mathrm{Derf}(x) = \\mathrm{erf}(αx + s)$, where $\\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.",
    "paper_id": "2512.10938v1"
  },
  "https://arxiv.org/abs/2512.21041v1": {
    "content_path": "../data/contents/2512.21041v1.md",
    "pdf_path": "../data/contents/2512.21041v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:23.843984",
    "title": "When LLMs fall short in Deductive Coding: Model Comparison and Human AI Collaboration Workflow Design",
    "abstract": "Summary: With generative artificial intelligence driving the growth of dialogic data in education, automated coding is a promising direction for learning analytics to improve efficiency. This surge highlights the need to understand the nuances of student-AI interactions, especially those rare yet crucial. However, automated coding may struggle to capture these rare codes due to imbalanced data, while human coding remains time-consuming and labour-intensive. The current study examined the potential of large language models (LLMs) to approximate or replace humans in deductive, theory-driven coding, while also exploring how human-AI collaboration might support such coding tasks at scale. We compared the coding performance of small transformer classifiers (e.g., BERT) and LLMs in two datasets, with particular attention to imbalanced head-tail distributions in dialogue codes. Our results showed that LLMs did not outperform BERT-based models and exhibited systematic errors and biases in deductive coding tasks. We designed and evaluated a human-AI collaborative workflow that improved coding efficiency while maintaining coding reliability. Our findings reveal both the limitations of LLMs -- especially their difficulties with semantic similarity and theoretical interpretations and the indispensable role of human judgment -- while demonstrating the practical promise of human-AI collaborative workflows for coding.",
    "paper_id": "2512.21041v1"
  },
  "https://arxiv.org/abs/2512.19081v1": {
    "content_path": "../data/contents/2512.19081v1.md",
    "pdf_path": "../data/contents/2512.19081v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:24.822406",
    "title": "Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning",
    "abstract": "Summary: Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.",
    "paper_id": "2512.19081v1"
  },
  "https://arxiv.org/abs/2512.14896v1": {
    "content_path": "../data/contents/2512.14896v1.md",
    "pdf_path": "../data/contents/2512.14896v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:25.375258",
    "title": "DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline",
    "abstract": "Summary: Objectives: To evaluate large language model (LLM) performance on pharmacy licensure-style question-answering (QA) tasks and develop an external knowledge integration method to improve their accuracy.\n  Methods: We benchmarked eleven existing LLMs with varying parameter sizes (8 billion to 70+ billion) using a 141-question pharmacy dataset. We measured baseline accuracy for each model without modification. We then developed a three-step retrieval-augmented generation (RAG) pipeline, DrugRAG, that retrieves structured drug knowledge from validated sources and augments model prompts with evidence-based context. This pipeline operates externally to the models, requiring no changes to model architecture or parameters.\n  Results: Baseline accuracy ranged from 46% to 92%, with GPT-5 (92%) and o3 (89%) achieving the highest scores. Models with fewer than 8 billion parameters scored below 50%. DrugRAG improved accuracy across all tested models, with gains ranging from 7 to 21 percentage points (e.g., Gemma 3 27B: 61% to 71%, Llama 3.1 8B: 46% to 67%) on the 141-item benchmark.\n  Conclusion: We demonstrate that external structured drug knowledge integration through DrugRAG measurably improves LLM accuracy on pharmacy tasks without modifying the underlying models. This approach provides a practical pipeline for enhancing pharmacy-focused AI applications with evidence-based information.",
    "paper_id": "2512.14896v1"
  },
  "https://arxiv.org/abs/2512.10932v1": {
    "content_path": "../data/contents/2512.10932v1.md",
    "pdf_path": "../data/contents/2512.10932v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:48.264102",
    "title": "BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models",
    "abstract": "Summary: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.",
    "paper_id": "2512.10932v1"
  },
  "https://arxiv.org/abs/2512.21039v1": {
    "content_path": "../data/contents/2512.21039v1.md",
    "pdf_path": "../data/contents/2512.21039v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:48.810497",
    "title": "Agentic Multi-Persona Framework for Evidence-Aware Fake News Detection",
    "abstract": "Summary: The rapid proliferation of online misinformation poses significant risks to public trust, policy, and safety, necessitating reliable automated fake news detection. Existing methods often struggle with multimodal content, domain generalization, and explainability. We propose AMPEND-LS, an agentic multi-persona evidence-grounded framework with LLM-SLM synergy for multimodal fake news detection. AMPEND-LS integrates textual, visual, and contextual signals through a structured reasoning pipeline powered by LLMs, augmented with reverse image search, knowledge graph paths, and persuasion strategy analysis. To improve reliability, we introduce a credibility fusion mechanism combining semantic similarity, domain trustworthiness, and temporal context, and a complementary SLM classifier to mitigate LLM uncertainty and hallucinations. Extensive experiments across three benchmark datasets demonstrate that AMPEND-LS consistently outperformed state-of-the-art baselines in accuracy, F1 score, and robustness. Qualitative case studies further highlight its transparent reasoning and resilience against evolving misinformation. This work advances the development of adaptive, explainable, and evidence-aware systems for safeguarding online information integrity.",
    "paper_id": "2512.21039v1"
  },
  "https://arxiv.org/abs/2512.19070v1": {
    "content_path": "../data/contents/2512.19070v1.md",
    "pdf_path": "../data/contents/2512.19070v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:52.256864",
    "title": "Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding",
    "abstract": "Summary: Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model's dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)",
    "paper_id": "2512.19070v1"
  },
  "https://arxiv.org/abs/2512.14895v1": {
    "content_path": "../data/contents/2512.14895v1.md",
    "pdf_path": "../data/contents/2512.14895v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:52.683599",
    "title": "Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections",
    "abstract": "Summary: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.",
    "paper_id": "2512.14895v1"
  },
  "https://arxiv.org/abs/2512.10931v1": {
    "content_path": "../data/contents/2512.10931v1.md",
    "pdf_path": "../data/contents/2512.10931v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:54.521536",
    "title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs",
    "abstract": "Summary: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.",
    "paper_id": "2512.10931v1"
  },
  "https://arxiv.org/abs/2512.21024v1": {
    "content_path": "../data/contents/2512.21024v1.md",
    "pdf_path": "../data/contents/2512.21024v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:54.841541",
    "title": "Policy-Conditioned Policies for Multi-Agent Task Solving",
    "abstract": "Summary: In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \\textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \\textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.",
    "paper_id": "2512.21024v1"
  },
  "https://arxiv.org/abs/2512.19069v1": {
    "content_path": "../data/contents/2512.19069v1.md",
    "pdf_path": "../data/contents/2512.19069v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:55.921767",
    "title": "Can abstract concepts from LLM improve SLM performance?",
    "abstract": "Summary: Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\\% of accuracy improvement for Qwen3-0.6B.",
    "paper_id": "2512.19069v1"
  },
  "https://arxiv.org/abs/2512.19723v1": {
    "content_path": "../data/contents/2512.19723v1.md",
    "pdf_path": "../data/contents/2512.19723v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:58.702402",
    "title": "End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment",
    "abstract": "Summary: This paper introduces a novel end-to-end framework that efficiently integrates data quality assessment with machine learning (ML) model operations in real-time production environments. While existing approaches treat data quality assessment and ML systems as isolated processes, our framework addresses the critical gap between theoretical methods and practical implementation by combining dynamic drift detection, adaptive data quality metrics, and MLOps into a cohesive, lightweight system. The key innovation lies in its operational efficiency, enabling real-time, quality-driven ML decision-making with minimal computational overhead. We validate the framework in a steel manufacturing company's Electroslag Remelting (ESR) vacuum pumping process, demonstrating a 12% improvement in model performance (R2 = 94%) and a fourfold reduction in prediction latency. By exploring the impact of data quality acceptability thresholds, we provide actionable insights into balancing data quality standards and predictive performance in industrial applications. This framework represents a significant advancement in MLOps, offering a robust solution for time-sensitive, data-driven decision-making in dynamic industrial environments.",
    "paper_id": "2512.19723v1"
  },
  "https://arxiv.org/abs/2512.10926v2": {
    "content_path": "../data/contents/2512.10926v2.md",
    "pdf_path": "../data/contents/2512.10926v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:59.210137",
    "title": "Decoupled Q-Chunking",
    "abstract": "Summary: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences (\"chunks\") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.",
    "paper_id": "2512.10926v2"
  },
  "https://arxiv.org/abs/2512.21021v1": {
    "content_path": "../data/contents/2512.21021v1.md",
    "pdf_path": "../data/contents/2512.21021v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:28:59.414067",
    "title": "Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces",
    "abstract": "Summary: Consumer-to-consumer (C2C) marketplaces pose distinct retrieval challenges: short, ambiguous queries; noisy, user-generated listings; and strict production constraints. This paper reports our experiment to build a domain-aware Japanese text-embedding approach to improve the quality of search at Mercari, Japan's largest C2C marketplace. We experimented with fine-tuning on purchase-driven query-title pairs, using role-specific prefixes to model query-item asymmetry. To meet production constraints, we apply Matryoshka Representation Learning to obtain compact, truncation-robust embeddings. Offline evaluation on historical search logs shows consistent gains over a strong generic encoder, with particularly large improvements when replacing PCA compression with Matryoshka truncation. A manual assessment further highlights better handling of proper nouns, marketplace-specific semantics, and term-importance alignment. Additionally, an initial online A/B test demonstrates statistically significant improvements in revenue per user and search-flow efficiency, with transaction frequency maintained. Results show that domain-aware embeddings improve relevance and efficiency at scale and form a practical foundation for richer LLM-era search experiences.",
    "paper_id": "2512.21021v1"
  },
  "https://arxiv.org/abs/2512.19061v1": {
    "content_path": "../data/contents/2512.19061v1.md",
    "pdf_path": "../data/contents/2512.19061v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:00.016759",
    "title": "Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation",
    "abstract": "Summary: Collaborative fraud, where multiple fraudulent accounts coordinate to exploit online payment systems, poses significant challenges due to the formation of complex network structures. Traditional detection methods that rely solely on high-confidence identity links suffer from limited coverage, while approaches using all available linkages often result in fragmented graphs with reduced clustering effectiveness. In this paper, we propose a novel graph-based fraud detection framework that addresses the challenge of large-scale heterogeneous graph clustering through a principled link transformation approach. Our method distinguishes between \\emph{hard links} (high-confidence identity relationships such as phone numbers, credit cards, and national IDs) and \\emph{soft links} (behavioral associations including device fingerprints, cookies, and IP addresses). We introduce a graph transformation technique that first identifies connected components via hard links, merges them into super-nodes, and then reconstructs a weighted soft-link graph amenable to efficient embedding and clustering. The transformed graph is processed using LINE (Large-scale Information Network Embedding) for representation learning, followed by HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for density-based cluster discovery. Experiments on a real-world payment platform dataset demonstrate that our approach achieves significant graph size reduction (from 25 million to 7.7 million nodes), doubles the detection coverage compared to hard-link-only baselines, and maintains high precision across identified fraud clusters. Our framework provides a scalable and practical solution for industrial-scale fraud detection systems.",
    "paper_id": "2512.19061v1"
  },
  "https://arxiv.org/abs/2512.14887v1": {
    "content_path": "../data/contents/2512.14887v1.md",
    "pdf_path": "../data/contents/2512.14887v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:00.277039",
    "title": "Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media",
    "abstract": "Summary: News sources play a central role in democratic societies by shaping political and social discourse through specific topics, viewpoints and voices. Understanding these dynamics is essential for assessing whether the media landscape offers a balanced and fair account of public debate. In earlier work, we introduced a pipeline that, given a news corpus, i) uses a hybrid human-machine approach to identify the range of viewpoints expressed about a given topic, and ii) classifies relevant claims with respect to the identified viewpoints, defined as sets of semantically and ideologically congruent claims (e.g., positions arguing that immigration positively impacts the UK economy). In this paper, we improve this pipeline by i) fine-tuning Large Language Models (LLMs) for viewpoint classification and ii) enriching claim representations with semantic descriptions of relevant actors drawn from Wikidata. We evaluate our approach against alternative solutions on a benchmark centred on the UK immigration debate. Results show that while both mechanisms independently improve classification performance, their integration yields the best results, particularly when using LLMs capable of processing long inputs.",
    "paper_id": "2512.14887v1"
  },
  "https://arxiv.org/abs/2512.10922v1": {
    "content_path": "../data/contents/2512.10922v1.md",
    "pdf_path": "../data/contents/2512.10922v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:00.684164",
    "title": "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale",
    "abstract": "Summary: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.",
    "paper_id": "2512.10922v1"
  },
  "https://arxiv.org/abs/2509.25292": {
    "content_path": "../data/contents/2509.25292.md",
    "pdf_path": "../data/contents/2509.25292.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:01.311276",
    "title": "A Measurement Study of Model Context Protocol",
    "abstract": "Large-scale empirical analysis of MCP markets/servers/clients (validity, security/maintenance risks), useful for MCP infra strategy. citeturn0academia17",
    "paper_id": "2509.25292"
  },
  "https://arxiv.org/abs/2512.21017v1": {
    "content_path": "../data/contents/2512.21017v1.md",
    "pdf_path": "../data/contents/2512.21017v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:01.568880",
    "title": "Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy",
    "abstract": "Summary: With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.",
    "paper_id": "2512.21017v1"
  },
  "https://arxiv.org/abs/2512.19057v1": {
    "content_path": "../data/contents/2512.19057v1.md",
    "pdf_path": "../data/contents/2512.19057v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:09.151855",
    "title": "Efficient Personalization of Generative Models via Optimal Experimental Design",
    "abstract": "Summary: Preference learning from human feedback has the ability to align generative models with the needs of end-users. Human feedback is costly and time-consuming to obtain, which creates demand for data-efficient query selection methods. This work presents a novel approach that leverages optimal experimental design to ask humans the most informative preference queries, from which we can elucidate the latent reward function modeling user preferences efficiently. We formulate the problem of preference query selection as the one that maximizes the information about the underlying latent preference model. We show that this problem has a convex optimization formulation, and introduce a statistically and computationally efficient algorithm ED-PBRL that is supported by theoretical guarantees and can efficiently construct structured queries such as images or text. We empirically present the proposed framework by personalizing a text-to-image generative model to user-specific styles, showing that it requires less preference queries compared to random query selection.",
    "paper_id": "2512.19057v1"
  },
  "https://arxiv.org/abs/2512.14880v1": {
    "content_path": "../data/contents/2512.14880v1.md",
    "pdf_path": "../data/contents/2512.14880v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:09.996630",
    "title": "Task Matrices: Linear Maps for Cross-Model Finetuning Transfer",
    "abstract": "Summary: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.",
    "paper_id": "2512.14880v1"
  },
  "https://arxiv.org/abs/2512.10918v1": {
    "content_path": "../data/contents/2512.10918v1.md",
    "pdf_path": "../data/contents/2512.10918v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:11.487509",
    "title": "CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio for Social Co-Viewing Experiences",
    "abstract": "Summary: Social presence is central to the enjoyment of watching content together, yet modern media consumption is increasingly solitary. We investigate whether multi-agent conversational AI systems can recreate the dynamics of shared viewing experiences across diverse content types. We present CompanionCast, a general framework for orchestrating multiple role-specialized AI agents that respond to video content using multimodal inputs, speech synthesis, and spatial audio. Distinctly, CompanionCast integrates an LLM-as-a-Judge module that iteratively scores and refines conversations across five dimensions (relevance, authenticity, engagement, diversity, personality consistency). We validate this framework through sports viewing, a domain with rich dynamics and strong social traditions, where a pilot study with soccer fans suggests that multi-agent interaction improves perceived social presence compared to solo viewing. We contribute: (1) a generalizable framework for orchestrating multi-agent conversations around multimodal video content, (2) a novel evaluator-agent pipeline for conversation quality control, and (3) exploratory evidence of increased social presence in AI-mediated co-viewing. We discuss challenges and future directions for applying this approach to diverse viewing contexts including entertainment, education, and collaborative watching experiences.",
    "paper_id": "2512.10918v1"
  },
  "https://arxiv.org/abs/2512.21010v1": {
    "content_path": "../data/contents/2512.21010v1.md",
    "pdf_path": "../data/contents/2512.21010v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:11.820241",
    "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
    "abstract": "Summary: The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation ($N=100,000$ iterations) is used to approximate the statistically robust Expected Win Score ($E[S_m]$), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity ($T_k$), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.",
    "paper_id": "2512.21010v1"
  },
  "https://arxiv.org/abs/2512.19769v1": {
    "content_path": "../data/contents/2512.19769v1.md",
    "pdf_path": "../data/contents/2512.19769v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:12.061902",
    "title": "A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows",
    "abstract": "Summary: Building deployment-ready LLM agents requires complex orchestration of tools, data sources, and control flow logic, yet existing systems tightly couple agent logic to specific programming languages and deployment models. We present a declarative system that separates agent workflow specification from implementation, enabling the same pipeline definition to execute across multiple backend languages (Java, Python, Go) and deployment environments (cloud-native, on-premises).\n  Our key insight is that most agent workflows consist of common patterns -- data serialization, filtering, RAG retrieval, API orchestration -- that can be expressed through a unified DSL rather than imperative code. This approach transforms agent development from application programming to configuration, where adding new tools or fine-tuning agent behaviors requires only pipeline specification changes, not code deployment. Our system natively supports A/B testing of agent strategies, allowing multiple pipeline variants to run on the same backend infrastructure with automatic metric collection and comparison.\n  We evaluate our approach on real-world e-commerce workflows at PayPal, processing millions of daily interactions. Our results demonstrate 60% reduction in development time, and 3x improvement in deployment velocity compared to imperative implementations. The language's declarative approach enables non-engineers to modify agent behaviors safely, while maintaining sub-100ms orchestration overhead. We show that complex workflows involving product search, personalization, and cart management can be expressed in under 50 lines of DSL compared to 500+ lines of imperative code.",
    "paper_id": "2512.19769v1"
  },
  "https://arxiv.org/abs/2512.14879v1": {
    "content_path": "../data/contents/2512.14879v1.md",
    "pdf_path": "../data/contents/2512.14879v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:12.488903",
    "title": "Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse",
    "abstract": "Summary: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.",
    "paper_id": "2512.14879v1"
  },
  "https://arxiv.org/abs/2512.10913v1": {
    "content_path": "../data/contents/2512.10913v1.md",
    "pdf_path": "../data/contents/2512.10913v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:15.601480",
    "title": "Reinforcement Learning in Financial Decision Making: A Systematic Review of Performance, Challenges, and Implementation Strategies",
    "abstract": "Summary: Reinforcement learning (RL) is an innovative approach to financial decision making, offering specialized solutions to complex investment problems where traditional methods fail. This review analyzes 167 articles from 2017--2025, focusing on market making, portfolio optimization, and algorithmic trading. It identifies key performance issues and challenges in RL for finance. Generally, RL offers advantages over traditional methods, particularly in market making. This study proposes a unified framework to address common concerns such as explainability, robustness, and deployment feasibility. Empirical evidence with synthetic data suggests that implementation quality and domain knowledge often outweigh algorithmic complexity. The study highlights the need for interpretable RL architectures for regulatory compliance, enhanced robustness in nonstationary environments, and standardized benchmarking protocols. Organizations should focus less on algorithm sophistication and more on market microstructure, regulatory constraints, and risk management in decision-making.",
    "paper_id": "2512.10913v1"
  },
  "https://arxiv.org/abs/2512.21002v1": {
    "content_path": "../data/contents/2512.21002v1.md",
    "pdf_path": "../data/contents/2512.21002v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:15.977161",
    "title": "Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation",
    "abstract": "Summary: Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\\%$ of tokens of every training sequence can retain, on average, $\\approx94\\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.",
    "paper_id": "2512.21002v1"
  },
  "https://arxiv.org/abs/2512.19027v1": {
    "content_path": "../data/contents/2512.19027v1.md",
    "pdf_path": "../data/contents/2512.19027v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:20.023374",
    "title": "Recontextualization Mitigates Specification Gaming without Modifying the Specification",
    "abstract": "Summary: Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models \"game\" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.",
    "paper_id": "2512.19027v1"
  },
  "https://arxiv.org/abs/2512.14865v1": {
    "content_path": "../data/contents/2512.14865v1.md",
    "pdf_path": "../data/contents/2512.14865v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:21.948449",
    "title": "Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction",
    "abstract": "Summary: End-to-end (E2E) spoken dialogue systems are increasingly replacing cascaded pipelines for voice-based human-AI interaction, processing raw audio directly without intermediate transcription. Existing benchmarks primarily evaluate these models on synthetic speech and single-turn tasks, leaving realistic multi-turn conversational ability underexplored. We introduce Audio MultiChallenge, an open-source benchmark to evaluate E2E spoken dialogue systems under natural multi-turn interaction patterns. Building on the text-based MultiChallenge framework, which evaluates Inference Memory, Instruction Retention, and Self Coherence, we introduce a new axis Voice Editing that tests robustness to mid-utterance speech repairs and backtracking. We further augment each axis to the audio modality, such as introducing Audio-Cue challenges for Inference Memory that require recalling ambient sounds and paralinguistic signals beyond semantic content. We curate 452 conversations from 47 speakers with 1,712 instance-specific rubrics through a hybrid audio-native agentic and human-in-the-loop pipeline that exposes model failures at scale while preserving natural disfluencies found in unscripted human speech. Our evaluation of proprietary and open-source models reveals that even frontier models struggle on our benchmark, with Gemini 3 Pro Preview (Thinking), our highest-performing model achieving a 54.65% pass rate. Error analysis shows that models fail most often on our new axes and that Self Coherence degrades with longer audio context. These failures reflect difficulty of tracking edits, audio cues, and long-range context in natural spoken dialogue. Audio MultiChallenge provides a reproducible testbed to quantify them and drive improvements in audio-native multi-turn interaction capability.",
    "paper_id": "2512.14865v1"
  },
  "https://arxiv.org/abs/2512.10903v1": {
    "content_path": "../data/contents/2512.10903v1.md",
    "pdf_path": "../data/contents/2512.10903v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:22.562222",
    "title": "Multi-Granular Node Pruning for Circuit Discovery",
    "abstract": "Summary: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.",
    "paper_id": "2512.10903v1"
  },
  "https://arxiv.org/abs/2512.20996v1": {
    "content_path": "../data/contents/2512.20996v1.md",
    "pdf_path": "../data/contents/2512.20996v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:22.886080",
    "title": "TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control",
    "abstract": "Summary: Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.",
    "paper_id": "2512.20996v1"
  },
  "https://arxiv.org/abs/2512.19025v2": {
    "content_path": "../data/contents/2512.19025v2.md",
    "pdf_path": "../data/contents/2512.19025v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:24.124035",
    "title": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
    "abstract": "Summary: Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have \"forgotten\" the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose Proximal Surrogate Generation (PSG), an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$β$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.",
    "paper_id": "2512.19025v2"
  },
  "https://arxiv.org/abs/2512.14860v1": {
    "content_path": "../data/contents/2512.14860v1.md",
    "pdf_path": "../data/contents/2512.14860v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:24.530946",
    "title": "Penetration Testing of Agentic AI: A Comparative Security Analysis Across Models and Frameworks",
    "abstract": "Summary: Agentic AI introduces security vulnerabilities that traditional LLM safeguards fail to address. Although recent work by Unit 42 at Palo Alto Networks demonstrated that ChatGPT-4o successfully executes attacks as an agent that it refuses in chat mode, there is no comparative analysis in multiple models and frameworks. We conducted the first systematic penetration testing and comparative evaluation of agentic AI systems, testing five prominent models (Claude 3.5 Sonnet, Gemini 2.5 Flash, GPT-4o, Grok 2, and Nova Pro) across two agentic AI frameworks (AutoGen and CrewAI) using a seven-agent architecture that mimics the functionality of a university information management system and 13 distinct attack scenarios that span prompt injection, Server Side Request Forgery (SSRF), SQL injection, and tool misuse. Our 130 total test cases reveal significant security disparities: AutoGen demonstrates a 52.3% refusal rate versus CrewAI's 30.8%, while model performance ranges from Nova Pro's 46.2% to Claude and Grok 2's 38.5%. Most critically, Grok 2 on CrewAI rejected only 2 of 13 attacks (15.4% refusal rate), and the overall refusal rate of 41.5% across all configurations indicates that more than half of malicious prompts succeeded despite enterprise-grade safety mechanisms. We identify six distinct defensive behavior patterns including a novel \"hallucinated compliance\" strategy where models fabricate outputs rather than executing or refusing attacks, and provide actionable recommendations for secure agent deployment. Complete attack prompts are also included in the Appendix to enable reproducibility.",
    "paper_id": "2512.14860v1"
  },
  "https://arxiv.org/abs/2512.10895v1": {
    "content_path": "../data/contents/2512.10895v1.md",
    "pdf_path": "../data/contents/2512.10895v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:24.810148",
    "title": "LLMs Can Assist with Proposal Selection at Large User Facilities",
    "abstract": "Summary: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.",
    "paper_id": "2512.10895v1"
  },
  "https://arxiv.org/abs/2512.20985v1": {
    "content_path": "../data/contents/2512.20985v1.md",
    "pdf_path": "../data/contents/2512.20985v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:25.038337",
    "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
    "abstract": "Summary: The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.",
    "paper_id": "2512.20985v1"
  },
  "https://arxiv.org/abs/2512.19019v1": {
    "content_path": "../data/contents/2512.19019v1.md",
    "pdf_path": "../data/contents/2512.19019v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:26.069178",
    "title": "Optimizer Dynamics at the Edge of Stability with Differential Privacy",
    "abstract": "Summary: Deep learning models can reveal sensitive information about individual training examples, and while differential privacy (DP) provides guarantees restricting such leakage, it also alters optimization dynamics in poorly understood ways. We study the training dynamics of neural networks under DP by comparing Gradient Descent (GD), and Adam to their privacy-preserving variants. Prior work shows that these optimizers exhibit distinct stability dynamics: full-batch methods train at the Edge of Stability (EoS), while mini-batch and adaptive methods exhibit analogous edge-of-stability behavior. At these regimes, the training loss and the sharpness--the maximum eigenvalue of the training loss Hessian--exhibit certain characteristic behavior. In DP training, per-example gradient clipping and Gaussian noise modify the update rule, and it is unclear whether these stability patterns persist. We analyze how clipping and noise change sharpness and loss evolution and show that while DP generally reduces the sharpness and can prevent optimizers from fully reaching the classical stability thresholds, patterns from EoS and analogous adaptive methods stability regimes persist, with the largest learning rates and largest privacy budgets approaching, and sometimes exceeding, these thresholds. These findings highlight the unpredictability introduced by DP in neural network optimization.",
    "paper_id": "2512.19019v1"
  },
  "https://arxiv.org/abs/2512.14856v2": {
    "content_path": "../data/contents/2512.14856v2.md",
    "pdf_path": "../data/contents/2512.14856v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:26.423860",
    "title": "T5Gemma 2: Seeing, Reading, and Understanding Longer",
    "abstract": "Summary: We introduce T5Gemma 2, the next generation of the T5Gemma family of lightweight open encoder-decoder models, featuring strong multilingual, multimodal and long-context capabilities. T5Gemma 2 follows the adaptation recipe (via UL2) in T5Gemma -- adapting a pretrained decoder-only model into an encoder-decoder model, and extends it from text-only regime to multimodal based on the Gemma 3 models. We further propose two methods to improve the efficiency: tied word embedding that shares all embeddings across encoder and decoder, and merged attention that unifies decoder self- and cross-attention into a single joint module. Experiments demonstrate the generality of the adaptation strategy over architectures and modalities as well as the unique strength of the encoder-decoder architecture on long context modeling. Similar to T5Gemma, T5Gemma 2 yields comparable or better pretraining performance and significantly improved post-training performance than its Gemma 3 counterpart. We release the pretrained models (270M-270M, 1B-1B and 4B-4B) to the community for future research.",
    "paper_id": "2512.14856v2"
  },
  "https://arxiv.org/abs/2512.10882v2": {
    "content_path": "../data/contents/2512.10882v2.md",
    "pdf_path": "../data/contents/2512.10882v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:27.690212",
    "title": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity",
    "abstract": "Summary: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze emotions, the emergence of multimodal generative Artificial Intelligence (AI) promises great advances. However, we lack evidence about the effectiveness of multimodal AI in analyzing emotions in political communication. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in the video-based analysis of emotional arousal, using two complementary datasets of human-labeled video recordings. It finds that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and exhibit little to no demographic bias. However, in recordings of real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in multimodal political analysis and contributes a suitable replicable framework.",
    "paper_id": "2512.10882v2"
  },
  "https://arxiv.org/abs/2512.20983v1": {
    "content_path": "../data/contents/2512.20983v1.md",
    "pdf_path": "../data/contents/2512.20983v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:28.056944",
    "title": "Automatic Replication of LLM Mistakes in Medical Conversations",
    "abstract": "Summary: Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.",
    "paper_id": "2512.20983v1"
  },
  "https://arxiv.org/abs/2512.19012v2": {
    "content_path": "../data/contents/2512.19012v2.md",
    "pdf_path": "../data/contents/2512.19012v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:29.286142",
    "title": "DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation",
    "abstract": "Summary: Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.",
    "paper_id": "2512.19012v2"
  },
  "https://arxiv.org/abs/2512.15798v1": {
    "content_path": "../data/contents/2512.15798v1.md",
    "pdf_path": "../data/contents/2512.15798v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:30.785747",
    "title": "DP-Bench: A Benchmark for Evaluating Data Product Creation Systems",
    "abstract": "Summary: A data product is created with the intention of solving a specific problem, addressing a specific business usecase or meeting a particular need, going beyond just serving data as a raw asset. Data products enable end users to gain greater insights about their data. Since it was first introduced over a decade ago, there has been considerable work, especially in industry, to create data products manually or semi-automatically. However, there exists hardly any benchmark to evaluate automatic data product creation. In this work, we present a benchmark, first of its kind, for this task. We call it DP-Bench. We describe how this benchmark was created by taking advantage of existing work in ELT (Extract-Load-Transform) and Text-to-SQL benchmarks. We also propose a number of LLM based approaches that can be considered as baselines for generating data products automatically. We make the DP-Bench and supplementary materials available in https://huggingface.co/datasets/ibm-research/dp-bench .",
    "paper_id": "2512.15798v1"
  },
  "https://arxiv.org/abs/2512.10877v1": {
    "content_path": "../data/contents/2512.10877v1.md",
    "pdf_path": "../data/contents/2512.10877v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:31.410623",
    "title": "Guided Transfer Learning for Discrete Diffusion Models",
    "abstract": "Summary: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.",
    "paper_id": "2512.10877v1"
  },
  "https://arxiv.org/abs/2512.20973v1": {
    "content_path": "../data/contents/2512.20973v1.md",
    "pdf_path": "../data/contents/2512.20973v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:31.968570",
    "title": "DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination",
    "abstract": "Summary: Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.",
    "paper_id": "2512.20973v1"
  },
  "https://arxiv.org/abs/2512.19011v1": {
    "content_path": "../data/contents/2512.19011v1.md",
    "pdf_path": "../data/contents/2512.19011v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:32.186514",
    "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
    "abstract": "Summary: Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.\n  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.\n  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.",
    "paper_id": "2512.19011v1"
  },
  "https://arxiv.org/abs/2512.14851v1": {
    "content_path": "../data/contents/2512.14851v1.md",
    "pdf_path": "../data/contents/2512.14851v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:35.797030",
    "title": "Unreliable Uncertainty Estimates with Monte Carlo Dropout",
    "abstract": "Summary: Reliable uncertainty estimation is crucial for machine learning models, especially in safety-critical domains. While exact Bayesian inference offers a principled approach, it is often computationally infeasible for deep neural networks. Monte Carlo dropout (MCD) was proposed as an efficient approximation to Bayesian inference in deep learning by applying neuron dropout at inference time \\citep{gal2016dropout}. Hence, the method generates multiple sub-models yielding a distribution of predictions to estimate uncertainty. We empirically investigate its ability to capture true uncertainty and compare to Gaussian Processes (GP) and Bayesian Neural Networks (BNN). We find that MCD struggles to accurately reflect the underlying true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions as observed in Bayesian models. The findings suggest that uncertainty estimates from MCD, as implemented and evaluated in these experiments, is not as reliable as those from traditional Bayesian approaches for capturing epistemic and aleatoric uncertainty.",
    "paper_id": "2512.14851v1"
  },
  "https://arxiv.org/abs/2512.11922v1": {
    "content_path": "../data/contents/2512.11922v1.md",
    "pdf_path": "../data/contents/2512.11922v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:36.319262",
    "title": "Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use",
    "abstract": "Summary: Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.",
    "paper_id": "2512.11922v1"
  },
  "https://arxiv.org/abs/2512.20968v1": {
    "content_path": "../data/contents/2512.20968v1.md",
    "pdf_path": "../data/contents/2512.20968v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:36.731843",
    "title": "Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality",
    "abstract": "Summary: Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.\n  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.",
    "paper_id": "2512.20968v1"
  },
  "https://arxiv.org/abs/2512.19004v1": {
    "content_path": "../data/contents/2512.19004v1.md",
    "pdf_path": "../data/contents/2512.19004v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:36.990504",
    "title": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models",
    "abstract": "Summary: Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.\n  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.",
    "paper_id": "2512.19004v1"
  },
  "https://arxiv.org/abs/2512.14846v1": {
    "content_path": "../data/contents/2512.14846v1.md",
    "pdf_path": "../data/contents/2512.14846v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:37.433189",
    "title": "MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber",
    "abstract": "Summary: Traditional, centralized security tools often miss adaptive, multi-vector attacks. We present the Multi-Agent LLM Cyber Defense Framework (MALCDF), a practical setup where four large language model (LLM) agents-Detection, Intelligence, Response, and Analysis-work together in real time. Agents communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages, and produce audit-friendly outputs (e.g., MITRE ATT&CK mappings).\n  For evaluation, we keep the test simple and consistent: all reported metrics come from the same 50-record live stream derived from the CICIDS2017 feature schema. CICIDS2017 is used for configuration (fields/schema) and to train a practical ML baseline. The ML-IDS baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the 50-record stream, with no overlap between training and test records.\n  In experiments, MALCDF reaches 90.0% detection accuracy, 85.7% F1-score, and 9.1% false-positive rate, with 6.8s average per-event latency. It outperforms the lightweight ML-IDS baseline and a single-LLM setup on accuracy while keeping end-to-end outputs consistent. Overall, this hands-on build suggests that coordinating simple LLM agents with secure, ontology-aligned messaging can improve practical, real-time cyber defense.",
    "paper_id": "2512.14846v1"
  },
  "https://arxiv.org/abs/2512.10863v1": {
    "content_path": "../data/contents/2512.10863v1.md",
    "pdf_path": "../data/contents/2512.10863v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:43.424077",
    "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
    "abstract": "Summary: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.",
    "paper_id": "2512.10863v1"
  },
  "https://arxiv.org/abs/2512.20967v1": {
    "content_path": "../data/contents/2512.20967v1.md",
    "pdf_path": "../data/contents/2512.20967v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:46.091624",
    "title": "Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions",
    "abstract": "Summary: As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \\emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\\mathcal{O}(\\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\\%.",
    "paper_id": "2512.20967v1"
  },
  "https://arxiv.org/abs/2512.19001v1": {
    "content_path": "../data/contents/2512.19001v1.md",
    "pdf_path": "../data/contents/2512.19001v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:46.497405",
    "title": "ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management",
    "abstract": "Summary: As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided \"Pretrain-then-Reinforce\" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.",
    "paper_id": "2512.19001v1"
  },
  "https://arxiv.org/abs/2512.14698v1": {
    "content_path": "../data/contents/2512.14698v1.md",
    "pdf_path": "../data/contents/2512.14698v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:47.797372",
    "title": "TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs",
    "abstract": "Summary: This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.",
    "paper_id": "2512.14698v1"
  },
  "https://arxiv.org/abs/2512.10858v1": {
    "content_path": "../data/contents/2512.10858v1.md",
    "pdf_path": "../data/contents/2512.10858v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:48.783097",
    "title": "Scaling Behavior of Discrete Diffusion Language Models",
    "abstract": "Summary: Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.\n  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.",
    "paper_id": "2512.10858v1"
  },
  "https://arxiv.org/abs/2512.20963v1": {
    "content_path": "../data/contents/2512.20963v1.md",
    "pdf_path": "../data/contents/2512.20963v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:55.168563",
    "title": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
    "abstract": "Summary: Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized \"spiky\" representations, whereas (ii) generalization arises when the model captures local data statistics, producing \"balanced\" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.",
    "paper_id": "2512.20963v1"
  },
  "https://arxiv.org/abs/2512.18999v1": {
    "content_path": "../data/contents/2512.18999v1.md",
    "pdf_path": "../data/contents/2512.18999v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:55.392274",
    "title": "Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework",
    "abstract": "Summary: When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.",
    "paper_id": "2512.18999v1"
  },
  "https://arxiv.org/abs/2512.14697v1": {
    "content_path": "../data/contents/2512.14697v1.md",
    "pdf_path": "../data/contents/2512.14697v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:29:56.963167",
    "title": "Spherical Leech Quantization for Visual Tokenization and Generation",
    "abstract": "Summary: Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($Λ_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.",
    "paper_id": "2512.14697v1"
  },
  "https://arxiv.org/abs/2512.10857v1": {
    "content_path": "../data/contents/2512.10857v1.md",
    "pdf_path": "../data/contents/2512.10857v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:01.486008",
    "title": "Generative Modeling from Black-box Corruptions via Self-Consistent Stochastic Interpolants",
    "abstract": "Summary: Transport-based methods have emerged as a leading paradigm for building generative models from large, clean datasets. However, in many scientific and engineering domains, clean data are often unavailable: instead, we only observe measurements corrupted through a noisy, ill-conditioned channel. A generative model for the original data thus requires solving an inverse problem at the level of distributions. In this work, we introduce a novel approach to this task based on Stochastic Interpolants: we iteratively update a transport map between corrupted and clean data samples using only access to the corrupted dataset as well as black box access to the corruption channel. Under appropriate conditions, this iterative procedure converges towards a self-consistent transport map that effectively inverts the corruption channel, thus enabling a generative model for the clean data. We refer to the resulting method as the self-consistent stochastic interpolant (SCSI). It (i) is computationally efficient compared to variational alternatives, (ii) highly flexible, handling arbitrary nonlinear forward models with only black-box access, and (iii) enjoys theoretical guarantees. We demonstrate superior performance on inverse problems in natural image processing and scientific reconstruction, and establish convergence guarantees of the scheme under appropriate assumptions.",
    "paper_id": "2512.10857v1"
  },
  "https://arxiv.org/abs/2512.20959v1": {
    "content_path": "../data/contents/2512.20959v1.md",
    "pdf_path": "../data/contents/2512.20959v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:01.849209",
    "title": "Can Agentic AI Match the Performance of Human Data Scientists?",
    "abstract": "Summary: Data science plays a critical role in transforming complex data into actionable insights across numerous domains. Recent developments in large language models (LLMs) have significantly automated data science workflows, but a fundamental question persists: Can these agentic AI systems truly match the performance of human data scientists who routinely leverage domain-specific knowledge? We explore this question by designing a prediction task where a crucial latent variable is hidden in relevant image data instead of tabular features. As a result, agentic AI that generates generic codes for modeling tabular data cannot perform well, while human experts could identify the important hidden variable using domain knowledge. We demonstrate this idea with a synthetic dataset for property insurance. Our experiments show that agentic AI that relies on generic analytics workflow falls short of methods that use domain-specific insights. This highlights a key limitation of the current agentic AI for data science and underscores the need for future research to develop agentic AI systems that can better recognize and incorporate domain knowledge.",
    "paper_id": "2512.20959v1"
  },
  "https://arxiv.org/abs/2512.18996v1": {
    "content_path": "../data/contents/2512.18996v1.md",
    "pdf_path": "../data/contents/2512.18996v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:02.040870",
    "title": "Modular Layout Synthesis (MLS): Front-end Code via Structure Normalization and Constrained Generation",
    "abstract": "Summary: Automated front-end engineering drastically reduces development cycles and minimizes manual coding overhead. While Generative AI has shown promise in translating designs to code, current solutions often produce monolithic scripts, failing to natively support modern ecosystems like React, Vue, or Angular. Furthermore, the generated code frequently suffers from poor modularity, making it difficult to maintain. To bridge this gap, we introduce Modular Layout Synthesis (MLS), a hierarchical framework that merges visual understanding with structural normalization. Initially, a visual-semantic encoder maps the screen capture into a serialized tree topology, capturing the essential layout hierarchy. Instead of simple parsing, we apply heuristic deduplication and pattern recognition to isolate reusable blocks, creating a framework-agnostic schema. Finally, a constraint-based generation protocol guides the LLM to synthesize production-ready code with strict typing and component props. Evaluations show that MLS significantly outperforms existing baselines, ensuring superior code reusability and structural integrity across multiple frameworks",
    "paper_id": "2512.18996v1"
  },
  "https://arxiv.org/abs/2512.14693v2": {
    "content_path": "../data/contents/2512.14693v2.md",
    "pdf_path": "../data/contents/2512.14693v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:02.393303",
    "title": "Universal Reasoning Model",
    "abstract": "Summary: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/UbiquantAI/URM.",
    "paper_id": "2512.14693v2"
  },
  "https://arxiv.org/abs/2512.10821v1": {
    "content_path": "../data/contents/2512.10821v1.md",
    "pdf_path": "../data/contents/2512.10821v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:04.892354",
    "title": "Agile Deliberation: Concept Deliberation for Subjective Visual Classification",
    "abstract": "Summary: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through \"concept deliberation\", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called \"Agile Deliberation\" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.",
    "paper_id": "2512.10821v1"
  },
  "https://arxiv.org/abs/2512.20957v1": {
    "content_path": "../data/contents/2512.20957v1.md",
    "pdf_path": "../data/contents/2512.20957v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:05.301653",
    "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
    "abstract": "Summary: Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
    "paper_id": "2512.20957v1"
  },
  "https://arxiv.org/abs/2512.18980v1": {
    "content_path": "../data/contents/2512.18980v1.md",
    "pdf_path": "../data/contents/2512.18980v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:05.952314",
    "title": "OPBO: Order-Preserving Bayesian Optimization",
    "abstract": "Summary: Bayesian optimization is an effective method for solving expensive black-box optimization problems. Most existing methods use Gaussian processes (GP) as the surrogate model for approximating the black-box objective function, it is well-known that it can fail in high-dimensional space (e.g., dimension over 500). We argue that the reliance of GP on precise numerical fitting is fundamentally ill-suited in high-dimensional space, where it leads to prohibitive computational complexity. In order to address this, we propose a simple order-preserving Bayesian optimization (OPBO) method, where the surrogate model preserves the order, instead of the value, of the black-box objective function. Then we can use a simple but effective OP neural network (NN) to replace GP as the surrogate model. Moreover, instead of searching for the best solution from the acquisition model, we select good-enough solutions in the ordinal set to reduce computational cost. The experimental results show that for high-dimensional (over 500) black-box optimization problems, the proposed OPBO significantly outperforms traditional BO methods based on regression NN and GP. The source code is available at https://github.com/pengwei222/OPBO.",
    "paper_id": "2512.18980v1"
  },
  "https://arxiv.org/abs/2512.14691v2": {
    "content_path": "../data/contents/2512.14691v2.md",
    "pdf_path": "../data/contents/2512.14691v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:19.734679",
    "title": "MMGR: Multi-Modal Generative Reasoning",
    "abstract": "Summary: Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.",
    "paper_id": "2512.14691v2"
  },
  "https://arxiv.org/abs/2512.10817v1": {
    "content_path": "../data/contents/2512.10817v1.md",
    "pdf_path": "../data/contents/2512.10817v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:25.781995",
    "title": "Extrapolation of Periodic Functions Using Binary Encoding of Continuous Numerical Values",
    "abstract": "Summary: We report the discovery that binary encoding allows neural networks to extrapolate periodic functions beyond their training bounds. We introduce Normalized Base-2 Encoding (NB2E) as a method for encoding continuous numerical values and demonstrate that, using this input encoding, vanilla multi-layer perceptrons (MLP) successfully extrapolate diverse periodic signals without prior knowledge of their functional form. Internal activation analysis reveals that NB2E induces bit-phase representations, enabling MLPs to learn and extrapolate signal structure independently of position.",
    "paper_id": "2512.10817v1"
  },
  "https://arxiv.org/abs/2512.20954v1": {
    "content_path": "../data/contents/2512.20954v1.md",
    "pdf_path": "../data/contents/2512.20954v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:26.176362",
    "title": "Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models",
    "abstract": "Summary: Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary \"thinking tokens\" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.",
    "paper_id": "2512.20954v1"
  },
  "https://arxiv.org/abs/2512.18978v1": {
    "content_path": "../data/contents/2512.18978v1.md",
    "pdf_path": "../data/contents/2512.18978v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:27.086628",
    "title": "Outlier detection in mixed-attribute data: a semi-supervised approach with fuzzy approximations and relative entropy",
    "abstract": "Summary: Outlier detection is a critical task in data mining, aimed at identifying objects that significantly deviate from the norm. Semi-supervised methods improve detection performance by leveraging partially labeled data but typically overlook the uncertainty and heterogeneity of real-world mixed-attribute data. This paper introduces a semi-supervised outlier detection method, namely fuzzy rough sets-based outlier detection (FROD), to effectively handle these challenges. Specifically, we first utilize a small subset of labeled data to construct fuzzy decision systems, through which we introduce the attribute classification accuracy based on fuzzy approximations to evaluate the contribution of attribute sets in outlier detection. Unlabeled data is then used to compute fuzzy relative entropy, which provides a characterization of outliers from the perspective of uncertainty. Finally, we develop the detection algorithm by combining attribute classification accuracy with fuzzy relative entropy. Experimental results on 16 public datasets show that FROD is comparable with or better than leading detection algorithms. All datasets and source codes are accessible at https://github.com/ChenBaiyang/FROD. This manuscript is the accepted author version of a paper published by Elsevier. The final published version is available at https://doi.org/10.1016/j.ijar.2025.109373",
    "paper_id": "2512.18978v1"
  },
  "https://arxiv.org/abs/2512.14687v2": {
    "content_path": "../data/contents/2512.14687v2.md",
    "pdf_path": "../data/contents/2512.14687v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:27.893513",
    "title": "Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization",
    "abstract": "Summary: Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. We release an online demo at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/, with plans to release the full dataset in the near future. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.",
    "paper_id": "2512.14687v2"
  },
  "https://arxiv.org/abs/2512.11013v1": {
    "content_path": "../data/contents/2512.11013v1.md",
    "pdf_path": "../data/contents/2512.11013v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:28.310494",
    "title": "PIAST: Rapid Prompting with In-context Augmentation for Scarce Training data",
    "abstract": "Summary: LLMs are highly sensitive to prompt design, but handcrafting effective prompts is difficult and often requires intricate crafting of few-shot examples. We propose a fast automatic prompt construction algorithm that augments human instructions by generating a small set of few shot examples. Our method iteratively replaces/drops/keeps few-shot examples using Monte Carlo Shapley estimation of example utility. For faster execution, we use aggressive subsampling and a replay buffer for faster evaluations. Our method can be run using different compute time budgets. On a limited budget, we outperform existing automatic prompting methods on text simplification and GSM8K and obtain second best results on classification and summarization. With an extended, but still modest compute budget we set a new state of the art among automatic prompting methods on classification, simplification and GSM8K. Our results show that carefully constructed examples, rather than exhaustive instruction search, are the dominant lever for fast and data efficient prompt engineering. Our code is available at https://github.com/Batorskq/PIAST.",
    "paper_id": "2512.11013v1"
  },
  "https://arxiv.org/abs/2512.20953v1": {
    "content_path": "../data/contents/2512.20953v1.md",
    "pdf_path": "../data/contents/2512.20953v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:29.608930",
    "title": "Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications",
    "abstract": "Summary: The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D parallelism in such environments, addressing critical issues such as the need for symmetric tensor parallelism, efficient gradient synchronization in asymmetric pipeline parallelism, and the trade-offs between memory utilization and computational efficiency. Building upon these insights, we introduce AutoHet, a novel system that automatically identifies the optimal parallelism plan for distributed training on heterogeneous GPUs. AutoHet supports asymmetric 3D parallelism structures and facilitates fine-grained workload distribution. We propose a theoretical model that frames the device grouping and load balancing as an optimization problem to minimize per-iteration training time, thus effectively balancing computing power and memory usage across GPUs with diverse capabilities. To enable elastic training upon spot instance preemption, AutoHet presents an efficient recovery strategy that prioritizes to retrieve training states from local nodes, and only downloads the missing checkpoints from the cloud storage. Our extensive evaluation, conducted on three large-scale models and utilizing combinations of three different GPU types, demonstrates that AutoHet outperforms existing DNN training systems, achieving up to a 1.79$\\times$ speedup in training throughput compared with Megatron-LM and Whale, and a 4.38$\\times$ speedup of recovery speed compared to a spot instance baseline.",
    "paper_id": "2512.20953v1"
  },
  "https://arxiv.org/abs/2512.18965v1": {
    "content_path": "../data/contents/2512.18965v1.md",
    "pdf_path": "../data/contents/2512.18965v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:30.140643",
    "title": "Lag Operator SSMs: A Geometric Framework for Structured State Space Modeling",
    "abstract": "Summary: Structured State Space Models (SSMs), which are at the heart of the recently popular Mamba architecture, are powerful tools for sequence modeling. However, their theoretical foundation relies on a complex, multi-stage process of continuous-time modeling and subsequent discretization, which can obscure intuition. We introduce a direct, first-principles framework for constructing discrete-time SSMs that is both flexible and modular. Our approach is based on a novel lag operator, which geometrically derives the discrete-time recurrence by measuring how the system's basis functions \"slide\" and change from one timestep to the next. The resulting state matrices are computed via a single inner product involving this operator, offering a modular design space for creating novel SSMs by flexibly combining different basis functions and time-warping schemes. To validate our approach, we demonstrate that a specific instance exactly recovers the recurrence of the influential HiPPO model. Numerical simulations confirm our derivation, providing new theoretical tools for designing flexible and robust sequence models.",
    "paper_id": "2512.18965v1"
  },
  "https://arxiv.org/abs/2512.14686v1": {
    "content_path": "../data/contents/2512.14686v1.md",
    "pdf_path": "../data/contents/2512.14686v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:32.463598",
    "title": "Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean",
    "abstract": "Summary: Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $α$ of the noise. Nonetheless, existing complexity results often cover only the case $α\\in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $α$ approaches $1$. This paper tackles the general case of noise with tail index $α\\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $α\\in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.",
    "paper_id": "2512.14686v1"
  },
  "https://arxiv.org/abs/2512.10805v1": {
    "content_path": "../data/contents/2512.10805v1.md",
    "pdf_path": "../data/contents/2512.10805v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:34.059932",
    "title": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders",
    "abstract": "Summary: Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.",
    "paper_id": "2512.10805v1"
  },
  "https://arxiv.org/abs/2512.20950v1": {
    "content_path": "../data/contents/2512.20950v1.md",
    "pdf_path": "../data/contents/2512.20950v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:34.347694",
    "title": "MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment",
    "abstract": "Summary: This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.",
    "paper_id": "2512.20950v1"
  },
  "https://arxiv.org/abs/2512.18956v1": {
    "content_path": "../data/contents/2512.18956v1.md",
    "pdf_path": "../data/contents/2512.18956v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:37.934123",
    "title": "Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection",
    "abstract": "Summary: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.",
    "paper_id": "2512.18956v1"
  },
  "https://arxiv.org/abs/2512.14806v4": {
    "content_path": "../data/contents/2512.14806v4.md",
    "pdf_path": "../data/contents/2512.14806v4.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:38.527803",
    "title": "Let the Barbarians In: How AI Can Accelerate Systems Performance Research",
    "abstract": "Summary: Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight.\n  Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.",
    "paper_id": "2512.14806v4"
  },
  "https://arxiv.org/abs/2512.10794v1": {
    "content_path": "../data/contents/2512.10794v1.md",
    "pdf_path": "../data/contents/2512.10794v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:55.037427",
    "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
    "abstract": "Summary: Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \\textit{global} \\revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \\emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",
    "paper_id": "2512.10794v1"
  },
  "https://arxiv.org/abs/2512.20949v1": {
    "content_path": "../data/contents/2512.20949v1.md",
    "pdf_path": "../data/contents/2512.20949v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:55.401269",
    "title": "Neural Probe-Based Hallucination Detection for Large Language Models",
    "abstract": "Summary: Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.",
    "paper_id": "2512.20949v1"
  },
  "https://arxiv.org/abs/2512.18951v1": {
    "content_path": "../data/contents/2512.18951v1.md",
    "pdf_path": "../data/contents/2512.18951v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:56.657706",
    "title": "Learning Through Little Eyes: Attribute Discrimination Beyond Objects",
    "abstract": "Summary: Infants learn to recognize not only object categories but also fine grained attributes such as color, size, and texture within their first two years of life. Prior work explores Childs View for Contrastive Learning (CVCL), a CLIP style model trained on infant egocentric video as a computational model of early infant learning, but it focuses only on class level recognition. This leaves it unclear whether infant scale learning also supports attribute discrimination. To address this, we introduce a benchmark that systematically varies color, size, and texture, allowing controlled tests of within class attribute recognition. Comparing CVCL with CLIP shows clear differences. CVCL is better at size discrimination, while CLIP achieves higher accuracy on color discrimination. Both models represent texture in image embeddings but fail to ground texture linguistically, suggesting a gap between visual and language spaces.",
    "paper_id": "2512.18951v1"
  },
  "https://arxiv.org/abs/2512.14681v1": {
    "content_path": "../data/contents/2512.14681v1.md",
    "pdf_path": "../data/contents/2512.14681v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:57.510588",
    "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
    "abstract": "Summary: Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
    "paper_id": "2512.14681v1"
  },
  "https://arxiv.org/abs/2512.10793v1": {
    "content_path": "../data/contents/2512.10793v1.md",
    "pdf_path": "../data/contents/2512.10793v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:57.961134",
    "title": "LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification",
    "abstract": "Summary: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.",
    "paper_id": "2512.10793v1"
  },
  "https://arxiv.org/abs/2512.20948v1": {
    "content_path": "../data/contents/2512.20948v1.md",
    "pdf_path": "../data/contents/2512.20948v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:59.099560",
    "title": "Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study",
    "abstract": "Summary: Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.",
    "paper_id": "2512.20948v1"
  },
  "https://arxiv.org/abs/2512.18950v1": {
    "content_path": "../data/contents/2512.18950v1.md",
    "pdf_path": "../data/contents/2512.18950v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:59.441075",
    "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
    "abstract": "Summary: We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.",
    "paper_id": "2512.18950v1"
  },
  "https://arxiv.org/abs/2512.14805v1": {
    "content_path": "../data/contents/2512.14805v1.md",
    "pdf_path": "../data/contents/2512.14805v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:30:59.921056",
    "title": "Sharing State Between Prompts and Programs",
    "abstract": "Summary: The rise of large language models (LLMs) has introduced a new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language -- natural language code -- for the LLM to execute.\n  An emerging area of research enables interoperability between natural language code and formal languages such as Python. We present a novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. We present a schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as a natural function interface.\n  We implement shared program state in the Nightjar programming system. Nightjar enables programmers to write Python programs that contain natural code that shares the Python program state. We show that Nightjar programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using Nightjar is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations).",
    "paper_id": "2512.14805v1"
  },
  "https://arxiv.org/abs/2512.10791v1": {
    "content_path": "../data/contents/2512.10791v1.md",
    "pdf_path": "../data/contents/2512.10791v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:01.560522",
    "title": "The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality",
    "abstract": "Summary: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .",
    "paper_id": "2512.10791v1"
  },
  "https://arxiv.org/abs/2512.20938v1": {
    "content_path": "../data/contents/2512.20938v1.md",
    "pdf_path": "../data/contents/2512.20938v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:02.326519",
    "title": "Pioneering Multimodal Emotion Recognition in the Era of Large Models: From Closed Sets to Open Vocabularies",
    "abstract": "Summary: Recent advances in multimodal large language models (MLLMs) have demonstrated remarkable multi- and cross-modal integration capabilities. However, their potential for fine-grained emotion understanding remains systematically underexplored. While open-vocabulary multimodal emotion recognition (MER-OV) has emerged as a promising direction to overcome the limitations of closed emotion sets, no comprehensive evaluation of MLLMs in this context currently exists. To address this, our work presents the first large-scale benchmarking study of MER-OV on the OV-MERD dataset, evaluating 19 mainstream MLLMs, including general-purpose, modality-specialized, and reasoning-enhanced architectures. Through systematic analysis of model reasoning capacity, fusion strategies, contextual utilization, and prompt design, we provide key insights into the capabilities and limitations of current MLLMs for MER-OV. Our evaluation reveals that a two-stage, trimodal (audio, video, and text) fusion approach achieves optimal performance in MER-OV, with video emerging as the most critical modality. We further identify a surprisingly narrow gap between open- and closed-source LLMs. These findings establish essential benchmarks and offer practical guidelines for advancing open-vocabulary and fine-grained affective computing, paving the way for more nuanced and interpretable emotion AI systems. Associated code will be made publicly available upon acceptance.",
    "paper_id": "2512.20938v1"
  },
  "https://arxiv.org/abs/2512.18940v1": {
    "content_path": "../data/contents/2512.18940v1.md",
    "pdf_path": "../data/contents/2512.18940v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:02.558923",
    "title": "FASTRIC: Prompt Specification Language for Verifiable LLM Interactions",
    "abstract": "Summary: Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.",
    "paper_id": "2512.18940v1"
  },
  "https://arxiv.org/abs/2512.14652v1": {
    "content_path": "../data/contents/2512.14652v1.md",
    "pdf_path": "../data/contents/2512.14652v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:03.357705",
    "title": "Segmental Attention Decoding With Long Form Acoustic Encodings",
    "abstract": "Summary: We address the fundamental incompatibility of attention-based encoder-decoder (AED) models with long-form acoustic encodings. AED models trained on segmented utterances learn to encode absolute frame positions by exploiting limited acoustic context beyond segment boundaries, but fail to generalize when decoding long-form segments where these cues vanish. The model loses ability to order acoustic encodings due to permutation invariance of keys and values in cross-attention. We propose four modifications: (1) injecting explicit absolute positional encodings into cross-attention for each decoded segment, (2) long-form training with extended acoustic context to eliminate implicit absolute position encoding, (3) segment concatenation to cover diverse segmentations needed during training, and (4) semantic segmentation to align AED-decoded segments with training segments. We show these modifications close the accuracy gap between continuous and segmented acoustic encodings, enabling auto-regressive use of the attention decoder.",
    "paper_id": "2512.14652v1"
  },
  "https://arxiv.org/abs/2512.10789v1": {
    "content_path": "../data/contents/2512.10789v1.md",
    "pdf_path": "../data/contents/2512.10789v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:04.956742",
    "title": "Natural Language Interface for Firewall Configuration",
    "abstract": "Summary: This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.",
    "paper_id": "2512.10789v1"
  },
  "https://arxiv.org/abs/2512.20934v1": {
    "content_path": "../data/contents/2512.20934v1.md",
    "pdf_path": "../data/contents/2512.20934v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:05.428109",
    "title": "Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning",
    "abstract": "Summary: Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.",
    "paper_id": "2512.20934v1"
  },
  "https://arxiv.org/abs/2512.18934v1": {
    "content_path": "../data/contents/2512.18934v1.md",
    "pdf_path": "../data/contents/2512.18934v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:05.783874",
    "title": "When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models",
    "abstract": "Summary: Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.",
    "paper_id": "2512.18934v1"
  },
  "https://arxiv.org/abs/2512.14645v1": {
    "content_path": "../data/contents/2512.14645v1.md",
    "pdf_path": "../data/contents/2512.14645v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:07.644172",
    "title": "TiME: Tiny Monolingual Encoders for Efficient NLP Pipelines",
    "abstract": "Summary: Today, a lot of research on language models is focused on large, general-purpose models. However, many NLP pipelines only require models with a well-defined, small set of capabilities. While large models are capable of performing the tasks of those smaller models, they are simply not fast enough to process large amounts of data or offer real-time responses. Furthermore, they often use unnecessarily large amounts of energy, leading to sustainability concerns and problems when deploying them on battery-powered devices. In our work, we show how to train small models for such efficiency-critical applications. As opposed to many off-the-shelf NLP pipelines, our models use modern training techniques such as distillation, and offer support for low-resource languages. We call our models TiME (Tiny Monolingual Encoders) and comprehensively evaluate them on a range of common NLP tasks, observing an improved trade-off between benchmark performance on one hand, and throughput, latency and energy consumption on the other. Along the way, we show that distilling monolingual models from multilingual teachers is possible, and likewise distilling models with absolute positional embeddings from teachers with relative positional embeddings.",
    "paper_id": "2512.14645v1"
  },
  "https://arxiv.org/abs/2512.10787v2": {
    "content_path": "../data/contents/2512.10787v2.md",
    "pdf_path": "../data/contents/2512.10787v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:08.386752",
    "title": "Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly",
    "abstract": "Summary: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \\textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \\textbf{context dilution}, where distractors crowd out relevant information. We propose \\textbf{SEAL-RAG}, a training-free controller that adopts a \\textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\\textbf{S}earch $\\rightarrow$ \\textbf{E}xtract $\\rightarrow$ \\textbf{A}ssess $\\rightarrow$ \\textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \\textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \\textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \\textbf{HotpotQA} and \\textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \\textbf{+3--13 pp} and evidence precision by \\textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \\textbf{+8.0 pp} in accuracy and maintains \\textbf{96\\%} evidence precision compared to 22\\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.",
    "paper_id": "2512.10787v2"
  },
  "https://arxiv.org/abs/2512.20932v1": {
    "content_path": "../data/contents/2512.20932v1.md",
    "pdf_path": "../data/contents/2512.20932v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:08.630124",
    "title": "Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy",
    "abstract": "Summary: This paper presents a marketing analytics framework that operationalizes subscription pricing as a dynamic, guardrailed decision system, uniting multivariate demand forecasting, segment-level price elasticity, and churn propensity to optimize revenue, margin, and retention. The approach blends seasonal time-series models with tree-based learners, runs Monte Carlo scenario tests to map risk envelopes, and solves a constrained optimization that enforces business guardrails on customer experience, margin floors, and allowable churn. Validated across heterogeneous SaaS portfolios, the method consistently outperforms static tiers and uniform uplifts by reallocating price moves toward segments with higher willingness-to-pay while protecting price-sensitive cohorts. The system is designed for real-time recalibration via modular APIs and includes model explainability for governance and compliance. Managerially, the framework functions as a strategy playbook that clarifies when to shift from flat to dynamic pricing, how to align pricing with CLV and MRR targets, and how to embed ethical guardrails, enabling durable growth without eroding customer trust.",
    "paper_id": "2512.20932v1"
  },
  "https://arxiv.org/abs/2512.18930v1": {
    "content_path": "../data/contents/2512.18930v1.md",
    "pdf_path": "../data/contents/2512.18930v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:18.359592",
    "title": "LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer",
    "abstract": "Summary: Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent embeddings of generative image models. Trained on artistic data, our SAE learns an emergent, largely disentangled set of stylistic and compositional concepts, corresponding to style-related elements pertaining brushwork, texture, and color palette, as well as semantic and structural concepts. We call it LouvreSAE and use it to construct style profiles: compact, decomposable steering vectors that enable style transfer without any model updates or optimization. Unlike prior concept-based style transfer methods, our method requires no fine-tuning, no LoRA training, and no additional inference passes, enabling direct steering of artistic styles from only a few reference images. We validate our method on ArtBench10, achieving or surpassing existing methods on style evaluations (VGG Style Loss and CLIP Score Style) while being 1.7-20x faster and, critically, interpretable.",
    "paper_id": "2512.18930v1"
  },
  "https://arxiv.org/abs/2512.19721v1": {
    "content_path": "../data/contents/2512.19721v1.md",
    "pdf_path": "../data/contents/2512.19721v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:18.829508",
    "title": "Sign-Aware Multistate Jaccard Kernels and Geometry for Real and Complex-Valued Signals",
    "abstract": "Summary: We introduce a sign-aware, multistate Jaccard/Tanimoto framework that extends overlap-based distances from nonnegative vectors and measures to arbitrary real- and complex-valued signals while retaining bounded metric and positive-semidefinite kernel structure. Formally, the construction is a set- and measure-theoretic geometry: signals are represented as atomic measures on a signed state space, and similarity is given by a generalized Jaccard overlap of these measures. Each signal is embedded into a nonnegative multistate representation, using positive/negative splits for real signals, Cartesian and polar decompositions for complex signals, and user-defined state partitions for refined regime analysis. Applying the Tanimoto construction to these embeddings yields a family of $[0,1]$ distances that satisfy the triangle inequality and define positive-semidefinite kernels usable directly in kernel methods and graph-based learning. Beyond pairwise distances, we develop coalition analysis via Möbius inversion, which decomposes signal magnitude into nonnegative, additive contributions with exact budget closure across coalitions of signals. Normalizing the same embeddings produces probability measures on coordinate -- state configurations, so that the distance becomes a monotone transform of total variation and admits a regime -- intensity decomposition. The resulting construction yields a single, mechanistically interpretable distance that simultaneously provides bounded metric structure, positive-semidefinite kernels, probabilistic semantics, and transparent budget accounting within one sign-aware framework, supporting correlograms, feature engineering, similarity graphs, and other analytical tools in scientific and financial applications.",
    "paper_id": "2512.19721v1"
  },
  "https://arxiv.org/abs/2512.10785v1": {
    "content_path": "../data/contents/2512.10785v1.md",
    "pdf_path": "../data/contents/2512.10785v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:22.534005",
    "title": "Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving",
    "abstract": "Summary: Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future.",
    "paper_id": "2512.10785v1"
  },
  "https://arxiv.org/abs/2512.20924v1": {
    "content_path": "../data/contents/2512.20924v1.md",
    "pdf_path": "../data/contents/2512.20924v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:23.004559",
    "title": "Clever Hans in Chemistry: Chemist Style Signals Confound Activity Prediction on Public Benchmarks",
    "abstract": "Summary: Can machine learning models identify which chemist made a molecule from structure alone? If so, models trained on literature data may exploit chemist intent rather than learning causal structure-activity relationships. We test this by linking CHEMBL assays to publication authors and training a 1,815-class classifier to predict authors from molecular fingerprints, achieving 60% top-5 accuracy under scaffold-based splitting. We then train an activity model that receives only a protein identifier and an author-probability vector derived from structure, with no direct access to molecular descriptors. This author-only model achieves predictive power comparable to a simple baseline that has access to structure. This reveals a \"Clever Hans\" failure mode: models can predict bioactivity largely by inferring chemist goals and favorite targets without requiring a lab-independent understanding of chemistry. We analyze the sources of this leakage, propose author-disjoint splits, and recommend dataset practices to decouple chemist intent from biological outcomes.",
    "paper_id": "2512.20924v1"
  },
  "https://arxiv.org/abs/2512.18925v1": {
    "content_path": "../data/contents/2512.18925v1.md",
    "pdf_path": "../data/contents/2512.18925v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:23.310696",
    "title": "An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects",
    "abstract": "Summary: While Large Language Models (LLMs) have demonstrated remarkable capabilities, research shows that their effectiveness depends not only on explicit prompts but also on the broader context provided. This requirement is especially pronounced in software engineering, where the goals, architecture, and collaborative conventions of an existing project play critical roles in response quality. To support this, many AI coding assistants have introduced ways for developers to author persistent, machine-readable directives that encode a project's unique constraints. Although this practice is growing, the content of these directives remains unstudied.\n  This paper presents a large-scale empirical study to characterize this emerging form of developer-provided context. Through a qualitative analysis of 401 open-source repositories containing cursor rules, we developed a comprehensive taxonomy of project context that developers consider essential, organized into five high-level themes: Conventions, Guidelines, Project Information, LLM Directives, and Examples. Our study also explores how this context varies across different project types and programming languages, offering implications for the next generation of context-aware AI developer tools.",
    "paper_id": "2512.18925v1"
  },
  "https://arxiv.org/abs/2512.14801v1": {
    "content_path": "../data/contents/2512.14801v1.md",
    "pdf_path": "../data/contents/2512.14801v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:23.918853",
    "title": "Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis",
    "abstract": "Summary: OpenAI has recently argued that hallucinations in large language models result primarily from misaligned evaluation incentives that reward confident guessing rather than epistemic humility. On this view, hallucination is a contingent behavioral artifact, remediable through improved benchmarks and reward structures. In this paper, we challenge that interpretation. Drawing on previous work on structural hallucination and empirical experiments using a Licensing Oracle, we argue that hallucination is not an optimization failure but an architectural inevitability of the transformer model.\n  Transformers do not represent the world; they model statistical associations among tokens. Their embedding spaces form a pseudo-ontology derived from linguistic co-occurrence rather than world-referential structure. At ontological boundary conditions - regions where training data is sparse or incoherent - the model necessarily interpolates fictional continuations in order to preserve coherence. No incentive mechanism can modify this structural dependence on pattern completion.\n  Our empirical results demonstrate that hallucination can only be eliminated through external truth-validation and abstention modules, not through changes to incentives, prompting, or fine-tuning. The Licensing Oracle achieves perfect abstention precision across domains precisely because it supplies grounding that the transformer lacks.\n  We conclude that hallucination is a structural property of generative architectures and that reliable AI requires hybrid systems that distinguish linguistic fluency from epistemic responsibility.",
    "paper_id": "2512.14801v1"
  },
  "https://arxiv.org/abs/2512.10780v1": {
    "content_path": "../data/contents/2512.10780v1.md",
    "pdf_path": "../data/contents/2512.10780v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:24.447805",
    "title": "Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting",
    "abstract": "Summary: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.",
    "paper_id": "2512.10780v1"
  },
  "https://arxiv.org/abs/2512.20920v1": {
    "content_path": "../data/contents/2512.20920v1.md",
    "pdf_path": "../data/contents/2512.20920v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:24.725354",
    "title": "RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks",
    "abstract": "Summary: Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU.",
    "paper_id": "2512.20920v1"
  },
  "https://arxiv.org/abs/2512.18920v1": {
    "content_path": "../data/contents/2512.18920v1.md",
    "pdf_path": "../data/contents/2512.18920v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:30.584328",
    "title": "Narrative Scaffolding: Transforming Data-Driven Sensemaking Through Narrative-First Exploration",
    "abstract": "Summary: When exploring data, analysts construct narratives about what the data means by asking questions, generating visualizations, reflecting on patterns, and revising their interpretations as new insights emerge. Yet existing analysis tools treat narrative as an afterthought, breaking the link between reasoning, reflection, and the evolving story from exploration. Consequently, analysts lose the ability to see how their reasoning evolves, making it harder to reflect systematically or build coherent explanations. To address this gap, we propose Narrative Scaffolding, a framework for narrative-driven exploration that positions narrative construction as the primary interface for exploration and reasoning. We implement this framework in a system that externalizes iterative reasoning through narrative-first entry, semantically aligned view generation, and reflection support via insight provenance and inquiry tracking. In a within-subject study N=20, we demonstrate that narrative scaffolding facilitates broader exploration, deeper reflection, and more defensible narratives. An evaluation with visualization literacy experts (N = 6) confirmed that the system produced outputs aligned with narrative intent and facilitated intentional exploration.",
    "paper_id": "2512.18920v1"
  },
  "https://arxiv.org/abs/2512.14622v1": {
    "content_path": "../data/contents/2512.14622v1.md",
    "pdf_path": "../data/contents/2512.14622v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:31.582182",
    "title": "Beyond Text-to-SQL: Autonomous Research-Driven Database Exploration with DAR",
    "abstract": "Summary: Large language models can already query databases, yet most existing systems remain reactive: they rely on explicit user prompts and do not actively explore data. We introduce DAR (Data Agnostic Researcher), a multi-agent system that performs end-to-end database research without human-initiated queries. DAR orchestrates specialized AI agents across three layers: initialization (intent inference and metadata extraction), execution (SQL and AI-based query synthesis with iterative validation), and synthesis (report generation with built-in quality control). All reasoning is executed directly inside BigQuery using native generative AI functions, eliminating data movement and preserving data governance. On a realistic asset-incident dataset, DAR completes the full analytical task in 16 minutes, compared to 8.5 hours for a professional analyst (approximately 32x times faster), while producing useful pattern-based insights and evidence-grounded recommendations. Although human experts continue to offer deeper contextual interpretation, DAR excels at rapid exploratory analysis. Overall, this work shifts database interaction from query-driven assistance toward autonomous, research-driven exploration within cloud data warehouses.",
    "paper_id": "2512.14622v1"
  },
  "https://arxiv.org/abs/2512.10772v1": {
    "content_path": "../data/contents/2512.10772v1.md",
    "pdf_path": "../data/contents/2512.10772v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:32.219579",
    "title": "Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation",
    "abstract": "Summary: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.",
    "paper_id": "2512.10772v1"
  },
  "https://arxiv.org/abs/2512.20916v1": {
    "content_path": "../data/contents/2512.20916v1.md",
    "pdf_path": "../data/contents/2512.20916v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:32.517855",
    "title": "MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model",
    "abstract": "Summary: Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant potential in recommendation systems. However, the effective application of MLLMs to multimodal sequential recommendation remains unexplored: A) Existing methods primarily leverage the multimodal semantic understanding capabilities of pre-trained MLLMs to generate item embeddings or semantic IDs, thereby enhancing traditional recommendation models. These approaches generate item representations that exhibit limited interpretability, and pose challenges when transferring to language model-based recommendation systems. B) Other approaches convert user behavior sequence into image-text pairs and perform recommendation through multiple MLLM inference, incurring prohibitive computational and time costs. C) Current MLLM-based recommendation systems generally neglect the integration of collaborative signals. To address these limitations while balancing recommendation performance, interpretability, and computational cost, this paper proposes MultiModal Summarization-and-Retrieval-Augmented Sequential Recommendation. Specifically, we first employ MLLM to summarize items into concise keywords and fine-tune the model using rewards that incorporate summary length, information loss, and reconstruction difficulty, thereby enabling adaptive adjustment of the summarization policy. Inspired by retrieval-augmented generation, we then transform collaborative signals into corresponding keywords and integrate them as supplementary context. Finally, we apply supervised fine-tuning with multi-task learning to align the MLLM with the multimodal sequential recommendation. Extensive evaluations on common recommendation datasets demonstrate the effectiveness of MMSRARec, showcasing its capability to efficiently and interpretably understand user behavior histories and item information for accurate recommendations.",
    "paper_id": "2512.20916v1"
  },
  "https://arxiv.org/abs/2512.18918v1": {
    "content_path": "../data/contents/2512.18918v1.md",
    "pdf_path": "../data/contents/2512.18918v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:34.487107",
    "title": "Needles in a haystack: using forensic network science to uncover insider trading",
    "abstract": "Summary: Although the automation and digitisation of anti-financial crime investigation has made significant progress in recent years, detecting insider trading remains a unique challenge, partly due to the limited availability of labelled data. To address this challenge, we propose using a data-driven networks approach that flags groups of corporate insiders who report coordinated transactions that are indicative of insider trading. Specifically, we leverage data on 2.9 million trades reported to the U.S. Securities and Exchange Commission (SEC) by company insiders (C-suite executives, board members and major shareholders) between 2014 and 2024. Our proposed algorithm constructs weighted edges between insiders based on the temporal similarity of their trades over the 10-year timeframe. Within this network we then uncover trends that indicate insider trading by focusing on central nodes and anomalous subgraphs. To highlight the validity of our approach we evaluate our findings with reference to two null models, generated by running our algorithm on synthetic empirically calibrated and shuffled datasets. The results indicate that our approach can be used to detect pairs or clusters of insiders whose behaviour suggests insider trading and/or market manipulation.",
    "paper_id": "2512.18918v1"
  },
  "https://arxiv.org/abs/2512.14620v1": {
    "content_path": "../data/contents/2512.14620v1.md",
    "pdf_path": "../data/contents/2512.14620v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:37.795554",
    "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction",
    "abstract": "Summary: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.",
    "paper_id": "2512.14620v1"
  },
  "https://arxiv.org/abs/2512.10758v1": {
    "content_path": "../data/contents/2512.10758v1.md",
    "pdf_path": "../data/contents/2512.10758v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:38.185746",
    "title": "Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework",
    "abstract": "Summary: The rapid adoption of generative AI has undermined traditional modular assessments in computing education, creating a disconnect between academic evaluation and industry practice. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and multi-year empirical validation.\n  We make three contributions. First, we establish two theoretical results: (1) assessments composed of interconnected problems, where outputs feed into subsequent stages, are more AI-resilient than modular assessments because current language models struggle with sustained multi-step reasoning and context; and (2) semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution patterns. These results challenge common policy and institutional guidance that promotes open-ended assessments as the primary safeguard for academic integrity.\n  Second, we validate these results using data from four university data science courses (N = 138). While students achieve near-perfect scores on AI-assisted modular homework, performance drops by roughly 30 percentage points on proctored exams, indicating substantial AI score inflation. Interconnected projects remain strongly correlated with modular assessments, suggesting they measure the same underlying skills while resisting AI misuse. Proctored exams show weaker alignment, implying they may assess test-taking ability rather than intended learning outcomes.\n  Third, we translate these findings into a practical assessment design framework. The proposed approach enables educators to create assessments that promote integrative thinking, reflect real-world AI-augmented workflows, and naturally resist trivial delegation to generative AI, thereby helping restore academic integrity.",
    "paper_id": "2512.10758v1"
  },
  "https://arxiv.org/abs/2512.20914v1": {
    "content_path": "../data/contents/2512.20914v1.md",
    "pdf_path": "../data/contents/2512.20914v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:38.646944",
    "title": "Invariant Feature Extraction Through Conditional Independence and the Optimal Transport Barycenter Problem: the Gaussian case",
    "abstract": "Summary: A methodology is developed to extract $d$ invariant features $W=f(X)$ that predict a response variable $Y$ without being confounded by variables $Z$ that may influence both $X$ and $Y$.\n  The methodology's main ingredient is the penalization of any statistical dependence between $W$ and $Z$ conditioned on $Y$, replaced by the more readily implementable plain independence between $W$ and the random variable $Z_Y = T(Z,Y)$ that solves the [Monge] Optimal Transport Barycenter Problem for $Z\\mid Y$. In the Gaussian case considered in this article, the two statements are equivalent.\n  When the true confounders $Z$ are unknown, other measurable contextual variables $S$ can be used as surrogates, a replacement that involves no relaxation in the Gaussian case if the covariance matrix $Σ_{ZS}$ has full range. The resulting linear feature extractor adopts a closed form in terms of the first $d$ eigenvectors of a known matrix. The procedure extends with little change to more general, non-Gaussian / non-linear cases.",
    "paper_id": "2512.20914v1"
  },
  "https://arxiv.org/abs/2512.18915v1": {
    "content_path": "../data/contents/2512.18915v1.md",
    "pdf_path": "../data/contents/2512.18915v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:39.178965",
    "title": "QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits",
    "abstract": "Summary: As computation shifts from the cloud to the edge to reduce processing latency and network traffic, the resulting Computing Continuum (CC) creates a dynamic environment where it is challenging to meet strict Quality of Service (QoS) requirements and avoid service instance overload. Existing methods often prioritize global metrics, overlooking per-client QoS, which is crucial for latency-sensitive and reliability-critical applications. We propose QEdgeProxy, a decentralized QoS-aware load balancer that acts as a proxy between IoT devices and service instances in CC. We formulate the load balancing problem as a Multi-Player Multi-Armed Bandit (MP-MAB) with heterogeneous rewards, where each load balancer autonomously selects service instances that maximize the probability of meeting its clients' QoS targets by using Kernel Density Estimation (KDE) to estimate QoS success probabilities. It also incorporates an adaptive exploration mechanism to recover rapidly from performance shifts and non-stationary conditions. We present a Kubernetes-native QEdgeProxy implementation and evaluate it on an emulated CC testbed deployed on a K3s cluster with realistic network conditions and a latency-sensitive edge-AI workload. Results show that QEdgeProxy significantly outperforms proximity-based and reinforcement-learning baselines in per-client QoS satisfaction, while adapting effectively to load surges and instance availability changes.",
    "paper_id": "2512.18915v1"
  },
  "https://arxiv.org/abs/2512.14619v1": {
    "content_path": "../data/contents/2512.14619v1.md",
    "pdf_path": "../data/contents/2512.14619v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:39.646529",
    "title": "ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning",
    "abstract": "Summary: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.",
    "paper_id": "2512.14619v1"
  },
  "https://arxiv.org/abs/2512.10756v1": {
    "content_path": "../data/contents/2512.10756v1.md",
    "pdf_path": "../data/contents/2512.10756v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:41.053706",
    "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
    "abstract": "Summary: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
    "paper_id": "2512.10756v1"
  },
  "https://arxiv.org/abs/2512.20908v1": {
    "content_path": "../data/contents/2512.20908v1.md",
    "pdf_path": "../data/contents/2512.20908v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:42.874607",
    "title": "Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation",
    "abstract": "Summary: Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.",
    "paper_id": "2512.20908v1"
  },
  "https://arxiv.org/abs/2512.18908v1": {
    "content_path": "../data/contents/2512.18908v1.md",
    "pdf_path": "../data/contents/2512.18908v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:43.091793",
    "title": "Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage",
    "abstract": "Summary: Mass Casualty Incidents can overwhelm emergency medical systems and resulting delays or errors in the assessment of casualties can lead to preventable deaths. We present a decision support framework that fuses outputs from multiple computer vision models, estimating signs of severe hemorrhage, respiratory distress, physical alertness, or visible trauma, into a Bayesian network constructed entirely from expert-defined rules. Unlike traditional data-driven models, our approach does not require training data, supports inference with incomplete information, and is robust to noisy or uncertain observations. We report performance for two missions involving 11 and 9 casualties, respectively, where our Bayesian network model substantially outperformed vision-only baselines during evaluation of our system in the DARPA Triage Challenge (DTC) field scenarios. The accuracy of physiological assessment improved from 15% to 42% in the first scenario and from 19% to 46% in the second, representing nearly threefold increase in performance. More importantly, overall triage accuracy increased from 14% to 53% in all patients, while the diagnostic coverage of the system expanded from 31% to 95% of the cases requiring assessment. These results demonstrate that expert-knowledge-guided probabilistic reasoning can significantly enhance automated triage systems, offering a promising approach to supporting emergency responders in MCIs. This approach enabled Team Chiron to achieve 4th place out of 11 teams during the 1st physical round of the DTC.",
    "paper_id": "2512.18908v1"
  },
  "https://arxiv.org/abs/2512.14617v1": {
    "content_path": "../data/contents/2512.14617v1.md",
    "pdf_path": "../data/contents/2512.14617v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:45.737157",
    "title": "Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes",
    "abstract": "Summary: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.",
    "paper_id": "2512.14617v1"
  },
  "https://arxiv.org/abs/2512.11920v1": {
    "content_path": "../data/contents/2512.11920v1.md",
    "pdf_path": "../data/contents/2512.11920v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:46.193189",
    "title": "CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving",
    "abstract": "Summary: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \\textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.",
    "paper_id": "2512.11920v1"
  },
  "https://arxiv.org/abs/2512.20905v1": {
    "content_path": "../data/contents/2512.20905v1.md",
    "pdf_path": "../data/contents/2512.20905v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:46.456582",
    "title": "DiEC: Diffusion Embedded Clustering",
    "abstract": "Summary: Deep clustering hinges on learning representations that are inherently clusterable. However, using a single encoder to produce a fixed embedding ignores the representation trajectory formed by a pretrained diffusion model across network hierarchies and noise timesteps, where clusterability varies substantially. We propose DiEC (Diffusion Embedded Clustering), which performs unsupervised clustering by directly reading internal activations from a pretrained diffusion U-Net.\n  DiEC formulates representation selection as a two-dimensional search over layer x timestep, and exploits a weak-coupling property to decompose it into two stages. Specifically, we first fix the U-Net bottleneck layer as the Clustering-friendly Middle Layer (CML), and then use Optimal Timestep Search (OTS) to identify the clustering-optimal timestep (t*). During training, we extract bottleneck features at the fixed t* and obtain clustering representations via a lightweight residual mapping. We optimize a DEC-style KL self-training objective, augmented with adaptive graph regularization and entropy regularization to strengthen cluster structures. In parallel, we introduce a denoising-consistency branch at random timesteps to stabilize the representations and preserve generative consistency. Experiments show that DiEC achieves competitive clustering performance on multiple standard benchmarks.",
    "paper_id": "2512.20905v1"
  },
  "https://arxiv.org/abs/2512.18906v1": {
    "content_path": "../data/contents/2512.18906v1.md",
    "pdf_path": "../data/contents/2512.18906v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:48.237969",
    "title": "Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations",
    "abstract": "Summary: Over the years, automatic MT metrics have hillclimbed benchmarks and presented strong and sometimes human-level agreement with human ratings. Yet they remain black-box, offering little insight into their decision-making and often failing under real-world out-of-distribution (OOD) inputs. We introduce Remedy-R, a reasoning-driven generative MT metric trained with reinforcement learning from pairwise translation preferences, without requiring error-span annotations or distillation from closed LLMs. Remedy-R produces step-by-step analyses of accuracy, fluency, and completeness, followed by a final score, enabling more interpretable assessments. With only 60K training pairs across two language pairs, Remedy-R remains competitive with top scalar metrics and GPT-4-based judges on WMT22-24 meta-evaluation, generalizes to other languages, and exhibits strong robustness on OOD stress tests. Moreover, Remedy-R models generate self-reflective feedback that can be reused for translation improvement. Building on this finding, we introduce Remedy-R Agent, a simple evaluate-revise pipeline that leverages Remedy-R's evaluation analysis to refine translations. This agent consistently improves translation quality across diverse models, including Qwen2.5, ALMA-R, GPT-4o-mini, and Gemini-2.0-Flash, suggesting that Remedy-R's reasoning captures translation-relevant information and is practically useful.",
    "paper_id": "2512.18906v1"
  },
  "https://arxiv.org/abs/2512.14604v1": {
    "content_path": "../data/contents/2512.14604v1.md",
    "pdf_path": "../data/contents/2512.14604v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:49.650019",
    "title": "LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts",
    "abstract": "Summary: Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.",
    "paper_id": "2512.14604v1"
  },
  "https://arxiv.org/abs/2512.10739v2": {
    "content_path": "../data/contents/2512.10739v2.md",
    "pdf_path": "../data/contents/2512.10739v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:50.560728",
    "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
    "abstract": "Summary: Large Reasoning Models (LRMs) have expanded the mathematical reasoning frontier through Chain-of-Thought (CoT) techniques and Reinforcement Learning with Verifiable Rewards (RLVR), capable of solving AIME-level problems. However, the performance of LRMs is heavily dependent on the extended reasoning context length. For solving ultra-hard problems like those in the International Mathematical Olympiad (IMO), the required reasoning complexity surpasses the space that an LRM can explore in a single round. Previous works attempt to extend the reasoning context of LRMs but remain prompt-based and built upon proprietary models, lacking systematic structures and training pipelines. Therefore, this paper introduces Intern-S1-MO, a long-horizon math agent that conducts multi-round hierarchical reasoning, composed of an LRM-based multi-agent system including reasoning, summary, and verification. By maintaining a compact memory in the form of lemmas, Intern-S1-MO can more freely explore the lemma-rich reasoning spaces in multiple reasoning stages, thereby breaking through the context constraints for IMO-level math problems. Furthermore, we propose OREAL-H, an RL framework for training the LRM using the online explored trajectories to simultaneously bootstrap the reasoning ability of LRM and elevate the overall performance of Intern-S1-MO. Experiments show that Intern-S1-MO can obtain 26 out of 35 points on the non-geometry problems of IMO2025, matching the performance of silver medalists. It also surpasses the current advanced LRMs on inference benchmarks such as HMMT2025, AIME2025, and CNMO2025. In addition, our agent officially participates in CMO2025 and achieves a score of 102/126 under the judgment of human experts, reaching the gold medal level.",
    "paper_id": "2512.10739v2"
  },
  "https://arxiv.org/abs/2512.20900v1": {
    "content_path": "../data/contents/2512.20900v1.md",
    "pdf_path": "../data/contents/2512.20900v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:52.534390",
    "title": "When Experts Speak:Sequential LLM-Bayesian Learning for Startup Success Prediction",
    "abstract": "Summary: Evaluating startups is inherently challenging in entrepreneurial finance, where investors confront severe information asymmetry and limited quantitative data. Leveraging a novel expert network call data, we develop an LLM-Bayesian model that analyzes these conversations at the question-answer turn level, extracting semantic and evaluative signals via large language models (LLMs) and aggregating them in a sequential Bayesian architecture. The model dynamically updates beliefs as additional expert calls occur and attenuates contradictory assessments, which are absent from existing text-based screening tools. Empirically, our model outperforms state-of-the-art benchmarks by 6.691% in F1-score and increases portfolio-level Return on Investment by 15.255%. Attention and ablation analyses reveal that conversational cues are particularly informative for technologically complex startups, young firms, diverse founding teams, and firms with low public visibility. By converting expert dialogue into continually updated probabilities, our model advances research in entrepreneurial finance and information systems and offers policy implications for improving funding outcomes for informationally disadvantaged startups.",
    "paper_id": "2512.20900v1"
  },
  "https://arxiv.org/abs/2512.18901v1": {
    "content_path": "../data/contents/2512.18901v1.md",
    "pdf_path": "../data/contents/2512.18901v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:52.972355",
    "title": "Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models",
    "abstract": "Summary: We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.",
    "paper_id": "2512.18901v1"
  },
  "https://arxiv.org/abs/2512.19720v1": {
    "content_path": "../data/contents/2512.19720v1.md",
    "pdf_path": "../data/contents/2512.19720v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:53.890244",
    "title": "Per-Axis Weight Deltas for Frequent Model Updates",
    "abstract": "Summary: Serving many task-specialized LLM variants is often limited by the large size of fine-tuned checkpoints and the resulting cold-start latency. Since fine-tuned weights differ from their base model by relatively small structured residuals, a natural approach is to represent them as compressed deltas. We propose a simple 1-bit delta scheme that stores only the sign of the weight difference together with lightweight per-axis (row/column) FP16 scaling factors, learned from a small calibration set. This design preserves the compactness of 1-bit deltas while more accurately capturing variation across weight dimensions, leading to improved reconstruction quality over scalar alternatives. From a systems perspective, a streamlined loader that transfers packed deltas in a single operation per module reduces cold-start latency and storage overhead, with artifacts several times smaller than a full FP16 checkpoint. The method is drop-in, requires minimal calibration data, and maintains inference efficiency by avoiding dense reconstruction. Our experimental setup and source code are available at https://github.com/kuiumdjiev/Per-Axis-Weight-Deltas-for-Frequent-Model-Updates.",
    "paper_id": "2512.19720v1"
  },
  "https://arxiv.org/abs/2512.10734v2": {
    "content_path": "../data/contents/2512.10734v2.md",
    "pdf_path": "../data/contents/2512.10734v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:56.958459",
    "title": "Textual Data Bias Detection and Mitigation -- An Extensible Pipeline with Experimental Evaluation",
    "abstract": "Summary: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.",
    "paper_id": "2512.10734v2"
  },
  "https://arxiv.org/abs/2502.12110": {
    "content_path": "../data/contents/2502.12110.md",
    "pdf_path": "../data/contents/2502.12110.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:57.387486",
    "title": "A-MEM: Agentic Memory for LLM Agents",
    "abstract": "Novel agentic memory system using Zettelkasten principles for dynamic knowledge organization in agents.",
    "paper_id": "2502.12110"
  },
  "https://arxiv.org/abs/2512.20896v1": {
    "content_path": "../data/contents/2512.20896v1.md",
    "pdf_path": "../data/contents/2512.20896v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:31:57.656274",
    "title": "Accurate and Diverse Recommendations via Propensity-Weighted Linear Autoencoders",
    "abstract": "Summary: In real-world recommender systems, user-item interactions are Missing Not At Random (MNAR), as interactions with popular items are more frequently observed than those with less popular ones. Missing observations shift recommendations toward frequently interacted items, which reduces the diversity of the recommendation list. To alleviate this problem, Inverse Propensity Scoring (IPS) is widely used and commonly models propensities based on a power-law function of item interaction frequency. However, we found that such power-law-based correction overly penalizes popular items and harms their recommendation performance. We address this issue by redefining the propensity score to allow broader item recommendation without excessively penalizing popular items. The proposed score is formulated by applying a sigmoid function to the logarithm of the item observation frequency, maintaining the simplicity of power-law scoring while allowing for more flexible adjustment. Furthermore, we incorporate the redefined propensity score into a linear autoencoder model, which tends to favor popular items, and evaluate its effectiveness. Experimental results revealed that our method substantially improves the diversity of items in the recommendation list without sacrificing recommendation accuracy.",
    "paper_id": "2512.20896v1"
  },
  "https://arxiv.org/abs/2512.18880v1": {
    "content_path": "../data/contents/2512.18880v1.md",
    "pdf_path": "../data/contents/2512.18880v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:05.647997",
    "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
    "abstract": "Summary: Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.",
    "paper_id": "2512.18880v1"
  },
  "https://arxiv.org/abs/2512.14576v1": {
    "content_path": "../data/contents/2512.14576v1.md",
    "pdf_path": "../data/contents/2512.14576v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:05.938327",
    "title": "Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies",
    "abstract": "Summary: This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.",
    "paper_id": "2512.14576v1"
  },
  "https://arxiv.org/abs/2512.10732v1": {
    "content_path": "../data/contents/2512.10732v1.md",
    "pdf_path": "../data/contents/2512.10732v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:10.547293",
    "title": "TriHaRd: Higher Resilience for TEE Trusted Time",
    "abstract": "Summary: Accurately measuring time passing is critical for many applications. However, in Trusted Execution Environments (TEEs) such as Intel SGX, the time source is outside the Trusted Computing Base: a malicious host can manipulate the TEE's notion of time, jumping in time or affecting perceived time speed. Previous work (Triad) proposes protocols for TEEs to maintain a trustworthy time source by building a cluster of TEEs that collaborate with each other and with a remote Time Authority to maintain a continuous notion of passing time. However, such approaches still allow an attacker to control the operating system and arbitrarily manipulate their own TEE's perceived clock speed. An attacker can even propagate faster passage of time to honest machines participating in Triad's trusted time protocol, causing them to skip to timestamps arbitrarily far in the future. We propose TriHaRd, a TEE trusted time protocol achieving high resilience against clock speed and offset manipulations, notably through Byzantine-resilient clock updates and consistency checks. We empirically show that TriHaRd mitigates known attacks against Triad.",
    "paper_id": "2512.10732v1"
  },
  "https://arxiv.org/abs/2512.20885v1": {
    "content_path": "../data/contents/2512.20885v1.md",
    "pdf_path": "../data/contents/2512.20885v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:11.208142",
    "title": "From GNNs to Symbolic Surrogates via Kolmogorov-Arnold Networks for Delay Prediction",
    "abstract": "Summary: Accurate prediction of flow delay is essential for optimizing and managing modern communication networks. We investigate three levels of modeling for this task. First, we implement a heterogeneous GNN with attention-based message passing, establishing a strong neural baseline. Second, we propose FlowKANet in which Kolmogorov-Arnold Networks replace standard MLP layers, reducing trainable parameters while maintaining competitive predictive performance. FlowKANet integrates KAMP-Attn (Kolmogorov-Arnold Message Passing with Attention), embedding KAN operators directly into message-passing and attention computation. Finally, we distill the model into symbolic surrogate models using block-wise regression, producing closed-form equations that eliminate trainable weights while preserving graph-structured dependencies. The results show that KAN layers provide a favorable trade-off between efficiency and accuracy and that symbolic surrogates emphasize the potential for lightweight deployment and enhanced transparency.",
    "paper_id": "2512.20885v1"
  },
  "https://arxiv.org/abs/2512.18858v1": {
    "content_path": "../data/contents/2512.18858v1.md",
    "pdf_path": "../data/contents/2512.18858v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:11.593022",
    "title": "Adapting Skill Ratings to Luck-Based Hidden-Information Games",
    "abstract": "Summary: Rating systems play a crucial role in evaluating player skill across competitive environments. The Elo rating system, originally designed for deterministic and information-complete games such as chess, has been widely adopted and modified in various domains. However, the traditional Elo rating system only considers game outcomes for rating calculation and assumes uniform initial states across players. This raises important methodological challenges in skill modelling for popular partially randomized incomplete-information games such as Rummy. In this paper, we examine the limitations of conventional Elo ratings when applied to luck-driven environments and propose a modified Elo framework specifically tailored for Rummy. Our approach incorporates score-based performance metrics and explicitly models the influence of initial hand quality to disentangle skill from luck. Through extensive simulations involving 270,000 games across six strategies of varying sophistication, we demonstrate that our proposed system achieves stable convergence, superior discriminative power, and enhanced predictive accuracy compared to traditional Elo formulations. The framework maintains computational simplicity while effectively capturing the interplay of skill, strategy, and randomness, with broad applicability to other stochastic competitive environments.",
    "paper_id": "2512.18858v1"
  },
  "https://arxiv.org/abs/2512.14565v1": {
    "content_path": "../data/contents/2512.14565v1.md",
    "pdf_path": "../data/contents/2512.14565v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:12.109634",
    "title": "Pairwise Comparison for Bias Identification and Quantification",
    "abstract": "Summary: Linguistic bias in online news and social media is widespread but difficult to measure. Yet, its identification and quantification remain difficult due to subjectivity, context dependence, and the scarcity of high-quality gold-label datasets. We aim to reduce annotation effort by leveraging pairwise comparison for bias annotation. To overcome the costliness of the approach, we evaluate more efficient implementations of pairwise comparison-based rating. We achieve this by investigating the effects of various rating techniques and the parameters of three cost-aware alternatives in a simulation environment. Since the approach can in principle be applied to both human and large language model annotation, our work provides a basis for creating high-quality benchmark datasets and for quantifying biases and other subjective linguistic aspects.\n  The controlled simulations include latent severity distributions, distance-calibrated noise, and synthetic annotator bias to probe robustness and cost-quality trade-offs. In applying the approach to human-labeled bias benchmark datasets, we then evaluate the most promising setups and compare them to direct assessment by large language models and unmodified pairwise comparison labels as baselines. Our findings support the use of pairwise comparison as a practical foundation for quantifying subjective linguistic aspects, enabling reproducible bias analysis. We contribute an optimization of comparison and matchmaking components, an end-to-end evaluation including simulation and real-data application, and an implementation blueprint for cost-aware large-scale annotation",
    "paper_id": "2512.14565v1"
  },
  "https://arxiv.org/abs/2512.10720v1": {
    "content_path": "../data/contents/2512.10720v1.md",
    "pdf_path": "../data/contents/2512.10720v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:17.790719",
    "title": "Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality",
    "abstract": "Summary: Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the simplest causal explanation -- can endow the latent representations of diffusion vision and autoregressive language models with clear causal interpretation and robust, component-wise identifiable control. We introduce a novel theoretical framework for hierarchical selection models, where higher-level concepts emerge from the constrained composition of lower-level variables, better capturing the complex dependencies in data generation. Under theoretically derived minimality conditions (manifesting as sparsity or compression constraints), we show that learned representations can be equivalent to the true latent variables of the data-generating process. Empirically, applying these constraints to leading generative models allows us to extract their innate hierarchical concept graphs, offering fresh insights into their internal knowledge organization. Furthermore, these causally grounded concepts serve as levers for fine-grained model steering, paving the way for transparent, reliable systems.",
    "paper_id": "2512.10720v1"
  },
  "https://arxiv.org/abs/2512.20884v1": {
    "content_path": "../data/contents/2512.20884v1.md",
    "pdf_path": "../data/contents/2512.20884v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:18.061246",
    "title": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents",
    "abstract": "Summary: Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.",
    "paper_id": "2512.20884v1"
  },
  "https://arxiv.org/abs/2512.20677v1": {
    "content_path": "../data/contents/2512.20677v1.md",
    "pdf_path": "../data/contents/2512.20677v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:21.092450",
    "title": "Automated Red-Teaming Framework for Large Language Model Security Assessment: A Comprehensive Attack Generation and Detection System",
    "abstract": "Summary: As large language models (LLMs) are increasingly deployed in high-stakes domains, ensuring their security and alignment has become a critical challenge. Existing red-teaming practices depend heavily on manual testing, which limits scalability and fails to comprehensively cover the vast space of potential adversarial behaviors. This paper introduces an automated red-teaming framework that systematically generates, executes, and evaluates adversarial prompts to uncover security vulnerabilities in LLMs. Our framework integrates meta-prompting-based attack synthesis, multi-modal vulnerability detection, and standardized evaluation protocols spanning six major threat categories -- reward hacking, deceptive alignment, data exfiltration, sandbagging, inappropriate tool use, and chain-of-thought manipulation. Experiments on the GPT-OSS-20B model reveal 47 distinct vulnerabilities, including 21 high-severity and 12 novel attack patterns, achieving a $3.9\\times$ improvement in vulnerability discovery rate over manual expert testing while maintaining 89\\% detection accuracy. These results demonstrate the framework's effectiveness in enabling scalable, systematic, and reproducible AI safety evaluations. By providing actionable insights for improving alignment robustness, this work advances the state of automated LLM red-teaming and contributes to the broader goal of building secure and trustworthy AI systems.",
    "paper_id": "2512.20677v1"
  },
  "https://arxiv.org/abs/2512.14562v1": {
    "content_path": "../data/contents/2512.14562v1.md",
    "pdf_path": "../data/contents/2512.14562v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:21.345818",
    "title": "Polypersona: Persona-Grounded LLM for Synthetic Survey Responses",
    "abstract": "Summary: This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.",
    "paper_id": "2512.14562v1"
  },
  "https://arxiv.org/abs/2512.10713v2": {
    "content_path": "../data/contents/2512.10713v2.md",
    "pdf_path": "../data/contents/2512.10713v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:23.342395",
    "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
    "abstract": "Summary: Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
    "paper_id": "2512.10713v2"
  },
  "https://arxiv.org/abs/2512.20877v1": {
    "content_path": "../data/contents/2512.20877v1.md",
    "pdf_path": "../data/contents/2512.20877v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:23.734622",
    "title": "Architectural Trade-offs in Small Language Models Under Compute Constraints",
    "abstract": "Summary: We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.",
    "paper_id": "2512.20877v1"
  },
  "https://arxiv.org/abs/2512.18857v1": {
    "content_path": "../data/contents/2512.18857v1.md",
    "pdf_path": "../data/contents/2512.18857v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:24.545383",
    "title": "CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning",
    "abstract": "Summary: Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.",
    "paper_id": "2512.18857v1"
  },
  "https://arxiv.org/abs/2512.14561v1": {
    "content_path": "../data/contents/2512.14561v1.md",
    "pdf_path": "../data/contents/2512.14561v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:24.807883",
    "title": "Agreement Between Large Language Models and Human Raters in Essay Scoring: A Research Synthesis",
    "abstract": "Summary: Despite the growing promise of large language models (LLMs) in automatic essay scoring (AES), empirical findings regarding their reliability compared to human raters remain mixed. Following the PRISMA 2020 guidelines, we synthesized 65 published and unpublished studies from January 2022 to August 2025 that examined agreement between LLMs and human raters in AES. Across studies, reported LLM-human agreement was generally moderate to good, with agreement indices (e.g., Quadratic Weighted Kappa, Pearson correlation, and Spearman's rho) mostly ranging between 0.30 and 0.80. Substantial variability in agreement levels was observed across studies, reflecting differences in study-specific factors as well as the lack of standardized reporting practices. Implications and directions for future research are discussed.",
    "paper_id": "2512.14561v1"
  },
  "https://arxiv.org/abs/2512.19703v1": {
    "content_path": "../data/contents/2512.19703v1.md",
    "pdf_path": "../data/contents/2512.19703v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:25.558021",
    "title": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
    "abstract": "Summary: The dominant paradigm for Audio-Text Retrieval (ATR) relies on mini-batch-based contrastive learning. This process, however, is inherently limited by what we formalize as the Gradient Locality Bottleneck (GLB), which structurally prevents models from leveraging out-of-batch knowledge and thus impairs fine-grained and long-tail learning. While external knowledge-enhanced methods can alleviate the GLB, we identify a critical, unaddressed side effect: the Representation-Drift Mismatch (RDM), where a static knowledge base becomes progressively misaligned with the evolving model, turning guidance into noise. To address this dual challenge, we propose the Adaptive Self-improving Knowledge (ASK) framework, a model-agnostic, plug-and-play solution. ASK breaks the GLB via multi-grained knowledge injection, systematically mitigates RDM through dynamic knowledge refinement, and introduces a novel adaptive reliability weighting scheme to ensure consistent knowledge contributes to optimization. Experimental results on two benchmark datasets with superior, state-of-the-art performance justify the efficacy of our proposed ASK framework.",
    "paper_id": "2512.19703v1"
  },
  "https://arxiv.org/abs/2512.20861v1": {
    "content_path": "../data/contents/2512.20861v1.md",
    "pdf_path": "../data/contents/2512.20861v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:25.889703",
    "title": "Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs",
    "abstract": "Summary: Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\\times$ speedups and $3\\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr .",
    "paper_id": "2512.20861v1"
  },
  "https://arxiv.org/abs/2512.18853v1": {
    "content_path": "../data/contents/2512.18853v1.md",
    "pdf_path": "../data/contents/2512.18853v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:31.474218",
    "title": "VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference",
    "abstract": "Summary: The integrity of data visualizations is increasingly threatened by image editing techniques that enable subtle yet deceptive tampering. Through a formative study, we define this challenge and categorize tampering techniques into two primary types: data manipulation and visual encoding manipulation. To address this, we present VizDefender, a framework for tampering detection and analysis. The framework integrates two core components: 1) a semi-fragile watermark module that protects the visualization by embedding a location map to images, which allows for the precise localization of tampered regions while preserving visual quality, and 2) an intent analysis module that leverages Multimodal Large Language Models (MLLMs) to interpret manipulation, inferring the attacker's intent and misleading effects. Extensive evaluations and user studies demonstrate the effectiveness of our methods.",
    "paper_id": "2512.18853v1"
  },
  "https://arxiv.org/abs/2512.14559v1": {
    "content_path": "../data/contents/2512.14559v1.md",
    "pdf_path": "../data/contents/2512.14559v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:31.897356",
    "title": "Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions",
    "abstract": "Summary: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.",
    "paper_id": "2512.14559v1"
  },
  "https://arxiv.org/abs/2512.10696v1": {
    "content_path": "../data/contents/2512.10696v1.md",
    "pdf_path": "../data/contents/2512.10696v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:32.756967",
    "title": "Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution",
    "abstract": "Summary: Procedural memory enables large language model (LLM) agents to internalize \"how-to\" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a \"passive accumulation\" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\\textbf{ReMe}$ ($\\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\\texttt{reme.library}$ dataset to facilitate further research.",
    "paper_id": "2512.10696v1"
  },
  "https://arxiv.org/abs/2512.20856v1": {
    "content_path": "../data/contents/2512.20856v1.md",
    "pdf_path": "../data/contents/2512.20856v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:33.036649",
    "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
    "abstract": "Summary: We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.",
    "paper_id": "2512.20856v1"
  },
  "https://arxiv.org/abs/2512.18841v1": {
    "content_path": "../data/contents/2512.18841v1.md",
    "pdf_path": "../data/contents/2512.18841v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:33.373144",
    "title": "MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models",
    "abstract": "Summary: Despite advances in mathematical reasoning capabilities, Large Language Models (LLMs) still struggle with calculation verification when using established prompting techniques. We present MDToC (Metacognitive Dynamic Tree of Concepts), a three-phase approach that constructs a concept tree, develops accuracy-verified calculations for each concept, and employs majority voting to evaluate competing solutions. Evaluations across CHAMP, MATH, and Game-of-24 benchmarks demonstrate our MDToC's effectiveness, with GPT-4-Turbo achieving 58.1\\% on CHAMP, 86.6\\% on MATH, and 85\\% on Game-of-24 - outperforming GoT by 5\\%, 5.4\\%, and 4\\% on all these tasks, respectively, without hand-engineered hints. MDToC consistently surpasses existing prompting methods across all backbone models, yielding improvements of up to 7.6\\% over ToT and 6.2\\% over GoT, establishing metacognitive calculation verification as a promising direction for enhanced mathematical reasoning.",
    "paper_id": "2512.18841v1"
  },
  "https://arxiv.org/abs/2512.14554v4": {
    "content_path": "../data/contents/2512.14554v4.md",
    "pdf_path": "../data/contents/2512.14554v4.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:35.424129",
    "title": "VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models",
    "abstract": "Summary: The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, the Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems. To facilitate access and reproducibility, we provide a public landing page for this benchmark at https://vilegalbench.cmcai.vn/.",
    "paper_id": "2512.14554v4"
  },
  "https://arxiv.org/abs/2512.10691v1": {
    "content_path": "../data/contents/2512.10691v1.md",
    "pdf_path": "../data/contents/2512.10691v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:36.834972",
    "title": "Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning",
    "abstract": "Summary: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning (\"thinking\") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.",
    "paper_id": "2512.10691v1"
  },
  "https://arxiv.org/abs/2512.20854v1": {
    "content_path": "../data/contents/2512.20854v1.md",
    "pdf_path": "../data/contents/2512.20854v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:37.109260",
    "title": "How important is Recall for Measuring Retrieval Quality?",
    "abstract": "Summary: In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.",
    "paper_id": "2512.20854v1"
  },
  "https://arxiv.org/abs/2512.18834v1": {
    "content_path": "../data/contents/2512.18834v1.md",
    "pdf_path": "../data/contents/2512.18834v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:37.677241",
    "title": "AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus",
    "abstract": "Summary: We present AraMix, a deduplicated Arabic pretraining corpus containing approximately 178 billion tokens across 179 million documents. Rather than scraping the web again, AraMix demonstrates that substantial value lies in systematically reusing and curating existing pretraining datasets: we combine seven publicly available Arabic web datasets, apply quality filtering designed specifically for Arabic text to re-filter some datasets, and perform cross-dataset deduplication, both MinHash and sentence-level. This approach reveals that nearly 60% of tokens across these independently collected corpora are duplicates, redundancy that any new scraping efforts will reproduce. Our work suggests that for lower resource languages, investment in curation pipelines for existing data yields greater returns than additional web crawls, an approach that allowed us to curate the largest heavily filtered publicly available Arabic pretraining corpus.",
    "paper_id": "2512.18834v1"
  },
  "https://arxiv.org/abs/2512.14549v1": {
    "content_path": "../data/contents/2512.14549v1.md",
    "pdf_path": "../data/contents/2512.14549v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:38.713077",
    "title": "Dual Language Models: Balancing Training Efficiency and Overfitting Resilience",
    "abstract": "Summary: This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.",
    "paper_id": "2512.14549v1"
  },
  "https://arxiv.org/abs/2512.10688v5": {
    "content_path": "../data/contents/2512.10688v5.md",
    "pdf_path": "../data/contents/2512.10688v5.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:39.022867",
    "title": "Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition",
    "abstract": "Summary: Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users' genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant \"popularity direction\" where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.",
    "paper_id": "2512.10688v5"
  },
  "https://arxiv.org/abs/2512.20848v1": {
    "content_path": "../data/contents/2512.20848v1.md",
    "pdf_path": "../data/contents/2512.20848v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:39.400531",
    "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
    "abstract": "Summary: We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.",
    "paper_id": "2512.20848v1"
  },
  "https://arxiv.org/abs/2512.18832v1": {
    "content_path": "../data/contents/2512.18832v1.md",
    "pdf_path": "../data/contents/2512.18832v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:40.765932",
    "title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?",
    "abstract": "Summary: Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.",
    "paper_id": "2512.18832v1"
  },
  "https://arxiv.org/abs/2512.14531v1": {
    "content_path": "../data/contents/2512.14531v1.md",
    "pdf_path": "../data/contents/2512.14531v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:41.241997",
    "title": "VersatileFFN: Achieving Parameter Efficiency in LLMs via Adaptive Wide-and-Deep Reuse",
    "abstract": "Summary: The rapid scaling of Large Language Models (LLMs) has achieved remarkable performance, but it also leads to prohibitive memory costs. Existing parameter-efficient approaches such as pruning and quantization mainly compress pretrained models without enhancing architectural capacity, thereby hitting the representational ceiling of the base model. In this work, we propose VersatileFFN, a novel feed-forward network (FFN) that enables flexible reuse of parameters in both width and depth dimensions within a fixed parameter budget. Inspired by the dual-process theory of cognition, VersatileFFN comprises two adaptive pathways: a width-versatile path that generates a mixture of sub-experts from a single shared FFN, mimicking sparse expert routing without increasing parameters, and a depth-versatile path that recursively applies the same FFN to emulate deeper processing for complex tokens. A difficulty-aware gating dynamically balances the two pathways, steering \"easy\" tokens through the efficient width-wise route and allocating deeper iterative refinement to \"hard\" tokens. Crucially, both pathways reuse the same parameters, so all additional capacity comes from computation rather than memory. Experiments across diverse benchmarks and model scales demonstrate the effectiveness of the method. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/VersatileFFN.",
    "paper_id": "2512.14531v1"
  },
  "https://arxiv.org/abs/2512.10687v1": {
    "content_path": "../data/contents/2512.10687v1.md",
    "pdf_path": "../data/contents/2512.10687v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:43.864366",
    "title": "Challenges of Evaluating LLM Safety for User Welfare",
    "abstract": "Summary: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.",
    "paper_id": "2512.10687v1"
  },
  "https://arxiv.org/abs/2512.20845v1": {
    "content_path": "../data/contents/2512.20845v1.md",
    "pdf_path": "../data/contents/2512.20845v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:44.131383",
    "title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs",
    "abstract": "Summary: LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.",
    "paper_id": "2512.20845v1"
  },
  "https://arxiv.org/abs/2512.18826v1": {
    "content_path": "../data/contents/2512.18826v1.md",
    "pdf_path": "../data/contents/2512.18826v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:44.408581",
    "title": "Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection",
    "abstract": "Summary: This survey reviews hyperbolic graph embedding models, and evaluate them on anomaly detection, highlighting their advantages over Euclidean methods in capturing complex structures. Evaluating models like \\textit{HGCAE}, \\textit{\\(\\mathcal{P}\\)-VAE}, and \\textit{HGCN} demonstrates high performance, with \\textit{\\(\\mathcal{P}\\)-VAE} achieving an F1-score of 94\\% on the \\textit{Elliptic} dataset and \\textit{HGCAE} scoring 80\\% on \\textit{Cora}. In contrast, Euclidean methods like \\textit{DOMINANT} and \\textit{GraphSage} struggle with complex data. The study emphasizes the potential of hyperbolic spaces for improving anomaly detection, and provides an open-source library to foster further research in this field.",
    "paper_id": "2512.18826v1"
  },
  "https://arxiv.org/abs/2512.14527v1": {
    "content_path": "../data/contents/2512.14527v1.md",
    "pdf_path": "../data/contents/2512.14527v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:45.827228",
    "title": "Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence",
    "abstract": "Summary: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \\emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.",
    "paper_id": "2512.14527v1"
  },
  "https://arxiv.org/abs/2512.10669v1": {
    "content_path": "../data/contents/2512.10669v1.md",
    "pdf_path": "../data/contents/2512.10669v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:53.595364",
    "title": "Learning by Analogy: A Causal Framework for Composition Generalization",
    "abstract": "Summary: Compositional generalization -- the ability to understand and generate novel combinations of learned concepts -- enables models to extend their capabilities beyond limited experiences. While effective, the data structures and principles that enable this crucial capability remain poorly understood. We propose that compositional generalization fundamentally requires decomposing high-level concepts into basic, low-level concepts that can be recombined across similar contexts, similar to how humans draw analogies between concepts. For example, someone who has never seen a peacock eating rice can envision this scene by relating it to their previous observations of a chicken eating rice.\n  In this work, we formalize these intuitive processes using principles of causal modularity and minimal changes. We introduce a hierarchical data-generating process that naturally encodes different levels of concepts and their interaction mechanisms. Theoretically, we demonstrate that this approach enables compositional generalization supporting complex relations between composed concepts, advancing beyond prior work that assumes simpler interactions like additive effects. Critically, we also prove that this latent hierarchical structure is provably recoverable (identifiable) from observable data like text-image pairs, a necessary step for learning such a generative process. To validate our theory, we apply insights from our theoretical framework and achieve significant improvements on benchmark datasets.",
    "paper_id": "2512.10669v1"
  },
  "https://arxiv.org/abs/2512.20822v1": {
    "content_path": "../data/contents/2512.20822v1.md",
    "pdf_path": "../data/contents/2512.20822v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:53.934875",
    "title": "MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs",
    "abstract": "Summary: Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.",
    "paper_id": "2512.20822v1"
  },
  "https://arxiv.org/abs/2512.18803v1": {
    "content_path": "../data/contents/2512.18803v1.md",
    "pdf_path": "../data/contents/2512.18803v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:54.720841",
    "title": "Quantifying the Lifelong Impact of Resilience Interventions via Agent-Based LLM Simulation",
    "abstract": "Summary: Establishing the long-term, causal impact of psychological interventions on life outcomes is a grand challenge for the social sciences, caught between the limitations of correlational longitudinal studies and short-term randomized controlled trials (RCTs). This paper introduces Large-Scale Agent-based Longitudinal Simulation (LALS), a framework that resolves this impasse by simulating multi-decade, counterfactual life trajectories. The methodology employs a \"digital clone\" design where 2,500 unique LLM-based agent personas (grounded in a curated corpus of 3,917 empirical research articles) are each cloned across a 2x2 factorial experiment. Specifically, the simulation models the efficacy of extended psychological resilience training (Intervention vs. Control) either in childhood or as a young adult (age 6 vs. age 18). Comparing digital clones enables exceptionally precise causal inference. The simulation provides a quantitative, causal estimate of a resilience intervention's lifelong effects, revealing significant reductions in mortality, a lower incidence of dementia, and a substantial increase in accumulated wealth. Crucially, the results uncover a crucial developmental window: the intervention administered at age 6 produced more than double the positive impact on lifetime wealth compared to the same intervention at age 18. These benefits were most pronounced for agents from low-socioeconomic backgrounds, highlighting a powerful buffering effect. The LALS framework serves as a \"computational wind tunnel\" for social science, offering a new paradigm for generating and testing causal hypotheses about the complex, lifelong dynamics that shape human capital and well-being.",
    "paper_id": "2512.18803v1"
  },
  "https://arxiv.org/abs/2512.14503v1": {
    "content_path": "../data/contents/2512.14503v1.md",
    "pdf_path": "../data/contents/2512.14503v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:55.148442",
    "title": "RecGPT-V2 Technical Report",
    "abstract": "Summary: Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.\n  To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.",
    "paper_id": "2512.14503v1"
  },
  "https://arxiv.org/abs/2512.10665v1": {
    "content_path": "../data/contents/2512.10665v1.md",
    "pdf_path": "../data/contents/2512.10665v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:55.986044",
    "title": "On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity",
    "abstract": "Summary: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.",
    "paper_id": "2512.10665v1"
  },
  "https://arxiv.org/abs/2512.20817v1": {
    "content_path": "../data/contents/2512.20817v1.md",
    "pdf_path": "../data/contents/2512.20817v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:57.617671",
    "title": "EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading",
    "abstract": "Summary: Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.",
    "paper_id": "2512.20817v1"
  },
  "https://arxiv.org/abs/2512.18792v1": {
    "content_path": "../data/contents/2512.18792v1.md",
    "pdf_path": "../data/contents/2512.18792v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:58.326291",
    "title": "The Dead Salmons of AI Interpretability",
    "abstract": "Summary: In a striking neuroscience study, the authors placed a dead salmon in an MRI scanner and showed it images of humans in social situations. Astonishingly, standard analyses of the time reported brain regions predictive of social emotions. The explanation, of course, was not supernatural cognition but a cautionary tale about misapplied statistical inference. In AI interpretability, reports of similar ''dead salmon'' artifacts abound: feature attribution, probing, sparse auto-encoding, and even causal analyses can produce plausible-looking explanations for randomly initialized neural networks. In this work, we examine this phenomenon and argue for a pragmatic statistical-causal reframing: explanations of computational systems should be treated as parameters of a (statistical) model, inferred from computational traces. This perspective goes beyond simply measuring statistical variability of explanations due to finite sampling of input data; interpretability methods become statistical estimators, and findings should be tested against explicit and meaningful alternative computational hypotheses, with uncertainty quantified with respect to the postulated statistical model. It also highlights important theoretical issues, such as the identifiability of common interpretability queries, which we argue is critical to understand the field's susceptibility to false discoveries, poor generalizability, and high variance. More broadly, situating interpretability within the standard toolkit of statistical inference opens promising avenues for future work aimed at turning AI interpretability into a pragmatic and rigorous science.",
    "paper_id": "2512.18792v1"
  },
  "https://arxiv.org/abs/2512.14500v1": {
    "content_path": "../data/contents/2512.14500v1.md",
    "pdf_path": "../data/contents/2512.14500v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:58.787047",
    "title": "C-ing Clearly: Enhanced Binary Code Explanations using C code",
    "abstract": "Summary: Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.",
    "paper_id": "2512.14500v1"
  },
  "https://arxiv.org/abs/2512.10659v2": {
    "content_path": "../data/contents/2512.10659v2.md",
    "pdf_path": "../data/contents/2512.10659v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:59.663526",
    "title": "DCFO: Density-Based Counterfactuals for Outliers - Additional Material",
    "abstract": "Summary: Outlier detection identifies data points that significantly deviate from the majority of the data distribution. Explaining outliers is crucial for understanding the underlying factors that contribute to their detection, validating their significance, and identifying potential biases or errors. Effective explanations provide actionable insights, facilitating preventive measures to avoid similar outliers in the future. Counterfactual explanations clarify why specific data points are classified as outliers by identifying minimal changes required to alter their prediction. Although valuable, most existing counterfactual explanation methods overlook the unique challenges posed by outlier detection, and fail to target classical, widely adopted outlier detection algorithms. Local Outlier Factor (LOF) is one the most popular unsupervised outlier detection methods, quantifying outlierness through relative local density. Despite LOF's widespread use across diverse applications, it lacks interpretability. To address this limitation, we introduce Density-based Counterfactuals for Outliers (DCFO), a novel method specifically designed to generate counterfactual explanations for LOF. DCFO partitions the data space into regions where LOF behaves smoothly, enabling efficient gradient-based optimisation. Extensive experimental validation on 50 OpenML datasets demonstrates that DCFO consistently outperforms benchmarked competitors, offering superior proximity and validity of generated counterfactuals.",
    "paper_id": "2512.10659v2"
  },
  "https://arxiv.org/abs/2512.20812v1": {
    "content_path": "../data/contents/2512.20812v1.md",
    "pdf_path": "../data/contents/2512.20812v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:32:59.946823",
    "title": "Semantic Deception: When Reasoning Models Can't Compute an Addition",
    "abstract": "Summary: Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.",
    "paper_id": "2512.20812v1"
  },
  "https://arxiv.org/abs/2512.18779v1": {
    "content_path": "../data/contents/2512.18779v1.md",
    "pdf_path": "../data/contents/2512.18779v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:00.398676",
    "title": "From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure",
    "abstract": "Summary: Modern experimental platforms such as particle accelerators, fusion devices, telescopes, and industrial process control systems expose tens to hundreds of thousands of control and diagnostic channels accumulated over decades of evolution. Operators and AI systems rely on informal expert knowledge, inconsistent naming conventions, and fragmented documentation to locate signals for monitoring, troubleshooting, and automated control, creating a persistent bottleneck for reliability, scalability, and language-model-driven interfaces. We formalize semantic channel finding-mapping natural-language intent to concrete control-system signals-as a general problem in complex experimental infrastructure, and introduce a four-paradigm framework to guide architecture selection across facility-specific data regimes. The paradigms span (i) direct in-context lookup over curated channel dictionaries, (ii) constrained hierarchical navigation through structured trees, (iii) interactive agent exploration using iterative reasoning and tool-based database queries, and (iv) ontology-grounded semantic search that decouples channel meaning from facility-specific naming conventions. We demonstrate each paradigm through proof-of-concept implementations at four operational facilities spanning two orders of magnitude in scale-from compact free-electron lasers to large synchrotron light sources-and diverse control-system architectures, from clean hierarchies to legacy environments. These implementations achieve 90-97% accuracy on expert-curated operational queries.",
    "paper_id": "2512.18779v1"
  },
  "https://arxiv.org/abs/2512.14491v1": {
    "content_path": "../data/contents/2512.14491v1.md",
    "pdf_path": "../data/contents/2512.14491v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:01.717511",
    "title": "Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification",
    "abstract": "Summary: Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.",
    "paper_id": "2512.14491v1"
  },
  "https://arxiv.org/abs/2512.10656v1": {
    "content_path": "../data/contents/2512.10656v1.md",
    "pdf_path": "../data/contents/2512.10656v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:02.109837",
    "title": "Token Sample Complexity of Attention",
    "abstract": "Summary: As context windows in large language models continue to expand, it is essential to characterize how attention behaves at extreme sequence lengths. We introduce token-sample complexity: the rate at which attention computed on $n$ tokens converges to its infinite-token limit. We estimate finite-$n$ convergence bounds at two levels: pointwise uniform convergence of the attention map, and convergence of moments for the transformed token distribution. For compactly supported (and more generally sub-Gaussian) distributions, our first result shows that the attention map converges uniformly on a ball of radius $R$ at rate $C(R)/\\sqrt{n}$, where $C(R)$ grows exponentially with $R$. For large $R$, this estimate loses practical value, and our second result addresses this issue by establishing convergence rates for the moments of the transformed distribution (the token output of the attention layer). In this case, the rate is $C'(R)/n^β$ with $β<\\tfrac{1}{2}$, and $C'(R)$ depends polynomially on the size of the support of the distribution. The exponent $β$ depends on the attention geometry and the spectral properties of the tokens distribution. We also examine the regime in which the attention parameter tends to infinity and the softmax approaches a hardmax, and in this setting, we establish a logarithmic rate of convergence. Experiments on synthetic Gaussian data and real BERT models on Wikipedia text confirm our predictions.",
    "paper_id": "2512.10656v1"
  },
  "https://arxiv.org/abs/2512.20811v1": {
    "content_path": "../data/contents/2512.20811v1.md",
    "pdf_path": "../data/contents/2512.20811v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:02.354481",
    "title": "Weighted MCC: A Robust Measure of Multiclass Classifier Performance for Observations with Individual Weights",
    "abstract": "Summary: Several performance measures are used to evaluate binary and multiclass classification tasks.\n  But individual observations may often have distinct weights, and none of these measures are sensitive to such varying weights.\n  We propose a new weighted Pearson-Matthews Correlation Coefficient (MCC) for binary classification as well as weighted versions of related multiclass measures. The weighted MCC varies between $-1$ and $1$. But crucially, the weighted MCC values are higher for classifiers that perform better on highly weighted observations, and hence is able to distinguish them from classifiers that have a similar overall performance and ones that perform better on the lowly weighted observations.\n  Furthermore, we prove that the weighted measures are robust with respect to the choice of weights in a precise manner:\n  if the weights are changed by at most $ε$, the value of the weighted measure changes at most by a factor of $ε$ in the binary case\n  and by a factor of $ε^2$ in the multiclass case.\n  Our computations demonstrate that the weighted measures clearly identify classifiers that perform better on higher weighted observations, while the unweighted measures remain completely indifferent to the choices of weights.",
    "paper_id": "2512.20811v1"
  },
  "https://arxiv.org/abs/2512.18755v1": {
    "content_path": "../data/contents/2512.18755v1.md",
    "pdf_path": "../data/contents/2512.18755v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:03.698288",
    "title": "MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking",
    "abstract": "Summary: The rapid advancement of large language models (LLMs) has intensified concerns about the robustness of their safety alignment. While existing jailbreak studies explore both single-turn and multi-turn strategies, most implicitly assume a static safety boundary and fail to account for how contextual interactions dynamically influence model behavior, leading to limited stability and generalization. Motivated by this gap, we propose MEEA (Mere Exposure Effect Attack), a psychology-inspired, fully automated black-box framework for evaluating multi-turn safety robustness, grounded in the mere exposure effect. MEEA leverages repeated low-toxicity semantic exposure to induce a gradual shift in a model's effective safety threshold, enabling progressive erosion of alignment constraints over sustained interactions. Concretely, MEEA constructs semantically progressive prompt chains and optimizes them using a simulated annealing strategy guided by semantic similarity, toxicity, and jailbreak effectiveness. Extensive experiments on both closed-source and open-source models, including GPT-4, Claude-3.5, and DeepSeek-R1, demonstrate that MEEA consistently achieves higher attack success rates than seven representative baselines, with an average Attack Success Rate (ASR) improvement exceeding 20%. Ablation studies further validate the necessity of both annealing-based optimization and contextual exposure mechanisms. Beyond improved attack effectiveness, our findings indicate that LLM safety behavior is inherently dynamic and history-dependent, challenging the common assumption of static alignment boundaries and highlighting the need for interaction-aware safety evaluation and defense mechanisms. Our code is available at: https://github.com/Carney-lsz/MEEA",
    "paper_id": "2512.18755v1"
  },
  "https://arxiv.org/abs/2512.14490v1": {
    "content_path": "../data/contents/2512.14490v1.md",
    "pdf_path": "../data/contents/2512.14490v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:03.988044",
    "title": "PushGen: Push Notifications Generation with LLM",
    "abstract": "Summary: We present PushGen, an automated framework for generating high-quality push notifications comparable to human-crafted content. With the rise of generative models, there is growing interest in leveraging LLMs for push content generation. Although LLMs make content generation straightforward and cost-effective, maintaining stylistic control and reliable quality assessment remains challenging, as both directly impact user engagement. To address these issues, PushGen combines two key components: (1) a controllable category prompt technique to guide LLM outputs toward desired styles, and (2) a reward model that ranks and selects generated candidates. Extensive offline and online experiments demonstrate its effectiveness, which has been deployed in large-scale industrial applications, serving hundreds of millions of users daily.",
    "paper_id": "2512.14490v1"
  },
  "https://arxiv.org/abs/2512.15751v1": {
    "content_path": "../data/contents/2512.15751v1.md",
    "pdf_path": "../data/contents/2512.15751v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:04.679558",
    "title": "GLOW: Graph-Language Co-Reasoning for Agentic Workflow Performance Prediction",
    "abstract": "Summary: Agentic Workflows (AWs) have emerged as a promising paradigm for solving complex tasks. However, the scalability of automating their generation is severely constrained by the high cost and latency of execution-based evaluation. Existing AW performance prediction methods act as surrogates but fail to simultaneously capture the intricate topological dependencies and the deep semantic logic embedded in AWs. To address this limitation, we propose GLOW, a unified framework for AW performance prediction that combines the graph-structure modeling capabilities of GNNs with the reasoning power of LLMs. Specifically, we introduce a graph-oriented LLM, instruction-tuned on graph tasks, to extract topologically aware semantic features, which are fused with GNN-encoded structural representations. A contrastive alignment strategy further refines the latent space to distinguish high-quality AWs. Extensive experiments on FLORA-Bench show that GLOW outperforms state-of-the-art baselines in prediction accuracy and ranking utility.",
    "paper_id": "2512.15751v1"
  },
  "https://arxiv.org/abs/2512.20806v1": {
    "content_path": "../data/contents/2512.20806v1.md",
    "pdf_path": "../data/contents/2512.20806v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:05.337708",
    "title": "Safety Alignment of LMs via Non-cooperative Games",
    "abstract": "Summary: Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.",
    "paper_id": "2512.20806v1"
  },
  "https://arxiv.org/abs/2512.18748v2": {
    "content_path": "../data/contents/2512.18748v2.md",
    "pdf_path": "../data/contents/2512.18748v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:05.578288",
    "title": "Code2Doc: A Quality-First Curated Dataset for Code Documentation",
    "abstract": "Summary: The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.\n  We introduce Code2Doc, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6% satisfy all quality constraints.\n  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.",
    "paper_id": "2512.18748v2"
  },
  "https://arxiv.org/abs/2512.14481v1": {
    "content_path": "../data/contents/2512.14481v1.md",
    "pdf_path": "../data/contents/2512.14481v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:06.074310",
    "title": "SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models",
    "abstract": "Summary: Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.",
    "paper_id": "2512.14481v1"
  },
  "https://arxiv.org/abs/2512.10611v2": {
    "content_path": "../data/contents/2512.10611v2.md",
    "pdf_path": "../data/contents/2512.10611v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:11.477593",
    "title": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs",
    "abstract": "Summary: Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.",
    "paper_id": "2512.10611v2"
  },
  "https://arxiv.org/abs/2512.20798v1": {
    "content_path": "../data/contents/2512.20798v1.md",
    "pdf_path": "../data/contents/2512.20798v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:11.720703",
    "title": "A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents",
    "abstract": "Summary: As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant \"deliberative misalignment\", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.",
    "paper_id": "2512.20798v1"
  },
  "https://arxiv.org/abs/2512.18747v1": {
    "content_path": "../data/contents/2512.18747v1.md",
    "pdf_path": "../data/contents/2512.18747v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:12.495798",
    "title": "IPCV: Information-Preserving Compression for MLLM Visual Encoders",
    "abstract": "Summary: Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV.",
    "paper_id": "2512.18747v1"
  },
  "https://arxiv.org/abs/2512.14474v1": {
    "content_path": "../data/contents/2512.14474v1.md",
    "pdf_path": "../data/contents/2512.14474v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:13.089567",
    "title": "Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling",
    "abstract": "Summary: Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.",
    "paper_id": "2512.14474v1"
  },
  "https://arxiv.org/abs/2512.10610v1": {
    "content_path": "../data/contents/2512.10610v1.md",
    "pdf_path": "../data/contents/2512.10610v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:13.682808",
    "title": "Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing",
    "abstract": "Summary: We present Thinking While Driving, a concurrent routing framework that integrates LLMs into a graph-based traffic environment. Unlike approaches that require agents to stop and deliberate, our system enables LLM-based route planning while agents are moving, significantly reducing intersection wait times. Under high traffic, agents average just 0.75 seconds of decision latency. To coordinate many agents in real-time, we implement a non-blocking asynchronous architecture using Unity coroutines and a dedicated request manager. The environment is a weighted undirected graph with live congestion metrics, updated continuously by the agents to enable shared perception. Our results show LLM-driven agents can dynamically adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding, all while maintaining real-time performance. This work provides a reproducible framework for future research in adaptive routing and multi-agent cooperation.",
    "paper_id": "2512.10610v1"
  },
  "https://arxiv.org/abs/2512.20796v1": {
    "content_path": "../data/contents/2512.20796v1.md",
    "pdf_path": "../data/contents/2512.20796v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:14.465869",
    "title": "Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?",
    "abstract": "Summary: We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.",
    "paper_id": "2512.20796v1"
  },
  "https://arxiv.org/abs/2512.18746v1": {
    "content_path": "../data/contents/2512.18746v1.md",
    "pdf_path": "../data/contents/2512.18746v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:18.632832",
    "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
    "abstract": "Summary: Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
    "paper_id": "2512.18746v1"
  },
  "https://arxiv.org/abs/2512.14792v1": {
    "content_path": "../data/contents/2512.14792v1.md",
    "pdf_path": "../data/contents/2512.14792v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:25.904786",
    "title": "IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection",
    "abstract": "Summary: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a \"Correctness-Congruence Gap\" where LLMs can become proficient \"coders\" but remain limited \"architects\" in fulfilling nuanced user intent.",
    "paper_id": "2512.14792v1"
  },
  "https://arxiv.org/abs/2512.10601v1": {
    "content_path": "../data/contents/2512.10601v1.md",
    "pdf_path": "../data/contents/2512.10601v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:32.547437",
    "title": "Multi-Objective Reward and Preference Optimization: Theory and Algorithms",
    "abstract": "Summary: This thesis develops theoretical frameworks and algorithms that advance constrained reinforcement learning (RL) across control, preference learning, and alignment of large language models. The first contribution addresses constrained Markov Decision Processes (CMDPs) under the average-cost criterion through the Average-Constrained Policy Optimization (ACPO) algorithm. ACPO integrates sensitivity analysis with trust-region updates to ensure stable constraint handling, achieving state-of-the-art empirical performance with theoretical guarantees. Constrained RL is then extended to finite-horizon settings via e-COP, the first policy optimization method for episodic CMDPs. Built on an episodic policy difference lemma, e-COP offers provable performance, simplicity, and scalability in safety-critical environments. The thesis then investigates reinforcement learning from human preferences. warmPref-PS introduces a posterior sampling strategy for linear bandits that integrates offline preference data from heterogeneous raters into online learning. Explicit modeling of rater competence yields substantial regret reduction and more efficient data collection for RLHF. The PSPL algorithm further advances preference-based RL by jointly sampling reward models and transition dynamics from pairwise trajectory comparisons, providing Bayesian simple-regret guarantees and robust empirical identification of optimal policies. The final contribution applies these methods to large-scale model alignment. A multi-objective constrained optimization view yields MOPO, an iterative algorithm with closed-form updates that scales to multi-billion-parameter language models and remains robust across alignment settings. Collectively, the thesis unifies constrained RL across average-cost, episodic, and preference-driven paradigms, delivering theoretical advances and practical tools for safe and aligned decision-making.",
    "paper_id": "2512.10601v1"
  },
  "https://arxiv.org/abs/2512.20795v1": {
    "content_path": "../data/contents/2512.20795v1.md",
    "pdf_path": "../data/contents/2512.20795v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:34.471436",
    "title": "RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale",
    "abstract": "Summary: Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and tightly coupled, agent-driven control within a single execution campaign. These workflows impose heterogeneous and often conflicting requirements on runtime systems, spanning MPI executables, persistent AI services, fine-grained tasks, and low-latency AI-HPC coupling. Existing systems typically address only subsets of these requirements, limiting their ability to support emerging AI-HPC applications at scale. We present RHAPSODY, a multi-runtime middleware that enables concurrent execution of heterogeneous AI-HPC workloads through uniform abstractions for tasks, services, resources, and execution policies. Rather than replacing existing runtimes, RHAPSODY composes and coordinates them, allowing simulation codes, inference services, and agentic workflows to coexist within a single job allocation on leadership-class HPC platforms. We evaluate RHAPSODY with Dragon and vLLM on multiple HPC systems using representative heterogeneous, inference-at-scale, and tightly coupled AI-HPC workflows. Our results show that RHAPSODY introduces minimal runtime overhead, sustains increasing heterogeneity at scale, achieves near-linear scaling for high-throughput inference workloads, and data- and control-efficient coupling between AI and HPC tasks in agentic workflows.",
    "paper_id": "2512.20795v1"
  },
  "https://arxiv.org/abs/2512.18745v1": {
    "content_path": "../data/contents/2512.18745v1.md",
    "pdf_path": "../data/contents/2512.18745v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:55.978619",
    "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
    "abstract": "Summary: The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .",
    "paper_id": "2512.18745v1"
  },
  "https://arxiv.org/abs/2512.14465v1": {
    "content_path": "../data/contents/2512.14465v1.md",
    "pdf_path": "../data/contents/2512.14465v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:56.403931",
    "title": "Context-Picker: Dynamic context selection using multi-stage reinforcement learning",
    "abstract": "Summary: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \\emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \\emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \\emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines \"minimal sufficient sets\" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.",
    "paper_id": "2512.14465v1"
  },
  "https://arxiv.org/abs/2512.10600v1": {
    "content_path": "../data/contents/2512.10600v1.md",
    "pdf_path": "../data/contents/2512.10600v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:33:58.083886",
    "title": "Authority Backdoor: A Certifiable Backdoor Mechanism for Authoring DNNs",
    "abstract": "Summary: Deep Neural Networks (DNNs), as valuable intellectual property, face unauthorized use. Existing protections, such as digital watermarking, are largely passive; they provide only post-hoc ownership verification and cannot actively prevent the illicit use of a stolen model. This work proposes a proactive protection scheme, dubbed ``Authority Backdoor,\" which embeds access constraints directly into the model. In particular, the scheme utilizes a backdoor learning framework to intrinsically lock a model's utility, such that it performs normally only in the presence of a specific trigger (e.g., a hardware fingerprint). But in its absence, the DNN's performance degrades to be useless. To further enhance the security of the proposed authority scheme, the certifiable robustness is integrated to prevent an adaptive attacker from removing the implanted backdoor. The resulting framework establishes a secure authority mechanism for DNNs, combining access control with certifiable robustness against adversarial attacks. Extensive experiments on diverse architectures and datasets validate the effectiveness and certifiable robustness of the proposed framework.",
    "paper_id": "2512.10600v1"
  },
  "https://arxiv.org/abs/2512.21337v1": {
    "content_path": "../data/contents/2512.21337v1.md",
    "pdf_path": "../data/contents/2512.21337v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:00.404198",
    "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
    "abstract": "Summary: We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/",
    "paper_id": "2512.21337v1"
  },
  "https://arxiv.org/abs/2512.18737v1": {
    "content_path": "../data/contents/2512.18737v1.md",
    "pdf_path": "../data/contents/2512.18737v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:01.218921",
    "title": "PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation",
    "abstract": "Summary: The estimation of individual treatment effects (ITE) focuses on predicting the outcome changes that result from a change in treatment. A fundamental challenge in observational data is that while we need to infer outcome differences under alternative treatments, we can only observe each individual's outcome under a single treatment. Existing approaches address this limitation either by training with inferred pseudo-outcomes or by creating matched instance pairs. However, recent work has largely overlooked the potential impact of post-treatment variables on the outcome. This oversight prevents existing methods from fully capturing outcome variability, resulting in increased variance in counterfactual predictions. This paper introduces Pseudo-outcome Imputation with Post-treatment Variables for Counterfactual Regression (PIPCFR), a novel approach that incorporates post-treatment variables to improve pseudo-outcome imputation. We analyze the challenges inherent in utilizing post-treatment variables and establish a novel theoretical bound for ITE risk that explicitly connects post-treatment variables to ITE estimation accuracy. Unlike existing methods that ignore these variables or impose restrictive assumptions, PIPCFR learns effective representations that preserve informative components while mitigating bias. Empirical evaluations on both real-world and simulated datasets demonstrate that PIPCFR achieves significantly lower ITE errors compared to existing methods.",
    "paper_id": "2512.18737v1"
  },
  "https://arxiv.org/abs/2512.14448v1": {
    "content_path": "../data/contents/2512.14448v1.md",
    "pdf_path": "../data/contents/2512.14448v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:01.800615",
    "title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space",
    "abstract": "Summary: Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.",
    "paper_id": "2512.14448v1"
  },
  "https://arxiv.org/abs/2512.10596v1": {
    "content_path": "../data/contents/2512.10596v1.md",
    "pdf_path": "../data/contents/2512.10596v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:03.385864",
    "title": "Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval",
    "abstract": "Summary: Semantic retrieval of remote sensing (RS) images is a critical task fundamentally challenged by the \\textquote{semantic gap}, the discrepancy between a model's low-level visual features and high-level human concepts. While large Vision-Language Models (VLMs) offer a promising path to bridge this gap, existing methods often rely on costly, domain-specific training, and there is a lack of benchmarks to evaluate the practical utility of VLM-generated text in a zero-shot retrieval context. To address this research gap, we introduce the Remote Sensing Rich Text (RSRT) dataset, a new benchmark featuring multiple structured captions per image. Based on this dataset, we propose a fully training-free, text-only retrieval reference called TRSLLaVA. Our methodology reformulates cross-modal retrieval as a text-to-text (T2T) matching problem, leveraging rich text descriptions as queries against a database of VLM-generated captions within a unified textual embedding space. This approach completely bypasses model training or fine-tuning. Experiments on the RSITMD and RSICD benchmarks show our training-free method is highly competitive with state-of-the-art supervised models. For instance, on RSITMD, our method achieves a mean Recall of 42.62\\%, nearly doubling the 23.86\\% of the standard zero-shot CLIP baseline and surpassing several top supervised models. This validates that high-quality semantic representation through structured text provides a powerful and cost-effective paradigm for remote sensing image retrieval.",
    "paper_id": "2512.10596v1"
  },
  "https://arxiv.org/abs/2512.21334v1": {
    "content_path": "../data/contents/2512.21334v1.md",
    "pdf_path": "../data/contents/2512.21334v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:04.629776",
    "title": "Streaming Video Instruction Tuning",
    "abstract": "Summary: We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.",
    "paper_id": "2512.21334v1"
  },
  "https://arxiv.org/abs/2512.18735v1": {
    "content_path": "../data/contents/2512.18735v1.md",
    "pdf_path": "../data/contents/2512.18735v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:05.037790",
    "title": "$M^3-Verse$: A \"Spot the Difference\" Challenge for Large Multimodal Models",
    "abstract": "Summary: Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.",
    "paper_id": "2512.18735v1"
  },
  "https://arxiv.org/abs/2512.14429v1": {
    "content_path": "../data/contents/2512.14429v1.md",
    "pdf_path": "../data/contents/2512.14429v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:11.748272",
    "title": "Seismology modeling agent: A smart assistant for geophysical researchers",
    "abstract": "Summary: To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.",
    "paper_id": "2512.14429v1"
  },
  "https://arxiv.org/abs/2512.10589v1": {
    "content_path": "../data/contents/2512.10589v1.md",
    "pdf_path": "../data/contents/2512.10589v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:14.298399",
    "title": "THeGAU: Type-Aware Heterogeneous Graph Autoencoder and Augmentation",
    "abstract": "Summary: Heterogeneous Graph Neural Networks (HGNNs) are effective for modeling Heterogeneous Information Networks (HINs), which encode complex multi-typed entities and relations. However, HGNNs often suffer from type information loss and structural noise, limiting their representational fidelity and generalization. We propose THeGAU, a model-agnostic framework that combines a type-aware graph autoencoder with guided graph augmentation to improve node classification. THeGAU reconstructs schema-valid edges as an auxiliary task to preserve node-type semantics and introduces a decoder-driven augmentation mechanism to selectively refine noisy structures. This joint design enhances robustness, accuracy, and efficiency while significantly reducing computational overhead. Extensive experiments on three benchmark HIN datasets (IMDB, ACM, and DBLP) demonstrate that THeGAU consistently outperforms existing HGNN methods, achieving state-of-the-art performance across multiple backbones.",
    "paper_id": "2512.10589v1"
  },
  "https://arxiv.org/abs/2512.21333v1": {
    "content_path": "../data/contents/2512.21333v1.md",
    "pdf_path": "../data/contents/2512.21333v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:17.741354",
    "title": "Fast SAM2 with Text-Driven Token Pruning",
    "abstract": "Summary: Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.",
    "paper_id": "2512.21333v1"
  },
  "https://arxiv.org/abs/2512.18733v1": {
    "content_path": "../data/contents/2512.18733v1.md",
    "pdf_path": "../data/contents/2512.18733v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:18.205848",
    "title": "Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection",
    "abstract": "Summary: Large language model (LLM)-based multi-agent systems (MAS) have shown strong capabilities in solving complex tasks. As MAS become increasingly autonomous in various safety-critical tasks, detecting malicious agents has become a critical security concern. Although existing graph anomaly detection (GAD)-based defenses can identify anomalous agents, they mainly rely on coarse sentence-level information and overlook fine-grained lexical cues, leading to suboptimal performance. Moreover, the lack of interpretability in these methods limits their reliability and real-world applicability. To address these limitations, we propose XG-Guard, an explainable and fine-grained safeguarding framework for detecting malicious agents in MAS. To incorporate both coarse and fine-grained textual information for anomalous agent identification, we utilize a bi-level agent encoder to jointly model the sentence- and token-level representations of each agent. A theme-based anomaly detector further captures the evolving discussion focus in MAS dialogues, while a bi-level score fusion mechanism quantifies token-level contributions for explanation. Extensive experiments across diverse MAS topologies and attack scenarios demonstrate robust detection performance and strong interpretability of XG-Guard.",
    "paper_id": "2512.18733v1"
  },
  "https://arxiv.org/abs/2512.14427v1": {
    "content_path": "../data/contents/2512.14427v1.md",
    "pdf_path": "../data/contents/2512.14427v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:18.445808",
    "title": "Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models",
    "abstract": "Summary: The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.",
    "paper_id": "2512.14427v1"
  },
  "https://arxiv.org/abs/2512.10576v1": {
    "content_path": "../data/contents/2512.10576v1.md",
    "pdf_path": "../data/contents/2512.10576v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:18.945165",
    "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp",
    "abstract": "Summary: DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.",
    "paper_id": "2512.10576v1"
  },
  "https://arxiv.org/abs/2512.21218v1": {
    "content_path": "../data/contents/2512.21218v1.md",
    "pdf_path": "../data/contents/2512.21218v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:23.387819",
    "title": "Latent Implicit Visual Reasoning",
    "abstract": "Summary: While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.",
    "paper_id": "2512.21218v1"
  },
  "https://arxiv.org/abs/2512.18730v1": {
    "content_path": "../data/contents/2512.18730v1.md",
    "pdf_path": "../data/contents/2512.18730v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:23.725525",
    "title": "A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models",
    "abstract": "Summary: Large language models (LLMs) trained via KL-regularized reinforcement learning demonstrate strong instruction following, self-correction, and reasoning abilities. Yet their theoretical underpinnings remain limited. We exploit the closed-form energy-based model (EBM) structure of the optimal KL-regularized policy to provide a unified variational analysis of LLMs.\n  For instruction-tuned models, under natural assumptions on reward potentials and pretraining symmetry, we prove that the transition kernel satisfies detailed balance with respect to a scalar potential encoding response quality. This yields monotonic KL convergence to a high-quality stationary distribution, bounded hitting times to superior states, and exponential mixing governed by the spectral gap.\n  For reasoning models trained with verifiable rewards (RLVR), we show the objective is equivalent to expected KL minimization toward an optimal reasoning distribution, with the suboptimality gap reducing to the Bernoulli KL between target and current accuracies along the natural gradient flow. This helps explain empirical entropy-accuracy trade-offs.",
    "paper_id": "2512.18730v1"
  },
  "https://arxiv.org/abs/2512.14779v1": {
    "content_path": "../data/contents/2512.14779v1.md",
    "pdf_path": "../data/contents/2512.14779v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:25.924269",
    "title": "Evaluating Weather Forecasts from a Decision Maker's Perspective",
    "abstract": "Summary: Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.",
    "paper_id": "2512.14779v1"
  },
  "https://arxiv.org/abs/2512.10575v1": {
    "content_path": "../data/contents/2512.10575v1.md",
    "pdf_path": "../data/contents/2512.10575v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:27.218079",
    "title": "RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems",
    "abstract": "Summary: Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.",
    "paper_id": "2512.10575v1"
  },
  "https://arxiv.org/abs/2512.21194v1": {
    "content_path": "../data/contents/2512.21194v1.md",
    "pdf_path": "../data/contents/2512.21194v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:39.378377",
    "title": "VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs",
    "abstract": "Summary: Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.",
    "paper_id": "2512.21194v1"
  },
  "https://arxiv.org/abs/2512.18725v2": {
    "content_path": "../data/contents/2512.18725v2.md",
    "pdf_path": "../data/contents/2512.18725v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:39.762188",
    "title": "ML Inference Scheduling with Predictable Latency",
    "abstract": "Summary: Machine learning (ML) inference serving systems can schedule requests to improve GPU utilization and to meet service level objectives (SLOs) or deadlines. However, improving GPU utilization may compromise latency-sensitive scheduling, as concurrent tasks contend for GPU resources and thereby introduce interference. Given that interference effects introduce unpredictability in scheduling, neglecting them may compromise SLO or deadline satisfaction. Nevertheless, existing interference prediction approaches remain limited in several respects, which may restrict their usefulness for scheduling. First, they are often coarse-grained, which ignores runtime co-location dynamics and thus restricts their accuracy in interference prediction. Second, they tend to use a static prediction model, which may not effectively cope with different workload characteristics. In this paper, we evaluate the potential limitations of existing interference prediction approaches, finding that coarse-grained methods can lead to noticeable deviations in prediction accuracy and that static models degrade considerably under changing workloads.",
    "paper_id": "2512.18725v2"
  },
  "https://arxiv.org/abs/2512.14420v1": {
    "content_path": "../data/contents/2512.14420v1.md",
    "pdf_path": "../data/contents/2512.14420v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:42.917864",
    "title": "DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning",
    "abstract": "Summary: Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.",
    "paper_id": "2512.14420v1"
  },
  "https://arxiv.org/abs/2512.21095v1": {
    "content_path": "../data/contents/2512.21095v1.md",
    "pdf_path": "../data/contents/2512.21095v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:47.529100",
    "title": "UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters",
    "abstract": "Summary: Text and formulas constitute the core informational components of many documents. Accurately and efficiently recognizing both is crucial for developing robust and generalizable document parsing systems. Recently, vision-language models (VLMs) have achieved impressive unified recognition of text and formulas. However, they are large-sized and computationally demanding, restricting their usage in many applications. In this paper, we propose UniRec-0.1B, a unified recognition model with only 0.1B parameters. It is capable of performing text and formula recognition at multiple levels, including characters, words, lines, paragraphs, and documents. To implement this task, we first establish UniRec40M, a large-scale dataset comprises 40 million text, formula and their mix samples, enabling the training of a powerful yet lightweight model. Secondly, we identify two challenges when building such a lightweight but unified expert model. They are: structural variability across hierarchies and semantic entanglement between textual and formulaic content. To tackle these, we introduce a hierarchical supervision training that explicitly guides structural comprehension, and a semantic-decoupled tokenizer that separates text and formula representations. Finally, we develop a comprehensive evaluation benchmark covering Chinese and English documents from multiple domains and with multiple levels. Experimental results on this and public benchmarks demonstrate that UniRec-0.1B outperforms both general-purpose VLMs and leading document parsing expert models, while achieving a 2-9$\\times$ speedup, validating its effectiveness and efficiency. Codebase and Dataset: https://github.com/Topdu/OpenOCR.",
    "paper_id": "2512.21095v1"
  },
  "https://arxiv.org/abs/2512.18722v1": {
    "content_path": "../data/contents/2512.18722v1.md",
    "pdf_path": "../data/contents/2512.18722v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:48.794682",
    "title": "Generating Risky Samples with Conformity Constraints via Diffusion Models",
    "abstract": "Summary: Although neural networks achieve promising performance in many tasks, they may still fail when encountering some examples and bring about risks to applications. To discover risky samples, previous literature attempts to search for patterns of risky samples within existing datasets or inject perturbation into them. Yet in this way the diversity of risky samples is limited by the coverage of existing datasets. To overcome this limitation, recent works adopt diffusion models to produce new risky samples beyond the coverage of existing datasets. However, these methods struggle in the conformity between generated samples and expected categories, which could introduce label noise and severely limit their effectiveness in applications. To address this issue, we propose RiskyDiff that incorporates the embeddings of both texts and images as implicit constraints of category conformity. We also design a conformity score to further explicitly strengthen the category conformity, as well as introduce the mechanisms of embedding screening and risky gradient guidance to boost the risk of generated samples. Extensive experiments reveal that RiskyDiff greatly outperforms existing methods in terms of the degree of risk, generation quality, and conformity with conditioned categories. We also empirically show the generalization ability of the models can be enhanced by augmenting training data with generated samples of high conformity.",
    "paper_id": "2512.18722v1"
  },
  "https://arxiv.org/abs/2512.14417v1": {
    "content_path": "../data/contents/2512.14417v1.md",
    "pdf_path": "../data/contents/2512.14417v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:49.604552",
    "title": "PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals",
    "abstract": "Summary: Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created",
    "paper_id": "2512.14417v1"
  },
  "https://arxiv.org/abs/2512.21008v1": {
    "content_path": "../data/contents/2512.21008v1.md",
    "pdf_path": "../data/contents/2512.21008v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:50.102885",
    "title": "GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs",
    "abstract": "Summary: Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness.\n  In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.",
    "paper_id": "2512.21008v1"
  },
  "https://arxiv.org/abs/2512.18683v1": {
    "content_path": "../data/contents/2512.18683v1.md",
    "pdf_path": "../data/contents/2512.18683v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:50.367161",
    "title": "CIRR: Causal-Invariant Retrieval-Augmented Recommendation with Faithful Explanations under Distribution Shift",
    "abstract": "Summary: Recent advances in retrieval-augmented generation (RAG) have shown promise in enhancing recommendation systems with external knowledge. However, existing RAG-based recommenders face two critical challenges: (1) vulnerability to distribution shifts across different environments (e.g., time periods, user segments), leading to performance degradation in out-of-distribution (OOD) scenarios, and (2) lack of faithful explanations that can be verified against retrieved evidence. In this paper, we propose CIRR, a Causal-Invariant Retrieval-Augmented Recommendation framework that addresses both challenges simultaneously. CIRR learns environment-invariant user preference representations through causal inference, which guide a debiased retrieval process to select relevant evidence from multiple sources. Furthermore, we introduce consistency constraints that enforce faithfulness between retrieved evidence, generated explanations, and recommendation outputs. Extensive experiments on two real-world datasets demonstrate that CIRR achieves robust performance under distribution shifts, reducing performance degradation from 15.4% (baseline) to only 5.6% in OOD scenarios, while providing more faithful and interpretable explanations (26% improvement in faithfulness score) compared to state-of-the-art baselines.",
    "paper_id": "2512.18683v1"
  },
  "https://arxiv.org/abs/2512.14410v1": {
    "content_path": "../data/contents/2512.14410v1.md",
    "pdf_path": "../data/contents/2512.14410v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:51.463388",
    "title": "Pattern Recognition of Aluminium Arbitrage in Global Trade Data",
    "abstract": "Summary: As the global economy transitions toward decarbonization, the aluminium sector has become a focal point for strategic resource management. While policies such as the Carbon Border Adjustment Mechanism (CBAM) aim to reduce emissions, they have inadvertently widened the price arbitrage between primary metal, scrap, and semi-finished goods, creating new incentives for market optimization. This study presents a unified, unsupervised machine learning framework to detect and classify emerging trade anomalies within UN Comtrade data (2020 to 2024). Moving beyond traditional rule-based monitoring, we apply a four-layer analytical pipeline utilizing Forensic Statistics, Isolation Forests, Network Science, and Deep Autoencoders. Contrary to the hypothesis that Sustainability Arbitrage would be the primary driver, empirical results reveal a contradictory and more severe phenomenon of Hardware Masking. Illicit actors exploit bi-directional tariff incentives by misclassifying scrap as high-count heterogeneous goods to justify extreme unit-price outliers of >$160/kg, a 1,900% markup indicative of Trade-Based Money Laundering (TBML) rather than commercial arbitrage. Topologically, risk is not concentrated in major exporters but in high-centrality Shadow Hubs that function as pivotal nodes for illicit rerouting. These actors execute a strategy of Void-Shoring, systematically suppressing destination data to Unspecified Code to fracture mirror statistics and sever forensic trails. Validated by SHAP (Shapley Additive Explanations), the results confirm that price deviation is the dominant predictor of anomalies, necessitating a paradigm shift in customs enforcement from physical volume checks to dynamic, algorithmic valuation auditing.",
    "paper_id": "2512.14410v1"
  },
  "https://arxiv.org/abs/2512.20986v1": {
    "content_path": "../data/contents/2512.20986v1.md",
    "pdf_path": "../data/contents/2512.20986v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:52.073979",
    "title": "AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs",
    "abstract": "Summary: The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs. Traditional defenses, based on static filters and rigid rules, are insufficient to address the semantic complexity of these new attacks. We argue that a paradigm shift is needed -- from passive filtering to active protection and autonomous reasoning. We introduce AegisAgent, an autonomous agent system designed to ensure the security of LLM-driven HAR systems. Instead of merely blocking threats, AegisAgent functions as a cognitive guardian. It autonomously perceives potential semantic inconsistencies, reasons about the user's true intent by consulting a dynamic memory of past interactions, and acts by generating and executing a multi-step verification and repair plan. We implement AegisAgent as a lightweight, full-stack prototype and conduct a systematic evaluation on 15 common attacks with five state-of-the-art LLM-based HAR systems on three public datasets. Results show it reduces attack success rate by 30\\% on average while incurring only 78.6 ms of latency overhead on a GPU workstation. Our work makes the first step towards building secure and trustworthy LLM-driven HAR systems.",
    "paper_id": "2512.20986v1"
  },
  "https://arxiv.org/abs/2512.18682v1": {
    "content_path": "../data/contents/2512.18682v1.md",
    "pdf_path": "../data/contents/2512.18682v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:34:54.345221",
    "title": "Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design",
    "abstract": "Summary: In the high-cost simulation-driven design domain, translating ambiguous design requirements into a mathematical optimization formulation is a bottleneck for optimizing product performance. This process is time-consuming and heavily reliant on expert knowledge. While large language models (LLMs) offer potential for automating this task, existing approaches either suffer from poor formalization that fails to accurately align with the design intent or rely on solver feedback for data filtering, which is unavailable due to the high simulation costs. To address this challenge, we propose APF, a framework for solver-independent, automated problem formulation via LLMs designed to automatically convert engineers' natural language requirements into executable optimization models. The core of this framework is an innovative pipeline for automatically generating high-quality data, which overcomes the difficulty of constructing suitable fine-tuning datasets in the absence of high-cost solver feedback with the help of data generation and test instance annotation. The generated high-quality dataset is used to perform supervised fine-tuning on LLMs, significantly enhancing their ability to generate accurate and executable optimization problem formulations. Experimental results on antenna design demonstrate that APF significantly outperforms the existing methods in both the accuracy of requirement formalization and the quality of resulting radiation efficiency curves in meeting the design goals.",
    "paper_id": "2512.18682v1"
  },
  "https://arxiv.org/abs/2512.14400v1": {
    "content_path": "../data/contents/2512.14400v1.md",
    "pdf_path": "../data/contents/2512.14400v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:13.715773",
    "title": "GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion",
    "abstract": "Summary: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.",
    "paper_id": "2512.14400v1"
  },
  "https://arxiv.org/abs/2512.20940v1": {
    "content_path": "../data/contents/2512.20940v1.md",
    "pdf_path": "../data/contents/2512.20940v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:16.082304",
    "title": "ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments",
    "abstract": "Summary: Vision-Language Navigation in Continuous Environments (VLN-CE) requires an embodied agent to navigate towards target in continuous environments, following natural language instructions. While current graph-based methods offer an efficient, structured approach by abstracting the environment into a topological map and simplifying the action space to waypoint selection, they lag behind methods based on Large Vision-Language Models (LVLMs) in leveraging large-scale data and advanced training paradigms. In this paper, we try to bridge this gap by introducing ETP-R1, a framework that applies the paradigm of scaling up data and Reinforcement Fine-Tuning (RFT) to a graph-based VLN-CE model. To build a strong foundation, we first construct a high-quality, large-scale pretraining dataset using the Gemini API. This dataset consists of diverse, low-hallucination instructions for topological trajectories, providing rich supervision for our graph-based policy to map language to topological paths. This foundation is further strengthened by unifying data from both R2R and RxR tasks for joint pretraining. Building on this, we introduce a three-stage training paradigm, which culminates in the first application of closed-loop, online RFT to a graph-based VLN-CE model, powered by the Group Relative Policy Optimization (GRPO) algorithm. Extensive experiments demonstrate that our approach is highly effective, establishing new state-of-the-art performance across all major metrics on both the R2R-CE and RxR-CE benchmarks. Our code is available at https://github.com/Cepillar/ETP-R1.",
    "paper_id": "2512.20940v1"
  },
  "https://arxiv.org/abs/2512.18674v1": {
    "content_path": "../data/contents/2512.18674v1.md",
    "pdf_path": "../data/contents/2512.18674v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:16.755388",
    "title": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
    "abstract": "Summary: Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines.",
    "paper_id": "2512.18674v1"
  },
  "https://arxiv.org/abs/2512.14395v2": {
    "content_path": "../data/contents/2512.14395v2.md",
    "pdf_path": "../data/contents/2512.14395v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:18.177895",
    "title": "Massive Editing for Large Language Models Based on Dynamic Weight Generation",
    "abstract": "Summary: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.",
    "paper_id": "2512.14395v2"
  },
  "https://arxiv.org/abs/2512.20936v1": {
    "content_path": "../data/contents/2512.20936v1.md",
    "pdf_path": "../data/contents/2512.20936v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:20.508662",
    "title": "Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation",
    "abstract": "Summary: Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.",
    "paper_id": "2512.20936v1"
  },
  "https://arxiv.org/abs/2512.18670v1": {
    "content_path": "../data/contents/2512.18670v1.md",
    "pdf_path": "../data/contents/2512.18670v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:38.744608",
    "title": "Demonstration-Guided Continual Reinforcement Learning in Dynamic Environments",
    "abstract": "Summary: Reinforcement learning (RL) excels in various applications but struggles in dynamic environments where the underlying Markov decision process evolves. Continual reinforcement learning (CRL) enables RL agents to continually learn and adapt to new tasks, but balancing stability (preserving prior knowledge) and plasticity (acquiring new knowledge) remains challenging. Existing methods primarily address the stability-plasticity dilemma through mechanisms where past knowledge influences optimization but rarely affects the agent's behavior directly, which may hinder effective knowledge reuse and efficient learning. In contrast, we propose demonstration-guided continual reinforcement learning (DGCRL), which stores prior knowledge in an external, self-evolving demonstration repository that directly guides RL exploration and adaptation. For each task, the agent dynamically selects the most relevant demonstration and follows a curriculum-based strategy to accelerate learning, gradually shifting from demonstration-guided exploration to fully self-exploration. Extensive experiments on 2D navigation and MuJoCo locomotion tasks demonstrate its superior average performance, enhanced knowledge transfer, mitigation of forgetting, and training efficiency. The additional sensitivity analysis and ablation study further validate its effectiveness.",
    "paper_id": "2512.18670v1"
  },
  "https://arxiv.org/abs/2512.14391v1": {
    "content_path": "../data/contents/2512.14391v1.md",
    "pdf_path": "../data/contents/2512.14391v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:39.969644",
    "title": "RePo: Language Models with Context Re-Positioning",
    "abstract": "Summary: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.",
    "paper_id": "2512.14391v1"
  },
  "https://arxiv.org/abs/2512.20926v1": {
    "content_path": "../data/contents/2512.20926v1.md",
    "pdf_path": "../data/contents/2512.20926v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:40.750993",
    "title": "Uncovering Hierarchical Structure in LLM Embeddings with $δ$-Hyperbolicity, Ultrametricity, and Neighbor Joining",
    "abstract": "Summary: The rapid advancement of large language models (LLMs) has enabled significant strides in various fields. This paper introduces a novel approach to evaluate the effectiveness of LLM embeddings in the context of inherent geometric properties. We investigate the structural properties of these embeddings through three complementary metrics $δ$-hyperbolicity, Ultrametricity, and Neighbor Joining. $δ$-hyperbolicity, a measure derived from geometric group theory, quantifies how much a metric space deviates from being a tree-like structure. In contrast, ultrametricity characterizes strictly hierarchical structures where distances obey a strong triangle inequality. While Neighbor Joining quantifies how tree-like the distance relationships are, it does so specifically with respect to the tree reconstructed by the Neighbor Joining algorithm. By analyzing the embeddings generated by LLMs using these metrics, we uncover to what extent the embedding space reflects an underlying hierarchical or tree-like organization. Our findings reveal that LLM embeddings exhibit varying degrees of hyperbolicity and ultrametricity, which correlate with their performance in the underlying machine learning tasks.",
    "paper_id": "2512.20926v1"
  },
  "https://arxiv.org/abs/2512.18669v1": {
    "content_path": "../data/contents/2512.18669v1.md",
    "pdf_path": "../data/contents/2512.18669v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:41.851895",
    "title": "IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling",
    "abstract": "Summary: LLM-based tutors are typically single-turn assistants that lack persistent representations of learner knowledge, making it difficult to provide principled, transparent, and long-term pedagogical support. We introduce IntelliCode, a multi-agent LLM tutoring system built around a centralized, versioned learner state that integrates mastery estimates, misconceptions, review schedules, and engagement signals. A StateGraph Orchestrator coordinates six specialized agents: skill assessment, learner profiling, graduated hinting, curriculum selection, spaced repetition, and engagement monitoring, each operating as a pure transformation over the shared state under a single-writer policy. This architecture enables auditable mastery updates, proficiency-aware hints, dependency-aware curriculum adaptation, and safety-aligned prompting.\n  The demo showcases an end-to-end tutoring workflow: a learner attempts a DSA problem, receives a conceptual hint when stuck, submits a corrected solution, and immediately sees mastery updates and a personalized review interval. We report validation results with simulated learners, showing stable state updates, improved task success with graduated hints, and diverse curriculum coverage. IntelliCode demonstrates how persistent learner modeling, orchestrated multi-agent reasoning, and principled instructional design can be combined to produce transparent and reliable LLM-driven tutoring.",
    "paper_id": "2512.18669v1"
  },
  "https://arxiv.org/abs/2512.14358v2": {
    "content_path": "../data/contents/2512.14358v2.md",
    "pdf_path": "../data/contents/2512.14358v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:42.469959",
    "title": "TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation",
    "abstract": "Summary: Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.",
    "paper_id": "2512.14358v2"
  },
  "https://arxiv.org/abs/2512.15922": {
    "content_path": "../data/contents/2512.15922.md",
    "pdf_path": "../data/contents/2512.15922.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:42.769738",
    "title": "Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems",
    "abstract": "Very recent GraphRAG-style retrieval method using spreading activation over auto-constructed graphs to improve multi-hop evidence retrieval.",
    "paper_id": "2512.15922"
  },
  "https://arxiv.org/abs/2512.20901v1": {
    "content_path": "../data/contents/2512.20901v1.md",
    "pdf_path": "../data/contents/2512.20901v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:47.589569",
    "title": "Benchmarking and Enhancing VLM for Compressed Image Understanding",
    "abstract": "Summary: With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.",
    "paper_id": "2512.20901v1"
  },
  "https://arxiv.org/abs/2512.18661v1": {
    "content_path": "../data/contents/2512.18661v1.md",
    "pdf_path": "../data/contents/2512.18661v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:51.929949",
    "title": "ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting",
    "abstract": "Summary: Financial time series forecasting is fundamentally an information fusion challenge, yet most existing models rely on static architectures that struggle to integrate heterogeneous knowledge sources or adjust to rapid regime shifts. Conventional approaches, relying exclusively on historical price sequences, often neglect the semantic drivers of volatility such as policy uncertainty and market narratives. To address these limitations, we propose the ASTIF (Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting), a hybrid intelligent system that adapts its forecasting strategy in real time through confidence-based meta-learning. The framework integrates three complementary components. A dual-channel Small Language Model using MirrorPrompt extracts semantic market cues alongside numerical trends. A hybrid LSTM Random Forest model captures sequential temporal dependencies. A confidence-aware meta-learner functions as an adaptive inference layer, modulating each predictor's contribution based on its real-time uncertainty.\n  Experimental evaluation on a diverse dataset of AI-focused cryptocurrencies and major technology stocks from 2020 to 2024 shows that ASTIF outperforms leading deep learning and Transformer baselines (e.g., Informer, TFT). The ablation studies further confirm the critical role of the adaptive meta-learning mechanism, which successfully mitigates risk by shifting reliance between semantic and temporal channels during market turbulence. The research contributes a scalable, knowledge-based solution for fusing quantitative and qualitative data in non-stationary environments.",
    "paper_id": "2512.18661v1"
  },
  "https://arxiv.org/abs/2512.14354v1": {
    "content_path": "../data/contents/2512.14354v1.md",
    "pdf_path": "../data/contents/2512.14354v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:53.207050",
    "title": "Enhancing Interpretability for Vision Models via Shapley Value Optimization",
    "abstract": "Summary: Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.",
    "paper_id": "2512.14354v1"
  },
  "https://arxiv.org/abs/2512.09487": {
    "content_path": "../data/contents/2512.09487.md",
    "pdf_path": "../data/contents/2512.09487.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:54.473950",
    "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning",
    "abstract": "December 2025 hybrid text+graph RAG framework using RL to adaptively decide when/what to retrieve—relevant to agentic retrieval.",
    "paper_id": "2512.09487"
  },
  "https://arxiv.org/abs/2512.20858v1": {
    "content_path": "../data/contents/2512.20858v1.md",
    "pdf_path": "../data/contents/2512.20858v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:56.568879",
    "title": "ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction",
    "abstract": "Summary: Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.\n  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.\n  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.\n  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.",
    "paper_id": "2512.20858v1"
  },
  "https://arxiv.org/abs/2512.18660v1": {
    "content_path": "../data/contents/2512.18660v1.md",
    "pdf_path": "../data/contents/2512.18660v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:35:57.082046",
    "title": "PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval",
    "abstract": "Summary: Remote sensing (RS) image-text retrieval faces significant challenges in real-world datasets due to the presence of Pseudo-Matched Pairs (PMPs), semantically mismatched or weakly aligned image-text pairs, which hinder the learning of reliable cross-modal alignments. To address this issue, we propose a novel retrieval framework that leverages Cross-Modal Gated Attention and a Positive-Negative Awareness Attention mechanism to mitigate the impact of such noisy associations. The gated module dynamically regulates cross-modal information flow, while the awareness mechanism explicitly distinguishes informative (positive) cues from misleading (negative) ones during alignment learning. Extensive experiments on three benchmark RS datasets, i.e., RSICD, RSITMD, and RS5M, demonstrate that our method consistently achieves state-of-the-art performance, highlighting its robustness and effectiveness in handling real-world mismatches and PMPs in RS image-text retrieval tasks.",
    "paper_id": "2512.18660v1"
  },
  "https://arxiv.org/abs/2512.14341v1": {
    "content_path": "../data/contents/2512.14341v1.md",
    "pdf_path": "../data/contents/2512.14341v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:00.026350",
    "title": "Towards Transferable Defense Against Malicious Image Edits",
    "abstract": "Summary: Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.",
    "paper_id": "2512.14341v1"
  },
  "https://arxiv.org/abs/2512.12694": {
    "content_path": "../data/contents/2512.12694.md",
    "pdf_path": "../data/contents/2512.12694.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:02.501115",
    "title": "Hybrid Retrieval-Augmented Generation for Robust Multilingual Document Question Answering",
    "abstract": "Recent hybrid retrieval pipeline with query expansion + RRF fusion aimed at robustness on noisy/OCR’d multilingual corpora.",
    "paper_id": "2512.12694"
  },
  "https://arxiv.org/abs/2512.20839v1": {
    "content_path": "../data/contents/2512.20839v1.md",
    "pdf_path": "../data/contents/2512.20839v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:05.378459",
    "title": "Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference",
    "abstract": "Summary: Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.",
    "paper_id": "2512.20839v1"
  },
  "https://arxiv.org/abs/2512.18658v1": {
    "content_path": "../data/contents/2512.18658v1.md",
    "pdf_path": "../data/contents/2512.18658v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:08.643369",
    "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
    "abstract": "Summary: Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.",
    "paper_id": "2512.18658v1"
  },
  "https://arxiv.org/abs/2512.14332v1": {
    "content_path": "../data/contents/2512.14332v1.md",
    "pdf_path": "../data/contents/2512.14332v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:16.012021",
    "title": "Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring",
    "abstract": "Summary: The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.",
    "paper_id": "2512.14332v1"
  },
  "https://arxiv.org/abs/2511.19987": {
    "content_path": "../data/contents/2511.19987.md",
    "pdf_path": "../data/contents/2511.19987.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:17.457766",
    "title": "R²R: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers",
    "abstract": "New reranker post-training framework for multi-domain specialization (routing + abstraction) directly relevant to RAG reranking.",
    "paper_id": "2511.19987"
  },
  "https://arxiv.org/abs/2512.20789v1": {
    "content_path": "../data/contents/2512.20789v1.md",
    "pdf_path": "../data/contents/2512.20789v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:17.880418",
    "title": "X-GridAgent: An LLM-Powered Agentic AI System for Assisting Power Grid Analysis",
    "abstract": "Summary: The growing complexity of power system operations has created an urgent need for intelligent, automated tools to support reliable and efficient grid management. Conventional analysis tools often require significant domain expertise and manual effort, which limits their accessibility and adaptability. To address these challenges, this paper presents X-GridAgent, a novel large language model (LLM)-powered agentic AI system designed to automate complex power system analysis through natural language queries. The system integrates domain-specific tools and specialized databases under a three-layer hierarchical architecture comprising planning, coordination, and action layers. This architecture offers high flexibility and adaptability to previously unseen tasks, while providing a modular and extensible framework that can be readily expanded to incorporate new tools, data sources, or analytical capabilities. To further enhance performance, we introduce two novel algorithms: (1) LLM-driven prompt refinement with human feedback, and (2) schema-adaptive hybrid retrieval-augmented generation (RAG) for accurate information retrieval from large-scale structured grid datasets. Experimental evaluations across a variety of user queries and power grid cases demonstrate the effectiveness and reliability of X-GridAgent in automating interpretable and rigorous power system analysis.",
    "paper_id": "2512.20789v1"
  },
  "https://arxiv.org/abs/2512.18634v1": {
    "content_path": "../data/contents/2512.18634v1.md",
    "pdf_path": "../data/contents/2512.18634v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:19.801771",
    "title": "From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers",
    "abstract": "Summary: Transformers can implement both generalizable algorithms (e.g., induction heads) and simple positional shortcuts (e.g., memorizing fixed output positions). In this work, we study how the choice of pretraining data distribution steers a shallow transformer toward one behavior or the other. Focusing on a minimal trigger-output prediction task -- copying the token immediately following a special trigger upon its second occurrence -- we present a rigorous analysis of gradient-based training of a single-layer transformer. In both the infinite and finite sample regimes, we prove a transition in the learned mechanism: if input sequences exhibit sufficient diversity, measured by a low ``max-sum'' ratio of trigger-to-trigger distances, the trained model implements an induction head and generalizes to unseen contexts; by contrast, when this ratio is large, the model resorts to a positional shortcut and fails to generalize out-of-distribution (OOD). We also reveal a trade-off between the pretraining context length and OOD generalization, and derive the optimal pretraining distribution that minimizes computational cost per sample. Finally, we validate our theoretical predictions with controlled synthetic experiments, demonstrating that broadening context distributions robustly induces induction heads and enables OOD generalization. Our results shed light on the algorithmic biases of pretrained transformers and offer conceptual guidelines for data-driven control of their learned behaviors.",
    "paper_id": "2512.18634v1"
  },
  "https://arxiv.org/abs/2512.14321v1": {
    "content_path": "../data/contents/2512.14321v1.md",
    "pdf_path": "../data/contents/2512.14321v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:23.610131",
    "title": "Multi-Agent Medical Decision Consensus Matrix System: An Intelligent Collaborative Framework for Oncology MDT Consultations",
    "abstract": "Summary: Multidisciplinary team (MDT) consultations are the gold standard for cancer care decision-making, yet current practice lacks structured mechanisms for quantifying consensus and ensuring decision traceability. We introduce a Multi-Agent Medical Decision Consensus Matrix System that deploys seven specialized large language model agents, including an oncologist, a radiologist, a nurse, a psychologist, a patient advocate, a nutritionist and a rehabilitation therapist, to simulate realistic MDT workflows. The framework incorporates a mathematically grounded consensus matrix that uses Kendall's coefficient of concordance to objectively assess agreement. To further enhance treatment recommendation quality and consensus efficiency, the system integrates reinforcement learning methods, including Q-Learning, PPO and DQN. Evaluation across five medical benchmarks (MedQA, PubMedQA, DDXPlus, MedBullets and SymCat) shows substantial gains over existing approaches, achieving an average accuracy of 87.5% compared with 83.8% for the strongest baseline, a consensus achievement rate of 89.3% and a mean Kendall's W of 0.823. Expert reviewers rated the clinical appropriateness of system outputs at 8.9/10. The system guarantees full evidence traceability through mandatory citations of clinical guidelines and peer-reviewed literature, following GRADE principles. This work advances medical AI by providing structured consensus measurement, role-specialized multi-agent collaboration and evidence-based explainability to improve the quality and efficiency of clinical decision-making.",
    "paper_id": "2512.14321v1"
  },
  "https://arxiv.org/abs/2511.16681": {
    "content_path": "../data/contents/2511.16681.md",
    "pdf_path": "../data/contents/2511.16681.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:27.159278",
    "title": "Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search",
    "abstract": "Targets vector DB retrieval efficiency for RAG via multi-resolution indexing and query-adaptive control; includes FAISS/Qdrant integration claim.",
    "paper_id": "2511.16681"
  },
  "https://arxiv.org/abs/2512.20781v1": {
    "content_path": "../data/contents/2512.20781v1.md",
    "pdf_path": "../data/contents/2512.20781v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:27.554196",
    "title": "Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints",
    "abstract": "Summary: Composed Image Retrieval (CIR) aims to find a target image that aligns with user intent, expressed through a reference image and a modification text. While Zero-shot CIR (ZS-CIR) methods sidestep the need for labeled training data by leveraging pretrained vision-language models, they often rely on a single fused query that merges all descriptive cues of what the user wants, tending to dilute key information and failing to account for what they wish to avoid. Moreover, current CIR benchmarks assume a single correct target per query, overlooking the ambiguity in modification texts. To address these challenges, we propose Soft Filtering with Textual constraints (SoFT), a training-free, plug-and-play filtering module for ZS-CIR. SoFT leverages multimodal large language models (LLMs) to extract two complementary constraints from the reference-modification pair: prescriptive (must-have) and proscriptive (must-avoid) constraints. These serve as semantic filters that reward or penalize candidate images to re-rank results, without modifying the base retrieval model or adding supervision. In addition, we construct a two-stage dataset pipeline that refines CIR benchmarks. We first identify multiple plausible targets per query to construct multi-target triplets, capturing the open-ended nature of user intent. Then guide multimodal LLMs to rewrite the modification text to focus on one target, while referencing contrastive distractors to ensure precision. This enables more comprehensive and reliable evaluation under varying ambiguity levels. Applied on top of CIReVL, a ZS-CIR retriever, SoFT raises R@5 to 65.25 on CIRR (+12.94), mAP@50 to 27.93 on CIRCO (+6.13), and R@50 to 58.44 on FashionIQ (+4.59), demonstrating broad effectiveness.",
    "paper_id": "2512.20781v1"
  },
  "https://arxiv.org/abs/2512.18623v1": {
    "content_path": "../data/contents/2512.18623v1.md",
    "pdf_path": "../data/contents/2512.18623v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:28.583692",
    "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction",
    "abstract": "Summary: Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.\n  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.\n  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.",
    "paper_id": "2512.18623v1"
  },
  "https://arxiv.org/abs/2512.14320v1": {
    "content_path": "../data/contents/2512.14320v1.md",
    "pdf_path": "../data/contents/2512.14320v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:35.244128",
    "title": "Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity",
    "abstract": "Summary: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.",
    "paper_id": "2512.14320v1"
  },
  "https://arxiv.org/abs/2511.07025": {
    "content_path": "../data/contents/2511.07025.md",
    "pdf_path": "../data/contents/2511.07025.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:35.493130",
    "title": "Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks",
    "abstract": "Open-weights embedding model focused on multilingual/cross-lingual retrieval with instruction-awareness—strong candidate embedder for RAG.",
    "paper_id": "2511.07025"
  },
  "https://arxiv.org/abs/2512.20778v1": {
    "content_path": "../data/contents/2512.20778v1.md",
    "pdf_path": "../data/contents/2512.20778v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:35.732570",
    "title": "Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication",
    "abstract": "Summary: Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.",
    "paper_id": "2512.20778v1"
  },
  "https://arxiv.org/abs/2512.18622v1": {
    "content_path": "../data/contents/2512.18622v1.md",
    "pdf_path": "../data/contents/2512.18622v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:36.652953",
    "title": "A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback",
    "abstract": "Summary: Text2SQL, the task of generating SQL queries from natural language text, is a critical challenge in data engineering. Recently, Large Language Models (LLMs) have demonstrated superior performance for this task due to their advanced comprehension and generation capabilities. However, privacy and cost considerations prevent companies from using Text2SQL solutions based on external LLMs offered as a service. Rather, small LLMs (SLMs) that are openly available and can hosted in-house are adopted. These SLMs, in turn, lack the generalization capabilities of larger LLMs, which impairs their effectiveness for complex tasks such as Text2SQL. To address these limitations, we propose MATS, a novel Text2SQL framework designed specifically for SLMs. MATS uses a multi-agent mechanism that assigns specialized roles to auxiliary agents, reducing individual workloads and fostering interaction. A training scheme based on reinforcement learning aligns these agents using feedback obtained during execution, thereby maintaining competitive performance despite a limited LLM size. Evaluation results using on benchmark datasets show that MATS, deployed on a single- GPU server, yields accuracy that are on-par with large-scale LLMs when using significantly fewer parameters. Our source code and data are available at https://github.com/thanhdath/mats-sql.",
    "paper_id": "2512.18622v1"
  },
  "https://arxiv.org/abs/2512.14313v1": {
    "content_path": "../data/contents/2512.14313v1.md",
    "pdf_path": "../data/contents/2512.14313v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:36.883652",
    "title": "Dynamic Context Selection for Retrieval-Augmented Generation: Mitigating Distractors and Positional Bias",
    "abstract": "Summary: Retrieval Augmented Generation (RAG) enhances language model performance by incorporating external knowledge retrieved from large corpora, which makes it highly suitable for tasks such as open domain question answering. Standard RAG systems typically rely on a fixed top k retrieval strategy, which can either miss relevant information or introduce semantically irrelevant passages, known as distractors, that degrade output quality. Additionally, the positioning of retrieved passages within the input context can influence the model attention and generation outcomes. Context placed in the middle tends to be overlooked, which is an issue known as the \"lost in the middle\" phenomenon. In this work, we systematically analyze the impact of distractors on generation quality, and quantify their effects under varying conditions. We also investigate how the position of relevant passages within the context window affects their influence on generation. Building on these insights, we propose a context-size classifier that dynamically predicts the optimal number of documents to retrieve based on query-specific informational needs. We integrate this approach into a full RAG pipeline, and demonstrate improved performance over fixed k baselines.",
    "paper_id": "2512.14313v1"
  },
  "https://arxiv.org/abs/2412.15605": {
    "content_path": "../data/contents/2412.15605.md",
    "pdf_path": "../data/contents/2412.15605.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:37.358245",
    "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks",
    "abstract": "Proposes Cache-Augmented Generation (CAG) as an alternative to RAG that eliminates retrieval latency by preloading documents into extended context windows.",
    "paper_id": "2412.15605"
  },
  "https://arxiv.org/abs/2512.20760v1": {
    "content_path": "../data/contents/2512.20760v1.md",
    "pdf_path": "../data/contents/2512.20760v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:38.069637",
    "title": "Generalization of RLVR Using Causal Reasoning as a Testbed",
    "abstract": "Summary: Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.",
    "paper_id": "2512.20760v1"
  },
  "https://arxiv.org/abs/2512.18616v1": {
    "content_path": "../data/contents/2512.18616v1.md",
    "pdf_path": "../data/contents/2512.18616v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:40.841836",
    "title": "DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System",
    "abstract": "Summary: We present DASH (Deception-Augmented Shared mental model for Human-machine teaming), a novel framework that enhances mission resilience by embedding proactive deception into Shared Mental Models (SMM). Designed for mission-critical applications such as surveillance and rescue, DASH introduces \"bait tasks\" to detect insider threats, e.g., compromised Unmanned Ground Vehicles (UGVs), AI agents, or human analysts, before they degrade team performance. Upon detection, tailored recovery mechanisms are activated, including UGV system reinstallation, AI model retraining, or human analyst replacement. In contrast to existing SMM approaches that neglect insider risks, DASH improves both coordination and security. Empirical evaluations across four schemes (DASH, SMM-only, no-SMM, and baseline) show that DASH sustains approximately 80% mission success under high attack rates, eight times higher than the baseline. This work contributes a practical human-AI teaming framework grounded in shared mental models, a deception-based strategy for insider threat detection, and empirical evidence of enhanced robustness under adversarial conditions. DASH establishes a foundation for secure, adaptive human-machine teaming in contested environments.",
    "paper_id": "2512.18616v1"
  },
  "https://arxiv.org/abs/2512.14312v1": {
    "content_path": "../data/contents/2512.14312v1.md",
    "pdf_path": "../data/contents/2512.14312v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:41.534908",
    "title": "From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region",
    "abstract": "Summary: In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.",
    "paper_id": "2512.14312v1"
  },
  "https://arxiv.org/abs/2405.00451": {
    "content_path": "../data/contents/2405.00451.md",
    "pdf_path": "../data/contents/2405.00451.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:41.931903",
    "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning",
    "abstract": "Demonstrates using MCTS to iteratively collect preference data, breaking down instance-level rewards into step-level signals.",
    "paper_id": "2405.00451"
  },
  "https://arxiv.org/abs/2512.15567v1": {
    "content_path": "../data/contents/2512.15567v1.md",
    "pdf_path": "../data/contents/2512.15567v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:42.837708",
    "title": "Evaluating Large Language Models in Scientific Discovery",
    "abstract": "Summary: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific \"superintelligence\". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.",
    "paper_id": "2512.15567v1"
  },
  "https://arxiv.org/abs/2512.15550v1": {
    "content_path": "../data/contents/2512.15550v1.md",
    "pdf_path": "../data/contents/2512.15550v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:46.071806",
    "title": "CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing",
    "abstract": "Summary: Large language models (LLMs) are increasingly applied in long-context scenarios such as multi-turn conversations. However, long contexts pose significant challenges for inference efficiency, including high memory overhead from Key-Value (KV) cache and increased latency due to excessive memory accesses. Recent methods for dynamic KV selection struggle with trade-offs: block-level indexing degrades accuracy by retrieving irrelevant KV entries, while token-level indexing incurs high latency from inefficient retrieval mechanisms. In this paper, we propose CTKVR, a novel centroid-then-token KV retrieval scheme that addresses these limitations. CTKVR leverages a key observation: query vectors adjacent in position exhibit high similarity after Rotary Position Embedding (RoPE) and share most of their top-k KV cache entries. Based on this insight, CTKVR employs a two-stage retrieval strategy: lightweight centroids are precomputed during prefilling for centroid-grained indexing, followed by token-level refinement for precise KV retrieval. This approach balances retrieval efficiency and accuracy. To further enhance performance, we implement an optimized system for indexing construction and search using CPU-GPU co-execution. Experimentally, CTKVR achieves superior performance across multiple benchmarks with less than 1% accuracy degradation. Meanwhile, CTKVR delivers 3 times and 4 times throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware.",
    "paper_id": "2512.15550v1"
  },
  "https://arxiv.org/html/2505.14656v1": {
    "content_path": "../data/contents/2505.14656v1.md",
    "pdf_path": "../data/contents/2505.14656v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:46.603017",
    "title": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning (CATS)",
    "abstract": "Introduces cost-aware MCTS for LLM planning, achieving better task success rates than raw LLMs under budget constraints.",
    "paper_id": "2505.14656v1"
  },
  "https://arxiv.org/abs/2512.15538v1": {
    "content_path": "../data/contents/2512.15538v1.md",
    "pdf_path": "../data/contents/2512.15538v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:51.425020",
    "title": "Tracking Temporal Dynamics of Vector Sets with Gaussian Process",
    "abstract": "Summary: Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.",
    "paper_id": "2512.15538v1"
  },
  "https://arxiv.org/abs/2410.08146": {
    "content_path": "../data/contents/2410.08146.md",
    "pdf_path": "../data/contents/2410.08146.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:52.308081",
    "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
    "abstract": "Key paper on process reward models showing how to design effective process rewards measuring progress toward solutions.",
    "paper_id": "2410.08146"
  },
  "https://arxiv.org/abs/2512.15492v1": {
    "content_path": "../data/contents/2512.15492v1.md",
    "pdf_path": "../data/contents/2512.15492v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:52.663910",
    "title": "Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier",
    "abstract": "Summary: We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.",
    "paper_id": "2512.15492v1"
  },
  "https://arxiv.org/html/2502.10325v1": {
    "content_path": "../data/contents/2502.10325v1.md",
    "pdf_path": "../data/contents/2502.10325v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:53.166760",
    "title": "Process Reward Models for LLM Agents: Practical Framework and Directions (AgentPRM)",
    "abstract": "Introduces AgentPRM framework for training LLM agents with process rewards, showing 3B models outperform GPT-4o baselines.",
    "paper_id": "2502.10325v1"
  },
  "https://arxiv.org/abs/2512.15489v1": {
    "content_path": "../data/contents/2512.15489v1.md",
    "pdf_path": "../data/contents/2512.15489v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:53.416556",
    "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "abstract": "Summary: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).\n  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.\n  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.\n  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.",
    "paper_id": "2512.15489v1"
  },
  "https://arxiv.org/abs/2512.15483v1": {
    "content_path": "../data/contents/2512.15483v1.md",
    "pdf_path": "../data/contents/2512.15483v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:36:59.948981",
    "title": "Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs",
    "abstract": "Summary: Self-driving laboratories (SDLs) are combining recent technological advances in robotics, automation, and machine learning based data analysis and decision-making to perform autonomous experimentation toward human-directed goals without requiring any direct human intervention. SDLs are successfully used in materials science, chemistry, and beyond, to optimise processes, materials, and devices in a systematic and data-efficient way. At present, the most widely used algorithm to identify the most informative next experiment is Bayesian optimisation. While relatively simple to apply to a wide range of optimisation problems, standard Bayesian optimisation relies on a fixed experimental workflow with a clear set of optimisation parameters and one or more measurable objective functions. This excludes the possibility of making on-the-fly decisions about changes in the planned sequence of operations and including intermediate measurements in the decision-making process. Therefore, many real-world experiments need to be adapted and simplified to be converted to the common setting in self-driving labs. In this paper, we introduce an extension to Bayesian optimisation that allows flexible sampling of multi-stage workflows and makes optimal decisions based on intermediate observables, which we call proxy measurements. We systematically compare the advantage of taking into account proxy measurements over conventional Bayesian optimisation, in which only the final measurement is observed. We find that over a wide range of scenarios, proxy measurements yield a substantial improvement, both in the time to find good solutions and in the overall optimality of found solutions. This not only paves the way to use more complex and thus more realistic experimental workflows in autonomous labs but also to smoothly combine simulations and experiments in the next generation of SDLs.",
    "paper_id": "2512.15483v1"
  },
  "https://arxiv.org/abs/2512.15816v1": {
    "content_path": "../data/contents/2512.15816v1.md",
    "pdf_path": "../data/contents/2512.15816v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:00.563828",
    "title": "A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning",
    "abstract": "Summary: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.",
    "paper_id": "2512.15816v1"
  },
  "https://arxiv.org/abs/2512.15469v1": {
    "content_path": "../data/contents/2512.15469v1.md",
    "pdf_path": "../data/contents/2512.15469v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:01.640795",
    "title": "Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance",
    "abstract": "Summary: As machine learning models are increasingly deployed in high-stakes settings, e.g. as decision support systems in various societal sectors or in critical infrastructure, designers and auditors are facing the need to ensure that models satisfy a wider variety of requirements (e.g. compliance with regulations, fairness, computational constraints) beyond performance. Although most of them are the subject of ongoing studies, typical approaches face critical challenges: post-processing methods tend to compromise performance, which is often counteracted by fine-tuning or, worse, training from scratch, an often time-consuming or even unavailable strategy. This raises the following question: \"Can we efficiently edit models to satisfy requirements, without sacrificing their utility?\" In this work, we approach this with a unifying framework, in a data-driven manner, i.e. we learn to edit neural networks (NNs), where the editor is an NN itself - a graph metanetwork - and editing amounts to a single inference step. In particular, the metanetwork is trained on NN populations to minimise an objective consisting of two terms: the requirement to be enforced and the preservation of the NN's utility. We experiment with diverse tasks (the data minimisation principle, bias mitigation and weight pruning) improving the trade-offs between performance, requirement satisfaction and time efficiency compared to popular post-processing or re-training alternatives.",
    "paper_id": "2512.15469v1"
  },
  "https://arxiv.org/abs/2512.15468v1": {
    "content_path": "../data/contents/2512.15468v1.md",
    "pdf_path": "../data/contents/2512.15468v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:02.180746",
    "title": "How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?",
    "abstract": "Summary: The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.\n  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.",
    "paper_id": "2512.15468v1"
  },
  "https://arxiv.org/abs/2512.15466v1": {
    "content_path": "../data/contents/2512.15466v1.md",
    "pdf_path": "../data/contents/2512.15466v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:02.720341",
    "title": "On Assessing the Relevance of Code Reviews Authored by Generative Models",
    "abstract": "Summary: The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of \"usefulness\", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.",
    "paper_id": "2512.15466v1"
  },
  "https://arxiv.org/abs/2512.19737v1": {
    "content_path": "../data/contents/2512.19737v1.md",
    "pdf_path": "../data/contents/2512.19737v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:03.201885",
    "title": "Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach",
    "abstract": "Summary: Reliable prediction of train delays is essential for enhancing the robustness and efficiency of railway transportation systems. In this work, we reframe delay forecasting as a stochastic simulation task, modeling state-transition dynamics through imitation learning. We introduce Drift-Corrected Imitation Learning (DCIL), a novel self-supervised algorithm that extends DAgger by incorporating distance-based drift correction, thereby mitigating covariate shift during rollouts without requiring access to an external oracle or adversarial schemes. Our approach synthesizes the dynamical fidelity of event-driven models with the representational capacity of data-driven methods, enabling uncertainty-aware forecasting via Monte Carlo simulation. We evaluate DCIL using a comprehensive real-world dataset from \\textsc{Infrabel}, the Belgian railway infrastructure manager, which encompasses over three million train movements. Our results, focused on predictions up to 30 minutes ahead, demonstrate superior predictive performance of DCIL over traditional regression models and behavioral cloning on deep learning architectures, highlighting its effectiveness in capturing the sequential and uncertain nature of delay propagation in large-scale networks.",
    "paper_id": "2512.19737v1"
  },
  "https://arxiv.org/abs/2512.15442v1": {
    "content_path": "../data/contents/2512.15442v1.md",
    "pdf_path": "../data/contents/2512.15442v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:04.448770",
    "title": "Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting",
    "abstract": "Summary: Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.",
    "paper_id": "2512.15442v1"
  },
  "https://arxiv.org/abs/2410.02725": {
    "content_path": "../data/contents/2410.02725.md",
    "pdf_path": "../data/contents/2410.02725.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:05.079293",
    "title": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation",
    "abstract": "Novel approach for LLMs to self-evaluate and adaptively allocate inference compute, achieving 74% improvement efficiency.",
    "paper_id": "2410.02725"
  },
  "https://arxiv.org/abs/2512.15420v1": {
    "content_path": "../data/contents/2512.15420v1.md",
    "pdf_path": "../data/contents/2512.15420v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:19.503909",
    "title": "FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows",
    "abstract": "Summary: Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.",
    "paper_id": "2512.15420v1"
  },
  "https://arxiv.org/abs/2512.15398v1": {
    "content_path": "../data/contents/2512.15398v1.md",
    "pdf_path": "../data/contents/2512.15398v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:20.336273",
    "title": "Mapis: A Knowledge-Graph Grounded Multi-Agent Framework for Evidence-Based PCOS Diagnosis",
    "abstract": "Summary: Polycystic Ovary Syndrome (PCOS) constitutes a significant public health issue affecting 10% of reproductive-aged women, highlighting the critical importance of developing effective diagnostic tools. Previous machine learning and deep learning detection tools are constrained by their reliance on large-scale labeled data and an lack of interpretability. Although multi-agent systems have demonstrated robust capabilities, the potential of such systems for PCOS detection remains largely unexplored. Existing medical multi-agent frameworks are predominantly designed for general medical tasks, suffering from insufficient domain integration and a lack of specific domain knowledge. To address these challenges, we propose Mapis, the first knowledge-grounded multi-agent framework explicitly designed for guideline-based PCOS diagnosis. Specifically, it built upon the 2023 International Guideline into a structured collaborative workflow that simulates the clinical diagnostic process. It decouples complex diagnostic tasks across specialized agents: a gynecological endocrine agent and a radiology agent collaborative to verify inclusion criteria, while an exclusion agent strictly rules out other causes. Furthermore, we construct a comprehensive PCOS knowledge graph to ensure verifiable, evidence-based decision-making. Extensive experiments on public benchmarks and specialized clinical datasets, benchmarking against nine diverse baselines, demonstrate that Mapis significantly outperforms competitive methods. On the clinical dataset, it surpasses traditional machine learning models by 13.56%, single-agent by 6.55%, and previous medical multi-agent systems by 7.05% in Accuracy.",
    "paper_id": "2512.15398v1"
  },
  "https://arxiv.org/abs/2512.15397v1": {
    "content_path": "../data/contents/2512.15397v1.md",
    "pdf_path": "../data/contents/2512.15397v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:20.897550",
    "title": "ORACLE: Time-Dependent Recursive Summary Graphs for Foresight on News Data Using LLMs",
    "abstract": "Summary: ORACLE turns daily news into week-over-week, decision-ready insights for one of the Finnish University of Applied Sciences. The platform crawls and versions news, applies University-specific relevance filtering, embeds content, classifies items into PESTEL dimensions and builds a concise Time-Dependent Recursive Summary Graph (TRSG): two clustering layers summarized by an LLM and recomputed weekly. A lightweight change detector highlights what is new, removed or changed, then groups differences into themes for PESTEL-aware analysis. We detail the pipeline, discuss concrete design choices that make the system stable in production and present a curriculum-intelligence use case with an evaluation plan.",
    "paper_id": "2512.15397v1"
  },
  "https://arxiv.org/abs/2512.15396v1": {
    "content_path": "../data/contents/2512.15396v1.md",
    "pdf_path": "../data/contents/2512.15396v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:23.057966",
    "title": "SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering",
    "abstract": "Summary: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.",
    "paper_id": "2512.15396v1"
  },
  "https://arxiv.org/abs/2512.15388v1": {
    "content_path": "../data/contents/2512.15388v1.md",
    "pdf_path": "../data/contents/2512.15388v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:23.907578",
    "title": "Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations",
    "abstract": "Summary: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.",
    "paper_id": "2512.15388v1"
  },
  "https://arxiv.org/abs/2512.15384v1": {
    "content_path": "../data/contents/2512.15384v1.md",
    "pdf_path": "../data/contents/2512.15384v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:24.148929",
    "title": "MedNuggetizer: Confidence-Based Information Nugget Extraction from Medical Documents",
    "abstract": "Summary: We present MedNuggetizer, https://mednugget-ai.de/; access is available upon request.}, a tool for query-driven extraction and clustering of information nuggets from medical documents to support clinicians in exploring underlying medical evidence. Backed by a large language model (LLM), \\textit{MedNuggetizer} performs repeated extractions of information nuggets that are then grouped to generate reliable evidence within and across multiple documents. We demonstrate its utility on the clinical use case of \\textit{antibiotic prophylaxis before prostate biopsy} by using major urological guidelines and recent PubMed studies as sources of information. Evaluation by domain experts shows that \\textit{MedNuggetizer} provides clinicians and researchers with an efficient way to explore long documents and easily extract reliable, query-focused medical evidence.",
    "paper_id": "2512.15384v1"
  },
  "https://arxiv.org/abs/2512.15383v1": {
    "content_path": "../data/contents/2512.15383v1.md",
    "pdf_path": "../data/contents/2512.15383v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:25.363972",
    "title": "Interpretable Multivariate Conformal Prediction with Fast Transductive Standardization",
    "abstract": "Summary: We propose a conformal prediction method for constructing tight simultaneous prediction intervals for multiple, potentially related, numerical outputs given a single input. This method can be combined with any multi-target regression model and guarantees finite-sample coverage. It is computationally efficient and yields informative prediction intervals even with limited data. The core idea is a novel \\emph{coordinate-wise} standardization procedure that makes residuals across output dimensions directly comparable, estimating suitable scaling parameters using the calibration data themselves. This does not require modeling of cross-output dependence nor auxiliary sample splitting. Implementing this idea requires overcoming technical challenges associated with transductive or full conformal prediction. Experiments on simulated and real data demonstrate this method can produce tighter prediction intervals than existing baselines while maintaining valid simultaneous coverage.",
    "paper_id": "2512.15383v1"
  },
  "https://arxiv.org/abs/2512.19735v2": {
    "content_path": "../data/contents/2512.19735v2.md",
    "pdf_path": "../data/contents/2512.19735v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:30.713028",
    "title": "Case Prompting to Mitigate Large Language Model Bias for ICU Mortality Prediction",
    "abstract": "Summary: Accurate mortality risk prediction for intensive care unit (ICU) patients is essential for clinical decision-making. Although large language models (LLMs) show promise in predicting outcomes from structured medical data, their predictions may exhibit demographic biases related to sex, age, and race, limiting their trustworthy use in clinical practice. Existing debiasing methods often reduce predictive performance, making it difficult to jointly optimize fairness and accuracy. In this study, we systematically examine bias in LLM-based ICU mortality prediction and propose a training-free, clinically adaptive prompting framework to simultaneously improve fairness and performance. We first develop a multi-dimensional bias assessment scheme for comprehensive model diagnosis. Building on this analysis, we introduce CAse Prompting (CAP), a novel prompting framework that integrates conventional debiasing prompts with case-based reasoning. CAP guides the model to learn from similar historical misprediction cases and their correct outcomes, enabling correction of biased reasoning patterns. Experiments on the MIMIC-IV dataset show that CAP substantially improves both predictive accuracy and fairness. CAP increases AUROC from 0.806 to 0.873 and AUPRC from 0.497 to 0.694, while reducing sex- and race-related disparities by over 90%. Feature reliance analysis further indicates highly consistent attention patterns across demographic groups, with similarity scores exceeding 0.98. These results demonstrate that LLMs exhibit measurable bias in ICU mortality prediction, and that a carefully designed prompting framework can effectively co-optimize fairness and performance without retraining, offering a transferable paradigm for equitable clinical decision support.",
    "paper_id": "2512.19735v2"
  },
  "https://arxiv.org/abs/2512.15378v1": {
    "content_path": "../data/contents/2512.15378v1.md",
    "pdf_path": "../data/contents/2512.15378v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:31.441520",
    "title": "A Regime-Aware Fusion Framework for Time Series Classification",
    "abstract": "Summary: Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.",
    "paper_id": "2512.15378v1"
  },
  "https://arxiv.org/abs/2512.15374v1": {
    "content_path": "../data/contents/2512.15374v1.md",
    "pdf_path": "../data/contents/2512.15374v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:33.109757",
    "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
    "abstract": "Summary: Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \\textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \\textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.",
    "paper_id": "2512.15374v1"
  },
  "https://arxiv.org/abs/2512.15372v1": {
    "content_path": "../data/contents/2512.15372v1.md",
    "pdf_path": "../data/contents/2512.15372v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:33.491127",
    "title": "Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models",
    "abstract": "Summary: Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.",
    "paper_id": "2512.15372v1"
  },
  "https://arxiv.org/abs/2512.15365v1": {
    "content_path": "../data/contents/2512.15365v1.md",
    "pdf_path": "../data/contents/2512.15365v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:34.000235",
    "title": "ArcBERT: An LLM-based Search Engine for Exploring Integrated Multi-Omics Metadata",
    "abstract": "Summary: Traditional search applications within Research Data Management (RDM) ecosystems are crucial in helping users discover and explore the structured metadata from the research datasets. Typically, text search engines require users to submit keyword-based queries rather than using natural language. However, using Large Language Models (LLMs) trained on domain-specific content for specialized natural language processing (NLP) tasks is becoming increasingly common. We present ArcBERT, an LLM-based system designed for integrated metadata exploration. ArcBERT understands natural language queries and relies on semantic matching, unlike traditional search applications. Notably, ArcBERT also understands the structure and hierarchies within the metadata, enabling it to handle diverse user querying patterns effectively.",
    "paper_id": "2512.15365v1"
  },
  "https://arxiv.org/abs/2512.15363v1": {
    "content_path": "../data/contents/2512.15363v1.md",
    "pdf_path": "../data/contents/2512.15363v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:35.188109",
    "title": "Revisiting Task-Oriented Dataset Search in the Era of Large Language Models: Challenges, Benchmark, and Solution",
    "abstract": "Summary: The search for suitable datasets is the critical \"first step\" in data-driven research, but it remains a great challenge. Researchers often need to search for datasets based on high-level task descriptions. However, existing search systems struggle with this task due to ambiguous user intent, task-to-dataset mapping and benchmark gaps, and entity ambiguity. To address these challenges, we introduce KATS, a novel end-to-end system for task-oriented dataset search from unstructured scientific literature. KATS consists of two key components, i.e., offline knowledge base construction and online query processing. The sophisticated offline pipeline automatically constructs a high-quality, dynamically updatable task-dataset knowledge graph by employing a collaborative multi-agent framework for information extraction, thereby filling the task-to-dataset mapping gap. To further address the challenge of entity ambiguity, a unique semantic-based mechanism is used for task entity linking and dataset entity resolution. For online retrieval, KATS utilizes a specialized hybrid query engine that combines vector search with graph-based ranking to generate highly relevant results. Additionally, we introduce CS-TDS, a tailored benchmark suite for evaluating task-oriented dataset search systems, addressing the critical gap in standardized evaluation. Experiments on our benchmark suite show that KATS significantly outperforms state-of-the-art retrieval-augmented generation frameworks in both effectiveness and efficiency, providing a robust blueprint for the next generation of dataset discovery systems.",
    "paper_id": "2512.15363v1"
  },
  "https://arxiv.org/abs/2512.15358v1": {
    "content_path": "../data/contents/2512.15358v1.md",
    "pdf_path": "../data/contents/2512.15358v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:35.649583",
    "title": "Dual-Density Inference for Efficient Language Model Reasoning",
    "abstract": "Summary: Large Language Models (LLMs) have shown impressive capabilities in complex reasoning tasks. However, current approaches employ uniform language density for both intermediate reasoning and final answers, leading to computational inefficiency. Our observation found that reasoning process serves a computational function for the model itself, while answering serves a communicative function for human understanding. This distinction enables the use of compressed, symbol-rich language for intermediate computations while maintaining human-readable final explanations. To address this inefficiency, we present Denser: \\underline{D}ual-d\\underline{ens}ity inf\\underline{er}ence, a novel framework that optimizes information density separately for reasoning and answering phases. Our framework implements this through three components: a query processing module that analyzes input problems, a high-density compressed reasoning mechanism for efficient intermediate computations, and an answer generation component that translates compressed reasoning into human-readable solutions. Experimental evaluation across multiple reasoning question answering benchmarks demonstrates that Denser reduces token consumption by up to 62\\% compared to standard Chain-of-Thought methods while preserving or improving accuracy. These efficiency gains are particularly significant for complex multi-step reasoning problems where traditional methods generate extensive explanations.",
    "paper_id": "2512.15358v1"
  },
  "https://arxiv.org/abs/2512.15353v1": {
    "content_path": "../data/contents/2512.15353v1.md",
    "pdf_path": "../data/contents/2512.15353v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:36.330290",
    "title": "Adversarial versification in portuguese as a jailbreak operator in LLMs",
    "abstract": "Summary: Recent evidence shows that the versification of prompts constitutes a highly effective adversarial mechanism against aligned LLMs. The study 'Adversarial poetry as a universal single-turn jailbreak mechanism in large language models' demonstrates that instructions routinely refused in prose become executable when rewritten as verse, producing up to 18 x more safety failures in benchmarks derived from MLCommons AILuminate. Manually written poems reach approximately 62% ASR, and automated versions 43%, with some models surpassing 90% success in single-turn interactions. The effect is structural: systems trained with RLHF, constitutional AI, and hybrid pipelines exhibit consistent degradation under minimal semiotic formal variation. Versification displaces the prompt into sparsely supervised latent regions, revealing guardrails that are excessively dependent on surface patterns. This dissociation between apparent robustness and real vulnerability exposes deep limitations in current alignment regimes. The absence of evaluations in Portuguese, a language with high morphosyntactic complexity, a rich metric-prosodic tradition, and over 250 million speakers, constitutes a critical gap. Experimental protocols must parameterise scansion, metre, and prosodic variation to test vulnerabilities specific to Lusophone patterns, which are currently ignored.",
    "paper_id": "2512.15353v1"
  },
  "https://arxiv.org/abs/2512.19734v1": {
    "content_path": "../data/contents/2512.19734v1.md",
    "pdf_path": "../data/contents/2512.19734v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:41.635257",
    "title": "The Deleuzian Representation Hypothesis",
    "abstract": "Summary: We propose an alternative to sparse autoencoders (SAEs) as a simple and effective unsupervised method for extracting interpretable concepts from neural networks. The core idea is to cluster differences in activations, which we formally justify within a discriminant analysis framework. To enhance the diversity of extracted concepts, we refine the approach by weighting the clustering using the skewness of activations. The method aligns with Deleuze's modern view of concepts as differences. We evaluate the approach across five models and three modalities (vision, language, and audio), measuring concept quality, diversity, and consistency. Our results show that the proposed method achieves concept quality surpassing prior unsupervised SAE variants while approaching supervised baselines, and that the extracted concepts enable steering of a model's inner representations, demonstrating their causal influence on downstream behavior.",
    "paper_id": "2512.19734v1"
  },
  "https://arxiv.org/abs/2512.15347v1": {
    "content_path": "../data/contents/2512.15347v1.md",
    "pdf_path": "../data/contents/2512.15347v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:58.083160",
    "title": "Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models",
    "abstract": "Summary: Group Relative Policy Optimization (GRPO) is a powerful technique for aligning generative models, but its effectiveness is bottlenecked by the conflict between large group sizes and prohibitive computational costs. In this work, we investigate the trade-off through empirical studies, yielding two key observations. First, we discover the reward clustering phenomenon in which many trajectories collapse toward the group-mean reward, offering limited optimization value. Second, we design a heuristic strategy named Optimal Variance Filtering (OVF), and verify that a high-variance subset of trajectories, selected by OVF can outperform the larger, unfiltered group. However, this static, post-sampling OVF approach still necessitates critical computational overhead, as it performs unnecessary sampling for trajectories that are ultimately discarded. To resolve this, we propose Pro-GRPO (Proactive GRPO), a novel dynamic framework that integrates latent feature-based trajectory pruning into the sampling process. Through the early termination of reward-clustered trajectories, Pro-GRPO reduces computational overhead. Leveraging its efficiency, Pro-GRPO employs an \"Expand-and-Prune\" strategy. This strategy first expands the size of initial sampling group to maximize trajectory diversity, then it applies multi-step OVF to the latents, avoiding prohibitive computational costs. Extensive experiments on both diffusion-based and flow-based models demonstrate the generality and effectiveness of our Pro-GRPO framework.",
    "paper_id": "2512.15347v1"
  },
  "https://arxiv.org/abs/2512.15335v1": {
    "content_path": "../data/contents/2512.15335v1.md",
    "pdf_path": "../data/contents/2512.15335v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:58.778811",
    "title": "Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference",
    "abstract": "Summary: Deep neural networks are widely deployed with quantization techniques to reduce memory and computational costs by lowering the numerical precision of their parameters. While quantization alters model parameters and their outputs, existing privacy analyses primarily focus on full-precision models, leaving a gap in understanding how bit-width reduction can affect privacy leakage. We present the first systematic study of the privacy-utility relationship in post-training quantization (PTQ), a versatile family of methods that can be applied to pretrained models without further training. Using membership inference attacks as our evaluation framework, we analyze three popular PTQ algorithms-AdaRound, BRECQ, and OBC-across multiple precision levels (4-bit, 2-bit, and 1.58-bit) on CIFAR-10, CIFAR-100, and TinyImageNet datasets. Our findings consistently show that low-precision PTQs can reduce privacy leakage. In particular, lower-precision models demonstrate up to an order of magnitude reduction in membership inference vulnerability compared to their full-precision counterparts, albeit at the cost of decreased utility. Additional ablation studies on the 1.58-bit quantization level show that quantizing only the last layer at higher precision enables fine-grained control over the privacy-utility trade-off. These results offer actionable insights for practitioners to balance efficiency, utility, and privacy protection in real-world deployments.",
    "paper_id": "2512.15335v1"
  },
  "https://arxiv.org/abs/2512.15813v1": {
    "content_path": "../data/contents/2512.15813v1.md",
    "pdf_path": "../data/contents/2512.15813v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:37:59.360891",
    "title": "CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory",
    "abstract": "Summary: Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.",
    "paper_id": "2512.15813v1"
  },
  "https://arxiv.org/abs/2512.15312v1": {
    "content_path": "../data/contents/2512.15312v1.md",
    "pdf_path": "../data/contents/2512.15312v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:01.727817",
    "title": "Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies",
    "abstract": "Summary: Extracting structured information from zeolite synthesis experimental procedures is critical for materials discovery, yet existing methods have not systematically evaluated Large Language Models (LLMs) for this domain-specific task. This work addresses a fundamental question: what is the efficacy of different prompting strategies when applying LLMs to scientific information extraction? We focus on four key subtasks: event type classification (identifying synthesis steps), trigger text identification (locating event mentions), argument role extraction (recognizing parameter types), and argument text extraction (extracting parameter values). We evaluate four prompting strategies - zero-shot, few-shot, event-specific, and reflection-based - across six state-of-the-art LLMs (Gemma-3-12b-it, GPT-5-mini, O4-mini, Claude-Haiku-3.5, DeepSeek reasoning and non-reasoning) using the ZSEE dataset of 1,530 annotated sentences. Results demonstrate strong performance on event type classification (80-90\\% F1) but modest performance on fine-grained extraction tasks, particularly argument role and argument text extraction (50-65\\% F1). GPT-5-mini exhibits extreme prompt sensitivity with 11-79\\% F1 variation. Notably, advanced prompting strategies provide minimal improvements over zero-shot approaches, revealing fundamental architectural limitations. Error analysis identifies systematic hallucination, over-generalization, and inability to capture synthesis-specific nuances. Our findings demonstrate that while LLMs achieve high-level understanding, precise extraction of experimental parameters requires domain-adapted models, providing quantitative benchmarks for scientific information extraction.",
    "paper_id": "2512.15312v1"
  },
  "https://arxiv.org/abs/2512.15306v1": {
    "content_path": "../data/contents/2512.15306v1.md",
    "pdf_path": "../data/contents/2512.15306v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:02.118826",
    "title": "LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs",
    "abstract": "Summary: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.",
    "paper_id": "2512.15306v1"
  },
  "https://arxiv.org/abs/2512.15302v1": {
    "content_path": "../data/contents/2512.15302v1.md",
    "pdf_path": "../data/contents/2512.15302v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:05.752985",
    "title": "Towards Proactive Personalization through Profile Customization for Individual Users in Dialogues",
    "abstract": "Summary: The deployment of Large Language Models (LLMs) in interactive systems necessitates a deep alignment with the nuanced and dynamic preferences of individual users. Current alignment techniques predominantly address universal human values or static, single-turn preferences, thereby failing to address the critical needs of long-term personalization and the initial user cold-start problem. To bridge this gap, we propose PersonalAgent, a novel user-centric lifelong agent designed to continuously infer and adapt to user preferences. PersonalAgent constructs and dynamically refines a unified user profile by decomposing dialogues into single-turn interactions, framing preference inference as a sequential decision-making task. Experiments show that PersonalAgent achieves superior performance over strong prompt-based and policy optimization baselines, not only in idealized but also in noisy conversational contexts, while preserving cross-session preference consistency. Furthermore, human evaluation confirms that PersonalAgent excels at capturing user preferences naturally and coherently. Our findings underscore the importance of lifelong personalization for developing more inclusive and adaptive conversational agents. Our code is available here.",
    "paper_id": "2512.15302v1"
  },
  "https://arxiv.org/abs/2512.15298v1": {
    "content_path": "../data/contents/2512.15298v1.md",
    "pdf_path": "../data/contents/2512.15298v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:06.484613",
    "title": "ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I",
    "abstract": "Summary: The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that \"Perception Errors\" were dominant, highlighting a \"Perception-Cognition Gap\" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a \"Calculation-Conceptualization Discrepancy,\" successfully performing calculations while failing to apply the underlying scientific concepts, and \"Process Hallucination,\" where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing \"AI-resistant questions\" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.",
    "paper_id": "2512.15298v1"
  },
  "https://arxiv.org/abs/2512.15285v1": {
    "content_path": "../data/contents/2512.15285v1.md",
    "pdf_path": "../data/contents/2512.15285v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:06.739180",
    "title": "Topological Metric for Unsupervised Embedding Quality Evaluation",
    "abstract": "Summary: Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.",
    "paper_id": "2512.15285v1"
  },
  "https://arxiv.org/abs/2512.15274v1": {
    "content_path": "../data/contents/2512.15274v1.md",
    "pdf_path": "../data/contents/2512.15274v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:07.529628",
    "title": "Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning",
    "abstract": "Summary: Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.",
    "paper_id": "2512.15274v1"
  },
  "https://arxiv.org/abs/2512.15269v1": {
    "content_path": "../data/contents/2512.15269v1.md",
    "pdf_path": "../data/contents/2512.15269v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:08.311007",
    "title": "Model inference for ranking from pairwise comparisons",
    "abstract": "Summary: We consider the problem of ranking objects from noisy pairwise comparisons, for example, ranking tennis players from the outcomes of matches. We follow a standard approach to this problem and assume that each object has an unobserved strength and that the outcome of each comparison depends probabilistically on the strengths of the comparands. However, we do not assume to know a priori how skills affect outcomes. Instead, we present an efficient algorithm for simultaneously inferring both the unobserved strengths and the function that maps strengths to probabilities. Despite this problem being under-constrained, we present experimental evidence that the conclusions of our Bayesian approach are robust to different model specifications. We include several case studies to exemplify the method on real-world data sets.",
    "paper_id": "2512.15269v1"
  },
  "https://arxiv.org/abs/2512.14744": {
    "content_path": "../data/contents/2512.14744.md",
    "pdf_path": "../data/contents/2512.14744.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:08.922883",
    "title": "VERAFI: Verified Agentic Financial Intelligence through Neurosymbolic Policy Generation",
    "abstract": "Proposes a verified agentic framework (GAAP/SEC/math policies) to reduce calculation/compliance errors in financial AI systems.",
    "paper_id": "2512.14744"
  },
  "https://arxiv.org/abs/2512.15254v1": {
    "content_path": "../data/contents/2512.15254v1.md",
    "pdf_path": "../data/contents/2512.15254v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:10.927375",
    "title": "Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models",
    "abstract": "Summary: Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.",
    "paper_id": "2512.15254v1"
  },
  "https://arxiv.org/abs/2510.20099": {
    "content_path": "../data/contents/2510.20099.md",
    "pdf_path": "../data/contents/2510.20099.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:11.367228",
    "title": "AI PB: A Grounded Generative Agent for Personalized Investment Insights",
    "abstract": "Describes a production-scale grounded investment-insights agent with orchestration, hybrid retrieval, and compliance constraints.",
    "paper_id": "2510.20099"
  },
  "https://arxiv.org/abs/2512.15235v1": {
    "content_path": "../data/contents/2512.15235v1.md",
    "pdf_path": "../data/contents/2512.15235v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:11.737354",
    "title": "FAME: Fictional Actors for Multilingual Erasure",
    "abstract": "Summary: LLMs trained on web-scale data raise concerns about privacy and the right to be forgotten. To address these issues, Machine Unlearning provides techniques to remove specific information from trained models without retraining from scratch. However, existing benchmarks for evaluating unlearning in LLMs face two major limitations: they focus only on English and support only entity-level forgetting (removing all information about a person). We introduce FAME (Fictional Actors for Multilingual Erasure), a synthetic benchmark for evaluating Machine Unlearning across five languages: English, French, German, Italian, and Spanish. FAME contains 1,000 fictional actor biographies and 20,000 question-answer pairs. Each biography includes information on 20 topics organized into structured categories (biography, career, achievements, personal information). This design enables both entity-level unlearning (i.e., forgetting entire identities) and instance-level unlearning (i.e., forgetting specific facts while retaining others). We provide two dataset splits to support these two different unlearning scenarios and enable systematic comparison of unlearning techniques across languages. Since FAME uses entirely fictional data, it ensures that the information was never encountered during model pretraining, allowing for a controlled evaluation of unlearning methods.",
    "paper_id": "2512.15235v1"
  },
  "https://arxiv.org/abs/2510.15949": {
    "content_path": "../data/contents/2510.15949.md",
    "pdf_path": "../data/contents/2510.15949.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:12.126814",
    "title": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination",
    "abstract": "Multi-agent trading framework with dynamic prompt optimization and order-aware action space (bridging model output to executable orders).",
    "paper_id": "2510.15949"
  },
  "https://arxiv.org/abs/2512.15231v1": {
    "content_path": "../data/contents/2512.15231v1.md",
    "pdf_path": "../data/contents/2512.15231v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:14.545736",
    "title": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications",
    "abstract": "Summary: The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).",
    "paper_id": "2512.15231v1"
  },
  "https://arxiv.org/abs/2508.16571": {
    "content_path": "../data/contents/2508.16571.md",
    "pdf_path": "../data/contents/2508.16571.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:15.221750",
    "title": "LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence",
    "abstract": "Agentic due-diligence benchmark + system (biotech VC memos); reports major analyst time savings and compares against Deep Research/Perplexity.",
    "paper_id": "2508.16571"
  },
  "https://arxiv.org/abs/2512.15229v1": {
    "content_path": "../data/contents/2512.15229v1.md",
    "pdf_path": "../data/contents/2512.15229v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:16.472468",
    "title": "O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization",
    "abstract": "Summary: We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.",
    "paper_id": "2512.15229v1"
  },
  "https://arxiv.org/abs/2505.10278": {
    "content_path": "../data/contents/2505.10278.md",
    "pdf_path": "../data/contents/2505.10278.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:17.250577",
    "title": "MASS: Multi-Agent Simulation Scaling for Portfolio Construction",
    "abstract": "Portfolio construction via scaling multi-agent simulations; paper includes an open-source implementation link for reproducibility/extension.",
    "paper_id": "2505.10278"
  },
  "https://arxiv.org/abs/2512.15226v1": {
    "content_path": "../data/contents/2512.15226v1.md",
    "pdf_path": "../data/contents/2512.15226v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:17.871556",
    "title": "Yes-MT's Submission to the Low-Resource Indic Language Translation Shared Task in WMT 2024",
    "abstract": "Summary: This paper presents the systems submitted by the Yes-MT team for the Low-Resource Indic Language Translation Shared Task at WMT 2024 (Pakray et al., 2024), focusing on translating between English and the Assamese, Mizo, Khasi, and Manipuri languages. The experiments explored various approaches, including fine-tuning pre-trained models like mT5 (Xue et al., 2020) and IndicBart (Dabre et al., 2021) in both multilingual and monolingual settings, LoRA (Hu et al., 2021) fine-tuning IndicTrans2 (Gala et al., 2023), zero-shot and few-shot prompting (Brown, 2020) with large language models (LLMs) like Llama 3 (Dubey et al., 2024) and Mixtral 8x7b (Jiang et al., 2024), LoRA supervised fine-tuning of Llama 3 (Mecklenburg et al., 2024), and training Transformer models (Vaswani, 2017) from scratch. The results were evaluated on the WMT23 Low-Resource Indic Language Translation Shared Task test data using SacreBLEU (Post, 2018) and CHRF (Popovic, 2015), highlighting the challenges of low-resource translation and the potential of LLMs for these tasks, particularly with fine-tuning.",
    "paper_id": "2512.15226v1"
  },
  "https://arxiv.org/abs/2512.15219v1": {
    "content_path": "../data/contents/2512.15219v1.md",
    "pdf_path": "../data/contents/2512.15219v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:18.646988",
    "title": "RFKG-CoT: Relation-Driven Adaptive Hop-count Selection and Few-Shot Path Guidance for Knowledge-Aware QA",
    "abstract": "Summary: Large language models (LLMs) often generate hallucinations in knowledge-intensive QA due to parametric knowledge limitations. While existing methods like KG-CoT improve reliability by integrating knowledge graph (KG) paths, they suffer from rigid hop-count selection (solely question-driven) and underutilization of reasoning paths (lack of guidance). To address this, we propose RFKG-CoT: First, it replaces the rigid hop-count selector with a relation-driven adaptive hop-count selector that dynamically adjusts reasoning steps by activating KG relations (e.g., 1-hop for direct \"brother\" relations, 2-hop for indirect \"father-son\" chains), formalized via a relation mask. Second, it introduces a few-shot in-context learning path guidance mechanism with CoT (think) that constructs examples in a \"question-paths-answer\" format to enhance LLMs' ability to understand reasoning paths. Experiments on four KGQA benchmarks show RFKG-CoT improves accuracy by up to 14.7 pp (Llama2-7B on WebQSP) over KG-CoT. Ablations confirm the hop-count selector and the path prompt are complementary, jointly transforming KG evidence into more faithful answers.",
    "paper_id": "2512.15219v1"
  },
  "https://arxiv.org/abs/2412.20138": {
    "content_path": "../data/contents/2412.20138.md",
    "pdf_path": "../data/contents/2412.20138.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:19.063434",
    "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework",
    "abstract": "Proposes a novel multi-agent trading framework with specialized roles (fundamental, sentiment, technical analysts) showing superior returns over baselines.",
    "paper_id": "2412.20138"
  },
  "https://arxiv.org/abs/2512.15210v1": {
    "content_path": "../data/contents/2512.15210v1.md",
    "pdf_path": "../data/contents/2512.15210v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:20.319026",
    "title": "Label-consistent clustering for evolving data",
    "abstract": "Summary: Data analysis often involves an iterative process, where solutions must be continuously refined in response to new data. Typically, as new data becomes available, an existing solution must be updated to incorporate the latest information. In addition to seeking a high-quality solution for the task at hand, it is also crucial to ensure consistency by minimizing drastic changes from previous solutions. Applying this approach across many iterations, ensures that the solution evolves gradually and smoothly.\n  In this paper, we study the above problem in the context of clustering, specifically focusing on the $k$-center problem. More precisely, we study the following problem: Given a set of points $X$, parameters $k$ and $b$, and a prior clustering solution $H$ for $X$, our goal is to compute a new solution $C$ for $X$, consisting of $k$ centers, which minimizes the clustering cost while introducing at most $b$ changes from $H$. We refer to this problem as label-consistent $k$-center, and we propose two constant-factor approximation algorithms for it. We complement our theoretical findings with an experimental evaluation demonstrating the effectiveness of our methods on real-world datasets.",
    "paper_id": "2512.15210v1"
  },
  "https://arxiv.org/html/2411.04788v1": {
    "content_path": "../data/contents/2411.04788v1.md",
    "pdf_path": "../data/contents/2411.04788v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:20.886055",
    "title": "Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research",
    "abstract": "Academic paper from ICAIF '24 demonstrating multi-agent collaboration outperforms single-agent models in investment research.",
    "paper_id": "2411.04788v1"
  },
  "https://arxiv.org/abs/2512.15176v1": {
    "content_path": "../data/contents/2512.15176v1.md",
    "pdf_path": "../data/contents/2512.15176v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:24.495762",
    "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
    "abstract": "Summary: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/",
    "paper_id": "2512.15176v1"
  },
  "https://arxiv.org/abs/2512.15163v1": {
    "content_path": "../data/contents/2512.15163v1.md",
    "pdf_path": "../data/contents/2512.15163v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:30.581306",
    "title": "MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers",
    "abstract": "Summary: Large language models (LLMs) are evolving into agentic systems that reason, plan, and operate external tools. The Model Context Protocol (MCP) is a key enabler of this transition, offering a standardized interface for connecting LLMs with heterogeneous tools and services. Yet MCP's openness and multi-server workflows introduce new safety risks that existing benchmarks fail to capture, as they focus on isolated attacks or lack real-world coverage. We present MCP-SafetyBench, a comprehensive benchmark built on real MCP servers that supports realistic multi-turn evaluation across five domains: browser automation, financial analysis, location navigation, repository management, and web search. It incorporates a unified taxonomy of 20 MCP attack types spanning server, host, and user sides, and includes tasks requiring multi-step reasoning and cross-server coordination under uncertainty. Using MCP-SafetyBench, we systematically evaluate leading open- and closed-source LLMs, revealing large disparities in safety performance and escalating vulnerabilities as task horizons and server interactions grow. Our results highlight the urgent need for stronger defenses and establish MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments.",
    "paper_id": "2512.15163v1"
  },
  "https://arxiv.org/abs/2512.15157v1": {
    "content_path": "../data/contents/2512.15157v1.md",
    "pdf_path": "../data/contents/2512.15157v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:31.464496",
    "title": "Extracting node comparison insights for the interactive exploration of property graphs",
    "abstract": "Summary: While scoring nodes in graphs to understand their importance (e.g., in terms of centrality) has been investigated for decades, comparing nodes in property graphs based on their properties has not, to our knowledge, yet been addressed. In this paper, we propose an approach to automatically extract comparison of nodes in property graphs, to support the interactive exploratory analysis of said graphs. We first present a way of devising comparison indicators using the context of nodes to be compared. Then, we formally define the problem of using these indicators to group the nodes so that the comparisons extracted are both significant and not straightforward. We propose various heuristics for solving this problem. Our tests on real property graph databases show that simple heuristics can be used to obtain insights within minutes while slower heuristics are needed to obtain insights of higher quality.",
    "paper_id": "2512.15157v1"
  },
  "https://arxiv.org/abs/2512.15151v2": {
    "content_path": "../data/contents/2512.15151v2.md",
    "pdf_path": "../data/contents/2512.15151v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:31.842280",
    "title": "Rakuten Data Release: A Large-Scale and Long-Term Reviews Corpus for Hotel Domain",
    "abstract": "Summary: This paper presents a large-scale corpus of Rakuten Travel Reviews. Our collection contains 7.3 million customer reviews for 16 years, ranging from 2009 to 2024. Each record in the dataset contains the review text, its response from an accommodation, an anonymized reviewer ID, review date, accommodation ID, plan ID, plan title, room type, room name, purpose, accompanying group, and user ratings from different aspect categories, as well as an overall score. We present statistical information about our corpus and provide insights into factors driving data drift between 2019 and 2024 using statistical approaches.",
    "paper_id": "2512.15151v2"
  },
  "https://arxiv.org/abs/2512.15146v2": {
    "content_path": "../data/contents/2512.15146v2.md",
    "pdf_path": "../data/contents/2512.15146v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:33.049578",
    "title": "Beyond Majority Voting: Towards Fine-grained and More Reliable Reward Signal for Test-Time Reinforcement Learning",
    "abstract": "Summary: Test-time reinforcement learning mitigates the reliance on annotated data by using majority voting results as pseudo-labels, emerging as a complementary direction to reinforcement learning with verifiable rewards (RLVR) for improving reasoning ability of large language models (LLMs). However, this voting strategy often induces confirmation bias and suffers from sparse rewards, limiting the overall performance. In this work, we propose subgroup-specific step-wise confidence-weighted pseudo-label estimation (SCOPE), a framework integrating model confidence and dynamic subgroup partitioning to address these issues. Specifically, SCOPE integrates the proposed step-wise confidence into pseudo label deduction, prioritizing high-quality reasoning paths over simple frequency count. Furthermore, it dynamically partitions the candidate outputs pool into independent subgroups by balancing reasoning quality against exploration diversity. By deriving local consensus via repeat sampling for each sub group, SCOPE provides diverse supervision targets to encourage broader exploration. We conduct experiments across various models and benchmarks, experimental results show that SCOPE consistently outperforms recent baselines. Notably, SCOPE achieving relative improvements of 13.1% on challenging AIME 2025 and 8.1% on AMC. The code is released at https://github.com/szu-tera/SCOPE.",
    "paper_id": "2512.15146v2"
  },
  "https://arxiv.org/abs/2512.15140v1": {
    "content_path": "../data/contents/2512.15140v1.md",
    "pdf_path": "../data/contents/2512.15140v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:38.050851",
    "title": "Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany",
    "abstract": "Summary: This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).\n  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.\n  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?",
    "paper_id": "2512.15140v1"
  },
  "https://arxiv.org/abs/2512.15134v1": {
    "content_path": "../data/contents/2512.15134v1.md",
    "pdf_path": "../data/contents/2512.15134v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:38.790647",
    "title": "From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?",
    "abstract": "Summary: A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.",
    "paper_id": "2512.15134v1"
  },
  "https://arxiv.org/abs/2512.19730v1": {
    "content_path": "../data/contents/2512.19730v1.md",
    "pdf_path": "../data/contents/2512.19730v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:42.086770",
    "title": "ArcGen: Generalizing Neural Backdoor Detection Across Diverse Architectures",
    "abstract": "Summary: Backdoor attacks pose a significant threat to the security and reliability of deep learning models. To mitigate such attacks, one promising approach is to learn to extract features from the target model and use these features for backdoor detection. However, we discover that existing learning-based neural backdoor detection methods do not generalize well to new architectures not seen during the learning phase. In this paper, we analyze the root cause of this issue and propose a novel black-box neural backdoor detection method called ArcGen. Our method aims to obtain architecture-invariant model features, i.e., aligned features, for effective backdoor detection. Specifically, in contrast to existing methods directly using model outputs as model features, we introduce an additional alignment layer in the feature extraction function to further process these features. This reduces the direct influence of architecture information on the features. Then, we design two alignment losses to train the feature extraction function. These losses explicitly require that features from models with similar backdoor behaviors but different architectures are aligned at both the distribution and sample levels. With these techniques, our method demonstrates up to 42.5% improvements in detection performance (e.g., AUC) on unseen model architectures. This is based on a large-scale evaluation involving 16,896 models trained on diverse datasets, subjected to various backdoor attacks, and utilizing different model architectures. Our code is available at https://github.com/SeRAlab/ArcGen.",
    "paper_id": "2512.19730v1"
  },
  "https://arxiv.org/abs/2512.19729v1": {
    "content_path": "../data/contents/2512.19729v1.md",
    "pdf_path": "../data/contents/2512.19729v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:42.821141",
    "title": "High-Performance Self-Supervised Learning by Joint Training of Flow Matching",
    "abstract": "Summary: Diffusion models can learn rich representations during data generation, showing potential for Self-Supervised Learning (SSL), but they face a trade-off between generative quality and discriminative performance. Their iterative sampling also incurs substantial computational and energy costs, hindering industrial and edge AI applications. To address these issues, we propose the Flow Matching-based Foundation Model (FlowFM), which jointly trains a representation encoder and a conditional flow matching generator. This decoupled design achieves both high-fidelity generation and effective recognition. By using flow matching to learn a simpler velocity field, FlowFM accelerates and stabilizes training, improving its efficiency for representation learning. Experiments on wearable sensor data show FlowFM reduces training time by 50.4\\% compared to a diffusion-based approach. On downstream tasks, FlowFM surpassed the state-of-the-art SSL method (SSL-Wearables) on all five datasets while achieving up to a 51.0x inference speedup and maintaining high generative quality. The implementation code is available at https://github.com/Okita-Laboratory/jointOptimizationFlowMatching.",
    "paper_id": "2512.19729v1"
  },
  "https://arxiv.org/abs/2512.15120v1": {
    "content_path": "../data/contents/2512.15120v1.md",
    "pdf_path": "../data/contents/2512.15120v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:52.773218",
    "title": "Automatic Reward Shaping from Multi-Objective Human Heuristics",
    "abstract": "Summary: Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.",
    "paper_id": "2512.15120v1"
  },
  "https://arxiv.org/abs/2512.15117v2": {
    "content_path": "../data/contents/2512.15117v2.md",
    "pdf_path": "../data/contents/2512.15117v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:54.335245",
    "title": "\"I am here for you\": How relational conversational AI appeals to adolescents, especially those who are socially and emotionally vulnerable",
    "abstract": "Summary: General-purpose conversational AI chatbots and AI companions increasingly provide young adolescents with emotionally supportive conversations, raising questions about how conversational style shapes anthropomorphism and emotional reliance. In a preregistered online experiment with 284 adolescent-parent dyads, youth aged 11-15 and their parents read two matched transcripts in which a chatbot responded to an everyday social problem using either a relational style (first-person, affiliative, commitment language) or a transparent style (explicit nonhumanness, informational tone). Adolescents more often preferred the relational than the transparent style, whereas parents were more likely to prefer transparent style than adolescents. Adolescents rated the relational chatbot as more human-like, likable, trustworthy and emotionally close, while perceiving both styles as similarly helpful. Adolescents who preferred relational style had lower family and peer relationship quality and higher stress and anxiety than those preferring transparent style or both chatbots. These findings identify conversational style as a key design lever for youth AI safety, showing that relational framing heightens anthropomorphism, trust and emotional closeness and can be especially appealing to socially and emotionally vulnerable adolescents, who may be at increased risk for emotional reliance on conversational AI.",
    "paper_id": "2512.15117v2"
  },
  "https://arxiv.org/abs/2512.15116v1": {
    "content_path": "../data/contents/2512.15116v1.md",
    "pdf_path": "../data/contents/2512.15116v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:56.883290",
    "title": "FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation",
    "abstract": "Summary: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF",
    "paper_id": "2512.15116v1"
  },
  "https://arxiv.org/abs/2512.19728v1": {
    "content_path": "../data/contents/2512.19728v1.md",
    "pdf_path": "../data/contents/2512.19728v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:59.313559",
    "title": "Hard Negative Sample-Augmented DPO Post-Training for Small Language Models",
    "abstract": "Summary: Large language models (LLMs) continue to struggle with mathematical reasoning, and common post-training pipelines often reduce each generated solution to a binary outcome: correct or incorrect. This perspective is limiting in practice, as failures in chain-of-thought (CoT) reasoning are frequently structured; solutions may appear convincing while containing subtle logical, algebraic, or numerical flaws. Meanwhile, reinforcement learning from human feedback (RLHF) variants that rely on large reward models or LLM-as-a-judge signals are often expensive, difficult to scale, and unstable to iterate. We propose a lightweight and pragmatic post-training pipeline that targets such structured errors under realistic compute budgets. Starting from supervised fine-tuning (SFT) on MetaMathQA-style CoT data, we introduce a compact MathVerifier that decomposes a candidate solution into a six-dimensional error profile and aggregates it into interpretable wrongness and absurdity scores. These verifier signals serve two roles: (i) mining hard negatives that are near-correct yet structurally flawed, and (ii) defining per-sample importance weights that emphasize the most informative preference pairs. We integrate both into an offline Direct Preference Optimization (DPO) objective via a verifier-guided weighted formulation. Experiments on a 1.5B-parameter Qwen2.5 model show that verifier-guided, weighted DPO yields more targeted improvements than vanilla SFT and unweighted DPO, particularly on problems where solutions are numerically close to correct but logically inconsistent, while avoiding the overhead of training large reward models or relying on external judges.",
    "paper_id": "2512.19728v1"
  },
  "https://arxiv.org/abs/2512.15115v1": {
    "content_path": "../data/contents/2512.15115v1.md",
    "pdf_path": "../data/contents/2512.15115v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:38:59.584191",
    "title": "How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models",
    "abstract": "Summary: Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.",
    "paper_id": "2512.15115v1"
  },
  "https://arxiv.org/abs/2512.20651v1": {
    "content_path": "../data/contents/2512.20651v1.md",
    "pdf_path": "../data/contents/2512.20651v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:02.702159",
    "title": "Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence",
    "abstract": "Summary: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from \"memory\" to \"cognition\".",
    "paper_id": "2512.20651v1"
  },
  "https://arxiv.org/abs/2512.15112v1": {
    "content_path": "../data/contents/2512.15112v1.md",
    "pdf_path": "../data/contents/2512.15112v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:03.301670",
    "title": "Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption",
    "abstract": "Summary: Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.",
    "paper_id": "2512.15112v1"
  },
  "https://arxiv.org/abs/2512.15089v1": {
    "content_path": "../data/contents/2512.15089v1.md",
    "pdf_path": "../data/contents/2512.15089v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:03.965054",
    "title": "Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models",
    "abstract": "Summary: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.",
    "paper_id": "2512.15089v1"
  },
  "https://arxiv.org/abs/2512.15082v1": {
    "content_path": "../data/contents/2512.15082v1.md",
    "pdf_path": "../data/contents/2512.15082v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:05.591826",
    "title": "The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks",
    "abstract": "Summary: Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.",
    "paper_id": "2512.15082v1"
  },
  "https://arxiv.org/abs/2512.15081v1": {
    "content_path": "../data/contents/2512.15081v1.md",
    "pdf_path": "../data/contents/2512.15081v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:07.235553",
    "title": "Quantifying Return on Security Controls in LLM Systems",
    "abstract": "Summary: Although large language models (LLMs) are increasingly used in security-critical workflows, practitioners lack quantitative guidance on which safeguards are worth deploying. This paper introduces a decision-oriented framework and reproducible methodology that together quantify residual risk, convert adversarial probe outcomes into financial risk estimates and return-on-control (RoC) metrics, and enable monetary comparison of layered defenses for LLM-based systems. A retrieval-augmented generation (RAG) service is instantiated using the DeepSeek-R1 model over a corpus containing synthetic personally identifiable information (PII), and subjected to automated attacks with Garak across five vulnerability classes: PII leakage, latent context injection, prompt injection, adversarial attack generation, and divergence. For each (vulnerability, control) pair, attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions, calibrated from public breach-cost data, in 10,000-run Monte Carlo simulations to produce loss exceedance curves and expected losses. Three widely used mitigations, attribute-based access control (ABAC); named entity recognition (NER) redaction using Microsoft Presidio; and NeMo Guardrails, are then compared to a baseline RAG configuration. The baseline system exhibits very high attack success rates (>= 0.98 for PII, latent injection, and prompt injection), yielding a total simulated expected loss of $313k per attack scenario. ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83. NER redaction likewise eliminates PII leakage and attains an RoC of 5.97, while NeMo Guardrails provides only marginal benefit (RoC of 0.05).",
    "paper_id": "2512.15081v1"
  },
  "https://arxiv.org/html/2408.06361v1": {
    "content_path": "../data/contents/2408.06361v1.md",
    "pdf_path": "../data/contents/2408.06361v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:08.086187",
    "title": "Large Language Model Agent in Financial Trading: A Survey",
    "abstract": "Comprehensive survey of LLM agents in trading covering architectures, memory, and reasoning mechanisms.",
    "paper_id": "2408.06361v1"
  },
  "https://arxiv.org/abs/2512.15068v2": {
    "content_path": "../data/contents/2512.15068v2.md",
    "pdf_path": "../data/contents/2512.15068v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:08.657304",
    "title": "The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems",
    "abstract": "Summary: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. While current detection methods leverage embedding similarity and natural language inference (NLI), their reliability in safety-critical settings remains unproven. We apply conformal prediction to RAG hallucination detection, transforming heuristic scores into decision sets with finite-sample coverage guarantees (1-alpha). Using calibration sets of n=600, we demonstrate a fundamental dichotomy: on synthetic hallucinations (Natural Questions), embedding methods achieve 95% coverage with 0% False Positive Rate (FPR). However, on real hallucinations from RLHF-aligned models (HaluEval), the same methods fail catastrophically, yielding 100% FPR at target coverage. We analyze this failure through the lens of distributional tails, showing that while NLI models achieve acceptable AUC (0.81), the \"hardest\" hallucinations are semantically indistinguishable from faithful responses, forcing conformal thresholds to reject nearly all valid outputs. Crucially, GPT-4 as a judge achieves 7% FPR (95% CI:[3.4%, 13.7%]) on the same data, proving the task is solvable via reasoning but opaque to surface-level semantics--a phenomenon we term the \"Semantic Illusion.\"",
    "paper_id": "2512.15068v2"
  },
  "https://arxiv.org/html/2504.10789v1": {
    "content_path": "../data/contents/2504.10789v1.md",
    "pdf_path": "../data/contents/2504.10789v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:09.211892",
    "title": "Can Large Language Models Trade? Testing Financial Theories with LLM Agents",
    "abstract": "Research on LLM trading agents showing they can execute strategies and produce realistic market dynamics.",
    "paper_id": "2504.10789v1"
  },
  "https://arxiv.org/abs/2512.17952v1": {
    "content_path": "../data/contents/2512.17952v1.md",
    "pdf_path": "../data/contents/2512.17952v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:09.440062",
    "title": "Will AI Trade? A Computational Inversion of the No-Trade Theorem",
    "abstract": "Summary: Classic no-trade theorems attribute trade to heterogeneous beliefs. We re-examine this conclusion for AI agents, asking if trade can arise from computational limitations, under common beliefs. We model agents' bounded computational rationality within an unfolding game framework, where computational power determines the complexity of its strategy. Our central finding inverts the classic paradigm: a stable no-trade outcome (Nash equilibrium) is reached only when \"almost rational\" agents have slightly different computational power. Paradoxically, when agents possess identical power, they may fail to converge to equilibrium, resulting in persistent strategic adjustments that constitute a form of trade. This instability is exacerbated if agents can strategically under-utilize their computational resources, which eliminates any chance of equilibrium in Matching Pennies scenarios. Our results suggest that the inherent computational limitations of AI agents can lead to situations where equilibrium is not reached, creating a more lively and unpredictable trade environment than traditional models would predict.",
    "paper_id": "2512.17952v1"
  },
  "https://arxiv.org/abs/2512.15053v1": {
    "content_path": "../data/contents/2512.15053v1.md",
    "pdf_path": "../data/contents/2512.15053v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:09.943655",
    "title": "The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops",
    "abstract": "Summary: The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based \"prompt engineering,\" fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for \"Observable Software Engineering\" in the era of probabilistic computing.",
    "paper_id": "2512.15053v1"
  },
  "https://arxiv.org/abs/2512.15052v1": {
    "content_path": "../data/contents/2512.15052v1.md",
    "pdf_path": "../data/contents/2512.15052v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:11.700590",
    "title": "SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification",
    "abstract": "Summary: Disclaimer: Samples in this paper may be harmful and cause discomfort.\n  Multimodal large language models (MLLMs) enable multimodal generation but inherit toxic, biased, and NSFW signals from weakly curated pretraining corpora, causing safety risks, especially under adversarial triggers that late, opaque training-free detoxification methods struggle to handle. We propose SGM, a white-box neuron-level multimodal intervention that acts like safety glasses for toxic neurons: it selectively recalibrates a small set of toxic expert neurons via expertise-weighted soft suppression, neutralizing harmful cross-modal activations without any parameter updates. We establish MM-TOXIC-QA, a multimodal toxicity evaluation framework, and compare SGM with existing detoxification techniques. Experiments on open-source MLLMs show that SGM mitigates toxicity in standard and adversarial conditions, cutting harmful rates from 48.2\\% to 2.5\\% while preserving fluency and multimodal reasoning. SGM is extensible, and its combined defenses, denoted as SGM*, integrate with existing detoxification methods for stronger safety performance, providing an interpretable, low-cost solution for toxicity-controlled multimodal generation.",
    "paper_id": "2512.15052v1"
  },
  "https://arxiv.org/abs/2512.15042v1": {
    "content_path": "../data/contents/2512.15042v1.md",
    "pdf_path": "../data/contents/2512.15042v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:11.956648",
    "title": "DASH: Dialogue-Aware Similarity and Handshake Recognition for Topic Segmentation in Public-Channel Conversations",
    "abstract": "Summary: Dialogue Topic Segmentation (DTS) is crucial for understanding task-oriented public-channel communications, such as maritime VHF dialogues, which feature informal speech and implicit transitions. To address the limitations of traditional methods, we propose DASH-DTS, a novel LLM-based framework. Its core contributions are: (1) topic shift detection via dialogue handshake recognition; (2) contextual enhancement through similarity-guided example selection; and (3) the generation of selective positive and negative samples to improve model discrimination and robustness. Additionally, we release VHF-Dial, the first public dataset of real-world maritime VHF communications, to advance research in this domain. DASH-DTS provides interpretable reasoning and confidence scores for each segment. Experimental results demonstrate that our framework achieves several sota segmentation trusted accuracy on both VHF-Dial and standard benchmarks, establishing a strong foundation for stable monitoring and decision support in operational dialogues.",
    "paper_id": "2512.15042v1"
  },
  "https://arxiv.org/abs/2512.15038v2": {
    "content_path": "../data/contents/2512.15038v2.md",
    "pdf_path": "../data/contents/2512.15038v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:14.336334",
    "title": "LADY: Linear Attention for Autonomous Driving Efficiency without Transformers",
    "abstract": "Summary: End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.",
    "paper_id": "2512.15038v2"
  },
  "https://arxiv.org/abs/2512.15033v1": {
    "content_path": "../data/contents/2512.15033v1.md",
    "pdf_path": "../data/contents/2512.15033v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:15.331518",
    "title": "Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation",
    "abstract": "Summary: The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.",
    "paper_id": "2512.15033v1"
  },
  "https://arxiv.org/abs/2512.15031v1": {
    "content_path": "../data/contents/2512.15031v1.md",
    "pdf_path": "../data/contents/2512.15031v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:16.035148",
    "title": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub",
    "abstract": "Summary: Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.\n  We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate \\textit{Summaries of Conversation Dynamics} (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the \\textit{likelihood of derailment}. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.",
    "paper_id": "2512.15031v1"
  },
  "https://arxiv.org/abs/2512.15011v1": {
    "content_path": "../data/contents/2512.15011v1.md",
    "pdf_path": "../data/contents/2512.15011v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:17.157674",
    "title": "Epistemic diversity across language models mitigates knowledge collapse",
    "abstract": "Summary: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.",
    "paper_id": "2512.15011v1"
  },
  "https://arxiv.org/abs/2512.15008v1": {
    "content_path": "../data/contents/2512.15008v1.md",
    "pdf_path": "../data/contents/2512.15008v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:21.620359",
    "title": "Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets",
    "abstract": "Summary: Understanding how prices evolve over time often requires peeling back the layers of market noise to identify clear, structural behavior. Many of the tools commonly used for this purpose technical indicators, chart heuristics, or even sophisticated predictive models leave important questions unanswered. Technical indicators depend on platform-specific rules, and predictive systems typically offer little in terms of explanation. In settings that demand transparency or auditability, this poses a significant challenge. We introduce the Stock Pattern Assistant (SPA), a deterministic framework designed to extract monotonic price runs, attach relevant public events through a symmetric correlation window, and generate explanations that are factual, historical, and guardrailed. SPA relies only on daily OHLCV data and a normalized event stream, making the pipeline straight-forward to audit and easy to reproduce. To illustrate SPA's behavior in practice, we evaluate it across four equities-AAPL, NVDA, SCHW, and PGR-chosen to span a range of volatility regimes and sector characteristics. Although the evaluation period is modest, the results demonstrate how SPA consistently produces stable structural decompositions and contextual narratives. Ablation experiments further show how deterministic segmentation, event alignment, and constrained explanation each contribute to interpretability. SPA is not a forecasting system, nor is it intended to produce trading signals. Its value lies in offering a transparent, reproducible view of historical price structure that can complement analyst workflows, risk reviews, and broader explainable-AI pipelines.",
    "paper_id": "2512.15008v1"
  },
  "https://arxiv.org/abs/2512.15006v1": {
    "content_path": "../data/contents/2512.15006v1.md",
    "pdf_path": "../data/contents/2512.15006v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:22.751016",
    "title": "Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation",
    "abstract": "Summary: Skilled human interviewers can extract valuable information from experts. This raises a fundamental question: what makes some questions more effective than others? To address this, a quantitative evaluation of question-generation models is essential. Video question generation (VQG) is a topic for video question answering (VideoQA), where questions are generated for given answers. Their evaluation typically focuses on the ability to answer questions, rather than the quality of generated questions. In contrast, we focus on the question quality in eliciting unseen knowledge from human experts. For a continuous improvement of VQG models, we propose a protocol that evaluates the ability by simulating question-answering communication with experts using a question-to-answer retrieval. We obtain the retriever by constructing a novel dataset, EgoExoAsk, which comprises 27,666 QA pairs generated from Ego-Exo4D's expert commentary annotation. The EgoExoAsk training set is used to obtain the retriever, and the benchmark is constructed on the validation set with Ego-Exo4D video segments. Experimental results demonstrate our metric reasonably aligns with question generation settings: models accessing richer context are evaluated better, supporting that our protocol works as intended. The EgoExoAsk dataset is available in https://github.com/omron-sinicx/VQG4ExpertKnowledge .",
    "paper_id": "2512.15006v1"
  },
  "https://arxiv.org/abs/2512.15003v1": {
    "content_path": "../data/contents/2512.15003v1.md",
    "pdf_path": "../data/contents/2512.15003v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:23.206198",
    "title": "SeBERTis: A Framework for Producing Classifiers of Security-Related Issue Reports",
    "abstract": "Summary: Monitoring issue tracker submissions is a crucial software maintenance activity. A key goal is the prioritization of high risk, security-related bugs. If such bugs can be recognized early, the risk of propagation to dependent products and endangerment of stakeholder benefits can be mitigated. To assist triage engineers with this task, several automatic detection techniques, from Machine Learning (ML) models to prompting Large Language Models (LLMs), have been proposed. Although promising to some extent, prior techniques often memorize lexical cues as decision shortcuts, yielding low detection rate specifically for more complex submissions. As such, these classifiers do not yet reach the practical expectations of a real-time detector of security-related issues. To address these limitations, we propose SEBERTIS, a framework to train Deep Neural Networks (DNNs) as classifiers independent of lexical cues, so that they can confidently detect fully unseen security-related issues. SEBERTIS capitalizes on fine-tuning bidirectional transformer architectures as Masked Language Models (MLMs) on a series of semantically equivalent vocabulary to prediction labels (which we call Semantic Surrogates) when they have been replaced with a mask. Our SEBERTIS-trained classifier achieves a 0.9880 F1-score in detecting security-related issues of a curated corpus of 10,000 GitHub issue reports, substantially outperforming state-of-the-art issue classifiers, with 14.44%-96.98%, 15.40%-93.07%, and 14.90%-94.72% higher detection precision, recall, and F1-score over ML-based baselines. Our classifier also substantially surpasses LLM baselines, with an improvement of 23.20%-63.71%, 36.68%-85.63%, and 39.49%-74.53% for precision, recall, and F1-score.",
    "paper_id": "2512.15003v1"
  },
  "https://arxiv.org/abs/2512.15002v1": {
    "content_path": "../data/contents/2512.15002v1.md",
    "pdf_path": "../data/contents/2512.15002v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:24.406867",
    "title": "Dense Associative Memories with Analog Circuits",
    "abstract": "Summary: The increasing computational demands of modern AI systems have exposed fundamental limitations of digital hardware, driving interest in alternative paradigms for efficient large-scale inference. Dense Associative Memory (DenseAM) is a family of models that offers a flexible framework for representing many contemporary neural architectures, such as transformers and diffusion models, by casting them as dynamical systems evolving on an energy landscape. In this work, we propose a general method for building analog accelerators for DenseAMs and implementing them using electronic RC circuits, crossbar arrays, and amplifiers. We find that our analog DenseAM hardware performs inference in constant time independent of model size. This result highlights an asymptotic advantage of analog DenseAMs over digital numerical solvers that scale at least linearly with the model size. We consider three settings of progressively increasing complexity: XOR, the Hamming (7,4) code, and a simple language model defined on binary variables. We propose analog implementations of these three models and analyze the scaling of inference time, energy consumption, and hardware. Finally, we estimate lower bounds on the achievable time constants imposed by amplifier specifications, suggesting that even conservative existing analog technology can enable inference times on the order of tens to hundreds of nanoseconds. By harnessing the intrinsic parallelism and continuous-time operation of analog circuits, our DenseAM-based accelerator design offers a new avenue for fast and scalable AI hardware.",
    "paper_id": "2512.15002v1"
  },
  "https://arxiv.org/abs/2512.15000v1": {
    "content_path": "../data/contents/2512.15000v1.md",
    "pdf_path": "../data/contents/2512.15000v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T13:39:24.737687",
    "title": "DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding",
    "abstract": "Summary: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.",
    "paper_id": "2512.15000v1"
  },
  "https://dev.to/datastax/the-best-embedding-models-for-information-retrieval-in-2025-3dp5": {
    "content_path": "../data/contents/38ba7a350f7883a2.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.098216",
    "title": "The Best Embedding Models for Information Retrieval in 2025"
  },
  "https://blog.voyageai.com/2025/07/23/voyage-context-3/": {
    "content_path": "../data/contents/6f4782193292842b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.132105",
    "title": "Introducing voyage-context-3: focused chunk-level details with global document context"
  },
  "https://microsoft.github.io/graphrag/": {
    "content_path": "../data/contents/8a6a98e4cbf1799c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.162083",
    "title": "Welcome - GraphRAG"
  },
  "https://cdn.openai.com/pdf/3a4153c8-c748-4b71-8e31-aecbde944f8d/oai_5_2_system-card.pdf": {
    "content_path": "../data/contents/28e7c670b8f38138.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.204238",
    "title": "oai_5_2_system-card.pdf"
  },
  "https://newsroom.morningstar.com/newsroom/news-archive/press-release-details/2025/Morningstar-and-PitchBook-Bring-Trusted-Investing-Intelligence-to-Apps-in-ChatGPT/default.aspx": {
    "content_path": "../data/contents/6e00e9f4d8ac2892.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.227770",
    "title": "Morningstar and PitchBook Bring Trusted Investing Intelligence to Apps in ChatGPT"
  },
  "https://openai.com/index/gpt-5-system-card-update-gpt-5-2/": {
    "content_path": "../data/contents/9c7a8b4e0f16bbb2.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.258334",
    "title": "Update to GPT-5 System Card: GPT-5.2"
  },
  "https://cloud.google.com/blog/products/ai-machine-learning/announcing-official-mcp-support-for-google-services": {
    "content_path": "../data/contents/0d5479ec8f02bb3f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.283326",
    "title": "Announcing official MCP support for Google services"
  },
  "https://github.blog/changelog/2025-12-18-github-copilot-now-supports-agent-skills/": {
    "content_path": "../data/contents/04c8caab56a3ced0.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.312528",
    "title": "GitHub Copilot now supports Agent Skills"
  },
  "https://docs.anthropic.com/en/docs/build-with-claude/pdf-support": {
    "content_path": "../data/contents/4e384f9d07b8b173.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.335699",
    "title": "Not Found - Claude Docs"
  },
  "https://weaviate.io/blog/chunking-strategies-for-rag": {
    "content_path": "../data/contents/ab638dcb2898d887.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.359287",
    "title": "Chunking Strategies to Improve Your RAG Performance | Weaviate"
  },
  "https://modelcontextprotocol.io/": {
    "content_path": "../data/contents/50a58b4f655899e0.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.383179",
    "title": "What is the Model Context Protocol (MCP)? - Model Context Protocol"
  },
  "https://developers.openai.com/codex/skills": {
    "content_path": "../data/contents/30cff081c6061526.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.408910",
    "title": "Agent Skills"
  },
  "https://tradingagents-ai.github.io/": {
    "content_path": "../data/contents/54ffb3357e2192a9.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.437892",
    "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework"
  },
  "https://www.applied-ai.com/briefings/pdf-parsing-benchmark/": {
    "content_path": "../data/contents/40eb032f7dedb592.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.460890",
    "title": "The State of PDF Parsing: What 800+ Documents and 7 Frontier LLMs Taught Us About Parser Selection"
  },
  "https://docs.github.com/en/copilot/concepts/agents/about-agent-skills": {
    "content_path": "../data/contents/c3200a78bfd5aa52.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.486627",
    "title": "About Agent Skills - GitHub Docs"
  },
  "https://www.firecrawl.dev/blog/best-open-source-rag-frameworks": {
    "content_path": "../data/contents/6de523a9c70e7df6.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.511292",
    "title": "15 Best Open-Source RAG Frameworks in 2025"
  },
  "https://github.com/QwenLM/Qwen3-Embedding": {
    "content_path": "../data/contents/d92a1223d1c07cf8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.543368",
    "title": "GitHub - QwenLM/Qwen3-Embedding"
  },
  "https://code.visualstudio.com/docs/copilot/customization/agent-skills": {
    "content_path": "../data/contents/8e69f2f080de682f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.568937",
    "title": "Use Agent Skills in VS Code"
  },
  "https://modal.com/blog/mteb-leaderboard-article": {
    "content_path": "../data/contents/7d5375c00012478f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.596723",
    "title": "Top embedding models on the MTEB leaderboard"
  },
  "https://venturebeat.com/data-infrastructure/aws-claims-90-vector-cost-savings-with-s3-vectors-ga-calls-it-complementary": {
    "content_path": "../data/contents/47faf99e4f4958a0.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.648476",
    "title": "AWS claims 90% vector cost savings with S3 Vectors GA, calls it 'complementary' - analysts split on what that means for vector databases"
  },
  "https://www.techradar.com/pro/anthropic-takes-the-fight-to-openai-with-enterprise-ai-tools-and-theyre-going-open-source-too": {
    "content_path": "../data/contents/5ba119ab4ba55097.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.706381",
    "title": "Anthropic takes the fight to OpenAI with enterprise AI tools - and they're going open source too"
  },
  "https://aclanthology.org/2024.findings-acl.854/": {
    "content_path": "../data/contents/c88ef24812fd9415.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.736928",
    "title": "Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models"
  },
  "https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview": {
    "content_path": "../data/contents/0af08141749a1839.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.774290",
    "title": "RAG and generative AI - Azure AI Search"
  },
  "https://github.com/deepseek-ai/Janus": {
    "content_path": "../data/contents/f5f013c9dea7a426.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.798727",
    "title": "GitHub - deepseek-ai/Janus: Janus-Series: Unified Multimodal Understanding and Generation Models"
  },
  "https://www.zenml.io/blog/best-embedding-models-for-rag": {
    "content_path": "../data/contents/9d4a9011cc6d940f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.820871",
    "title": "9 Best Embedding Models for RAG to Try This Year - ZenML Blog"
  },
  "https://huggingface.co/deepseek-ai/DeepSeek-R1": {
    "content_path": "../data/contents/3ce63bb1cf5241de.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.843788",
    "title": "deepseek-ai/DeepSeek-R1 · Hugging Face"
  },
  "https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses/": {
    "content_path": "../data/contents/64b1c32d8f2dfd9c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.875578",
    "title": "Finding the Best Chunking Strategy for Accurate AI Responses"
  },
  "https://venturebeat.com/ai/beyond-rag-how-cache-augmented-generation-reduces-latency-complexity-for-smaller-workloads": {
    "content_path": "../data/contents/6bdb38b65b91845f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.896776",
    "title": "Beyond RAG: How cache-augmented generation reduces latency, complexity for smaller workloads"
  },
  "https://blog.n8n.io/ai-agent-orchestration-frameworks/": {
    "content_path": "../data/contents/d0c25edb6c6c56e6.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.918851",
    "title": "AI Agent Orchestration Frameworks: Which One Works Best for You?"
  },
  "https://weaviate.io/blog/hybrid-search-explained": {
    "content_path": "../data/contents/6a7fecda3e84014d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.941194",
    "title": "Hybrid Search Explained | Weaviate"
  },
  "https://langcopilot.com/posts/2025-09-18-top-rag-frameworks-2024-complete-guide": {
    "content_path": "../data/contents/9b6d05667538a944.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.963052",
    "title": "Best RAG Frameworks 2025: LangChain vs LlamaIndex vs Haystack (Benchmarks Inside)"
  },
  "https://venturebeat.com/ai/deepmind-new-inference-time-scaling-technique-improves-planning-accuracy-in-llms": {
    "content_path": "../data/contents/ce7e36444d104b7f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:26.984361",
    "title": "DeepMind’s new inference-time scaling technique improves planning accuracy in LLMs"
  },
  "https://news.microsoft.com/source/2025/10/12/lseg-and-microsoft-transform-access-to-ai-ready-financial-data-in-customer-workflows/": {
    "content_path": "../data/contents/a3183fc317cf5afc.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.006697",
    "title": "LSEG and Microsoft announce next phase of strategic partnership to transform financial services with agentic AI"
  },
  "https://docs.mistral.ai/models/mistral-large-3-25-12": {
    "content_path": "../data/contents/3ab28585a19b11ad.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.027841",
    "title": "Mistral Large 3 - Mistral AI"
  },
  "https://venturebeat.com/ai/anthropic-launches-enterprise-agent-skills-and-opens-the-standard": {
    "content_path": "../data/contents/64b95d41777175d7.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.049738",
    "title": "Anthropic launches enterprise ‘Agent Skills’ and opens the standard, challenging OpenAI in workplace AI"
  },
  "https://www.cbinsights.com/research/report/top-fintech-startups-2025/": {
    "content_path": "../data/contents/0e6942f8251ffc85.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.084282",
    "title": "Fintech 100: The most promising fintech startups of 2025"
  },
  "https://docs.mistral.ai/api/endpoint/ocr": {
    "content_path": "../data/contents/5cfdc693dfa97f05.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.106684",
    "title": "Ocr"
  },
  "https://deepmind.google/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/": {
    "content_path": "../data/contents/2695d658d523e068.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.128353",
    "title": "FACTS Grounding: A new benchmark for evaluating the factuality of large language models"
  },
  "https://github.com/deepseek-ai/DeepSeek-R1": {
    "content_path": "../data/contents/afad87e802e53736.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.150079",
    "title": "GitHub - deepseek-ai/DeepSeek-R1"
  },
  "https://github.com/Yuliang-Liu/MonkeyOCR": {
    "content_path": "../data/contents/a345fc0b547ca168.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.172342",
    "title": "GitHub - Yuliang-Liu/MonkeyOCR: A lightweight LMM-based Document Parsing Model"
  },
  "https://agents.md/": {
    "content_path": "../data/contents/e7c64f1131461e39.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.193994",
    "title": "AGENTS.md"
  },
  "https://google.github.io/adk-docs/": {
    "content_path": "../data/contents/10eae276fc8f2aed.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.215536",
    "title": "Agent Development Kit"
  },
  "https://www.elastic.co/search-labs/blog/elastic-semantic-reranker-part-3": {
    "content_path": "../data/contents/8696c14db8386a2f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.237469",
    "title": "Exploring depth in a 'retrieve-and-rerank' pipeline"
  },
  "https://techcrunch.com/2025/11/26/here-are-the-49-us-ai-startups-that-have-raised-100m-or-more-in-2025/": {
    "content_path": "../data/contents/d4b2238f8e8fb3ef.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.259402",
    "title": "Here are the 49 US AI startups that have raised $100M or more in 2025"
  },
  "https://docs.mistral.ai/models/ministral-3-8b-25-12": {
    "content_path": "../data/contents/8ef7d9384aab8b2b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.280707",
    "title": "Ministral 3 8B - Mistral AI"
  },
  "https://www.llamaindex.ai/blog/introducing-agentworkflow-a-powerful-system-for-building-ai-agent-systems": {
    "content_path": "../data/contents/35d6922c8bb3c8c5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.303047",
    "title": "Introducing AgentWorkflow: A Powerful System for Building AI Agent Systems"
  },
  "https://weaviate.io/blog/late-interaction-overview": {
    "content_path": "../data/contents/eb9ae5d459ae951d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.325150",
    "title": "An Overview of Late Interaction Retrieval Models: ColBERT, ColPali, and ColQwen | Weaviate"
  },
  "https://arcprize.org/blog/analyzing-o3-with-arc-agi": {
    "content_path": "../data/contents/88db56cf1adeb58c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.349066",
    "title": "Analyzing o3 and o4-mini with ARC-AGI"
  },
  "https://aclanthology.org/2025.findings-emnlp.672/": {
    "content_path": "../data/contents/7dafc587461a33aa.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.385720",
    "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search"
  },
  "https://magazine.sebastianraschka.com/p/llm-research-papers-2025-list-one": {
    "content_path": "../data/contents/6be4c92ed8e99fc5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.408795",
    "title": "LLM Research Papers: The 2025 List (January to June)"
  },
  "https://github.com/github/github-mcp-server/releases": {
    "content_path": "../data/contents/fce99c6096ffc39d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.430511",
    "title": "Releases · github/github-mcp-server"
  },
  "https://github.com/DEEP-PolyU/Awesome-GraphRAG": {
    "content_path": "../data/contents/a3e2363052836e28.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.451945",
    "title": "GitHub - DEEP-PolyU/Awesome-GraphRAG: Awesome-GraphRAG: A curated list of resources (surveys, papers, benchmarks, and opensource projects) on graph-based retrieval-augmented generation."
  },
  "https://docling-project.github.io/docling/usage/vision_models/": {
    "content_path": "../data/contents/4a76b600be2893ce.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.473510",
    "title": "Vision models - Docling"
  },
  "https://huggingface.co/blog/NormalUhr/deepseek-r1-explained": {
    "content_path": "../data/contents/e9919b689abceae3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.495876",
    "title": "From Zero to Reasoning Hero: How DeepSeek-R1 Leverages Reinforcement Learning to Master Complex Reasoning"
  },
  "https://internvl.github.io/blog/2024-12-05-InternVL-2.5/": {
    "content_path": "../data/contents/0f2ee14f1da6d91d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.517777",
    "title": "InternVL2.5: Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling"
  },
  "https://en.wikipedia.org/wiki/OpenAI_o3": {
    "content_path": "../data/contents/8d8cfe888512b7eb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.539911",
    "title": "OpenAI o3"
  },
  "https://www.businesswire.com/news/home/20250908104474/en/Hebbia-Partners-With-FactSet-to-Power-AI-Driven-Financial-Research": {
    "content_path": "../data/contents/9119e15146df7555.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.563216",
    "title": "Hebbia Partners With FactSet to Power AI-Driven Financial Research"
  },
  "https://www.whatllm.org/blog/state-of-llms-december-2025": {
    "content_path": "../data/contents/8c3c8335d270075e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.603712",
    "title": "The State of LLMs: December 2025"
  },
  "https://aclanthology.org/2024.acl-long.331/": {
    "content_path": "../data/contents/e0e166a7de5546fa.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.631455",
    "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?"
  },
  "https://aclanthology.org/2024.acl-long.775/": {
    "content_path": "../data/contents/fb31083672f3961b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.655900",
    "title": "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models"
  },
  "https://developers.googleblog.com/announcing-the-agent-development-kit-for-go-build-powerful-ai-agents-with-your-favorite-languages/": {
    "content_path": "../data/contents/7f018f1b071e4c71.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.678516",
    "title": "Announcing the Agent Development Kit for Go: Build Powerful AI Agents with Your Favorite Languages"
  },
  "https://github.com/QwenLM/Qwen3-VL": {
    "content_path": "../data/contents/88d1db358c011b9c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.701759",
    "title": "GitHub - QwenLM/Qwen3-VL: Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud."
  },
  "https://www.anthropic.com/engineering/claude-think-tool": {
    "content_path": "../data/contents/4d418dad93eb839e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.731944",
    "title": "The \"think\" tool: Enabling Claude to stop and think"
  },
  "https://deepmind.google/models/model-cards/": {
    "content_path": "../data/contents/32f83c1b97cebd06.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.759363",
    "title": "Model cards"
  },
  "https://github.com/Topdu/DocPTBench": {
    "content_path": "../data/contents/d80b5fd0206c73d3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.785357",
    "title": "GitHub - Topdu/DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation"
  },
  "https://aclanthology.org/2025.emnlp-industry.116/": {
    "content_path": "../data/contents/19de9934b810fb4c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.810516",
    "title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning"
  },
  "https://github.com/deepseek-ai/DeepSeek-Math-V2": {
    "content_path": "../data/contents/53f90b7cf1d4ed7f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.837245",
    "title": "GitHub - deepseek-ai/DeepSeek-Math-V2"
  },
  "https://github.com/anthropics/claude-agent-sdk-typescript": {
    "content_path": "../data/contents/af204ca9c0d805fe.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.862367",
    "title": "GitHub - anthropics/claude-agent-sdk-typescript"
  },
  "https://trec-rag.github.io/": {
    "content_path": "../data/contents/ab00faa92b4a0be0.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.900144",
    "title": "What is TREC RAG?"
  },
  "https://aclanthology.org/2023.findings-emnlp.123/": {
    "content_path": "data/contents/ab4a10102ce75699.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.616330",
    "title": "Towards Mitigating LLM Hallucination via Self Reflection - ACL Anthology",
    "summary": "The webpage describes a paper titled \"Towards Mitigating LLM Hallucination via Self Reflection.\" The abstract focuses on the challenge of **hallucination** in Large Language Models (LLMs), particularly in medical generative Question-Answering (QA) systems, where models generate unfaithful or nonsensical information.\n\nThe paper proposes an **interactive self-reflection methodology** that incorporates knowledge acquisition and answer generation. This feedback process aims to enhance the **factuality**, consistency, and entailment of generated answers, ultimately leading to more precise and accurate outputs and demonstrating superiority in **hallucination reduction**.\n\nWhile the user query covers a broad range of topics related to LLM reasoning and planning (including chain-of-thought, MCTS, grounding, etc.), this specific page focuses primarily on **hallucination reduction** and **factuality** through **self-reflection**.",
    "published_date": "2023-12-01T00:00:00.000Z",
    "score": null
  },
  "https://www.businesswire.com/news/home/20250715309806/en/Qdrant-Launches-Qdrant-Cloud-Inference-to-Unify-Embeddings-and-Vector-Search-Across-Multiple-Modalities": {
    "content_path": "../data/contents/e5434e1803ff6ea3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.948023",
    "title": "Qdrant Launches Qdrant Cloud Inference to Unify Embeddings and Vector Search Across Multiple Modalities"
  },
  "https://venturebeat.com/ai/how-test-time-scaling-unlocks-hidden-reasoning-abilities-in-small-language-models/": {
    "content_path": "../data/contents/3b95212648f32f9d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.971449",
    "title": "VentureBeat | Transformative tech coverage that matters"
  },
  "https://weaviate.io/blog/context-engineering": {
    "content_path": "../data/contents/e72521ad82ea8c30.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:27.996080",
    "title": "Context Engineering for AI Agents | Weaviate"
  },
  "https://docs.mistral.ai/getting-started/changelog": {
    "content_path": "../data/contents/e13eebd78d1d3ff9.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.020969",
    "title": "Changelog"
  },
  "https://aclanthology.org/2025.findings-emnlp.440/": {
    "content_path": "../data/contents/a01501afed37393d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.044920",
    "title": "LRPLAN: A Multi-Agent Collaboration of Large Language and Reasoning Models for Planning with Implicit & Explicit Constraints"
  },
  "https://www.llamaindex.ai/blog/pdf-parsing-llamaparse": {
    "content_path": "../data/contents/21765ad08f819049.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.070883",
    "title": "Parsing PDFs with LlamaParse: a how-to guide"
  },
  "https://github.com/EdinburghNLP/awesome-hallucination-detection": {
    "content_path": "../data/contents/e1c6235ee056e281.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.094568",
    "title": "GitHub - EdinburghNLP/awesome-hallucination-detection: List of papers on hallucination detection in LLMs."
  },
  "https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/": {
    "content_path": "../data/contents/c11eb7058f35f572.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.116671",
    "title": "The lethal trifecta for AI agents: private data, untrusted content, and external communication"
  },
  "https://github.com/awslabs/agent-squad": {
    "content_path": "../data/contents/ac9a86838323e249.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.144780",
    "title": "GitHub - awslabs/agent-squad: Flexible and powerful framework for managing multiple AI agents and handling complex conversations"
  },
  "https://github.blog/changelog/2025-12-18-github-copilot-now-supports-agent-skills": {
    "content_path": "../data/contents/296d0ff4fe931ec5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.169074",
    "title": "GitHub Copilot now supports Agent Skills"
  },
  "https://simonwillison.net/2024/Dec/19/gemini-thinking-mode/": {
    "content_path": "../data/contents/8317f7d272423294.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.190597",
    "title": "Gemini 2.0 Flash “Thinking mode”"
  },
  "https://aclanthology.org/2025.emnlp-industry.55/": {
    "content_path": "../data/contents/5657b23e33fcd07e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.212722",
    "title": "A Multi-Agent Framework for Quantitative Finance : An Application to Portfolio Management Analytics"
  },
  "https://www.linkedin.com/posts/tarikmoody_how-jp-morgan-built-an-ai-agent-for-investment-activity-7333955130725175298-nxMN": {
    "content_path": "../data/contents/dadfbe1dd30d9bad.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.234515",
    "title": "David Odomirok and Zheng Xue from JP Morgan Chase Private Bank reveal how they built \"Ask David\" - a sophisticated multi-agent AI system designed to automate investment research for thousands of financial products."
  },
  "https://news.mit.edu/2025/smarter-way-large-language-models-think-about-hard-problems-1204": {
    "content_path": "../data/contents/61af8e3d952b4c48.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.256790",
    "title": "A smarter way for large language models to think about hard problems"
  },
  "https://simonwillison.net/2024/Dec/11/gemini-2/": {
    "content_path": "../data/contents/485d75d37eb614ac.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.284147",
    "title": "Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode"
  },
  "https://aclanthology.org/2025.findings-naacl.12/": {
    "content_path": "../data/contents/98f35e33a849517f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.307722",
    "title": "HALLUCANA: Fixing LLM Hallucination with A Canary Lookahead"
  },
  "https://www.llamaindex.ai/blog/llamaindex-newsletter-2025-12-23": {
    "content_path": "../data/contents/0bd9e2e255931655.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.329259",
    "title": "LlamaIndex Newsletter 2025-12-23"
  },
  "https://www.nature.com/articles/s41586-025-09422-z": {
    "content_path": "../data/contents/b7585011e5b5019f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.351790",
    "title": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning"
  },
  "https://sebastianraschka.com/blog/2024/llm-research-papers-the-2024-list.html": {
    "content_path": "../data/contents/97e5e251df6646de.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.377170",
    "title": "LLM Research Papers: The 2024 List"
  },
  "https://aclanthology.org/2024.acl-long.506/": {
    "content_path": "../data/contents/ec854d0216220dc4.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.406867",
    "title": "InterrogateLLM: Zero-Resource Hallucination Detection in LLM-Generated Answers"
  },
  "https://modelcontextprotocol.io/docs/tools/inspector": {
    "content_path": "../data/contents/7fb233a1872a533a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.429612",
    "title": "MCP Inspector - Model Context Protocol"
  },
  "https://simonwillison.net/tags/ai-agents/": {
    "content_path": "../data/contents/7b86e61d04eb95bc.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.451351",
    "title": "Simon Willison on ai-agents"
  },
  "https://imdeepmind.com/docs/ml/mcp/langchain": {
    "content_path": "../data/contents/eb9b346a509eca3c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.478963",
    "title": "LangChain | imdeepmind"
  },
  "https://docs.swarms.world/en/latest/swarms/examples/agent_structured_outputs/": {
    "content_path": "../data/contents/cf4aff164c72d72b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.501483",
    "title": "Agent with Structured Outputs - Swarms"
  },
  "https://docs.mistral.ai/models/ocr-3-25-12": {
    "content_path": "../data/contents/8d32194546c2abd5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.523604",
    "title": "OCR 3 - Mistral AI"
  },
  "https://aclanthology.org/2025.coling-main.672/": {
    "content_path": "../data/contents/0294c460d287621f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.545951",
    "title": "Planning with Multi-Constraints via Collaborative Language Agents"
  },
  "https://www.businesswire.com/news/home/20251216371604/en/Hebbia-Empowers-Platform-Users-with-Preqin-Data-Transforming-Private-Markets-Workflows": {
    "content_path": "../data/contents/b775c19a8b7e5700.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.568135",
    "title": "Hebbia Empowers Platform Users with Preqin Data; Transforming Private Markets Workflows"
  },
  "https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk": {
    "content_path": "../data/contents/cf56e2618a633525.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.592292",
    "title": "Building agents with the Claude Agent SDK"
  },
  "https://blog.modelcontextprotocol.io/posts/2025-11-25-first-mcp-anniversary/": {
    "content_path": "../data/contents/47841e3b86684e64.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.623848",
    "title": "One Year of MCP: November 2025 Spec Release"
  },
  "https://huggingface.co/datasets/Salesforce/UniDoc-Bench": {
    "content_path": "../data/contents/88f5b16e4bf26e9a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.647298",
    "title": "Salesforce/UniDoc-Bench · Datasets at Hugging Face"
  },
  "https://openreview.net/forum?id=LYx4w3CAgy": {
    "content_path": "../data/contents/6d29b28de178f822.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.670244",
    "title": "LLM-Check: Investigating Detection of Hallucinations in Large..."
  },
  "https://aclanthology.org/2025.findings-emnlp.880/": {
    "content_path": "../data/contents/0971093f377db8c7.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.692656",
    "title": "Simple Factuality Probes Detect Hallucinations in Long-Form Natural Language Generation"
  },
  "https://github.com/simplescaling/s1": {
    "content_path": "../data/contents/6864978308395cc6.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.718955",
    "title": "GitHub - simplescaling/s1: s1: Simple test-time scaling"
  },
  "https://blog.google/products/gemini/gemini-3": {
    "content_path": "../data/contents/e193972d6870281a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.743690",
    "title": "A new era of intelligence with Gemini 3"
  },
  "https://developers.googleblog.com/en/gemini-embedding-available-gemini-api/": {
    "content_path": "../data/contents/0ee63f946998846f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.765926",
    "title": "Gemini Embedding now generally available in the Gemini API"
  },
  "https://platform.openai.com/docs/guides/agents-sdk": {
    "content_path": "../data/contents/5afbefadbecba1df.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.795052",
    "title": "Agents SDK | OpenAI API"
  },
  "https://diginomica.com/anthropic-openai-and-block-donate-ai-agent-projects-new-linux-foundation-body": {
    "content_path": "../data/contents/e2d2e60dde71567c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.817671",
    "title": "Anthropic, OpenAI, and Block donate AI agent projects to new Linux Foundation body"
  },
  "https://github.com/rednote-hilab/dots.ocr": {
    "content_path": "../data/contents/4cfc211903696669.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.841214",
    "title": "GitHub - rednote-hilab/dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model"
  },
  "https://simonwillison.net/2025/Dec/19/agent-skills/": {
    "content_path": "../data/contents/8ce2f1697c6e18cd.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.862842",
    "title": "Agent Skills"
  },
  "https://blog.google/technology/developers/build-with-gemini-3-flash/": {
    "content_path": "../data/contents/39b6abbd896c00a7.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.887287",
    "title": "Build with Gemini 3 Flash, frontier intelligence that scales with you"
  },
  "https://www.fairviewcapital.com/insights/agenticai": {
    "content_path": "../data/contents/e2f9582df999d91a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.919087",
    "title": "Preparing for the Agentic Era in Venture Capital"
  },
  "https://proceedings.neurips.cc/paper_files/paper/2024/hash/3c1e1fdf305195cd620c118aaa9717ad-Abstract-Conference.html": {
    "content_path": "../data/contents/86d9e8c90793e375.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.941421",
    "title": "LLM-Check: Investigating Detection of Hallucinations in Large Language Models"
  },
  "https://github.com/AI4Finance-Foundation/FinGPT": {
    "content_path": "../data/contents/c998e04bd4bf84f3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.963426",
    "title": "GitHub - AI4Finance-Foundation/FinGPT: FinGPT: Open-Source Financial Large Language Models!  Revolutionize 🔥    We release the trained model on HuggingFace."
  },
  "https://www.farsight-ai.com/news/farsight-integrates-with-pitchbook": {
    "content_path": "../data/contents/5ae876dbf910bb86.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:28.985790",
    "title": "Farsight | Farsight Integrates with PitchBook to Accelerate AI-Powered Financial Workflows"
  },
  "https://sebastianraschka.com/blog/2025/state-of-llm-reasoning-and-inference-scaling.html": {
    "content_path": "../data/contents/12b6f25d7e97fdbd.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.009211",
    "title": "Inference-Time Compute Scaling Methods to Improve Reasoning Models"
  },
  "https://www.sciencedirect.com/science/article/pii/S105752192500835X": {
    "content_path": "../data/contents/345962159ad4fdbb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.040474",
    "title": "Just a moment..."
  },
  "https://github.com/QwenLM/Qwen2.5-Omni": {
    "content_path": "../data/contents/46389008dd576250.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.063565",
    "title": "GitHub - QwenLM/Qwen2.5-Omni: Qwen2.5-Omni is an end-to-end multimodal model by Qwen team at Alibaba Cloud, capable of understanding text, audio, vision, video, and performing real-time speech generation."
  },
  "https://github.com/agentskills/agentskills": {
    "content_path": "../data/contents/48dbbdd8fd0d3029.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.085796",
    "title": "GitHub - agentskills/agentskills: Specification and documentation for Agent Skills"
  },
  "https://developers.llamaindex.ai/python/cloud/llamaparse/features/multimodal/": {
    "content_path": "../data/contents/02f5e8056d563371.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.110601",
    "title": "Multimodal Parsing"
  },
  "https://github.com/punkpeye/awesome-mcp-servers": {
    "content_path": "../data/contents/61a57c93de29a252.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.133789",
    "title": "GitHub - punkpeye/awesome-mcp-servers: A collection of MCP servers."
  },
  "https://www.businesswire.com/news/home/20251119343840/en/Qdrant-Introduces-Tiered-Multitenancy-to-Eliminate-Noisy-Neighbor-Problems-in-Vector-Search": {
    "content_path": "../data/contents/9dfbe9c5b522fc16.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.168149",
    "title": "Qdrant Introduces Tiered Multitenancy to Eliminate Noisy Neighbor Problems in Vector Search"
  },
  "https://lancedb.com/blog/announcing-lance-sdk/": {
    "content_path": "../data/contents/59e77b850ecf2756.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.190375",
    "title": "Announcing Lance SDK 1.0.0: What This Milestone Means for the Community"
  },
  "https://aclanthology.org/2025.findings-emnlp.972/": {
    "content_path": "../data/contents/c2e55641d878f5e3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.213037",
    "title": "Large Language Model Agents in Finance: A Survey Bridging Research, Practice, and Real-World Deployment"
  },
  "https://openai.github.io/openai-agents-js/": {
    "content_path": "../data/contents/a121198523233a40.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.235055",
    "title": "OpenAI Agents SDK TypeScript"
  },
  "https://developers.cloudflare.com/changelog/2025-03-17-new-workers-ai-models/": {
    "content_path": "../data/contents/907cd478d56abb0f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.259051",
    "title": "New models in Workers AI · Changelog"
  },
  "https://www.startuphub.ai/ai-news/artificial-intelligence/2025/contextforge-mcp-gateway-the-mcp-router-for-ai-agents/": {
    "content_path": "../data/contents/e6746d24a9c66a9d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.284304",
    "title": "ContextForge MCP Gateway: the MCP router for AI agents"
  },
  "https://github.com/deepseek-ai/DeepSeek-OCR": {
    "content_path": "../data/contents/2d89f19eb4a9858c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.309093",
    "title": "GitHub - deepseek-ai/DeepSeek-OCR: Contexts Optical Compression"
  },
  "https://docs.langchain.com/oss/python/langgraph/persistence": {
    "content_path": "../data/contents/99e58642fe77e6e1.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.331060",
    "title": "Persistence - Docs by LangChain"
  },
  "https://blog.google/technology/developers/gemini-3-pro-vision/": {
    "content_path": "../data/contents/ee822c33e91f3b25.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.358872",
    "title": "Gemini 3 Pro: the frontier of vision AI"
  },
  "https://github.com/richard-peng-xia/MMed-RAG": {
    "content_path": "../data/contents/22856035428ac2a2.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.381372",
    "title": "GitHub - richard-peng-xia/MMed-RAG: [ICLR'25] MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models"
  },
  "https://github.com/wang-research-lab/verification": {
    "content_path": "../data/contents/fd29e8c19a84d963.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.405822",
    "title": "GitHub - wang-research-lab/verification: Code repo for \"Budget-aware Test-time Scaling via Discriminative Verification\""
  },
  "https://www.firecrawl.dev/blog/best-vector-databases-2025": {
    "content_path": "data/contents/1f5aa3a047645d42.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.531685",
    "title": "Best Vector Databases in 2025: A Complete Comparison Guide",
    "summary": "The webpage provides a comprehensive comparison guide for vector databases, focusing on their evaluation, architecture, performance metrics (like recall, latency, and QPS), features (like hybrid search and filtering), and operational realities (cost and simplicity).\n\nIt details several major vector databases, including:\n*   **Pinecone:** Managed, easy to use, good for commercial products.\n*   **Milvus:** Open-source, best for billion-scale deployments if you have ops expertise.\n*   **Weaviate:** Specialist in native **hybrid search** (combining vector, keyword, and metadata filters).\n*   **Qdrant:** Budget-friendly with the best free tier, excellent for filtering at moderate scale.\n*   **pgvector + pgvectorscale:** Ideal for integrating vector search into existing **PostgreSQL** infrastructure.\n*   **Elasticsearch:** Good for teams already using it for full-text search and analytics.\n\nThe guide also covers specialized options like ChromaDB (prototyping), Redis (ultra-low latency), FAISS (custom integration), Marqo (multi-modal search), DataStax Astra DB (real-time updates), MongoDB Atlas (unified document/vector queries), and Zilliz Cloud (managed Milvus).\n\nCrucially, the page outlines a complete **RAG pipeline** workflow, starting with data collection using **Firecrawl** to get clean web content, followed by **chunking strategies**, generating **embeddings** (using OpenAI models as an example), and storing them in the chosen vector database. It shows how to swap vector stores (Pinecone, Qdrant, Weaviate, ChromaDB, pgvector) with minimal code changes in the LangChain framework.\n\nWhile the page discusses the components necessary for RAG, including **embeddings** and the role of vector databases in **RAG architectures**, it does not explicitly detail:\n*   **RAG alternatives** (other than implicitly suggesting hybrid search or traditional search).\n*   Specifics on **new efficient models** for embeddings (it mentions using OpenAI's `text-embedding-3-small` but doesn't compare new models).\n*   Detailed discussion on **rerankers**.\n*   Specific strategies for **context window management** (though chunking is covered).\n\n**Summary relevant to the query:** The page extensively covers **Vector databases**, **RAG architectures**, **hybrid search**, and **chunking strategies",
    "published_date": "2025-10-09T00:00:00.000Z",
    "score": null
  },
  "https://www.ft.com/content/045dac3f-eb78-469d-a3ef-3495aefa6e8f": {
    "content_path": "../data/contents/d2aa5effff07a28e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.580446",
    "title": "Meet your new investment banker: an AI chatbot"
  },
  "https://github.com/harrytea/Awesome-Document-Understanding": {
    "content_path": "../data/contents/7d586c55b53b32eb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.693861",
    "title": "GitHub - harrytea/Awesome-Document-Understanding: Document Artifical Intelligence"
  },
  "https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/": {
    "content_path": "../data/contents/5970b79b7c6d9ec3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.745153",
    "title": "Agent Development Kit: Making it easy to build multi-agent applications"
  },
  "https://learn.microsoft.com/en-us/agent-framework/overview/agent-framework-overview": {
    "content_path": "../data/contents/464a0ce4d22bf72f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.769656",
    "title": "Introduction to Microsoft Agent Framework"
  },
  "https://github.com/OpenGVLab/InternVL": {
    "content_path": "../data/contents/cf32e5a8040179eb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.800426",
    "title": "GitHub - OpenGVLab/InternVL: [CVPR 2024 Oral] InternVL Family: A Pioneering Open-Source Alternative to GPT-4o.  接近GPT-4o表现的开源多模态对话模型"
  },
  "https://blog.google/technology/developers/file-search-gemini-api/": {
    "content_path": "../data/contents/d2035106137a936f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.823510",
    "title": "Introducing the File Search Tool in Gemini API"
  },
  "https://platform.openai.com/docs/guides/pdf-files": {
    "content_path": "../data/contents/d82c9336b5e1da06.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.846722",
    "title": "File inputs | OpenAI API"
  },
  "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models": {
    "content_path": "../data/contents/982823bf41e99a00.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.869649",
    "title": "GitHub - BradyFU/Awesome-Multimodal-Large-Language-Models: :sparkles::sparkles:Latest Advances on Multimodal Large Language Models"
  },
  "https://www.infoq.com/news/2025/12/pinecone-drn-vector-workloads/": {
    "content_path": "../data/contents/9382b9a6cfafebae.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.901760",
    "title": "Pinecone Introduces Dedicated Read Nodes in Public Preview for Predictable Vector Workloads"
  },
  "https://towardsdatascience.com/graphrag-in-practice-how-to-build-cost-efficient-high-recall-retrieval-systems/": {
    "content_path": "../data/contents/1eef463873626140.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.925526",
    "title": "GraphRAG in Practice: How to Build Cost-Efficient, High-Recall Retrieval Systems"
  },
  "https://blog.langchain.com/top-5-langgraph-agents-in-production-2024/": {
    "content_path": "../data/contents/4d69226aeddb6fa4.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.948091",
    "title": "Top 5 LangGraph Agents in Production 2024"
  },
  "https://openai.github.io/openai-agents-python/": {
    "content_path": "data/contents/52c5efffa5b82018.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.768215",
    "title": "OpenAI Agents SDK",
    "summary": "The OpenAI Agents SDK enables the building of agentic AI applications using a lightweight package with few abstractions. Its core primitives include **Agents** (LLMs with instructions and tools), **Handoffs** (for delegating tasks between agents), **Guardrails** (for validating inputs and outputs), and **Sessions** (for automatic conversation history management).\n\nThe SDK features a built-in **agent loop**, is **Python-first** for orchestration, supports turning Python functions into **function tools** with automatic schema generation, and includes built-in **tracing** for visualization, debugging, and evaluation. It is presented as a production-ready upgrade to their previous experimentation tool, Swarm.\n\nWhile the query lists many related concepts (e.g., LangChain, LlamaIndex, context compression, agent orchestration, various SDKs, deep research, etc.), the provided text specifically details the features and primitives of the **OpenAI Agents SDK**.",
    "published_date": "2011-06-09T00:00:00.000Z",
    "score": null
  },
  "https://www.farsight-ai.com/news/farsight-integrates-s-p-capital-iq-pro-data": {
    "content_path": "../data/contents/2fadfc680d639331.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:29.991988",
    "title": "Farsight | Farsight Integrates with S&P Capital IQ Pro"
  },
  "https://github.com/opendatalab/MinerU": {
    "content_path": "../data/contents/a00764589e2321fb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.207760",
    "title": "GitHub - opendatalab/MinerU: Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows."
  },
  "https://github.com/qdrant/qdrant/releases/tag/v1.16.0": {
    "content_path": "../data/contents/ee9773b3551b40e8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.260250",
    "title": "Release v1.16.0 · qdrant/qdrant"
  },
  "https://www.anthropic.com/news/claude-sonnet-4-5": {
    "content_path": "../data/contents/18816edf464a073a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.283666",
    "title": "Introducing Claude Sonnet 4.5"
  },
  "https://github.com/anthropics/skills": {
    "content_path": "../data/contents/82a901da1b9bedec.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.306244",
    "title": "GitHub - anthropics/skills: Public repository for Agent Skills"
  },
  "https://github.com/RUCBM/TOPS": {
    "content_path": "../data/contents/564c5225efffb48e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.331324",
    "title": "GitHub - RUCBM/TOPS: [NeurIPS 2025] Official repository for the paper \"Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning\""
  },
  "https://blog.modelcontextprotocol.io/posts/2025-09-26-mcp-next-version-update/": {
    "content_path": "../data/contents/b2fd6a536b360473.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.355207",
    "title": "Update on the Next MCP Protocol Release"
  },
  "https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/": {
    "content_path": "../data/contents/17be1e5a80549742.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.378705",
    "title": "Gemini 2.0 is now available to everyone"
  },
  "https://github.com/OpenBMB/MiniCPM-V": {
    "content_path": "../data/contents/d2937af26044db99.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.401727",
    "title": "GitHub - OpenBMB/MiniCPM-V: MiniCPM-V 4.5: A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone"
  },
  "https://developers.googleblog.com/building-agents-with-the-adk-and-the-new-interactions-api/": {
    "content_path": "../data/contents/1c4a9a88c965d0bf.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.424817",
    "title": "Building agents with the ADK and the new Interactions API"
  },
  "https://blog.modelcontextprotocol.io/posts/2025-12-09-mcp-joins-agentic-ai-foundation/": {
    "content_path": "../data/contents/cc519667518076ac.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.447047",
    "title": "MCP joins the Agentic AI Foundation"
  },
  "https://lilianweng.github.io/posts/2023-06-23-agent/": {
    "content_path": "../data/contents/19472db6d6f483d5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.473511",
    "title": "LLM Powered Autonomous Agents"
  },
  "https://github.com/db-Lee/Multi-RM": {
    "content_path": "../data/contents/8d6b27489989722a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.498798",
    "title": "GitHub - db-Lee/Multi-RM"
  },
  "https://aws.amazon.com/blogs/aws/amazon-s3-vectors-now-generally-available-with-increased-scale-and-performance/": {
    "content_path": "../data/contents/8913ac2b86cefead.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.521983",
    "title": "Amazon S3 Vectors now generally available with increased scale and performance | Amazon Web Services"
  },
  "https://aclanthology.org/2025.mrl-main.36/": {
    "content_path": "../data/contents/e27102f38272d9d7.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.545002",
    "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval"
  },
  "https://t2ragbench.demo.hcds.uni-hamburg.de/": {
    "content_path": "../data/contents/c76373cfe4d9d8bf.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.569219",
    "title": "T²-RAGBench"
  },
  "https://openai.com/index/learning-to-reason-with-llms/": {
    "content_path": "../data/contents/9edf2bd5938d8386.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.594146",
    "title": "Learning to reason with LLMs"
  },
  "https://fireworks.ai/blog/embeddings-and-reranking-announcement": {
    "content_path": "../data/contents/56debe621ad1c94c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.618880",
    "title": "Fireworks AI"
  },
  "https://www.businesswire.com/news/home/20250827824893/en/Hebbia-Integrates-with-Microsoft-Azure-AI-Foundry-to-Elevate-Financial-Analysis": {
    "content_path": "../data/contents/03746185f3242c40.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.642428",
    "title": "Hebbia Integrates with Microsoft Azure AI Foundry to Elevate Financial Analysis"
  },
  "https://blog.voyageai.com/2025/05/20/voyage-3-5/": {
    "content_path": "../data/contents/ea084d112a133578.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.665503",
    "title": "voyage-3.5 and voyage-3.5-lite: improved quality for a new retrieval frontier"
  },
  "https://blog.voyageai.com/2025/08/11/rerank-2-5/": {
    "content_path": "../data/contents/e2f0c8f7e86f4319.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.689924",
    "title": "rerank-2.5 and rerank-2.5-lite: instruction-following rerankers"
  },
  "https://block.xyz/inside/block-open-source-introduces-codename-goose": {
    "content_path": "../data/contents/30b63022f6ed2cd9.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.714348",
    "title": "Block Open Source Introduces “codename goose” — an Open Framework for AI Agents"
  },
  "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00660/120911/Automatically-Correcting-Large-Language-Models": {
    "content_path": "../data/contents/46bf8ae55bd1c595.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.751381",
    "title": "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies"
  },
  "https://www.junipersquare.com/news/juniper-square-adds-preqin-data-to-its-services-to-help-gps-accelerate-capital-raising": {
    "content_path": "../data/contents/6801a59004cffb93.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.779007",
    "title": "Juniper Square Adds Preqin Data to its Services to Help GPs Accelerate Capital Raising"
  },
  "https://huggingface.co/blog/nvidia/llama-embed-nemotron-8b": {
    "content_path": "../data/contents/1280110ec928bb31.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.802669",
    "title": "Llama‑Embed‑Nemotron‑8B Text Embedding Model Ranks First on Multilingual MTEB Leaderboard"
  },
  "https://blog.langchain.com/planning-agents/": {
    "content_path": "../data/contents/a90466537a8d8bec.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.826474",
    "title": "Plan-and-Execute Agents"
  },
  "https://ui.adsabs.harvard.edu/abs/arXiv:2501.14304": {
    "content_path": "../data/contents/6405fb403439adea.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.848853",
    "title": "MASTER: A Multi-Agent System with LLM Specialized MCTS"
  },
  "https://github.com/stefanoamorelli/sec-edgar-mcp": {
    "content_path": "../data/contents/1791ec5157aee040.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.871133",
    "title": "GitHub - stefanoamorelli/sec-edgar-mcp: A SEC EDGAR MCP (Model Context Protocol) Server"
  },
  "https://modelcontextprotocol.io/development/roadmap": {
    "content_path": "../data/contents/f1a3aa3683d683df.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.894021",
    "title": "Roadmap - Model Context Protocol"
  },
  "https://github.com/PaddlePaddle/PaddleOCR": {
    "content_path": "../data/contents/137ebc899af74dce.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.916775",
    "title": "GitHub - PaddlePaddle/PaddleOCR: Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages."
  },
  "https://blog.google/products/search/google-ai-mode-update-gemini-3-flash": {
    "content_path": "../data/contents/b107cfae4cf8be50.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.940322",
    "title": "Gemini 3 Flash is rolling out globally in Google Search"
  },
  "https://openai.com/index/new-chatgpt-images-is-here/": {
    "content_path": "../data/contents/e3939714c5d8d2b6.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:30.962718",
    "title": "The new ChatGPT Images is here"
  },
  "https://www.bdo.com/insights/industries/private-equity/ai-use-case-portfolio-for-private-equity": {
    "content_path": "../data/contents/35b5e47beda2c9cf.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.475120",
    "title": "AI Use Case Portfolio for Private Equity",
    "summary": "The webpage discusses the use of Artificial Intelligence (AI) across the private equity (PE) investment lifecycle, including deal sourcing, due diligence, portfolio monitoring, and exit strategy.\n\nRegarding your specific query about **multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg**:\n\n*   **Deal Sourcing & Investment Opportunity Analysis:** AI can help expedite deal sourcing by analyzing large volumes of data from potential targets to identify \"hidden gems.\"\n*   **Due Diligence Automation:** AI-enabled due diligence helps analyze lengthy documents and financial information to extract key insights and metrics (like revenue growth, EBITDA margins, operational red flags) in minutes.\n*   **Portfolio Monitoring:** AI can conduct real-time portfolio monitoring, identify performance red flags, and suggest recommendations.\n*   **Financial Report Generation/Data Analysis:** AI can help standardize reporting formats from portfolio companies (portcos), allowing fund managers to analyze data and generate insights more efficiently. Generative AI can create centralized dashboards.\n*   **Agents:** While the text frequently mentions \"AI\" and its capabilities, it does not explicitly use the term \"multi-agent systems\" or detail specific \"agents\" for these tasks, though the described functionalities align with what agents might perform.\n*   **Integrations (PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg):** The text mentions **PitchBook** in the context of reporting on dealmaking momentum, but it does not detail specific integrations of AI tools with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg** for the described use cases.\n\n**Summary:** The page confirms that AI is used extensively in PE for deal sourcing, due diligence automation, investment analysis, portfolio monitoring, and streamlining financial reporting. However, it does not explicitly confirm the use of **multi-agent systems** or detail integrations with **AlphaSense, Preqin, CapIQ, or Bloomberg**.",
    "published_date": "2025-09-12T00:00:00.000Z",
    "score": null
  },
  "https://github.blog/open-source/maintainers/mcp-joins-the-linux-foundation-what-this-means-for-developers-building-the-next-era-of-ai-tools-and-agents/": {
    "content_path": "../data/contents/4a2e0714073b4492.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.009163",
    "title": "MCP joins the Linux Foundation: What this means for developers building the next era of AI tools and agents"
  },
  "https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-opensearch-service-gpu-accelerated-auto-optimized-vector-indexes": {
    "content_path": "../data/contents/20cf820fa4530a10.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.032169",
    "title": "Amazon OpenSearch Service adds GPU-accelerated and auto-optimized vector indexes"
  },
  "https://blog.langchain.com/standard-message-content/": {
    "content_path": "../data/contents/a7ad32c405f6e3a9.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.054870",
    "title": "Standard message content"
  },
  "https://www.thirdbridge.com/en-us/about-us/media/news/third-bridge-integrates-with-claude-for-financial-services": {
    "content_path": "../data/contents/effae88c95c2ab9c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.078739",
    "title": "Third Bridge integrates expert insights with Anthropic’s Claude for Financial Services"
  },
  "https://github.com/deepseek-ai/DeepSeek-VL2": {
    "content_path": "../data/contents/16d591dac3c4a9ad.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.136349",
    "title": "GitHub - deepseek-ai/DeepSeek-VL2: DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding"
  },
  "https://github.com/THUDM/TreeRL": {
    "content_path": "../data/contents/1bcd0c231b097855.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.163428",
    "title": "GitHub - THUDM/TreeRL: TreeRL: LLM Reinforcement Learning with On-Policy Tree Search in ACL'25"
  },
  "https://alignment.anthropic.com/2025/inverse-scaling/": {
    "content_path": "../data/contents/890be24529425270.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.186821",
    "title": "Inverse Scaling in Test-Time Compute"
  },
  "https://nanonets.com/blog/table-extraction-using-llms-unlocking-structured-data-from-documents/": {
    "content_path": "../data/contents/eee4f176426e264f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.210436",
    "title": "Table Extraction using LLMs: Unlocking Structured Data from Documents"
  },
  "https://github.com/illuin-tech/colpali": {
    "content_path": "../data/contents/926b05f728b31e7c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.235246",
    "title": "GitHub - illuin-tech/colpali: The code used to train and run inference with the ColVision models, e.g. ColPali, ColQwen2, and ColSmol."
  },
  "https://www.gurustartups.com/reports/ai-powered-deal-sourcing-agents-for-private-equity": {
    "content_path": "../data/contents/28e3edcbbd6a9ea2.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.259118",
    "title": "AI-Powered Deal Sourcing Agents for Private Equity"
  },
  "https://lilianweng.github.io/posts/2025-05-01-thinking/": {
    "content_path": "../data/contents/ebb155c1ad2cd8bb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.281577",
    "title": "Why We Think"
  },
  "https://openai.com/index/introducing-agentkit/": {
    "content_path": "../data/contents/4bc53a82ae2ed156.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.303616",
    "title": "Introducing AgentKit"
  },
  "https://aclanthology.org/2025.findings-acl.1266/": {
    "content_path": "../data/contents/fb046790665277d0.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.330464",
    "title": "Tool learning via Inference-time Scaling and Cycle Verifier"
  },
  "https://americas.kioxia.com/en-us/business/news/2025/ssd-20251216-1.html": {
    "content_path": "../data/contents/fee7bca22e7b00c1.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.354142",
    "title": "KIOXIA AiSAQ Technology Integrated into Milvus Vector Database | KIOXIA - United States (English)"
  },
  "https://sacra.com/c/langchain/": {
    "content_path": "../data/contents/369e2f87af71fdbd.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.376996",
    "title": "LangChain valuation, funding & news"
  },
  "https://github.com/advisories/GHSA-wwqv-p2pp-99h5": {
    "content_path": "../data/contents/72fba11e00dc50fd.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.399413",
    "title": "CVE-2025-64439 - GitHub Advisory Database"
  },
  "https://www.infoq.com/news/2025/10/ai-agent-orchestration/": {
    "content_path": "../data/contents/fb000b11abd62733.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.422165",
    "title": "The Architectural Shift: AI Agents Become Execution Engines While Backends Retreat to Governance"
  },
  "https://openai.com/index/introducing-apps-in-chatgpt/": {
    "content_path": "../data/contents/c4e813cf9b62f8ae.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.445888",
    "title": "Introducing apps in ChatGPT and the new Apps SDK"
  },
  "https://docs.nvidia.com/aiqtoolkit/1.1.0/index.html": {
    "content_path": "../data/contents/8a730f63d6d426f3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.468610",
    "title": "NVIDIA Agent Intelligence Toolkit Overview — NVIDIA Agent Intelligence Toolkit (1.1.0)"
  },
  "https://langcopilot.com/posts/2025-10-11-document-chunking-for-rag-practical-guide": {
    "content_path": "../data/contents/0e8f2ecc200f30d8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.491611",
    "title": "Document Chunking for RAG: 9 Strategies Tested (70% Accuracy Boost 2025)"
  },
  "https://www.hebbia.com/blog/hebbia-integrates-with-microsoft-azure-ai-foundry-to-elevate-financial": {
    "content_path": "../data/contents/ea8d12bf73443622.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.515048",
    "title": "Hebbia Integrates with Microsoft Azure AI Foundry to Elevate Financial Analysis"
  },
  "https://www.llamaindex.ai/blog/announcing-workflows-1-0-a-lightweight-framework-for-agentic-systems": {
    "content_path": "../data/contents/3cc85b20edf8ce1c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.539875",
    "title": "Announcing Workflows 1.0: A Lightweight Framework for Agentic systems"
  },
  "https://karpathy.bearblog.dev/year-in-review-2025/": {
    "content_path": "../data/contents/ca5be40ee3c10c03.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.565519",
    "title": "2025 LLM Year in Review"
  },
  "https://ai.pydantic.dev/": {
    "content_path": "../data/contents/473e7956a86382e6.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.588777",
    "title": "Pydantic AI"
  },
  "https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/": {
    "content_path": "../data/contents/cba3f1a5a5c29c20.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.612158",
    "title": "LangChain & LangGraph 1.0 alpha releases"
  },
  "https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/ai-agent-design-patterns": {
    "content_path": "../data/contents/b6018c1ff9d0f480.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.634780",
    "title": "AI Agent Orchestration Patterns - Azure Architecture Center"
  },
  "https://www.promptlayer.com/research-papers/ai-tackles-multimodal-multi-document-qa": {
    "content_path": "../data/contents/6ff938ca4049a186.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.657397",
    "title": "VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation"
  },
  "https://supabase.com/docs/guides/ai/hybrid-search": {
    "content_path": "../data/contents/ae1458ac1d874dc5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.679898",
    "title": "Hybrid search | Supabase Docs"
  },
  "https://www.alpha-sense.com/press/alphasense-launches-financial-data/": {
    "content_path": "../data/contents/0c59828095d8c432.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.705324",
    "title": "AlphaSense Launches Financial Data to Offer Powerful Integrated View of Quantitative and Qualitative Market Intelligence"
  },
  "https://temporal.io/blog/announcing-openai-agents-sdk-integration": {
    "content_path": "../data/contents/b6b3a0eb99bfd8c8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.728370",
    "title": "Production-ready agents with the OpenAI Agents SDK + Temporal"
  },
  "https://blog.langchain.com/langgraph-platform-ga/": {
    "content_path": "../data/contents/19ab09b3083e0e85.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.751975",
    "title": "LangGraph Platform is now Generally Available: Deploy & manage long-running, stateful Agents"
  },
  "https://github.com/modelcontextprotocol/go-sdk": {
    "content_path": "../data/contents/b5b98b08f9034021.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.775604",
    "title": "GitHub - modelcontextprotocol/go-sdk: The official Go SDK for Model Context Protocol servers and clients. Maintained in collaboration with Google."
  },
  "https://openai.com/index/openai-o3-mini/": {
    "content_path": "../data/contents/650b567698aa0aa6.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.798206",
    "title": "OpenAI o3-mini"
  },
  "https://finster.ai/resources/news/finster-ai-partners-with-preqin-to-expand-alternative-assets-coverage": {
    "content_path": "../data/contents/a38ff6186a73226f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.841088",
    "title": "Finster AI partners with Preqin to expand alternative assets coverage"
  },
  "https://github.com/modelcontextprotocol/servers": {
    "content_path": "../data/contents/cb8f252ccaaf10b0.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.868621",
    "title": "GitHub - modelcontextprotocol/servers: Model Context Protocol Servers"
  },
  "https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents": {
    "content_path": "../data/contents/7f7a70a676497580.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.894319",
    "title": "Effective harnesses for long-running agents"
  },
  "https://openai.com/index/introducing-o3-and-o4-mini/": {
    "content_path": "../data/contents/bf92f3d905c3de0d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.917124",
    "title": "Introducing OpenAI o3 and o4-mini"
  },
  "https://www.anthropic.com/news/visible-extended-thinking": {
    "content_path": "../data/contents/8243dc3d3ead4c1c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.941696",
    "title": "Claude's extended thinking"
  },
  "https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation": {
    "content_path": "../data/contents/f8e84c3236afe2df.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.964504",
    "title": "Donating the Model Context Protocol and establishing the Agentic AI Foundation"
  },
  "https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B": {
    "content_path": "../data/contents/8017bf752ae83585.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:31.987507",
    "title": "opendatalab/MinerU2.5-2509-1.2B · Hugging Face"
  },
  "https://www.microsoft.com/en-us/dynamics-365/blog/it-professional/2025/11/11/dynamics-365-erp-model-context-protocol/": {
    "content_path": "../data/contents/6a44f2160d691e69.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.010332",
    "title": "Evolving the Dynamics 365 ERP Model Context Protocol (MCP) Server"
  },
  "https://aiera.com/newsroom/aiera-and-anthropic-integration/": {
    "content_path": "../data/contents/77e2312b9fa8e115.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.032462",
    "title": "Aiera and Anthropic Announce Integration of Aiera’s Trusted Market Intelligence into Claude for Financial Services"
  },
  "https://openai.com/index/new-tools-and-features-in-the-responses-api/": {
    "content_path": "../data/contents/9257c39d4172abdb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.054714",
    "title": "New tools and features in the Responses API"
  },
  "https://huggingface.co/blog/manu/colpali": {
    "content_path": "../data/contents/47b64b9e709c20fa.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.078347",
    "title": "ColPali: Efficient Document Retrieval with Vision Language Models 👀"
  },
  "https://modelcontextprotocol.io/specification/2025-11-25/changelog": {
    "content_path": "../data/contents/22e7d71eefcd7859.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.103763",
    "title": "Key Changes - Model Context Protocol"
  },
  "https://ir.elastic.co/news/news-details/2025/Elasticsearch-Open-Inference-API-now-Supports-Jina-AI-Embeddings-and-Rerank-Model/default.aspx": {
    "content_path": "../data/contents/27fa48fbb57f60ac.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.127059",
    "title": "Elasticsearch Open Inference API now Supports Jina AI Embeddings and Rerank Model"
  },
  "https://huggingface.co/blog/smolagents": {
    "content_path": "../data/contents/c1c6e8d4b5f3d596.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.149623",
    "title": "Introducing smolagents: simple agents that write actions in code."
  },
  "https://www.modelml.com/articles/model-ml-and-pitchbook": {
    "content_path": "../data/contents/dba5dcd29fca1a59.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.178119",
    "title": "Turning Private Market Data into Action: Model ML Announces Partnership with Pitchbook - ModelML"
  },
  "https://finster.ai/news/finster-partners-with-pitchbook-to-enhance-private-market-data": {
    "content_path": "../data/contents/2d83cd4d4a73333b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.201158",
    "title": "Finster partners with PitchBook to enhance private market data"
  },
  "https://docs.cohere.com/release-notes/": {
    "content_path": "../data/contents/e5530065dfef92c3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.224391",
    "title": "Release Notes | Cohere"
  },
  "https://cacm.acm.org/news/self-correction-in-large-language-models/": {
    "content_path": "../data/contents/967d4b2c5c7ef65c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.252692",
    "title": "Self-Correction in Large Language Models"
  },
  "https://devblogs.microsoft.com/semantic-kernel/semantic-kernel-adds-model-context-protocol-mcp-support-for-python/": {
    "content_path": "../data/contents/1bd8f749a672183a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.276260",
    "title": "Semantic Kernel adds Model Context Protocol (MCP) support for Python | Semantic Kernel"
  },
  "https://research.ibm.com/blog/docling-generative-AI": {
    "content_path": "../data/contents/24490e3d06213611.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.299143",
    "title": "A new tool to unlock data from enterprise documents for generative AI"
  },
  "https://github.com/pipiku915/FinMem-LLM-StockTrading": {
    "content_path": "../data/contents/585f9deb9362e455.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.323176",
    "title": "GitHub - pipiku915/FinMem-LLM-StockTrading: FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design"
  },
  "https://help.alpha-sense.com/hc/en-us/articles/46995476466323-AlphaSense-Product-Updates-November-2025": {
    "content_path": "../data/contents/96c7cf59506bf177.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.376551",
    "title": "AlphaSense Product Updates — November 2025"
  },
  "https://github.com/weaviate/weaviate/releases/tag/v1.35.0": {
    "content_path": "../data/contents/be6e8d5df809f202.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.407238",
    "title": "Release v1.35.0 - Operational Modes, Object TTL · weaviate/weaviate"
  },
  "https://www.microsoft.com/en-us/research/publication/towards-thinking-optimal-scaling-of-test-time-compute-for-llm-reasoning/": {
    "content_path": "../data/contents/1c36066b36146616.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.430851",
    "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning - Microsoft Research"
  },
  "https://blog.langchain.com/series-b/": {
    "content_path": "../data/contents/99a356595d547020.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.454509",
    "title": "LangChain raises $125M to build the platform for agent engineering"
  },
  "https://github.com/FlagOpen/FlagEmbedding": {
    "content_path": "../data/contents/d1fb7b5024710b09.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.477822",
    "title": "GitHub - FlagOpen/FlagEmbedding: Retrieval and Retrieval-augmented LLMs"
  },
  "https://rogo.ai/news/announcing-rogo-s-new-partnership-with-pitchbook": {
    "content_path": "../data/contents/61d80537681a8ff5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.500249",
    "title": "Announcing Rogo’s New Partnership with PitchBook | Rogo"
  },
  "https://ragflow.io/blog/rag-at-the-crossroads-mid-2025-reflections-on-ai-evolution": {
    "content_path": "../data/contents/4dee49aa7c9d4d41.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.523048",
    "title": "RAG at the Crossroads - Mid-2025 Reflections on AI’s Incremental Evolution"
  },
  "https://www.prnewswire.com/news-releases/alphasense-launches-deep-research-automating-in-depth-analysis-with-agentic-ai-on-high-value-content-302476710.html": {
    "content_path": "../data/contents/7edc53adab40968d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.545956",
    "title": "AlphaSense Launches Deep Research, Automating In-Depth Analysis with Agentic AI on High Value Content"
  },
  "https://github.com/modelcontextprotocol/typescript-sdk": {
    "content_path": "../data/contents/f7fb7d70e0ff6a4e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.568665",
    "title": "GitHub - modelcontextprotocol/typescript-sdk: The official TypeScript SDK for Model Context Protocol servers and clients"
  },
  "https://huggingface.co/ibm-granite/granite-docling-258M": {
    "content_path": "../data/contents/61948302a79f1961.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.593277",
    "title": "ibm-granite/granite-docling-258M · Hugging Face"
  },
  "https://aclanthology.org/2025.findings-naacl.113.pdf": {
    "content_path": "../data/contents/73c77c1303d70f78.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.616441",
    "title": "2025.findings-naacl.113.pdf"
  },
  "https://cookbook.openai.com/examples/structured_outputs_multi_agent": {
    "content_path": "../data/contents/4f0b332d93bb2892.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.643550",
    "title": "Structured Outputs for Multi-Agent Systems | OpenAI Cookbook"
  },
  "https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/": {
    "content_path": "../data/contents/b02ba91373fb6170.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.666172",
    "title": "Agents"
  },
  "https://github.com/huggingface/smolagents": {
    "content_path": "../data/contents/cce197194374b868.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.688993",
    "title": "GitHub - huggingface/smolagents: 🤗 smolagents: a barebones library for agents that think in code."
  },
  "https://github.com/google/adk-python": {
    "content_path": "../data/contents/b6c275efa5018657.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.712606",
    "title": "GitHub - google/adk-python: An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control."
  },
  "https://investor.spglobal.com/news-releases/news-details/2025/SP-Global-Agrees-to-Acquire-With-Intelligence-from-Motive-Partners-for-1-8-Billion-Establishing-Its-Leadership-in-Private-Markets-Intelligence/default.aspx": {
    "content_path": "../data/contents/7e14dff8ce94abfd.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.735145",
    "title": "S&P Global Agrees to Acquire With Intelligence from Motive Partners for $1.8 Billion, Establishing Its Leadership in Private Markets Intelligence"
  },
  "https://www.anthropic.com/news/claude-opus-4-5": {
    "content_path": "../data/contents/57f01cae307e1cb1.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.757907",
    "title": "Introducing Claude Opus 4.5"
  },
  "https://www.datacamp.com/blog/gemini-2-0-flash-experimental": {
    "content_path": "../data/contents/fbaa422b2f95b19d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.780655",
    "title": "Gemini 2.0 Flash Thinking Experimental: A Guide With Examples"
  },
  "https://www.microsoft.com/en-us/research/publication/mmed-rag-versatile-multimodal-rag-system-for-medical-vision-language-models/?lang=fr-ca%2Fbibtex%2F": {
    "content_path": "../data/contents/63e5352bb67f7bcc.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.803224",
    "title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models - Microsoft Research"
  },
  "https://simonw.substack.com/p/claude-skills-are-awesome-maybe-a": {
    "content_path": "../data/contents/b2b0d29ec94a3e25.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.826717",
    "title": "Claude Skills are awesome, maybe a bigger deal than MCP"
  },
  "https://www.alpha-sense.com/press/alphasense-supercharges-its-generative-ai-suite-with-groundbreaking-new-features/": {
    "content_path": "../data/contents/55ce953770f7110e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.849405",
    "title": "AlphaSense Supercharges its Generative AI Suite with Groundbreaking New Features | AlphaSense"
  },
  "https://openai.com/index/agentic-ai-foundation": {
    "content_path": "../data/contents/fa276c020c3018fb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.873548",
    "title": "OpenAI co-founds the Agentic AI Foundation under the Linux Foundation"
  },
  "https://github.com/DS4SD/docling": {
    "content_path": "../data/contents/3762bdd34293dd3a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.898046",
    "title": "GitHub - docling-project/docling: Get your documents ready for gen AI"
  },
  "https://github.com/pydantic/pydantic-ai": {
    "content_path": "../data/contents/e916d975160d16b9.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.921948",
    "title": "GitHub - pydantic/pydantic-ai: GenAI Agent Framework, the Pydantic way"
  },
  "https://github.com/openai/openai-agents-js": {
    "content_path": "../data/contents/66f58f93b5332a65.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.948611",
    "title": "GitHub - openai/openai-agents-js: A lightweight, powerful framework for multi-agent workflows and voice agents"
  },
  "https://huggingface.co/blog/train-reranker": {
    "content_path": "../data/contents/9a5d73af0fe988dc.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.973422",
    "title": "Training and Finetuning Reranker Models with Sentence Transformers v4"
  },
  "https://github.com/crewAIInc/crewAI": {
    "content_path": "../data/contents/715b6bc81ab4b4ff.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:32.996141",
    "title": "GitHub - crewAIInc/crewAI: Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks."
  },
  "https://openai.com/solutions/blueprints/mcpkit/": {
    "content_path": "../data/contents/5bafbc1ece7da668.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.019071",
    "title": "MCPKit: Secure, reliable data connectors for ChatGPT"
  },
  "https://www.globenewswire.com/news-release/2025/12/16/3206057/0/en/FactSet-Meets-Demand-for-AI-Ready-Data-First-to-Announce-MCP-Sans-Intermediary.html": {
    "content_path": "../data/contents/1c64bece2d36ede4.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.041911",
    "title": "FactSet Meets Demand for AI-Ready Data, First to Announce MCP Sans Intermediary"
  },
  "https://www.lseg.com/en/media-centre/press-releases/2025/lseg-expands-partnership-with-blackrock": {
    "content_path": "../data/contents/f75a3392ab148575.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.064863",
    "title": "LSEG expands partnership with BlackRock, strengthening private markets data offering with new Preqin® data feeds"
  },
  "https://www.promptingguide.ai/techniques/reflexion": {
    "content_path": "../data/contents/26fa5af0eee49a57.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.088152",
    "title": "Reflexion | Prompt Engineering Guide"
  },
  "https://docs.mistral.ai/capabilities/document_ai/basic_ocr/": {
    "content_path": "../data/contents/878c0ad26de5bbaa.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.111786",
    "title": "OCR Processor | Mistral Docs"
  },
  "https://docs.cohere.com/changelog/rerank-v4.0": {
    "content_path": "../data/contents/f85821654bd0494e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.139185",
    "title": "Cohere's Rerank v4.0 Model is Here! | Cohere"
  },
  "https://github.com/NovaSky-AI/SkyThought": {
    "content_path": "../data/contents/9c4a9ab9c5332057.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.162140",
    "title": "GitHub - NovaSky-AI/SkyThought: Sky-T1: Train your own O1 preview model within $450"
  },
  "https://platform.openai.com/docs/guides/function-calling": {
    "content_path": "../data/contents/ec4f8c98c7439855.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.185156",
    "title": "Function calling | OpenAI API"
  },
  "https://www.contextual.ai/blog/rerank-v2/": {
    "content_path": "../data/contents/c738390a97dfc30b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.240665",
    "title": "Open-Sourcing Reranker v2"
  },
  "https://milvus.io/docs/zh-hant/release_notes.md": {
    "content_path": "../data/contents/c1053977d8ff2822.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.263571",
    "title": "Release Notes | Milvus Documentation"
  },
  "https://blog.langchain.com/langsmith-agent-builder-now-in-public-beta/": {
    "content_path": "../data/contents/2a34984353a9e5ac.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.287248",
    "title": "LangSmith Agent Builder now in Public Beta"
  },
  "https://deepmind.google/models/gemini-image/pro/": {
    "content_path": "../data/contents/d1f8145d064c700f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.311000",
    "title": "Gemini 3 Pro Image (Nano Banana Pro)"
  },
  "https://jina.ai/news/jina-reranker-v2-for-agentic-rag-ultra-fast-multilingual-function-calling-and-code-search/": {
    "content_path": "../data/contents/4c5c0890b4587e99.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.333927",
    "title": "Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling & Code Search"
  },
  "https://aaif.io/news/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation-aaif-anchored-by-new-project-contributions-including-model-context-protocol-mcp-goose-and-agents-md/": {
    "content_path": "../data/contents/b6e218a60cbe1298.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.357133",
    "title": "Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF), Anchored by New Project Contributions Including Model Context Protocol (MCP), goose and AGENTS.md"
  },
  "https://github.com/RyanLiu112/GenPRM": {
    "content_path": "../data/contents/4e4cc753cdf6f276.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.380363",
    "title": "GitHub - RyanLiu112/GenPRM: [AAAI 2026] Official codebase for \"GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning\"."
  },
  "https://github.com/mukhal/thinkprm": {
    "content_path": "../data/contents/290c6b13cb198478.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.403492",
    "title": "GitHub - mukhal/ThinkPRM: Process Reward Models That Think"
  },
  "https://nature.com/articles/s44387-025-00045-3": {
    "content_path": "../data/contents/152e841d2d3d3d64.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.427696",
    "title": "Self-reflection enhances large language models towards substantial academic response"
  },
  "https://github.com/milad1378yz/MASPRM": {
    "content_path": "../data/contents/8cf908bac177573b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.450508",
    "title": "GitHub - milad1378yz/MASPRM"
  },
  "https://www.ravenpack.com/blog/ravenpack-and-preqin-bring-private-markets-intelligence-to-ai-workflows-on-bigdatacom": {
    "content_path": "../data/contents/0ae8387ba430500c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.474008",
    "title": "RavenPack and Preqin bring private markets intelligence to AI workflows on Bigdata.com"
  },
  "https://discuss.elastic.co/t/dec-18th-2025-en-using-es-ql-with-dense-vector-fields/384024": {
    "content_path": "../data/contents/bb4a7d9ec20eebd5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.619552",
    "title": "Dec 18th, 2025: [EN] Using ES|QL with dense_vector fields - Elastic Community and Ecosystem / Advent Calendar - Discuss the Elastic Stack"
  },
  "https://github.com/gta0804/MASS": {
    "content_path": "../data/contents/ae4c46dd0ac4cb80.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.832033",
    "title": "GitHub - gta0804/MASS: Official implementation of MASS: Multi-Agent Simulation Scaling for Portfolio Construction"
  },
  "https://www.llamaindex.ai/industry/finance": {
    "content_path": "../data/contents/e390e9e195d4c9fd.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.864234",
    "title": "Finance"
  },
  "https://github.com/testtimescaling/testtimescaling.github.io": {
    "content_path": "../data/contents/2939e434428f1fd1.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.892852",
    "title": "GitHub - testtimescaling/testtimescaling.github.io: \"what, how, where, and how well? a survey on test-time scaling in large language models\" repository"
  },
  "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-11-19": {
    "content_path": "../data/contents/b20f010e82ba1251.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.915828",
    "title": "LlamaIndex Newsletter 2024-11-19"
  },
  "https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning": {
    "content_path": "../data/contents/f9a78b9581c05ae7.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.942229",
    "title": "GitHub - SalesforceAIResearch/socratic-self-refine-reasoning"
  },
  "https://www.prnewswire.com/news-releases/sp-global-transforms-sp-capital-iq-pro-experience-with-the-launch-of-new-generative-ai-powered-capabilities-302300387.html": {
    "content_path": "../data/contents/4c54197eac9af0ec.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:33.976660",
    "title": "S&P Global Transforms S&P Capital IQ Pro Experience with the Launch of New Generative AI-Powered Capabilities"
  },
  "https://help.openai.com/en/articles/11909943-gpt-5-in-chatgpt": {
    "content_path": "../data/contents/7b896c8ea4475bff.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.012487",
    "title": "GPT-5.2 in ChatGPT | OpenAI Help Center"
  },
  "https://github.com/x-lance/neusym-rag": {
    "content_path": "../data/contents/d6d7ed8c91211307.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.036383",
    "title": "GitHub - OpenDFM/NeuSym-RAG: [ACL 2025] NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering"
  },
  "https://changelog.langchain.com/": {
    "content_path": "../data/contents/7ed681c3449c37e5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.063743",
    "title": "LangChain - Changelog"
  },
  "https://www.alpha-sense.com/press/alphasense-surpasses-500m-in-arr/": {
    "content_path": "../data/contents/88b0108b124fb436.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.087944",
    "title": "AlphaSense Surpasses $500M in ARR as Adoption of Applied AI Workflows Surges"
  },
  "https://github.com/applied-artificial-intelligence/pdf-parser-benchmark": {
    "content_path": "../data/contents/bdf2ddc1cd3e7092.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.112483",
    "title": "GitHub - applied-artificial-intelligence/pdf-parser-benchmark"
  },
  "https://www.infoq.com/news/2025/12/agentic-ai-foundation/": {
    "content_path": "../data/contents/b72fc6cc6641eb7c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.136406",
    "title": "OpenAI and Anthropic Donate AGENTS.md and Model Context Protocol to New Agentic AI Foundation"
  },
  "https://github.com/Filimoa/open-parse": {
    "content_path": "../data/contents/d5e5c0d0c5ec1108.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.160162",
    "title": "GitHub - Filimoa/open-parse: Improved file parsing for LLM’s"
  },
  "https://cybersecuritynews.com/langchain-vulnerability/": {
    "content_path": "../data/contents/11140700cba8ee8a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.184666",
    "title": "Critical Langchain Vulnerability Let attackers Exfiltrate Sensitive Secrets from AI systems"
  },
  "https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/": {
    "content_path": "../data/contents/6fae6f94fb70fe49.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.208186",
    "title": "An Easy Introduction to Multimodal Retrieval-Augmented Generation"
  },
  "https://github.com/openai/openai-agents-python/releases": {
    "content_path": "../data/contents/3497642ea1c0ecb5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.259911",
    "title": "Releases · openai/openai-agents-python"
  },
  "https://rogo.ai/news/whats-new-november-2025": {
    "content_path": "../data/contents/d94cfc7788202939.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.286115",
    "title": "What's New: November 2025 | Rogo"
  },
  "https://ai.google.dev/gemini-api/docs/image-generation": {
    "content_path": "../data/contents/5d0514efd1003d12.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.315590",
    "title": "Image generation with Gemini (aka Nano Banana & Nano Banana Pro)"
  },
  "https://huggingface.co/ibm-granite/granite-embedding-reranker-english-r2": {
    "content_path": "../data/contents/ec0af5d0d6e1597e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.339857",
    "title": "ibm-granite/granite-embedding-reranker-english-r2 · Hugging Face"
  },
  "https://www.microsoft.com/en-us/research/publication/putting-the-value-back-in-rl-better-test-time-scaling-by-unifying-llm-reasoners-with-verifiers/": {
    "content_path": "../data/contents/f6fb7a9d7cf0930a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.363808",
    "title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers - Microsoft Research"
  },
  "https://www.linuxfoundation.org/press/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation": {
    "content_path": "../data/contents/b6a13394a4d35548.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.387426",
    "title": "Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF), Anchored by New Project Contributions Including Model Context Protocol (MCP), goose and AGENTS.md"
  },
  "https://www.microsoft.com/en-us/research/blog/moving-to-graphrag-1-0-streamlining-ergonomics-for-developers-and-users/": {
    "content_path": "../data/contents/cddac036aa7bafef.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.410288",
    "title": "GraphRAG 1.0 includes major ergonomic and structural updates"
  },
  "https://platform.openai.com/docs/guides/responses-vs-chat-completions": {
    "content_path": "../data/contents/332f0bf19819d4d4.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.433970",
    "title": "Migrate to the Responses API | OpenAI API"
  },
  "https://github.com/microsoft/graphrag/releases": {
    "content_path": "../data/contents/93d5956a838cc8fc.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.457345",
    "title": "Releases · microsoft/graphrag"
  },
  "https://github.com/weaviate/weaviate/releases/tag/v1.34.0": {
    "content_path": "../data/contents/d4c49dd71447a219.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.482956",
    "title": "Release v1.34.0 - Flat Index 1Bit + 8Bit Rotational Quantization (Preview), Server-side Dynamic Batching (Preview) · weaviate/weaviate"
  },
  "https://huggingface.co/blog/Kseniase/testtimecompute": {
    "content_path": "../data/contents/f2ae6e1fa6b81e3f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.507530",
    "title": "What is test-time compute and how to scale it?"
  },
  "https://simonw.substack.com/p/the-lethal-trifecta-for-ai-agents": {
    "content_path": "../data/contents/136c7c20d34914fa.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.532529",
    "title": "The lethal trifecta for AI agents"
  },
  "https://huggingface.co/teslov/reranker": {
    "content_path": "../data/contents/70563ec542fc0833.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.556014",
    "title": "teslov/reranker · Hugging Face"
  },
  "https://www.hebbia.com/newsroom/hebbia-partners-with-pitchbook": {
    "content_path": "../data/contents/cbc16d9c59f587a4.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.579223",
    "title": "Hebbia Partners With PitchBook"
  },
  "https://www.waterstechnology.com/data-management/7952101/sp-debuts-genai-document-intelligence-for-capital-iq": {
    "content_path": "../data/contents/0105b79ede95cfc8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.602266",
    "title": "S&P debuts GenAI ‘Document Intelligence’ for Capital IQ"
  },
  "https://aws.amazon.com/about-aws/whats-new/2025/04/amazon-bedrock-knowledge-bases-hybrid-search-aurora-postgresql-mongo-db-atlas-vector-stores": {
    "content_path": "../data/contents/e7fca14b37298cda.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.625786",
    "title": "Amazon Bedrock Knowledge Bases now supports hybrid search for Aurora PostgreSQL and MongoDB Atlas vector stores"
  },
  "https://mistral.ai/news/codestral-embed": {
    "content_path": "../data/contents/9aa5a5a7403adb6a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.648853",
    "title": "Codestral Embed"
  },
  "https://www.implicator.ai/anthropics-finance-push-lands-while-revenue-closes-in-on-openai/": {
    "content_path": "../data/contents/b0b31bbb018804cb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.676323",
    "title": "Anthropic's finance push lands while revenue closes in on OpenAI"
  },
  "https://nature.com/articles/s41586-024-07421-0": {
    "content_path": "../data/contents/dd2a8c27bd25832d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.699593",
    "title": "Detecting hallucinations in large language models using semantic entropy"
  },
  "https://github.com/allenai/olmocr": {
    "content_path": "../data/contents/ae05babd8ff3098f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.723454",
    "title": "GitHub - allenai/olmocr: Toolkit for linearizing PDFs for LLM datasets/training"
  },
  "https://modelcontextprotocol.io/specification/2025-06-18/basic/index": {
    "content_path": "../data/contents/d17edd94be3b6ea8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.748438",
    "title": "Overview - Model Context Protocol"
  },
  "https://rpc.cfainstitute.org/research/the-automation-ahead-content-series/agentic-ai-for-finance": {
    "content_path": "data/contents/05be5bfeaeadabc6.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.587180",
    "title": "Agentic AI For Finance: Workflows, Tips, and Case Studies",
    "summary": "Agentic AI is increasingly being adopted in finance, with a focus on creating autonomous programs (agents) that can take actions on behalf of users. The article details the building blocks of these systems, including **Instructions**, **Tools** (for data retrieval, action execution, computation, and using other LLMs), **Information Retrieval** (internal RAG and external web search), **Memory**, and **Guardrails** for risk management.\n\nThe text outlines several workflow patterns applicable to finance:\n*   **Prompt Chaining:** Linear sequences for tasks like drafting credit memos.\n*   **Routing:** Decision points to select the correct sub-workflow (e.g., for multi-asset research).\n*   **Parallelization:** Running independent subtasks simultaneously (e.g., peer benchmarking).\n*   **Orchestrator–Workers:** A central system distributing tasks to specialized workers (e.g., automated portfolio rebalancing).\n*   **Evaluator–Optimizer:** Feedback loops for iterative refinement (e.g., iterative research gathering).\n\nMore autonomous systems utilize the **ReAct** (Reason + Act) framework, where a single LLM plans and executes steps cyclically. **Multi-agent orchestration** coordinates specialized agents toward a shared goal, often using the **Model Context Protocol (MCP)** for standardized tool interaction.\n\nThe article emphasizes that for high-risk, deterministic tasks in finance, predefined **workflows** are often preferred over highly autonomous **agents**. Case studies illustrate applications in:\n1.  **Fundamental Assessment:** Using regime-based screening (Expansionary, Inflationary, Stagflationary, Recession) for company analysis.\n2.  **Deep Research Sustainability Screening:** Employing parallelization and evaluation loops to score companies on sustainability dimensions.\n3.  **Portfolio Construction:** Augmenting traditional factor models with agentic sustainability and regime fit scores to create custom S&P 500 dividend portfolios.\n\nWhile the query specifically mentions **private equity, venture capital, due diligence automation, deal sourcing, portfolio monitoring, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, and Bloomberg**, the text discusses general financial workflows, data retrieval tools (mentioning Bloomberg BQL and LSEG as examples), and portfolio construction. It does not explicitly detail workflows tailored specifically for private equity or venture capital deal sourcing or due diligence automation using those specific platforms, though the underlying agentic principles apply.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://ai.google.dev/gemini-api/docs/thinking": {
    "content_path": "../data/contents/26c66a7b779c3128.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.795943",
    "title": "Gemini thinking"
  },
  "https://www.nature.com/articles/s41467-025-61040-5": {
    "content_path": "../data/contents/497f426051019dd8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.818943",
    "title": "Efficient GPT-4V level multimodal large language model for deployment on edge devices"
  },
  "https://www.alpha-sense.com/press/alphasense-innovations-in-end-to-end-ai-workflows/": {
    "content_path": "../data/contents/1f245634dd4154e2.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.842011",
    "title": "AlphaSense Innovations in End-to-End AI Workflows for Structured Financial Data, Expert Content, and Enterprise Intelligence Fuel Rapid Growth"
  },
  "https://openai.com/index/introducing-gpt-5-2-codex/": {
    "content_path": "../data/contents/072f05ea39e2d90f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.865990",
    "title": "Introducing GPT-5.2-Codex"
  },
  "https://www.anthropic.com/news/contextual-retrieval": {
    "content_path": "../data/contents/e1c689c17d931261.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.889716",
    "title": "Contextual Retrieval in AI Systems"
  },
  "https://www.llamaindex.ai/blog/building-blocks-of-llm-report-generation-beyond-basic-rag": {
    "content_path": "../data/contents/2cbbf4016246a6bb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.913340",
    "title": "Building Blocks of LLM Report Generation: Beyond Basic RAG"
  },
  "https://github.com/chroma-core/chroma/releases": {
    "content_path": "../data/contents/e1fdddb771f08dda.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.936089",
    "title": "Releases · chroma-core/chroma"
  },
  "https://aws.amazon.com/about-aws/whats-new/2025/11/the-aws-api-mcp-server-aws-marketplace": {
    "content_path": "../data/contents/b442e3d42991ea06.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:34.959181",
    "title": "The AWS API MCP Server is now available on AWS Marketplace"
  },
  "https://thenewstack.io/vector-search-is-reaching-its-limit-heres-what-comes-next/": {
    "content_path": "../data/contents/e9d7577c6ffcff37.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.241963",
    "title": "Vector Search Is Reaching Its Limit. Here’s What Comes Next"
  },
  "https://developers.googleblog.com/en/introducing-embeddinggemma/": {
    "content_path": "../data/contents/6fc65da75b5e15fe.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.267318",
    "title": "Introducing EmbeddingGemma: The Best-in-Class Open Model for On-Device Embeddings"
  },
  "https://www.anthropic.com/news/claude-for-financial-services": {
    "content_path": "../data/contents/f8527223e4270952.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.290090",
    "title": "Claude for Financial Services"
  },
  "https://mem0.ai/research": {
    "content_path": "../data/contents/f5529173e52f3c35.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.313241",
    "title": "Scalable Long-Term Memory for Production AI Agents"
  },
  "https://openai.com/index/new-tools-for-building-agents/": {
    "content_path": "../data/contents/78593f4b387f3e74.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.336465",
    "title": "New tools for building agents"
  },
  "https://docs.langchain.com/oss/javascript/langgraph/add-memory": {
    "content_path": "../data/contents/8ce506794284ce76.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.359454",
    "title": "Memory - Docs by LangChain"
  },
  "https://aws.amazon.com/about-aws/whats-new/2025/10/amazon-elasticache-vector-search/": {
    "content_path": "../data/contents/02d24985f179ec65.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.386736",
    "title": "Announcing vector search for Amazon ElastiCache"
  },
  "https://github.com/agiresearch/A-mem": {
    "content_path": "../data/contents/c170664a4b429250.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.409164",
    "title": "GitHub - agiresearch/A-mem: A-MEM: Agentic Memory for LLM Agents"
  },
  "https://github.com/IBM/docling-graph": {
    "content_path": "../data/contents/06a6ac003aefc990.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.432420",
    "title": "GitHub - IBM/docling-graph: Transform unstructured documents into validated, rich and queryable knowledge graphs."
  },
  "https://www.pwc.com/us/en/tech-effect/ai-analytics/ai-agents-for-finance.html": {
    "content_path": "../data/contents/77c3dad3425edb50.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.456202",
    "title": "How AI agents help drive a new finance operating model: What CFOs need to know"
  },
  "https://agentskills.io/home": {
    "content_path": "../data/contents/0a8f0e808225fe01.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.479191",
    "title": "Overview - Agent Skills"
  },
  "https://daloopa.com/blog/press-release/daloopa-expands-financial-data-model-context-protocol": {
    "content_path": "../data/contents/2666111cf67eb1bc.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.503038",
    "title": "Daloopa expands financial data Model Context Protocol (MCP) through a new connector with OpenAI - Daloopa Blog"
  },
  "https://simonw.substack.com/p/i-think-agent-may-finally-have-a": {
    "content_path": "../data/contents/bcfafae2344e58f3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.526287",
    "title": "I think \"agent\" may finally have a widely enough agreed upon definition to be useful jargon now"
  },
  "https://openai.com/index/hebbia/": {
    "content_path": "../data/contents/3795061684f1a721.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.549198",
    "title": "Automating 90% of finance and legal work with agents"
  },
  "https://github.com/openai/agents.md": {
    "content_path": "../data/contents/7853d5e260757614.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.572526",
    "title": "GitHub - agentsmd/agents.md: AGENTS.md — a simple, open format for guiding coding agents"
  },
  "https://www.anthropic.com/research/visible-extended-thinking": {
    "content_path": "../data/contents/63264f7cd4510d96.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.596659",
    "title": "Claude's extended thinking"
  },
  "https://techcommunity.microsoft.com/blog/azuredevcommunityblog/langchain-v1-is-now-generally-available/4462159": {
    "content_path": "../data/contents/4330d93c82b6a633.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.620235",
    "title": "LangChain v1 is now generally available!"
  },
  "https://github.com/HKUDS/RAG-Anything": {
    "content_path": "../data/contents/2f2d15a7a37b1639.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.643296",
    "title": "GitHub - HKUDS/RAG-Anything: \"RAG-Anything: All-in-One RAG Framework\""
  },
  "https://docs.contextual.ai/release-notes/latest": {
    "content_path": "../data/contents/453380d17362b87e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.666191",
    "title": "2025 Release Updates - Contextual AI Documentation"
  },
  "https://github.com/AI4Finance-Foundation/FinRobot": {
    "content_path": "../data/contents/438adb2525358f7f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.693728",
    "title": "GitHub - AI4Finance-Foundation/FinRobot: FinRobot: An Open-Source AI Agent Platform for Financial Analysis using LLMs 🚀 🚀 🚀"
  },
  "https://aws.amazon.com/solutions/guidance/multi-agent-orchestration-on-aws/": {
    "content_path": "../data/contents/22b7510022330e70.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.719402",
    "title": "Guidance for Multi-Agent Orchestration on AWS"
  },
  "https://www.v7labs.com/agents/ai-investment-memo-generation-agent": {
    "content_path": "../data/contents/503b4c9d579582ba.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.742878",
    "title": "AI Investment Memo Generation Agent | Automate Deal Memos | V7 Go"
  },
  "https://claude.com/blog/structured-outputs-on-the-claude-developer-platform": {
    "content_path": "../data/contents/87deb03ea67c43fb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.766715",
    "title": "Structured outputs on the Claude Developer Platform"
  },
  "https://github.com/sylphxltd/pdf-reader-mcp": {
    "content_path": "../data/contents/08e6f1fe05e2a6ad.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.789853",
    "title": "GitHub - SylphxAI/pdf-reader-mcp: 📄 Production-ready MCP server for PDF processing - 5-10x faster with parallel processing and 94%+ test coverage"
  },
  "https://dbadataverse.com/tech/postgresql/2025/12/pgvector-postgresql-vector-database-guide": {
    "content_path": "../data/contents/ca825dd936b3bb06.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.813951",
    "title": "PostgreSQL as Vector Database: Complete pgvector Installation & Configuration Guide [2025]"
  },
  "https://unstructured.io/blog/rag-vs-long-context-models-do-we-still-need-rag": {
    "content_path": "../data/contents/ecbac4c530ab0286.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.837120",
    "title": "RAG vs. Long-Context Models. Do we still need RAG? | Unstructured"
  },
  "https://www.stack-ai.com/blog/how-to-build-a-due-diligence-ai-agent": {
    "content_path": "../data/contents/940ff5f4d6198364.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.862581",
    "title": "How to Build a Due Diligence AI Agent"
  },
  "https://aws.amazon.com/blogs/machine-learning/harness-the-power-of-mcp-servers-with-amazon-bedrock-agents/": {
    "content_path": "../data/contents/009904daedde9c39.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.886057",
    "title": "Harness the power of MCP servers with Amazon Bedrock Agents | Amazon Web Services"
  },
  "https://www.prnewswire.com/news-releases/alphasense-launches-autonomous-ai-agent-interviewer-debuts-channel-checks-to-deliver-real-time-market-signals-across-all-sectors-of-the-economy-302532863.html": {
    "content_path": "../data/contents/10f1ef9239efea97.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.909769",
    "title": "AlphaSense Launches Autonomous AI Agent Interviewer, Debuts Channel Checks to Deliver Real-Time Market Signals Across All Sectors of the Economy"
  },
  "https://techintelpro.com/news/Finance/Investment/hebbia-integrates-preqin-data-for-ai-powered-private-markets": {
    "content_path": "../data/contents/226a67343cac4e65.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.932576",
    "title": "Hebbia Integrates Preqin Data for AI-Powered Private Markets"
  },
  "https://blog.langchain.com/langchain-langgraph-1dot0/": {
    "content_path": "../data/contents/826a0277bbd49e41.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.955543",
    "title": "LangChain and LangGraph Agent Frameworks Reach v1.0 Milestones"
  },
  "https://www.ark-invest.com/articles/analyst-research/ai-agents-could-transform-enterprise-spending": {
    "content_path": "../data/contents/54e166f9313b3562.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:35.980574",
    "title": "AI Agents Could Transform Enterprise Spending"
  },
  "https://www.workfusion.com/news/workfusion-introduces-edward-an-ai-agent-revolutionizing-enhanced-due-diligence-edd-high-risk-reviews-for-financial-institutions/": {
    "content_path": "../data/contents/18e4ddc02521acd5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.003684",
    "title": "WorkFusion Introduces Edward, an AI Agent Revolutionizing Enhanced Due Diligence (EDD)/High Risk Reviews for Financial Institutions | WorkFusion"
  },
  "https://github.com/microsoft/RD-Agent": {
    "content_path": "../data/contents/f5cb48f26baf9455.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.027108",
    "title": "GitHub - microsoft/RD-Agent: Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. 🔗https://aka.ms/RD-Agent-Tech-Report"
  },
  "https://github.com/modelcontextprotocol/python-sdk": {
    "content_path": "../data/contents/b275ec28fddd14c2.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.050017",
    "title": "GitHub - modelcontextprotocol/python-sdk: The official Python SDK for Model Context Protocol servers and clients"
  },
  "https://huggingface.co/docs/transformers/en/model_doc/table-transformer": {
    "content_path": "../data/contents/2565065e18805a11.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.096164",
    "title": "Table Transformer"
  },
  "https://github.com/Indic-Layout/IndicDLP": {
    "content_path": "../data/contents/92d0564923d1ff12.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.120766",
    "title": "GitHub - AI4Bharat/IndicDLP"
  },
  "https://www.blackrock.com/aladdin/discover/press-release-lseg-expands-partnership-with-blackrock": {
    "content_path": "../data/contents/abd7861d0383c19d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.144144",
    "title": "LSEG and BlackRock expand partnership with new Preqin® data feeds | Aladdin"
  },
  "https://github.com/AnswerDotAI/rerankers": {
    "content_path": "../data/contents/81605726eef598a7.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.171044",
    "title": "GitHub - AnswerDotAI/rerankers: A lightweight, low-dependency, unified API to use all common reranking and cross-encoder models."
  },
  "https://github.com/applied-artificial-intelligence/pdfsmith": {
    "content_path": "../data/contents/3257d3e161a3a6f6.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.194423",
    "title": "GitHub - applied-artificial-intelligence/pdfsmith: PDF to Markdown conversion with 19+ backend support including frontier LLMs"
  },
  "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/": {
    "content_path": "../data/contents/08aca1a4de71818f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.217635",
    "title": "Introducing Gemini 2.0: our new AI model for the agentic era"
  },
  "https://blog.cloudflare.com/remote-model-context-protocol-servers-mcp/": {
    "content_path": "../data/contents/3ffec7d0d6fe2e6a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.242457",
    "title": "Build and deploy Remote Model Context Protocol (MCP) servers to Cloudflare"
  },
  "https://huggingface.co/Qwen/QwQ-32B": {
    "content_path": "../data/contents/3fc33cc56d949e46.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.264882",
    "title": "Qwen/QwQ-32B · Hugging Face"
  },
  "https://www.contextual.ai/blog/introducing-instruction-following-reranker/": {
    "content_path": "../data/contents/3d094d2a1c3f89a8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.288044",
    "title": "Introducing the world’s first instruction-following reranker"
  },
  "https://platform.openai.com/docs/guides/structured-outputs": {
    "content_path": "../data/contents/b2986739165a92d2.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.313060",
    "title": "Structured model outputs | OpenAI API"
  },
  "https://blog.langchain.com/customers-harmonic/": {
    "content_path": "../data/contents/2ad35e8d17384d82.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.336478",
    "title": "How Harmonic built an investment agent with LangGraph and LangSmith— so VCs can focus on founders"
  },
  "https://github.com/yejining99/GuruAgents": {
    "content_path": "../data/contents/a9187e93cd4ef922.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.360936",
    "title": "GitHub - yejining99/GuruAgents"
  },
  "https://docs.anthropic.com/pt/docs/build-with-claude/pdf-support": {
    "content_path": "../data/contents/96384766e67388eb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.384004",
    "title": "Not Found - Claude Docs"
  },
  "https://agenta.ai/blog/the-guide-to-structured-outputs-and-function-calling-with-llms": {
    "content_path": "../data/contents/3241e59885a3e4a3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.407531",
    "title": "The guide to structured outputs and function calling with LLMs"
  },
  "https://becomingahacker.org/integrating-agentic-rag-with-mcp-servers-technical-implementation-guide-1aba8fd4e442?gi=9921cbf26fe6": {
    "content_path": "../data/contents/ee52fa939a70bb32.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.430635",
    "title": "Integrating Agentic RAG with MCP Servers: Technical Implementation Guide"
  },
  "https://qwenlm.github.io/blog/qwen2.5-vl/": {
    "content_path": "../data/contents/dc21f366b6384254.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.453603",
    "title": "Qwen2.5 VL! Qwen2.5 VL! Qwen2.5 VL!"
  },
  "https://aws.amazon.com/about-aws/whats-new/2025/04/pgvector-0-8-0-aurora-postgresql": {
    "content_path": "../data/contents/633c721d761d3831.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.478056",
    "title": "Announcing pgvector 0.8.0 support in Aurora PostgreSQL"
  },
  "https://docs.claude.com/en/docs/build-with-claude/structured-outputs": {
    "content_path": "../data/contents/d088346ef95486d3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.501084",
    "title": "Structured outputs"
  },
  "https://openai.com/index/rogo/": {
    "content_path": "../data/contents/582599151148f756.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.547772",
    "title": "Rogo scales AI-driven financial research with OpenAI o1"
  },
  "https://blog.google/technology/ai/nano-banana-pro": {
    "content_path": "../data/contents/e3529a602cca156f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.572751",
    "title": "Introducing Nano Banana Pro"
  },
  "https://www.prnewswire.com/news-releases/alphasense-launches-deep-research-automating-in-depth-analysis-with-agentic-ai-302476710.html": {
    "content_path": "../data/contents/fc05d15b15ae4b13.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.596997",
    "title": ""
  },
  "https://github.com/Open-Finance-Lab/AgenticTrading": {
    "content_path": "../data/contents/c7ab329b72ec79ee.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.620149",
    "title": "GitHub - Open-Finance-Lab/AgenticTrading"
  },
  "https://docs.langchain.com/oss/javascript/langchain/structured-output": {
    "content_path": "../data/contents/3053f50658ede53d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.644015",
    "title": "Structured output - Docs by LangChain"
  },
  "https://github.com/jina-ai/late-chunking": {
    "content_path": "../data/contents/b900ed1a75da394c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.696708",
    "title": "GitHub - jina-ai/late-chunking: Code for explaining and evaluating late chunking (chunked pooling)"
  },
  "https://qdrant.tech/blog/qdrant-deeplearning-ai-multi-vector-image-retrieval/": {
    "content_path": "../data/contents/7e887fb5faed29de.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.720968",
    "title": "New DeepLearning.AI Course on Multi-Vector Image Retrieval with ColPali and MUVERA"
  },
  "https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/azure-ai-search-outperforming-vector-search-with-hybrid-retrieval-and-reranking/3929167": {
    "content_path": "../data/contents/5d505bb7e6552da8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.745688",
    "title": "Azure AI Search: Outperforming vector search with hybrid retrieval and reranking"
  },
  "https://www.researchgate.net/publication/396731115_StockVision_A_Multi-Agent_AI_framework_for_Intelligent_Stock_Investment_and_Analysis_using_CrewAI": {
    "content_path": "../data/contents/7a35e74a02bcecd5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.768471",
    "title": "Just a moment..."
  },
  "https://github.com/ssmisya/PRMBench": {
    "content_path": "../data/contents/788a60a614608cd5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.791926",
    "title": "GitHub - ssmisya/PRMBench: [ACL' 25] The official code repository for PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models."
  },
  "https://www.prnewswire.com/news-releases/chronograph-and-anthropic-announce-integration-of-private-capital-data-into-claude-302595846.html": {
    "content_path": "../data/contents/a754fc3a5149b367.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.815370",
    "title": "Chronograph and Anthropic Announce Integration of Private Capital Data into Claude"
  },
  "https://siliconangle.com/2025/11/10/pitchbook-brings-generative-ai-private-markets-navigator-chatgpt-integration/": {
    "content_path": "../data/contents/5e20122f3fe94d00.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.841353",
    "title": "PitchBook brings generative AI to private markets with Navigator and ChatGPT integration - SiliconANGLE"
  },
  "https://www.prnewswire.com/news-releases/opensearch-3-0-enhances-vector-database-performance-search-infrastructure-and-scalability-to-meet-ai-driven-demand-302447824.html": {
    "content_path": "../data/contents/66f87fc3d76223df.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.864710",
    "title": "OpenSearch 3.0 Enhances Vector Database Performance, Search Infrastructure and Scalability to Meet AI-driven Demand"
  },
  "https://www.bloomberg.com/professional/insights/artificial-intelligence/banks-enter-agentic-ai-era-as-tech-race-heats-up-roi-in-focus/": {
    "content_path": "../data/contents/935017adcd845765.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.893203",
    "title": "Banks enter agentic AI era as tech race heats up, ROI in focus"
  },
  "https://weaviate.io/blog/muvera": {
    "content_path": "../data/contents/69040fc034405fe3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.916451",
    "title": "More efficient multi-vector embeddings with MUVERA | Weaviate"
  },
  "https://www.prnewswire.com/news-releases/juniper-square-adds-preqin-data-to-its-services-to-help-gps-accelerate-capital-raising-302595927.html": {
    "content_path": "../data/contents/01f6615b09cca17f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.940006",
    "title": "Juniper Square Adds Preqin Data to its Services to Help GPs Accelerate Capital Raising"
  },
  "https://qdrant.tech/blog/case-study-cosmos/": {
    "content_path": "../data/contents/42d895e712cc3bad.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.962790",
    "title": "How Cosmos delivered editorial-grade visual search with Qdrant"
  },
  "https://pitchbook.com/short-links/273vY7": {
    "content_path": "../data/contents/de45b5e19511fe2e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:36.986856",
    "title": "PitchBook’s Industry Leading Private Capital Market Intelligence is Now Accessible in Claude | PitchBook"
  },
  "https://towardsdatascience.com/mulitmodal-llms-interpreting-charts-b212f5c0aa1f/": {
    "content_path": "../data/contents/86198c44aeb491bf.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.010888",
    "title": "I Tested Frontline M-LLMs on Their Chart Interpretation Skills"
  },
  "https://github.com/Bessouat40/RAGLight": {
    "content_path": "../data/contents/f3bcfa8137062bf0.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.035597",
    "title": "GitHub - Bessouat40/RAGLight: RAGLight is a modular framework for Retrieval-Augmented Generation (RAG). It makes it easy to plug in different LLMs, embeddings, and vector stores, and now includes seamless MCP integration to connect external tools and data sources."
  },
  "https://pitchbook.com/media/press-releases/pitchbooks-industry-leading-private-capital-market-intelligence-is-now-accessible-in-claude": {
    "content_path": "../data/contents/cb0e0af236206eda.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.059389",
    "title": "PitchBook’s Industry Leading Private Capital Market Intelligence is Now Accessible in Claude | PitchBook"
  },
  "https://huggingface.co/blog/vlms-2025": {
    "content_path": "../data/contents/83de0e51351da7f7.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.082561",
    "title": "Vision Language Models (Better, faster, stronger)"
  },
  "https://blog.langchain.com/langmem-sdk-launch/": {
    "content_path": "../data/contents/3a8565df0061aada.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.111062",
    "title": "LangMem SDK for agent long-term memory"
  },
  "https://magazine.sebastianraschka.com/p/understanding-reasoning-llms": {
    "content_path": "../data/contents/3f2a9e705d9a4566.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.135682",
    "title": "Understanding Reasoning LLMs"
  },
  "https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/introducing-cohere-rerank-4-0-in-microsoft-foundry/4477076": {
    "content_path": "../data/contents/6e19b57fc1901cd3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.158958",
    "title": "Introducing Cohere Rerank 4.0 in Microsoft Foundry"
  },
  "https://github.com/modelcontextprotocol/modelcontextprotocol": {
    "content_path": "../data/contents/1f9a6ffe7d318ba3.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.182294",
    "title": "GitHub - modelcontextprotocol/modelcontextprotocol: Specification and documentation for the Model Context Protocol"
  },
  "https://huggingface.co/datasets/zhaosuifeng/FinRAGBench-V": {
    "content_path": "../data/contents/dc897da1e3e914f6.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.205718",
    "title": "zhaosuifeng/FinRAGBench-V · Datasets at Hugging Face"
  },
  "https://github.com/anthropics/claude-agent-sdk-python": {
    "content_path": "../data/contents/e027419555c8163a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.228715",
    "title": "GitHub - anthropics/claude-agent-sdk-python"
  },
  "https://www.kaggle.com/benchmarks/google/facts": {
    "content_path": "../data/contents/ae6c3c314e55b0c5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.251824",
    "title": "FACTS Benchmark Suite Leaderboard | Kaggle"
  },
  "https://github.com/docling-project/docling": {
    "content_path": "../data/contents/811ebbb72ff6d0f1.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.286395",
    "title": "GitHub - docling-project/docling: Get your documents ready for gen AI"
  },
  "https://medium.com/kx-systems/rag-llamaparse-advanced-pdf-parsing-for-retrieval-c393ab29891b": {
    "content_path": "../data/contents/5e422ddcef81704b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.309401",
    "title": "RAG + LlamaParse: Advanced PDF Parsing for Retrieval"
  },
  "https://changelog.langchain.com/announcements/langchain-1-1": {
    "content_path": "../data/contents/cefa1dc4b88b19ff.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.331925",
    "title": "LangChain 1.1"
  },
  "https://weaviate.io/blog/weaviate-1-32-release": {
    "content_path": "../data/contents/67543431340ebf3a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.357412",
    "title": "Weaviate 1.32 Release | Weaviate"
  },
  "https://github.com/lastmile-ai/mcp-agent": {
    "content_path": "../data/contents/9a7579136451220a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.381662",
    "title": "GitHub - lastmile-ai/mcp-agent: Build effective agents using Model Context Protocol and simple workflow patterns"
  },
  "https://www.alpha-sense.com/press/alphasense-acquires-carousel/": {
    "content_path": "../data/contents/1f61f46c731ab152.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.404441",
    "title": "AlphaSense Acquires Carousel to Power AI-Driven Excel Modeling"
  },
  "https://www.nature.com/articles/s41746-025-01893-8": {
    "content_path": "../data/contents/45f100e9a87bdce1.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.427569",
    "title": "Specialized curricula for training vision language models in retinal image analysis"
  },
  "https://changelog.langchain.com/announcements/langchain-1-0-now-generally-available": {
    "content_path": "../data/contents/67b5267ef6c2985d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.451011",
    "title": "LangChain 1.0 now generally available"
  },
  "https://www.datasite.com/en/company/news/datasite-acquires-leading-private-market-intelligence-company-grata-with--500-million-investment-commitment": {
    "content_path": "../data/contents/7181811ef3ed61e8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.481708",
    "title": "Datasite Acquires Leading Private Market Intelligence Company Grata with $500 Million Investment Commitment"
  },
  "https://github.com/modelcontextprotocol/registry": {
    "content_path": "../data/contents/0ab174b3cfd6fa55.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.515364",
    "title": "GitHub - modelcontextprotocol/registry: A community driven registry service for Model Context Protocol (MCP) servers."
  },
  "https://aws.amazon.com/blogs/aws/amazon-opensearch-service-improves-vector-database-performance-and-cost-with-gpu-acceleration-and-auto-optimization/": {
    "content_path": "../data/contents/5f15f9489199828f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.538588",
    "title": "Amazon OpenSearch Service improves vector database performance and cost with GPU acceleration and auto-optimization | Amazon Web Services"
  },
  "https://www.prnewswire.com/news-releases/sp-global-launches-sp-capital-iq-pro-document-intelligence-on-salesforces-agentexchange-transforming-document-analysis-with-generative-ai-302582333.html": {
    "content_path": "../data/contents/95abd64db2b2eb4d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.562638",
    "title": "S&P Global Launches S&P Capital IQ Pro Document Intelligence on Salesforce's AgentExchange, Transforming Document Analysis with Generative AI"
  },
  "https://www.nature.com/articles/s41586-024-07421-0": {
    "content_path": "../data/contents/1168071d7b84117d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.586219",
    "title": "Detecting hallucinations in large language models using semantic entropy"
  },
  "https://v7labs.com/agents/ai-data-room-analysis": {
    "content_path": "../data/contents/9fee9b94acef2147.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.609774",
    "title": "AI Data Room Analysis Agent | Automate Due Diligence | V7 Go"
  },
  "https://academic.oup.com/nsr/article/11/12/nwae403/7896414": {
    "content_path": "../data/contents/c653fe2a6bce8df1.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.632870",
    "title": "Just a moment..."
  },
  "https://menlovc.com/perspective/backing-trove-ai-agents-for-private-equity/": {
    "content_path": "data/contents/51368af8d3772e66.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.673548",
    "title": "Backing Trove AI: Agents for Private Equity",
    "summary": "Trove (formerly Mako) is an AI associate purpose-built for private equity firms, aiming to deliver associate-level AGI to free up professionals for judgment-driven work. It addresses the manual nature of financial workflows by directly integrating with a firm's apps and data sources (deal/portfolio files, emails, CRM) to create a searchable knowledge base.\n\nTrove's capabilities include:\n*   **Unlocking firm knowledge:** Indexing internal data for quick surfacing of metrics, files, and insights.\n*   **Investor-grade intelligence:** Independently executing private equity workstreams like analyzing data rooms, pulling metrics from past deals, and generating portfolio reports in the firm's style, using a specialized agent capable of complex reasoning and adaptation.\n*   **Security:** Utilizing a \"cloud-prem\" deployment model, allowing the application to reside entirely within the customer's private cloud.\n\nThe company recently secured a \\$7.1M seed round led by Menlo Ventures. The founders are Danny Goldman (CEO), who has private equity experience from Bain, and Shivaal Roy (CTO), who has experience building enterprise AI at Glean and Scale AI.",
    "published_date": "2025-10-14T15:00:00.000Z",
    "score": null
  },
  "https://www.mongodb.com/docs/atlas/architecture/current/solutions-library/fin-services-agentic-portfolio/": {
    "content_path": "../data/contents/950ed39ccd7909d6.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.684520",
    "title": "Agentic AI-Powered Investment Portfolio Management"
  },
  "https://docs.langchain.com/oss/python/releases/changelog": {
    "content_path": "../data/contents/fe40ac0fa5bdfd28.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.718022",
    "title": "Changelog - Docs by LangChain"
  },
  "https://blog.langchain.com/captide/": {
    "content_path": "../data/contents/983eb89ad12dedb7.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.742318",
    "title": "How Captide agents running on LangGraph Platform compress investment research from days to seconds"
  },
  "https://thenewstack.io/solving-the-rag-vs-long-context-model-dilemma/": {
    "content_path": "../data/contents/cc48021e0db9f297.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.765587",
    "title": "Solving the RAG vs. Long Context Model Dilemma"
  },
  "https://weaviate.io/blog/weaviate-1-34-release": {
    "content_path": "../data/contents/be5b0327cf48ac2b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.790013",
    "title": "Weaviate 1.34 Release | Weaviate"
  },
  "https://weaviate.io/blog/weaviate-1-31-release": {
    "content_path": "../data/contents/3b6fff12fa4393da.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.815972",
    "title": "Weaviate 1.31 Release | Weaviate"
  },
  "https://changelog.langchain.com/announcements/langchain-mcp-adapters-0-2-0": {
    "content_path": "../data/contents/3165c5696469ae21.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.841383",
    "title": "LangChain MCP Adapters 0.2.0"
  },
  "https://agentskills.io/specification": {
    "content_path": "../data/contents/f36ae4007ec3a8cf.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.867189",
    "title": "Specification - Agent Skills"
  },
  "https://weaviate.io/blog/weaviate-1-33-release": {
    "content_path": "../data/contents/ffc9c43022676736.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:37.976197",
    "title": "Weaviate 1.33 Release | Weaviate"
  },
  "https://www.llamaindex.ai/blog/making-coding-agents-safe-using-llamaindex": {
    "content_path": "../data/contents/e1d4807ab3d8e31e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:38.270656",
    "title": "Making Coding Agents Safe: Using LlamaIndex to Secure Filesystem Access"
  },
  "https://techcrunch.com/2025/10/28/mem0-raises-24m-from-yc-peak-xv-and-basis-set-to-build-the-memory-layer-for-ai-apps/": {
    "content_path": "../data/contents/64ea8e6e29aaf600.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:38.375902",
    "title": "Mem0 raises $24M from YC, Peak XV and Basis Set to build the memory layer for AI apps"
  },
  "https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-financial-analysis-agent-with-langgraph-and-strands-agents/": {
    "content_path": "../data/contents/15a4260c2d19b5c2.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:38.417522",
    "title": "Build an intelligent financial analysis agent with LangGraph and Strands Agents | Amazon Web Services"
  },
  "https://dl.acm.org/doi/10.1145/3657281": {
    "content_path": "../data/contents/597e25659b35a6b0.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:38.483906",
    "title": "Just a moment..."
  },
  "https://pitchbook.com/media/press-releases/pitchbook-expands-access-to-private-market-intelligence-through-new-llm-partnerships-with-finster-model-ml-and-farsight-ai": {
    "content_path": "../data/contents/3f793b34bb1fe479.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:38.877131",
    "title": "PitchBook Expands Access to Private Market Intelligence Through New LLM Partnerships with Finster, Model ML, and Farsight AI | PitchBook"
  },
  "https://github.com/alibaba/Logics-Parsing": {
    "content_path": "../data/contents/6ea2bc60027c614b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:38.932526",
    "title": "GitHub - alibaba/Logics-Parsing"
  },
  "https://www.datasite.com/en/company/news/datasite-acquires-leading-agentic-ai-company-blueflame": {
    "content_path": "../data/contents/1cc456b9932e0a83.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:38.962046",
    "title": "Datasite Acquires Leading Agentic AI Company Blueflame"
  },
  "https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling": {
    "content_path": "../data/contents/d47fff98db17933a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:38.986935",
    "title": "The State of LLM Reasoning Model Inference"
  },
  "https://www.mongodb.com/company/blog/product-release-announcements/rerank-2-5-and-rerank-2-5-lite-instruction-following-rerankers": {
    "content_path": "../data/contents/8d2ece95d196a875.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:39.012699",
    "title": "rerank-2.5 and rerank-2.5-lite: Instruction-Following Rerankers"
  },
  "https://www.anthropic.com/engineering/code-execution-with-mcp": {
    "content_path": "../data/contents/9b88bfec4a46afeb.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:39.037474",
    "title": "Code execution with MCP: building more efficient AI agents"
  },
  "https://support.intralinks.com/hc/en-us/articles/43880137019419-DealCentre-AI-December-2025": {
    "content_path": "../data/contents/896706b61d3fdb12.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:39.096184",
    "title": "DealCentre AI - December 2025"
  },
  "https://developer.nvidia.com/blog/an-easy-introduction-to-llm-reasoning-ai-agents-and-test-time-scaling/": {
    "content_path": "../data/contents/127476de1206de99.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:39.489712",
    "title": "An Easy Introduction to LLM Reasoning, AI Agents, and Test Time Scaling"
  },
  "https://www.alpha-sense.com/press/alphasense-launches-deep-research-automating-in-depth-analysis-with-agentic-ai-on-high-value-content/": {
    "content_path": "../data/contents/fb080354c907b0b9.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:39.568909",
    "title": "AlphaSense Launches Deep Research, Automating In-Depth Analysis with Agentic AI on High Value Content"
  },
  "https://aws.amazon.com/blogs/machine-learning/building-smarter-ai-agents-agentcore-long-term-memory-deep-dive/": {
    "content_path": "../data/contents/80a7f84c884235a7.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:39.638136",
    "title": "Building smarter AI agents: AgentCore long-term memory deep dive | Amazon Web Services"
  },
  "https://www.ibm.com/new/announcements/granite-docling-end-to-end-document-conversion": {
    "content_path": "../data/contents/70edcad25131473c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:39.861965",
    "title": "Granite Docling End To End Document Conversion"
  },
  "https://docs.smartly.ai/changelog/december-2025": {
    "content_path": "../data/contents/27cc7db70fa4700a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:39.915983",
    "title": "December 2025"
  },
  "https://github.com/milvus-io/milvus/releases": {
    "content_path": "../data/contents/5b36cdee93c60bdf.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:39.948844",
    "title": "Releases · milvus-io/milvus"
  },
  "https://milvus.io/docs/es/v2.5.x/release_notes.md": {
    "content_path": "../data/contents/916808a2381dbf17.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:39.974191",
    "title": "Release Notes Milvus v2.5.x documentation"
  },
  "https://v7labs.com/agents/ai-due-diligence-agent": {
    "content_path": "data/contents/0b62790cb516b73b.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.734535",
    "title": "AI Financial Due Diligence Agent",
    "summary": "The webpage describes an **AI Financial Due Diligence Agent** designed to automate the first-pass due diligence process for **Private Equity, Venture Capital, and Corporate Development**.\n\nKey capabilities relevant to your query include:\n*   **Due Diligence Automation:** It navigates Virtual Data Rooms (VDRs), extracts and normalizes financials, analyzes commercial contracts, and flags risks.\n*   **Financial Analysis:** It performs **Financial Statement Spreading** (extracting data from income statements, balance sheets, and cash flow statements) and populates standardized financial model templates.\n*   **Risk Identification:** It proactively searches for diligence risks like customer concentration, margin erosion, and problematic contract clauses.\n*   **Cap Table Analysis:** It parses capitalization tables to validate ownership and check for liquidation preference issues.\n*   **Data Integration:** It mentions the ability to **Import your files** from sources including **PitchBook** and **S&P Capital IQ** (CapIQ).\n\nThe page also mentions other related agents, such as an **AI Investment Analysis Agent** and a **Financial Valuation Agent**, which cover aspects like investment opportunity analysis and deal sourcing/analysis.",
    "published_date": "2025-09-03T00:00:00.000Z",
    "score": null
  },
  "https://www.wsj.com/articles/private-equity-backed-datasite-acquires-private-markets-intelligence-company-grata-aa3dda34": {
    "content_path": "../data/contents/f4d933f539d58911.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:40.647483",
    "title": "Exclusive | Private Equity-Backed Datasite Acquires Private-Markets Intelligence Company Grata"
  },
  "https://sakana.ai/ab-mcts/": {
    "content_path": "../data/contents/b7b709c8b69a5915.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:40.698307",
    "title": "Sakana AI"
  },
  "https://github.com/baaivision/Emu3.5": {
    "content_path": "../data/contents/77b7a814246d674c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:40.743734",
    "title": "GitHub - baaivision/Emu3.5: Native Multimodal Models are World Learners"
  },
  "https://aws.amazon.com/about-aws/whats-new/2025/10/coheres-embed-v4-multimodal-embeddings-bedrock/": {
    "content_path": "../data/contents/9358793d609554c6.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:40.840856",
    "title": "Cohere’s Embed v4 multimodal embeddings model now available on Amazon Bedrock"
  },
  "https://github.com/GarfieldHuang/dots.ocr": {
    "content_path": "../data/contents/24bbd42daad3291a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:41.176401",
    "title": "GitHub - GarfieldHuang/dots.ocr"
  },
  "https://www.newsletter.datadrivenvc.io/p/the-multi-agent-future-of-vc-will": {
    "content_path": "../data/contents/edca7559c1ae3efe.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:41.292110",
    "title": "🔥The Multi-Agent Future of VC: Will We See One-Person Billion-Dollar Funds?"
  },
  "https://milvus.io/docs/zh/v2.5.x/release_notes.md": {
    "content_path": "../data/contents/f5a3d1d8d5b2d354.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:41.406557",
    "title": "Release Notes Milvus v2.5.x documentation"
  },
  "https://www.databricks.com/blog/reranking-mosaic-ai-vector-search-faster-smarter-retrieval-rag-agents": {
    "content_path": "../data/contents/df7acebf0cc6999d.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:42.876631",
    "title": "Reranking in Mosaic AI Vector Search for Faster, Smarter Retrieval in RAG Agents"
  },
  "https://testtimescaling.github.io/": {
    "content_path": "../data/contents/13a44f4a975ec5de.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:42.926102",
    "title": "SOCIAL MEDIA TITLE TAG"
  },
  "https://changelog.langchain.com/announcements/langgraph-1-0-is-now-generally-available": {
    "content_path": "../data/contents/34fbd2de747866e5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:43.583838",
    "title": "LangGraph 1.0 is now generally available"
  },
  "https://www.ibm.com/think/topics/ai-agents-in-finance": {
    "content_path": "../data/contents/7ee8413d67ed4eef.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:43.944870",
    "title": "AI Agents in Finance"
  },
  "https://www.prnewswire.com/news-releases/alphasense-launches-autonomous-ai-agent-interviewer-debuts-channel-checks-302532863.html": {
    "content_path": "../data/contents/a9d57fe60fe5143a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:45.026328",
    "title": ""
  },
  "https://community.databricks.com/t5/technical-blog/the-ultimate-guide-to-chunking-strategies-for-rag-applications/ba-p/113089": {
    "content_path": "../data/contents/3214017834803dd8.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:46.166362",
    "title": "The Ultimate Guide to Chunking Strategies for RAG Applications with Databricks"
  },
  "https://www.mongodb.com/press/mongodb-extends-search-and-vector-search-capabilities-to-self-managed-offerings": {
    "content_path": "../data/contents/a8bccbb4aa1a0204.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:46.218166",
    "title": "MongoDB Extends Search And Vector Search Capabilities To Self-Managed Offerings"
  },
  "https://github.com/langchain-ai/langchain/releases": {
    "content_path": "../data/contents/f639e020ec7aafbe.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:46.780582",
    "title": "Releases · langchain-ai/langchain"
  },
  "https://www.ibm.com/think/topics/multimodal-rag": {
    "content_path": "../data/contents/f8f5d61e945ef961.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:46.908730",
    "title": "Multimodal RAG"
  },
  "https://github.com/langchain-ai/langgraph/releases": {
    "content_path": "../data/contents/4fa1b85bcbb569d1.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:47.067011",
    "title": "Releases · langchain-ai/langgraph"
  },
  "https://www.wired.com/story/openai-anthropic-and-block-are-teaming-up-on-ai-agent-standards": {
    "content_path": "../data/contents/d16135a7b3d82395.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:50.964289",
    "title": "OpenAI, Anthropic, and Block Are Teaming Up to Make AI Agents Play Nice"
  },
  "https://lilianweng.github.io/posts/2024-07-07-hallucination/": {
    "content_path": "../data/contents/d342305e68818550.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:51.075490",
    "title": "Extrinsic Hallucinations in LLMs"
  },
  "https://docs.pinecone.io/release-notes/2025": {
    "content_path": "../data/contents/f23b272f8fb9564b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:51.668017",
    "title": "2025 releases - Pinecone Docs"
  },
  "https://www.reuters.com/business/lseg-partners-with-anthropic-bring-financial-data-access-claude-users-2025-10-27/": {
    "content_path": "../data/contents/66b82514f8a9062a.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:51.852848",
    "title": "LSEG partners with Anthropic to bring financial data access to Claude users"
  },
  "https://www.reuters.com/markets/funds/agentic-ai-race-by-british-banks-raises-new-risks-regulator-2025-12-17/": {
    "content_path": "../data/contents/e92b11dbbc6f1dc9.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:55.566761",
    "title": "Agentic AI race by British banks raises new risks for regulator"
  },
  "https://qdrant.tech/articles/hybrid-search/": {
    "content_path": "../data/contents/3a5fff2a2ea11c65.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:55.622389",
    "title": "Hybrid Search Revamped - Building with Qdrant's Query API"
  },
  "https://www.digitalocean.com/community/tutorials/beyond-vector-databases-rag-without-embeddings": {
    "content_path": "../data/contents/16dd421158355949.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:56.128197",
    "title": "Beyond Vector Databases: RAG Architectures Without Embeddings"
  },
  "https://www.salesforce.com/ap/news/press-releases/2025/10/16/anthropic-regulated-industries-partnership-expansion-announcement/": {
    "content_path": "../data/contents/ddd6ca986b85cd41.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:56.180021",
    "title": "Anthropic and Salesforce Expand Strategic Partnership to Deliver Trusted AI for Regulated Industries"
  },
  "https://cohere.com/blog/rerank-4": {
    "content_path": "../data/contents/afe5e2cb18b44ad1.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:56.476625",
    "title": "Introducing Rerank 4: Cohere’s most powerful reranker yet"
  },
  "https://www.cloudflare.com/press/press-releases/2025/cloudflare-accelerates-ai-agent-development-remote-mcp/": {
    "content_path": "../data/contents/8e6839a4654ff3ff.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:56.516037",
    "title": "Cloudflare Accelerates AI Agent Development With The Industry's First Remote MCP Server | Cloudflare"
  },
  "https://blog.google/products/gemini/gemini-3-deep-think/": {
    "content_path": "../data/contents/02b6c7d970f3438b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:56.686039",
    "title": "Gemini 3 Deep Think is now available in the Gemini app."
  },
  "https://qdrant.tech/blog/qdrant-1.15.x/": {
    "content_path": "../data/contents/45ee9c9a4109ea1c.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:56.772495",
    "title": "Qdrant 1.15 - Smarter Quantization & better Text Filtering"
  },
  "https://a16z.com/a-deep-dive-into-mcp-and-the-future-of-ai-tooling/": {
    "content_path": "../data/contents/5e537f30f88b0f39.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:56.799001",
    "title": "A Deep Dive Into MCP and the Future of AI Tooling"
  },
  "https://www.intralinks.com/resources/blog/ai-search-transforms-ma-diligence": {
    "content_path": "../data/contents/4b7ac34b9ec5c901.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:56.846815",
    "title": "AI Search Transforms M&A Diligence"
  },
  "https://www.allvuesystems.com/news/allvue-private-equity-and-fund-administration-ai-knowledge-agent/": {
    "content_path": "../data/contents/f6c7c6ebfe6a1d5f.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:56.921903",
    "title": "Allvue Redefines Workflow Intelligence in Private Equity and Fund Administration with AI Knowledge Agent"
  },
  "https://www.pinecone.io/community/events/getting-started-2025-12/": {
    "content_path": "../data/contents/9cda7b7e31797d19.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:56.957369",
    "title": "Getting started with Pinecone (December 2025) | Pinecone"
  },
  "https://techxplore.com/news/2025-12-ai-agents-debate-mathematical.html": {
    "content_path": "../data/contents/6b090a17c5d3afbf.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:57.070410",
    "title": "AI agents debate their way to improved mathematical reasoning"
  },
  "https://kpmg.com/us/en/frv/reference-library/2024/guide-ai-and-automation-in-financial-reporting.html": {
    "content_path": "../data/contents/081b24106cc73249.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:57.107023",
    "title": "Guide: AI and automation in financial reporting"
  },
  "https://qdrant.tech/blog/qdrant-1.16.x/": {
    "content_path": "../data/contents/4a20e7268cc235a5.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:57.213076",
    "title": "Qdrant 1.16 - Tiered Multitenancy & Disk-Efficient Vector Search"
  },
  "https://www.lseg.com/en/media-centre/press-releases/2025/lseg-and-rogo-announce-strategic-partnership": {
    "content_path": "../data/contents/15a167fd03c2548e.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:57.248200",
    "title": "LSEG and Rogo Announce Strategic Partnership"
  },
  "https://cloud.google.com/blog/products/ai-machine-learning/gemini-2-5-pro-flash-on-vertex-ai": {
    "content_path": "../data/contents/97546edf44026030.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:57.328927",
    "title": "Gemini 2.5 on Vertex AI: Pro, Flash & Model Optimizer Live"
  },
  "https://www.accenture.com/us-en/blogs/strategy/ai-redefining-private-equity": {
    "content_path": "../data/contents/4bad3514a9560f59.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:57.429526",
    "title": "Agentic AI Is Redefining Private Equity in 2026"
  },
  "https://qdrant.tech/blog/2025-recap/": {
    "content_path": "../data/contents/0140276e64518d1b.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:57.781685",
    "title": "Qdrant 2025 Recap: Powering the Agentic Era"
  },
  "https://www.oracle.com/database/ai-native-database-26ai/": {
    "content_path": "../data/contents/b0878a6e3da23322.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:58.156705",
    "title": "Oracle introduces its AI-native database, Oracle AI Database 26ai"
  },
  "https://www.mongodb.com/products/updates/public-preview-mongodb-community-edition-now-offers-native-full-text-and-vector-search/": {
    "content_path": "../data/contents/f01a2a27a36398e4.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:58.746905",
    "title": "New in MongoDB | MongoDB"
  },
  "https://research.ibm.com/publications/docling-an-efficient-open-source-toolkit-for-ai-driven-document-conversion": {
    "content_path": "../data/contents/202093b269adc767.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:58.797868",
    "title": "Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion for AAAI 2025"
  },
  "https://www.calcalistech.com/ctechnews/article/bktsdpzole": {
    "content_path": "../data/contents/e2b49cbe3a0c4b96.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:58.830277",
    "title": "How AI became the ultimate partner for venture capitalists"
  },
  "https://www.marketplace.spglobal.com/en/solutions/document-intelligence-for-salesforce-agentforce-%283eba81d8-91b9-42e3-b2ef-e7887dbe8781%29": {
    "content_path": "../data/contents/fc1d6d3a6d1775f7.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:39:58.913826",
    "title": "Document Intelligence for Salesforce Agentforce Solution | S&P Global Marketplace"
  },
  "https://huggingface.co/nvidia/llama-embed-nemotron-8b": {
    "content_path": "../data/contents/6faea6582bd03105.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:40:01.558230",
    "title": "nvidia/llama-embed-nemotron-8b · Hugging Face"
  },
  "https://techcrunch.com/2025/12/10/google-is-going-all-in-on-mcp-servers-agent-ready-by-design/": {
    "content_path": "../data/contents/86cf18155b9252b4.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:40:03.863736",
    "title": "Google launches managed MCP servers that let AI agents simply plug into its tools"
  },
  "https://www.cloudthat.com/resources/blog/transforming-enterprise-search-with-cohere-embed-4-multimodal-embeddings": {
    "content_path": "../data/contents/e2e58e9c5e95c684.md",
    "source": "jina",
    "saved_at": "2025-12-28T13:40:06.390038",
    "title": "Transforming Enterprise Search with Cohere Embed 4 Multimodal Embeddings"
  },
  "https://github.com/mem0ai/mem0": {
    "content_path": "../data/contents/cddc9a2d075a5103.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:29.487462",
    "title": "GitHub - mem0ai/mem0: Universal memory layer for AI Agents"
  },
  "https://www.itpro.com/software/development/openais-skills-in-codex-service-aims-to-supercharge-agent-efficiency-for-developers": {
    "content_path": "../data/contents/dc4db569fb0f0dc8.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:29.538282",
    "title": "OpenAI's 'Skills in Codex' service aims to supercharge agent efficiency for developers"
  },
  "https://huggingface.co/Qwen/Qwen3-Embedding-8B": {
    "content_path": "../data/contents/fc4f46b5b34797ae.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:29.570856",
    "title": "Qwen/Qwen3-Embedding-8B · Hugging Face"
  },
  "https://simonwillison.net/": {
    "content_path": "../data/contents/8b0eaafedc559312.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:30.418684",
    "title": "Simon Willison’s Weblog"
  },
  "https://www.llamaindex.ai/blog/rag-is-dead-long-live-agentic-retrieval": {
    "content_path": "../data/contents/6835c35b4b36cc9e.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:30.469509",
    "title": "RAG is dead, long live agentic retrieval"
  },
  "https://venturebeat.com/ai/coheres-rerank-4-quadruples-the-context-window-to-cut-agent-errors-and-boost": {
    "content_path": "../data/contents/7fe1ec531f54dd34.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:30.503118",
    "title": "Cohere’s Rerank 4 quadruples the context window over 3.5 to cut agent errors and boost enterprise search accuracy"
  },
  "https://pypi.org/project/claude-agent-sdk/": {
    "content_path": "../data/contents/8d4640c611372f77.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:30.535615",
    "title": "claude-agent-sdk"
  },
  "https://aclanthology.org/2025.emnlp-main.532/": {
    "content_path": "../data/contents/2d69823d20c3ee47.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:30.562170",
    "title": "DSG-MCTS: A Dynamic Strategy-Guided Monte Carlo Tree Search for Diversified Reasoning in Large Language Models"
  },
  "https://pmc.ncbi.nlm.nih.gov/articles/PMC12546433/": {
    "content_path": "../data/contents/1c37d84efd1b563d.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:30.593293",
    "title": "Toward large reasoning models: A survey of reinforced reasoning with large language models"
  },
  "https://github.com/opendatalab/OmniDocBench": {
    "content_path": "../data/contents/c099fb6149f5b631.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:30.860244",
    "title": "GitHub - opendatalab/OmniDocBench: [CVPR 2025] A Comprehensive Benchmark for Document Parsing and Evaluation"
  },
  "https://nbrosse.github.io/posts/pdf-parsing/pdf-parsing.html": {
    "content_path": "../data/contents/c13c390dc59bcd1f.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:31.460236",
    "title": "PDF Parsing for LLM Input – Nicolas’ Notebook"
  },
  "https://huggingface.co/learn/cookbook/en/search_and_learn": {
    "content_path": "../data/contents/ec10051c3d6f07ee.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:31.510036",
    "title": "Scaling Test-Time Compute for Longer Thinking in LLMs - Hugging Face Open-Source AI Cookbook"
  },
  "https://research.google/blog/effective-large-language-model-adaptation-for-improved-grounding/": {
    "content_path": "../data/contents/7a500bdd3cc655e3.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:32.055800",
    "title": "Effective large language model adaptation for improved grounding"
  },
  "https://www.toltiq.com": {
    "content_path": "../data/contents/db2a28f4c4d8adba.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:32.105746",
    "title": "ToltIQ - AI for Private Equity Due Diligence"
  },
  "https://developers.googleblog.com/introducing-agent-development-kit-for-typescript-build-ai-agents-with-the-power-of-a-code-first-approach/": {
    "content_path": "../data/contents/c728d73f18b7e001.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:32.136115",
    "title": "Introducing Agent Development Kit for TypeScript: Build AI Agents with the Power of a Code-First Approach"
  },
  "https://www.sciencedirect.com/science/article/abs/pii/S156625352500867X": {
    "content_path": "../data/contents/4962e1e998bb72fc.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:32.358421",
    "title": "Scaling down, Powering up: A Survey on the Advancements of Small Vision-Language Models"
  },
  "https://blog.google/products/gemini/gemini-3-flash/": {
    "content_path": "../data/contents/a38c58274a67a650.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:32.406462",
    "title": "Gemini 3 Flash: frontier intelligence built for speed"
  },
  "https://docs.cohere.com/changelog/embed-multimodal-v4": {
    "content_path": "../data/contents/e119e376e4a51380.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:32.488728",
    "title": "Announcing Embed Multimodal v4 | Cohere"
  },
  "https://github.com/GaurangSriramanan/LLM_Check_Hallucination_Detection": {
    "content_path": "../data/contents/4d951618bb29ac4f.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:32.722582",
    "title": "GitHub - GaurangSriramanan/LLM_Check_Hallucination_Detection: LLM-Check: Investigating Detection of Hallucinations in Large Language Models (NeurIPS 2024)"
  },
  "https://github.com/iamarunbrahma/vision-parse": {
    "content_path": "../data/contents/9c0025737fff787d.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:32.760725",
    "title": "GitHub - iamarunbrahma/vision-parse: Parse PDFs into markdown using Vision LLMs"
  },
  "https://openreview.net/forum?id=ungnJ4O0AD": {
    "content_path": "../data/contents/75b7a69c805133f2.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:32.905612",
    "title": "ReflCtrl: Controlling LLM Reflection via Representation Engineering"
  },
  "https://www.bloomberg.com/company/stories/closing-the-agentic-ai-productionization-gap-bloomberg-embraces-mcp/": {
    "content_path": "../data/contents/55a6b4836655fc7f.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:33.016139",
    "title": "Closing the Agentic AI productionization gap: Bloomberg embraces MCP | Bloomberg LP"
  },
  "https://www.lseg.com/en/media-centre/press-releases/2025/lseg-announces-collaboration-with-anthropic": {
    "content_path": "../data/contents/ca7bc6006aed425f.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:33.130763",
    "title": "LSEG and Anthropic collaborate to make more financial data accessible to Claude for Enterprise customers"
  },
  "https://blog.voyageai.com/2025/01/07/voyage-3-large/": {
    "content_path": "../data/contents/4a27aaf7a0aff120.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:33.172644",
    "title": "voyage-3-large: the new state-of-the-art general-purpose embedding model"
  },
  "https://www.pwc.com/us/en/services/audit-assurance/library/ai-agents-for-finance-and-reporting.html": {
    "content_path": "../data/contents/9ef7686825b51738.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:33.257695",
    "title": "How AI agents are transforming finance and reporting"
  },
  "https://www.bloomberg.com/company/stories/bloombergs-ai-engineers-introduce-an-improved-agent-tool-calling-methodology-acl-2025/": {
    "content_path": "../data/contents/ae285e698d955555.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:33.347629",
    "title": "Bloomberg's AI engineers introduce an improved agent tool-calling methodology at ACL 2025 | Bloomberg LP"
  },
  "https://huggingface.co/vec-ai/lychee-rerank": {
    "content_path": "../data/contents/6227e46a0066a0e9.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:33.380190",
    "title": "vec-ai/lychee-rerank · Hugging Face"
  },
  "https://github.com/openai/openai-agents-python": {
    "content_path": "../data/contents/82fcf33f253d184e.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:33.439793",
    "title": "GitHub - openai/openai-agents-python: A lightweight, powerful framework for multi-agent workflows"
  },
  "https://opensearch.org/announcements/opensearch-3-0-enhances-vector-database-performance/": {
    "content_path": "../data/contents/d4a6f59e2563ec2f.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:33.744380",
    "title": "OpenSearch 3.0 Enhances Vector Database Performance, Search Infrastructure and Scalability to Meet AI-driven Demand"
  },
  "https://github.com/block/goose": {
    "content_path": "../data/contents/1e4478d8d92169d8.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:33.791913",
    "title": "GitHub - block/goose: an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM"
  },
  "https://huggingface.co/ds4sd/SmolDocling-256M-preview": {
    "content_path": "../data/contents/19fe887b6f61b689.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:34.025014",
    "title": "docling-project/SmolDocling-256M-preview · Hugging Face"
  },
  "https://huggingface.co/abdoelsayed/dear-8b-reranker-ce-v1": {
    "content_path": "../data/contents/cd7c69c688ca33fc.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:34.080779",
    "title": "abdoelsayed/dear-8b-reranker-ce-v1 · Hugging Face"
  },
  "https://www.llamaindex.ai/blog/llamaagents-build-serve-and-deploy-document-agents": {
    "content_path": "../data/contents/e5e542a9e2216829.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:34.118874",
    "title": "Announcing LlamaAgents Open Preview: Build, Serve & Deploy Document Agents"
  },
  "https://press.spglobal.com/2025-10-22-S-P-Global-Redefines-Financial-Insights-with-New-AI-Powered-Multi-Document-Research-and-Analysis-Tool-in-Capital-IQ-Pro-ChatIQ": {
    "content_path": "../data/contents/98c70c1668aa984c.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:34.429339",
    "title": "S&P Global Redefines Financial Insights with New AI-Powered Multi-Document Research and Analysis Tool in Capital IQ Pro ChatIQ"
  },
  "https://www.prnewswire.com/news-releases/alphasense-ranks-in-the-top-10-of-the-2025-cnbc-disruptor-50-list-highlighting-value-of-trusted-generative-ai-for-business-workflows-302476723.html": {
    "content_path": "../data/contents/5b68aab94bf3dd63.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:34.824407",
    "title": "AlphaSense Ranks in the Top 10 of the 2025 CNBC Disruptor 50 List, Highlighting Value of Trusted Generative AI for Business Workflows"
  },
  "https://www.alpha-sense.com/press/alphasense-launches-autonomous-ai-agent-interviewer-debuts-channel-checks-to-deliver-real-time-market-signals-across-all-sectors-of-the-economy/": {
    "content_path": "../data/contents/c60379facb1c3893.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:34.870732",
    "title": "AlphaSense Launches Autonomous AI Agent Interviewer"
  },
  "https://github.com/TauricResearch/TradingAgents": {
    "content_path": "../data/contents/626a5e4fcb233d8d.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:35.062809",
    "title": "GitHub - TauricResearch/TradingAgents: TradingAgents: Multi-Agents LLM Financial Trading Framework"
  },
  "https://www.vectara.com/blog/open-rag-benchmark-a-new-frontier-for-multimodal-pdf-understanding-in-rag": {
    "content_path": "../data/contents/b13d1c654ba9eab0.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:35.240191",
    "title": "Open RAG Benchmark: A New Frontier for Multimodal PDF Understanding in RAG"
  },
  "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5285532": {
    "content_path": "../data/contents/49a127dff35774a2.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:35.455472",
    "title": "Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting"
  },
  "https://github.com/ThreeSR/Awesome-Inference-Time-Scaling": {
    "content_path": "../data/contents/270246a4092e357d.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:35.913816",
    "title": "GitHub - ThreeSR/Awesome-Inference-Time-Scaling: Paper List of Inference/Test Time Scaling/Computing"
  },
  "https://pitchbook.com/media/press-releases/pitchbook-launches-new-generative-ai-experiences-with-the-introduction-of-pitchbook-navigator-and-upcoming-integration-with-openai": {
    "content_path": "../data/contents/6802b48e91c08778.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:36.233714",
    "title": "PitchBook Launches New Generative AI Experiences with the Introduction of PitchBook Navigator and Upcoming Integration with OpenAI | PitchBook"
  },
  "https://openai.com/index/agentic-ai-foundation/": {
    "content_path": "../data/contents/e76333c4e7576bff.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:36.695413",
    "title": "OpenAI co-founds the Agentic AI Foundation under the Linux Foundation"
  },
  "https://www.mongodb.com/company/blog/innovation/reimagining-investment-portfolio-management-with-agentic-ai": {
    "content_path": "../data/contents/e2fe36adf63d4a32.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:37.173431",
    "title": "Reimagining Investment Portfolio Management with Agentic AI"
  },
  "https://govclab.com/2025/04/12/ai-for-vc/": {
    "content_path": "../data/contents/77c01c14097d0970.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:38.070292",
    "title": "AI for Venture Capital (VC) - VC Lab"
  },
  "https://www.fnlondon.com/articles/blackrock-and-lseg-extend-partnership-with-addition-of-preqin-private-markets-data-ccd29571": {
    "content_path": "../data/contents/e50dca3b2e0d9440.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:38.899851",
    "title": "BlackRock and LSEG extend partnership with addition of Preqin private markets data"
  },
  "https://pitchbook.com/products/pitchbooks-ai-capabilities": {
    "content_path": "../data/contents/5cdf5e268c632d78.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:38.964640",
    "title": "AI & Machine Learning for Private Markets - PitchBook"
  },
  "https://azure.microsoft.com/en-us/blog/introducing-microsoft-agent-framework/": {
    "content_path": "../data/contents/813b55fe2f04b10a.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:40.149603",
    "title": "Introducing Microsoft Agent Framework"
  },
  "https://pitchbook.com/media/press-releases/pitchbook-announces-llm-partnerships-with-anthropic-perplexity-rogo-and-hebbia-to-expand-access-to-private-capital-market-data": {
    "content_path": "../data/contents/14cdad4e6203501b.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:41.777785",
    "title": "PitchBook Announces LLM Partnerships with Anthropic, Perplexity, Rogo and Hebbia to Expand Access to Private Capital Market Data | PitchBook"
  },
  "https://www.elastic.co/what-is/hybrid-search": {
    "content_path": "../data/contents/ceedece305bcc5ec.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:41.831592",
    "title": "A Comprehensive Hybrid Search Guide"
  },
  "https://www.fnlondon.com/articles/bnp-paribas-rolls-out-ai-portal-to-cut-pitch-time-for-dealmakers-8453517a": {
    "content_path": "../data/contents/e088990b84a49e3e.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:42.666080",
    "title": "BNP Paribas rolls out AI portal to cut pitch time for dealmakers"
  },
  "https://www.salesforce.com/news/stories/agentforce-for-financial-services-announcement/": {
    "content_path": "../data/contents/1f0c19ac4bef89c3.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:22:59.835935",
    "title": "Salesforce Introduces Agentforce for Financial Services to Address Shrinking Workforces and Rising Client Expectations"
  },
  "https://pinecone.io/learn/series/rag/rerankers/": {
    "content_path": "../data/contents/c0cf95547f06bae8.md",
    "source": "jina",
    "saved_at": "2025-12-28T14:23:03.418529",
    "title": "Rerankers and Two-Stage Retrieval | Pinecone"
  },
  "https://arxiv.org/html/2508.00554v2": {
    "content_path": "data/contents/e6f7bfce255d6bd1.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.958409",
    "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism",
    "summary": "The webpage describes **ContestTrade**, a novel multi-agent trading system inspired by corporate management structures, designed to enhance robustness against market noise in financial trading.\n\n**Key features relevant to your query:**\n\n*   **Multi-Agent System for Finance:** ContestTrade is explicitly a multi-agent system for financial trading.\n*   **Architecture:** It uses a two-team structure:\n    *   **Data Team:** Processes raw market data into concise textual factors suitable for LLMs.\n    *   **Research Team:** Uses these factors, along with specialized financial tools, to generate parallelized trading decisions.\n*   **Internal Contest Mechanism:** The core innovation is a real-time evaluation and ranking mechanism within each team, ensuring only outputs from top-performing agents are adopted, leading to self-optimization.\n*   **Financial Tool Integration (Deep Research):** Research Agents utilize a suite of specialized financial tools for in-depth analysis, including:\n    *   Web Search\n    *   Financial News\n    *   Corporate Announcements\n    *   Market Data (prices, technical indicators)\n    *   Financial Statement retrieval\n    *   Stock Analysis (Pattern, Price, Financial, Correlation, News)\n    *   Stock Screener\n\n**Information NOT explicitly mentioned in the context of ContestTrade:**\n\nThe text **does not mention** specific integrations with **PitchBook, AlphaSense, Preqin, or CapIQ**. While it mentions using \"corporate financial statements\" and \"diverse data sources,\" it does not list these specific commercial data providers as integrated tools. The system focuses on general data sources, web search, and market data access.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://hbr.org/2025/11/how-generative-ai-is-reshaping-venture-capital": {
    "content_path": "../data/contents/08bfc189d0154a8f.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.296422",
    "title": "How Generative AI Is Reshaping Venture Capital",
    "summary": "Generative AI is rapidly reshaping the venture capital world, fundamentally altering how startups are conceived, pitched, built, and funded. Global private AI investment reached a record high of $252.3 billion in 2024, with generative AI funding specifically soaring to $33.9 billion.\n\nThe summary does not mention multi-agent systems, private equity, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, or specific integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": "2025-11-18T13:25:17.000Z",
    "score": null
  },
  "https://www.glean.com/blog/ai-for-private-equity-and-venture-capital": {
    "content_path": "../data/contents/3646b83ff5b75512.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.345523",
    "title": "How leading PE and VC firms are using AI to unlock value faster",
    "summary": "The webpage discusses how leading Private Equity (PE) and Venture Capital (VC) firms are using AI to unlock value faster across the investment lifecycle.\n\nKey areas where AI is delivering impact include:\n*   **Faster deal execution:** Summarizing diligence documents, analyzing prior investments, and auto-drafting DDQ responses.\n*   **Smarter sourcing:** Identifying high-potential opportunities by scanning public and internal data.\n*   **Stronger portfolio oversight:** Gaining real-time insight into financial performance and operational risks for proactive support.\n*   **More responsive investor relations:** Centralizing institutional memory to quickly surface deal notes and performance data for LP communications.\n\nThe article highlights **Glean** as an enterprise-ready Work AI platform that connects to the systems PE/VC firms use (like CRMs, shared drives, and analytics platforms) to enable knowledge retrieval, streamline due diligence, and enhance portfolio management. Glean agents are noted to perform over 100 million actions per year, leading to measurable outcomes like time savings and reduced internal support requests.\n\n**Regarding the specific terms in your query:**\n\n*   **Multi-agent systems for finance, private equity, venture capital:** The page extensively covers the use of AI/agents in PE and VC.\n*   **Due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring:** These are explicitly mentioned as key areas where AI is transforming workflows.\n*   **Agents for data analysis, financial report generation:** The text mentions AI summarizing data, analyzing financials, and generating first drafts of investor letters/updates.\n*   **PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations:** **No specific mention** of integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg is found in the provided text. The text generally states Glean connects to the systems firms rely on, but does not list these specific financial data providers.",
    "published_date": "2025-06-17T00:00:00.000Z",
    "score": null
  },
  "https://www.pymnts.com/artificial-intelligence-2/2025/private-equity-turns-to-ai-as-a-new-value-engine/": {
    "content_path": "../data/contents/e1f980ab04fff9f6.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.372869",
    "title": "Private Equity Turns to AI as a New Value Engine",
    "summary": "The webpage discusses how **Private Equity (PE) firms are increasingly adopting Artificial Intelligence (AI)** as a core component for value creation, especially as deal cycles tighten.\n\nKey applications mentioned include:\n*   **Deal Sourcing and Due Diligence:** AI is being used to compress diligence timelines, standardize early analysis, and automate tasks. Examples include firms using AI agents to review Confidential Information Memorandums (CIMs) in minutes instead of hours, and internal platforms that generate company analyses in 15 minutes instead of weeks.\n*   **Portfolio Monitoring:** Oversight is shifting to continuous, data-driven cycles, using AI to run real-time simulations and spot operational issues (like customer churn or margin pressure) earlier than traditional quarterly financials allow.\n*   **Specific Tools/Implementations:** Firms are building AI stacks using tools like **ChatGPT Enterprise**, **Microsoft Copilot**, and **Blueflame AI**, alongside custom internal agents.\n\nThe text emphasizes that while AI is becoming an operational requirement, successful implementation still depends on human supervision, clear data, and rethinking processes, rather than just \"plug-and-play\" solutions. It also notes compliance friction related to security and model risk assessments.\n\n**Regarding your specific query keywords:**\nThe page confirms the use of **agents** (internal AI agents), **due diligence automation**, **investment opportunity analysis** (implied in deal sourcing), and **portfolio monitoring**. It discusses **data analysis** and **financial report generation** (implied through analysis and forecasting). However, it **does not mention** **finance**, **private equity**, **venture capital**, **deal sourcing**, **PitchBook**, **AlphaSense**, **Preqin**, **CapIQ**, or **Bloomberg integrations** specifically by name, although the entire context is within private equity finance.",
    "published_date": "2025-12-04T19:29:08.000Z",
    "score": null
  },
  "https://www.ey.com/en_ch/insights/strategy-transactions/ai-in-private-equity": {
    "content_path": "../data/contents/d58fb1e2c2b6e699.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.398328",
    "title": "How AI is sustainably transforming value creation in private equity",
    "summary": "The webpage discusses how Artificial Intelligence (AI) is transforming value creation in private equity (PE), establishing itself as a third pillar alongside financial engineering and operational excellence.\n\nRegarding your specific query about **agents and finance** applications:\n\n*   **Due Diligence Automation:** AI-based tools are used to accelerate the evaluation of large data volumes in due diligence through automated analysis of financial data, contracts, and market information. EY itself employs AI-driven tools for the automated review and analysis of large volumes of documents during due diligence to help PE firms identify risks quickly.\n*   **Investment Opportunity Analysis/Deal Sourcing:** AI enhances speed, precision, and competitiveness throughout the deal process. Examples include EQT using its platform \"Motherbrain\" to automate and analyze public and web data sources to identify potential targets, and Blackstone using an internal AI platform for pipeline screenings.\n*   **Portfolio Monitoring:** AI is used for dynamic KPI dashboards that enable real-time representation of key figures like IRR, multiple, and valuation changes.\n*   **Data Analysis and Financial Report Generation:** AI evaluates immense data volumes from various sources (financial reports, media, market data) in real-time for faster investment decisions. Generative AI tools are used to automatically create standardized and individual reports with commentary analyses.\n*   **Integrations (PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg):** The text mentions that AI evaluates data from various sources, including market data, and specifically references **Preqin** data regarding global buyout dry powder levels. It does not explicitly list integrations with PitchBook, AlphaSense, CapIQ, or Bloomberg, but the general capability to analyze vast external data sources is highlighted.\n\nIn summary, the page confirms the use of AI for many functions related to finance, due diligence, deal sourcing, portfolio monitoring, data analysis, and report generation within the PE context, but does not detail the use of \"multi-agent systems\" specifically, nor does it confirm integrations with all the listed third-party platforms.",
    "published_date": "2025-10-23T18:45:24.000Z",
    "score": null
  },
  "https://www.researchgate.net/publication/390948235_Advanced_Artificial_Intelligence_in_Private_Equity_and_Venture_Capital_A_Functional_Framework_for_Lifecycle_Transformation": {
    "content_path": "../data/contents/8b0a2691cffdebb4.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.423397",
    "title": "(PDF) Advanced Artificial Intelligence in Private Equity and Venture Capital A Functional Framework for Lifecycle Transformation",
    "summary": "The article discusses the transformative role of advanced Artificial Intelligence (AI) in Private Equity (PE) and Venture Capital (VC), covering the entire investment lifecycle.\n\nRegarding your specific query about **agents and finance**, the text explicitly mentions:\n\n*   **Agentic AI and Multi-Agent Systems (MAS):** These are detailed as core AI modalities being integrated into PE/VC workflows.\n*   **Use Cases:** Agentic AI is used for **autonomous task execution and orchestration**, including **deal scanning, KPI monitoring, and report generation**.\n*   **Specific Examples:**\n    *   **Blackstone** has integrated agentic workflows to automate performance alerts across portfolio companies.\n    *   Frameworks like **LangGraph and AutoGen** are being tested to support agent collaboration in **financial modeling, LP reporting, and scenario testing**.\n    *   In **Portfolio Management and Monitoring**, AI agents are deployed for real-time oversight.\n    *   In **Deal Sourcing**, agentic systems autonomously scan databases and compile investment briefs.\n\nThe article also covers AI applications in areas related to finance, such as:\n\n*   **Due Diligence Automation:** Agentic systems automatically scan data rooms and generate risk reports.\n*   **Investment Opportunity Analysis:** Covered under Deal Sourcing using predictive models and agentic tools.\n*   **Financial Report Generation:** LLMs are used to draft investor reports and communications, and agentic systems handle report generation.\n*   **Portfolio Monitoring:** AI agents provide real-time oversight.\n\nHowever, the text **does not explicitly mention** specific integrations with commercial platforms like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**. It focuses on the underlying AI technologies (LLMs, GNNs, RL, Agentic AI) and general data sources (news, databases, document repositories).\n\nThe user query asks for a summary of how **multi-agent systems (MAS)** and related AI technologies are used in **finance**, specifically covering **private equity (PE)** and **venture capital (VC)** for tasks like due diligence, deal sourcing, portfolio monitoring, data analysis, and integration with platforms like PitchBook, AlphaSense, Preqin, CapIQ, and Bloomberg.\n\nHere is a summary based on the provided text:\n\n**Multi-Agent Systems (MAS) and Agentic AI in PE/VC:**\n\n*   **Portfolio Management and Monitoring:** AI agents are explicitly mentioned as being assigned to specific portfolio companies or functions (e.g., cash flow tracking, compliance monitoring, hiring alerts). These agents continuously track Key Performance Indicators (KPIs), identify variances, and recommend actions or flag potential issues.\n*   **Due Diligence:** While the text focuses more on GNNs and RL for complex diligence scenarios, the general concept of agentic AI supports workflow automation, which is crucial in diligence.\n*   **Valuation and Financial Modeling:** Agentic AI systems automate data gathering from financial dashboards, capital statements, and third-party APIs, compiling this data into structured valuation models on a recurring cadence.\n*   **Investor Relations (IR) and Fundraising:** Agentic tools automate workflows such as document retrieval, pitch customization, and scheduled outreach based on triggers or LP segmentation.\n*   **Market and Sector Analysis:** Autonomous agents are deployed to monitor feeds from government bodies, research institutions, and global media outlets, continuously summarizing relevant signals for investment committees or sector teams.\n\n**Other Relevant AI Technologies Mentioned:**\n\n*   **Large Language Models (LLMs):** Used for narrative summarization (CEO reports, fund memos), drafting commentary, summarizing market research, and generating DDQ responses in IR.\n*   **Graph Neural Networks (GNNs):** Used for relationship mapping, such as identifying systemic supplier risk across a portfolio, modeling LP networks (influence paths), and assessing relational value in valuation.\n*   **Reinforcement Learning (RL):** Used to optimize resource allocation across a portfolio (directing operating partners or capital) and simulate optimal pricing paths for exits.\n*   **Neuro-symbolic AI:** Used to ensure auditability and explainability in diligence (pairing inference with rules) and compliance enforcement in valuation (ensuring rules like discount rates are applied consistently).\n\n**Data Integration (Platforms like PitchBook,",
    "published_date": "2025-04-20T00:00:00.000Z",
    "score": null
  },
  "https://www.accenture.com/us-en/blogs/business-functions-blog/private-equity-generative-ai": {
    "content_path": "../data/contents/5981ac38a1830911.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.452623",
    "title": "Unleashing the power of generative AI for private equity",
    "summary": "The webpage discusses the potential of **Generative AI** to revolutionize the **private equity (PE)** investment lifecycle, from fund-raising to exit planning, and for **portfolio companies (PortCos)**.\n\nKey areas mentioned where Generative AI can provide an edge for PE firms include:\n*   **Deal sourcing:** Screening thousands of potential targets and shortlisting them, potentially reducing time spent by 50–60%.\n*   **Due diligence:** Accelerating target screening through assisted research.\n\nThe text also mentions that Generative AI can impact business functions within PortCos such as customer service, knowledge management, sales and marketing, and product management, estimating a **6–7% productivity gain and a 3–4% revenue uplift** for PortCos.\n\nThe article recommends a four-step approach for implementation: **Educate, Explore, Experiment, and Execute.** It also stresses the need to **strengthen the digital core** (data environment), adopt a **business-driven mindset**, take a **people-centric approach**, accelerate **ecosystem innovation**, and level up **responsible AI**.\n\n**Regarding the specific terms in your query:**\n\n*   **Multi-agent systems for finance, private equity, venture capital:** The page focuses on Generative AI's application across the PE investment lifecycle, but does not explicitly mention \"multi-agent systems.\"\n*   **Due diligence automation, investment opportunity analysis, deal sourcing:** These are explicitly covered as areas where Generative AI can assist PE firms.\n*   **Portfolio monitoring:** Not explicitly detailed, though the overall lifecycle is mentioned.\n*   **Agents for data analysis, financial report generation:** \"Automating financial reporting\" is listed as a \"no-regret bet\" use case.\n*   **PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations:** None of these specific data providers or platforms are mentioned in the text.",
    "published_date": "2025-01-15T02:56:35.000Z",
    "score": null
  },
  "https://www.v7labs.com/blog/ai-for-private-equity-venture-capital": {
    "content_path": "../data/contents/77259f92e74ad93d.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.535180",
    "title": "5 Applications of AI in Venture Capital and Private Equity",
    "summary": "The webpage discusses the increasing integration of AI, particularly Generative AI and Large Language Models (LLMs), into Private Equity (PE) and Venture Capital (VC) operations.\n\n**Key applications of AI in PE/VC mentioned include:**\n\n*   **Deal Sourcing and Screening:** LLM-powered tools process thousands of startup pitches and analyze diverse data streams (patent filings, social media) to identify emerging companies and high-potential opportunities.\n*   **Due Diligence Automation:** AI parses complex financial statements, extracts and structures data from documents like investment memorandums (creating searchable knowledge bases), identifies regulatory red flags, and improves the depth and accuracy of analysis. Technologies like **IDP** and **RAG** are key here.\n*   **Operational Task Automation:** Automating back-office functions such as standardizing financial reports across subsidiaries, KPI tracking, invoice processing, and contract reviews, freeing up professionals for strategic work.\n*   **Competitive Intelligence and Market Analysis:** AI platforms digest real-time information from numerous sources (news, social media) to spot market patterns and gauge public perception.\n*   **Daily Task Quick Wins:** Junior associates use AI for tasks like email summarization and web searches, leading to compounding efficiency gains.\n\nThe page also highlights key technologies driving this adoption (**OCR, IDP, LLMs, RAG**) and implementation challenges, such as document complexity, data security (advocating for private deployment options), and the need for domain knowledge customization.\n\n**Regarding the specific terms in your query:**\n\n*   **Multi-agent systems for finance:** The text discusses orchestrating multiple LLM models chained together and automated through AI workflows, which aligns with the concept of multi-agent systems, though it doesn't explicitly use the term \"multi-agent systems.\"\n*   **Due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring:** These are explicitly covered as key use cases.\n*   **Agents for data analysis, financial report generation:** AI tools are used for data extraction, structuring data into CSVs, and generating various financial and operational reports.\n*   **PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations:** **No answer found.** The webpage discusses the *need* for AI to handle vast amounts of data but does not mention specific integrations with these named third-party data providers.",
    "published_date": "2025-01-20T00:00:00.000Z",
    "score": null
  },
  "https://www.pictet.com/global/en/insights/private-equity-and-ai": {
    "content_path": "../data/contents/e53b1f8ce6c74374.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.562744",
    "title": "AI adoption in private equity: insights and challenges",
    "summary": "The webpage discusses the adoption of AI in private equity (PE), based on a survey of PE general partners (GPs). It covers insights into current AI strategy adoption, perceived barriers (data/output quality, privacy, cybersecurity), and expected benefits across various corporate functions like data analysis, AI coding, and business process automation. It also touches upon the impact on portfolio companies and workforce size.\n\nHowever, the user query is highly specific, focusing on **\"agents\\_and\\_finance: Multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations.\"**\n\nWhile the webpage discusses AI adoption in private equity and mentions **data analysis** as a benefiting function, it **does not specifically mention or detail** multi-agent systems, due diligence automation, deal sourcing, portfolio monitoring, or integrations with specific platforms like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.\n\nNo answer found.",
    "published_date": "2025-07-01T20:51:16.000Z",
    "score": null
  },
  "https://hbr.org/2025/06/how-private-equity-firms-are-creating-value-with-ai?ab=HP-hero-latest-1": {
    "content_path": "../data/contents/d376647ff83ed15f.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:36.588931",
    "title": "How Private Equity Firms Are Creating Value with AI",
    "summary": "Private equity (PE) firms are increasingly using AI to rapidly realize value from their investments by improving portfolio company performance before selling them. The page mentions AI and machine learning as key areas of focus for the industry, along with related topics like Generative AI, Technology and analytics, Private equity, Analytics and data science, and Automation.\n\n**However, the provided text does not specifically mention:**\n\n*   Multi-agent systems for finance.\n*   Private equity or venture capital agents.\n*   Due diligence automation.\n*   Investment opportunity analysis or deal sourcing.\n*   Portfolio monitoring.\n*   Agents for data analysis or financial report generation.\n*   Integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.\n\n**No answer found** for the specific details requested in the user query regarding multi-agent systems and specific platform integrations.",
    "published_date": "2025-06-16T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2410.10594v2": {
    "content_path": "../data/contents/dd8d8933b191aea4.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.472173",
    "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents",
    "summary": "The user query asks for a summary related to **multimodal and generation** topics, specifically mentioning: Vision-language models (VLMs), multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.\n\nThe provided webpage describes **VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents**.\n\nHere is a summary of how the webpage content relates to the user query:\n\n**Vision-language models (VLMs) and Multimodal RAG:**\n*   The core of VisRAG is establishing a **VLM-based RAG pipeline**. It replaces the text-based retriever and generator of traditional RAG with VLM-based components (**VisRAG-Ret** and **VisRAG-Gen**).\n*   VisRAG directly embeds the document image using a VLM, bypassing the need for initial text parsing, thus maximizing the retention and utilization of visual and layout information crucial in multi-modality documents.\n\n**Document Understanding and PDF Parsing:**\n*   VisRAG is specifically designed to handle **multi-modality documents** (like textbooks and manuals) where text and images are interleaved, which traditional text-based RAG (TextRAG) struggles with due to reliance on a potentially error-prone **parsing stage** (OCR, layout recognition).\n*   The paper shows that directly encoding the document image with a VLM (VisRAG-Ret) performs better than using text extracted via OCR or even a VLM-based captioner for parsing.\n\n**Report Generation with LLMs (and related models):**\n*   The generation stage (**VisRAG-Gen**) uses VLMs (like MiniCPM-V 2.0, MiniCPM-V 2.6, and GPT-4o) to generate answers based on the retrieved document images.\n*   The results show that VisRAG-Gen significantly outperforms TextRAG-Gen when using the same underlying models (e.g., MiniCPM-V 2.0 or GPT-4o) because it leverages visual clues preserved in the image input.\n\n**Specific Models Mentioned:**\n*   The paper uses **MiniCPM-V 2.0** and **MiniCPM-V 2.6** (which supports multi-image input) and **GPT-\n\nThe webpage describes **VisRAG**, a novel **Retrieval-Augmented Generation (RAG)** paradigm that leverages **Vision-Language Models (VLMs)** for retrieval and generation on **multi-modality documents**, thereby eliminating the need for a separate document parsing stage required in traditional text-based RAG.\n\nThe user query covers several related topics: 'multimodal\\_and\\_generation: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, structured document output'.\n\nHere is how the page addresses the query components:\n\n*   **Vision-language models (VLMs) & Multimodal RAG:** VisRAG is explicitly proposed as a RAG paradigm utilizing VLMs for retrieval and generation on multi-modality documents.\n*   **Document understanding:** VisRAG is designed to handle multi-modal documents, suggesting improved document understanding compared to text-only methods.\n*   **PDF parsing / Chart/table extraction / Structured document output:** The paper highlights that VisRAG *eliminates the parsing stage* required in traditional text-based RAG, as it uses VLMs directly on document images. The appendix discusses pipeline-based parsing (Pytesseract, PPOCR) and model-based parsing (using MiniCPM-V 2.0/GPT-4V for descriptions) used for baseline comparisons, indicating an awareness of these tasks, even if VisRAG bypasses them.\n*   **Report generation with LLMs:** VisRAG performs generation based on retrieved multi-modal context.\n*   **GPT-4V, Claude vision, Gemini:** The paper mentions using **GPT-4o** (OpenAI's latest multimodal model) for synthesizing training data and for query filtering. It also references models like **MiniCPM-V 2.0** and **MiniCPM-V 2.6** (which builds upon Qwen2-7B).\n\n**Summary:** The page primarily details **VisRAG**, a multimodal RAG system that uses VLMs to process documents holistically, outperforming text-based RAG. It discusses the use of **GPT-4o** for data synthesis and mentions various document processing techniques (like OCR and model-based captioning) used for baselines, aligning well with the user's interest in multimodal RAG and V",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://www.analyticsvidhya.com/blog/2025/05/vision-rag/": {
    "content_path": "../data/contents/0849fb233ac74858.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.721374",
    "title": "What are Vision RAG Models?",
    "summary": "Vision RAG (Retrieval-Augmented Generation) extends traditional RAG systems to process both **textual and visual data** (like images and charts in PDFs) using **Vision Language Models (VLMs)** for indexing, retrieval, and processing.\n\nKey aspects mentioned include:\n\n*   **Multimodal Retrieval and Generation:** Ability to respond to questions about images and tables, not just text.\n*   **Direct Visual Embedding:** Uses VLMs to embed visual data, preserving semantic relationships better than traditional OCR.\n*   **Unified Search:** Enables meaningful search across mixed-modality content in a single vector space.\n*   **Implementation Example:** The article details **localGPT-Vision**, an end-to-end vision-based RAG system that supports VLMs like GPT-4o, Gemini, and various Llama models, working directly with visual document data without relying on OCR.\n*   **Applications:** Include medical imaging analysis, document search, customer support using submitted photos, and education.\n\nThe page focuses specifically on **Vision RAG Models** and the **localGPT-Vision** implementation, which utilizes VLMs to achieve multimodal understanding.",
    "published_date": "2025-05-16T17:20:58.000Z",
    "score": null
  },
  "https://github.com/deep-div/Multimodel-RAG": {
    "content_path": "../data/contents/656ad396afcdf2d9.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.107008",
    "title": "Search code, repositories, users, issues, pull requests...",
    "summary": "The webpage describes a GitHub repository named **Multimodel-RAG** by `deep-div`. This project implements a **Multimodal Retrieval-Augmented Generation (RAG)** system specifically designed to ingest **PDF documents** and generate outputs containing both **text and images**.\n\nKey aspects mentioned include:\n*   **Functionality:** Ingests PDFs, processes both textual and visual content, and generates combined text and image outputs by retrieving and grounding relevant information.\n*   **Frameworks Used:** Leverages **LangChain** (for PDF processing architecture) and **LlamaIndex** (using CLIP embeddings for multimodal representation).\n*   **Working Examples:** Provides a main implementation notebook and tutorials demonstrating independent implementations using LangChain and LlamaIndex separately.\n\nWhile the repository focuses on **Multimodal RAG**, **PDF parsing**, and **document understanding** (ingesting PDFs to generate text/image outputs), it does not explicitly detail the use of specific large models like GPT-4V, Claude vision, or Gemini, nor does it cover general report generation with LLMs or structured document output beyond the text/image generation from PDFs.",
    "published_date": "2025-05-24T09:59:49.000Z",
    "score": null
  },
  "https://reducto.ai/blog/reducto-chart-extraction": {
    "content_path": "../data/contents/9637f43ed1561b67.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.749870",
    "title": "How Reducto is building SOTA chart extractions",
    "summary": "Reducto is developing a state-of-the-art (SOTA) chart extraction method focused on achieving high numerical accuracy when extracting data from charts (like line, area, bar, and scatter plots) found in unstructured documents such as financial reports and academic papers.\n\nThe core challenge is that while current Vision Question Answering (VQA) models process charts individually and don't expose underlying data, and multimodal-embedding pipelines (like RAG) are better for search but still lack fine-grained accuracy, enterprises need structured, auditable chart data for downstream analytics and knowledge base construction.\n\nReducto's final approach involves training **multiple specialized, lightweight models** to interpret specific chart components (axes, legends, data series, etc.) to ensure point-level accuracy. This structured text output allows Large Language Models (LLMs) to operate over the data with higher throughput and better reasoning than relying solely on slower, less mature Vision-Language Models (VLMs).\n\nWhen compared against frontier models (like GPT 5.1, Gemini 3.0 Pro, LlamaParse), Reducto demonstrated superior accuracy, especially on complex charts like dense line charts and stacked bar charts, where other models often missed data points, struggled with overlaps, or provided only approximate values.",
    "published_date": "2025-12-03T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2410.10594": {
    "content_path": "../data/contents/fd0436bc523566c6.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.884945",
    "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents",
    "summary": "The webpage describes **VisRAG**, a novel **Vision-based Retrieval-augmented Generation (RAG)** pipeline designed for **multi-modality documents**.\n\nKey points related to your query:\n*   **Vision-language models (VLMs):** VisRAG establishes a VLM-based RAG pipeline.\n*   **Multimodal RAG:** It directly embeds the document as an image using a VLM for retrieval, unlike traditional text-based RAG.\n*   **Information Utilization:** This approach maximizes the retention and utilization of vision information (like layout and images) crucial in real-world documents, eliminating information loss from initial parsing.\n*   **Performance:** Experiments show VisRAG outperforms traditional RAG, achieving a 20–40% end-to-end performance gain.\n\nThe page focuses on the **VisRAG framework** for multimodal RAG using VLMs, but it does not specifically detail the use of GPT-4V, Claude vision, Gemini, document understanding, PDF parsing, chart/table extraction, or report generation with LLMs, although these topics are related to the general area of multimodal document processing that VisRAG addresses.",
    "published_date": "2024-10-14T00:00:00.000Z",
    "score": null
  },
  "https://openreview.net/forum?id=zG459X3Xge": {
    "content_path": "../data/contents/a621f1628d182bd8.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.920265",
    "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents | OpenReview",
    "summary": "The webpage describes **VisRAG**, a novel **Vision-based Retrieval-augmented Generation (RAG)** pipeline designed for **multi-modality documents**.\n\nUnlike traditional text-based RAG, VisRAG utilizes a **Vision-Language Model (VLM)** to directly embed the document as an image, maximizing the retention and utilization of visual information (like layout and images) that is lost during text parsing. This VLM-based RAG pipeline enhances the generation capabilities of the VLM. Experiments show that VisRAG outperforms traditional RAG, achieving a 20–40% end-to-end performance gain.\n\nThis directly relates to your query regarding **Vision-language models**, **multimodal RAG**, and the challenges of **document understanding** where traditional text parsing leads to information loss.",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/abs/2502.16636": {
    "content_path": "../data/contents/accc14a96d752245.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:37.011719",
    "title": "Computer Science > Computation and Language",
    "summary": "The provided webpage describes a paper titled **\"Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries.\"**\n\nThis paper introduces **Visual-RAG**, a new benchmark for evaluating Multimodal Large Language Models (MLLMs) in Retrieval-Augmented Generation (RAG) settings where the retrieval involves **text-to-image retrieval**. It specifically targets visually grounded, knowledge-intensive questions, assessing how models integrate retrieved images for answer generation. The study evaluates several open-source and proprietary MLLMs, concluding that while images provide strong evidence, current state-of-the-art models struggle with efficiently extracting and utilizing visual knowledge.\n\n**In relation to your query:**\n\nThe page directly addresses **Vision-language models** and **multimodal RAG**. It does not explicitly mention document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, or structured document output.\n\n**Summary relevant to your query:** The page focuses on **Visual-RAG**, a benchmark for **Vision-language models** that requires **text-to-image retrieval** and integration of visual evidence for answer generation, highlighting current limitations in multimodal RAG systems.",
    "published_date": "2025-02-23T00:00:00.000Z",
    "score": null
  },
  "https://huggingface.co/papers/2502.08826": {
    "content_path": "../data/contents/c79c8e066b0611e9.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.801854",
    "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation",
    "summary": "The provided webpage is a survey paper titled \"Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation.\"\n\nThe survey discusses:\n*   **Multimodal Retrieval-Augmented Generation (Multimodal RAG)** systems, which integrate multiple modalities (text, images, audio, video) to enhance AI outputs and mitigate LLM hallucinations/outdated knowledge.\n*   Advancements, challenges (like cross-modal alignment and reasoning), methodologies (retrieval, fusion, augmentation, generation), datasets, benchmarks, and evaluation in Multimodal RAG.\n*   Future research directions in this evolving field.\n\n**Regarding your specific query components:**\n\n*   **Vision-language models:** The survey covers multimodal learning, which includes vision-language models, as they are central to Multimodal RAG.\n*   **Multimodal RAG:** This is the primary focus of the entire survey.\n*   **Document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, structured document output:** The text **does not explicitly mention** these specific applications or models (like GPT-4V, Claude vision, Gemini) in detail, although they fall under the broader umbrella of multimodal AI and RAG that the paper surveys. The abstract focuses on the general framework of Multimodal RAG.\n\n**Summary based on the query:**\n\nThe page is a comprehensive survey on **Multimodal Retrieval-Augmented Generation (Multimodal RAG)** systems, detailing methodologies, datasets, and challenges in integrating multiple modalities (including vision) to enhance LLMs. While it covers the core concepts of **vision-language models** and **multimodal RAG**, it **does not specifically detail** document understanding, PDF parsing, chart/table extraction, report generation with LLMs, or mention specific models like GPT-4V, Claude vision, or Gemini.",
    "published_date": "2025-02-18T00:00:00.000Z",
    "score": null
  },
  "https://aclanthology.org/2025.sdp-1.20/": {
    "content_path": "../data/contents/ab9db714653aa27a.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.857470",
    "title": "ExpertNeurons at SciVQA-2025: Retrieval Augmented VQA with Vision Language Model (RAVQA-VLM) - ACL Anthology",
    "summary": "The webpage describes **RAVQA-VLM**, a novel **Retrieval-Augmented Generation (RAG)** architecture that utilizes a **Vision Language Model (VLM)** for the SciVQA challenge. This system is designed to answer closed-ended visual and nonvisual questions over scientific figures.\n\nKey aspects related to your query:\n*   **Vision-language models (VLM):** The system uses a vision-capable generative backbone (e.g., Phi-3.5, Pixtral-12B, Mixtral-24B-small, InterVL-3-14B).\n*   **Multimodal RAG:** It implements a Retrieval-Augmented Generation (RAG) architecture for multimodal scientific QA.\n*   **Document understanding/PDF parsing:** The system retrieves context passages from the full **PDF** of the source paper using a Dense Passage Retriever.\n\nThe page does not explicitly mention \"document understanding,\" \"PDF parsing,\" \"chart/table extraction,\" \"report generation with LLMs,\" \"GPT-4V,\" \"Claude vision,\" \"Gemini,\" or \"structured document output\" beyond the general context of processing scientific figures from PDFs.",
    "published_date": "2025-07-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2506.07785": {
    "content_path": "../data/contents/20f20b382a4c25d4.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.952587",
    "title": "Computer Science > Computer Vision and Pattern Recognition",
    "summary": "The webpage describes a research paper titled \"Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger.\"\n\nThis paper proposes a **multimodal RAG framework** called RCTS to enhance Large Vision Language Models (LVLMs) for Visual Question Answering (VQA). The framework includes:\n1.  A **Reasoning Context-enriched knowledge base** created using a self-consistent evaluation mechanism to capture intrinsic reasoning patterns.\n2.  A **Tree Search re-ranking method** (Monte Carlo Tree Search with Heuristic Rewards or MCTS-HR) to prioritize the most relevant retrieved knowledge examples.\n\nThe goal is to improve LVLMs by ensuring they leverage high-quality contextual reasoning, leading to better and more consistent responses, and achieving state-of-the-art performance on VQA datasets compared to In-Context Learning (ICL) and Vanilla-RAG methods.\n\nWhile the query mentions several topics like document understanding, PDF parsing, chart/table extraction, and report generation, the provided webpage specifically focuses on **Vision-language models** and **multimodal RAG** within the context of improving LVLMs for VQA tasks.",
    "published_date": "2025-06-09T00:00:00.000Z",
    "score": null
  },
  "https://huggingface.co/papers?q=Multimodal+retrieval-augmented+generation": {
    "content_path": "../data/contents/0fdf01746df2f819.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:37.361579",
    "title": "Daily Papers",
    "summary": "The webpage, titled \"Daily Papers,\" lists recently trending research papers. While the user query focuses on specific topics like **Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output**, the listed papers on the page include:\n\n*   Latent Implicit Visual Reasoning\n*   Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning\n*   Spatia: Video Generation with Updatable Spatial Memory\n*   Schoenfeld's Anatomy of Mathematical Reasoning by Language Models\n*   How Much 3D Do Video Foundation Models Encode?\n*   VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation\n*   GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training\n\nAlthough some papers touch upon **Vision** and **Language Models (LLMs)** (e.g., \"Agentic VLM Training,\" \"Mathematical Reasoning by Language Models\"), the page **does not explicitly detail or summarize** the specific areas requested in the query such as multimodal RAG, PDF parsing, chart/table extraction, or specific mentions of GPT-4V, Claude vision, or Gemini.\n\nNo answer found.",
    "published_date": "2025-10-22T00:00:00.000Z",
    "score": null
  },
  "https://docs.langchain.com/oss/python/langchain/mcp": {
    "content_path": "data/contents/1a23dbe21115c3a1.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.798650",
    "title": "Model Context Protocol (MCP)",
    "summary": "The user query lists several concepts related to building and managing AI agents, including infrastructure components, memory, context handling, frameworks, SDKs, function calling, orchestration, evaluation, and training methods.\n\nThe provided webpage focuses on the **Model Context Protocol (MCP)**, which is an open protocol standardizing how applications provide **tools and context to LLMs**.\n\nHere is how the webpage content relates to the query:\n\n*   **agent\\_infrastructure: MCP servers, tool use:** The page is entirely about MCP, which facilitates tool use via dedicated servers.\n*   **tool use, function calling, structured outputs:** The page details how MCP servers expose **Tools** (executable functions), support **Structured content** (like JSON), and **Multimodal tool content**.\n*   **agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** The page is documentation for **LangChain** and shows how its `langchain-mcp-adapters` library enables agents to use MCP tools.\n*   **agent memory, agentic memory, context compression, long context:** The page mentions **Short-term memory** and **Long-term memory** in the navigation, and discusses **Context engineering** and **Tool Interceptors** which can access runtime context (like state and store), but it does not detail specific implementations for agentic memory, context compression, or long context mechanisms beyond mentioning them in the navigation/advanced features.\n*   **agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, supervised fine-tuning, reinforcement learning:** The page covers **Context engineering** (via interceptors and runtime context access) and **Prompts** (loading reusable prompt templates from MCP servers). It also mentions **State updates and commands** in interceptors, which relates to orchestration. However, it does not discuss **agent evaluations**, **LLM reasoning trace evaluation**, **supervised fine-tuning**, or **reinforcement learning**.\n\n**Summary:**\n\nThe Model Context Protocol (MCP) documentation details an open protocol for providing **tools** and **context** to LLMs, primarily through **MCP servers**. It integrates with **LangChain** agents via the `langchain-mcp-adapters` library, enabling **tool use**, **function calling**, and handling **structured/multimodal content**. It also covers advanced features like **Tool Interceptors** for context engineering, accessing runtime",
    "published_date": "2025-05-12T00:00:00.000Z",
    "score": null
  },
  "https://developers.llamaindex.ai/python/examples/agent/anthropic_agent/": {
    "content_path": "../data/contents/cf9c6935dced283c.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.227235",
    "title": "Function Calling Anthropic Agent",
    "summary": "The webpage describes how to use the **Anthropic Agent** within the **LlamaIndex** framework, specifically highlighting its **function calling** capabilities.\n\nKey aspects covered include:\n\n*   **Prerequisites:** Requires `claude-3*` models for function calling support.\n*   **Initial Setup:** Involves importing necessary LlamaIndex components, defining custom tools (like `multiply` and `add` functions), and setting up the Anthropic LLM.\n*   **Agent Initialization:** Demonstrates initializing a `FunctionAgent` with defined tools.\n*   **Tool Use:** Shows an example of the agent using the defined calculator tools to solve a mathematical query.\n*   **Agent Memory/Context:** Explains that the default `.run()` method is stateless, but state can be maintained by passing a `Context` object.\n*   **RAG Pipeline Integration:** Details how to build an Anthropic agent over a Retrieval-Augmented Generation (RAG) pipeline by passing a query engine (built over an Uber 2021 PDF document) as a tool to the agent.\n\nRegarding your specific query terms:\n\n*   **agent\\_infrastructure:** The page focuses on the LlamaIndex agent infrastructure.\n*   **tool use:** Explicitly demonstrated with calculator functions and a RAG query engine.\n*   **agent memory/agentic memory:** Mentioned in the \"Managing Context/Memory\" section, noting the use of a `Context` object to maintain state.\n*   **agent frameworks:** **LlamaIndex** is the framework used.\n*   **LangChain, LlamaIndex:** **LlamaIndex** is central to the example. LangChain is not mentioned.\n*   **OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** The example uses the **Anthropic** LLM via LlamaIndex, and mentions using **OpenAI** embeddings for the RAG setup. Specific SDKs are not detailed beyond LlamaIndex integrations.\n*   **function calling:** This is the core feature demonstrated.\n*   **structured outputs:** Not explicitly detailed, but function calling inherently relies on structured tool definitions.\n*   **agent orchestration:** Implied through the setup of tools and the agent executing multi-step reasoning (like the math problem or the RAG query).",
    "published_date": null,
    "score": null
  },
  "https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph": {
    "content_path": "../data/contents/86f35bfde6fa8652.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.352135",
    "title": "LangChain Changelog",
    "summary": "The LangChain Changelog for March 1, 2025, announces the release of **LangChain MCP Adapters** for LangChain and LangGraph.\n\nThis package facilitates the use of **Anthropic Model Context Protocol (MCP) tools** with LangChain and LangGraph by:\n*   Converting **MCP tools** into tools compatible with LangChain and LangGraph.\n*   Enabling interaction with tools across multiple **MCP servers**.\n*   Integrating existing tool servers into LangGraph Agents.\n\nThis helps LangChain agents easily connect with the growing ecosystem of MCP tool servers and pull from multiple servers simultaneously.",
    "published_date": "2025-02-28T00:00:00.000Z",
    "score": null
  },
  "https://langchain-ai.github.io/langmem/": {
    "content_path": "../data/contents/e23ac778bb65b194.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.384372",
    "title": "Introduction",
    "summary": "The provided webpage introduces **LangMem**, a tool designed to help agents learn and adapt over time by providing tooling for memory management.\n\nKey features of LangMem include:\n*   A **Core memory API** compatible with any storage system.\n*   **Memory management tools** for agents to record and search information during active conversations (\"in the hot path\").\n*   A **Background memory manager** to automatically extract, consolidate, and update agent knowledge.\n*   **Native integration with LangGraph's Long-term Memory Store**.\n\nThe text demonstrates how to create an agent using `create_react_agent` that actively manages its own long-term memory by incorporating memory tools (`create_manage_memory_tool` and `create_search_memory_tool`) and a storage layer (like `InMemoryStore`). This allows the agent to store information (e.g., \"Remember that I prefer dark mode\") and retrieve it later based on context.\n\n**Regarding your specific query terms:**\n\nThe page directly discusses **agent memory** and **agent frameworks** (specifically mentioning **LangGraph** and its integration). It shows how to create an agent that manages memory.\n\nHowever, the page **does not mention or discuss** the following terms from your query:\n*   MCP servers\n*   tool use (beyond the memory tools provided by LangMem)\n*   agentic memory\n*   LangChain (only LangGraph is mentioned)\n*   LlamaIndex\n*   OpenAI Agents SDK\n*   Anthropic Agents SDK\n*   Google SDK\n*   function calling\n*   structured outputs\n*   agent orchestration",
    "published_date": null,
    "score": null
  },
  "https://platform.openai.com/docs/guides/function-calling?api-mode=chat": {
    "content_path": "../data/contents/32bf790818275d31.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.636587",
    "title": "Function calling",
    "summary": "The user query is a list of keywords related to agent infrastructure, frameworks, and capabilities. The provided webpage is a guide on **Function calling** (also known as **tool calling**) using the OpenAI API.\n\nHere is a summary of how the webpage content relates to the user's query:\n\n*   **Function calling / Tool use:** The core topic of the page is function calling, which is a mechanism to give models access to external functionality and data. This directly relates to the user's interest in **tool use**.\n*   **Structured outputs:** Function calling relies on defining tools using a JSON schema, which is a form of **structured output**. The page also explicitly mentions leveraging structured outputs when enabling **strict mode**.\n*   **Agent frameworks / SDKs (LangChain, LlamaIndex, OpenAI Agents SDK, etc.):** While the page details the raw OpenAI API mechanism for tool calling, this functionality is the foundation upon which agent frameworks like LangChain and LlamaIndex build their agent capabilities. The page focuses on the underlying mechanism rather than the higher-level frameworks.\n*   **Agent orchestration:** The five-step tool calling flow described on the page (request $\\rightarrow$ tool call $\\rightarrow$ execute $\\rightarrow$ return output $\\rightarrow$ final response) is a fundamental pattern used in **agent orchestration**.\n*   **MCP servers:** The text mentions that built-in tools enable the model to access the functionality of an **MCP server** (among other things like web search and code execution).\n*   **Agent memory / Agentic memory:** The page mentions the \"Conversation state\" and \"Background mode\" in the navigation sidebar, which are related concepts to memory in agent systems, but the main body of the text focuses on tool use, not memory mechanisms.\n*   **OpenAI Agents SDK / Anthropic Agents SDK / Google SDK:** The page describes the core API concept used by these SDKs but does not detail the specific implementation within those SDKs.\n\n**Summary:**\n\nThe webpage explains **Function calling** (or **tool calling**) in the OpenAI API, which allows models to interface with external systems using defined **tools** (functions or custom tools). This mechanism is crucial for **tool use** and **structured outputs** by defining functionality via JSON schemas. The described multi-step flow is a core component of **agent orchestration**. The page also notes that built-in tools can access functionality provided by an",
    "published_date": null,
    "score": null
  },
  "https://galileo.ai/blog/reflection-tuning-llms": {
    "content_path": "../data/contents/c1a6d5cb0662031f.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:38.940901",
    "title": "Reflection Tuning Explained: Self-Improving LLMs 101 | Galileo",
    "summary": "Reflection tuning is a technique that teaches AI models to critique and rewrite their own responses, creating a feedback loop for self-improvement. This process involves generating an answer, having the model (or an oracle) critique it for errors (like factual mistakes or logical flaws), and then generating an improved answer. This enhanced pair is then incorporated back into the training data.\n\n**Relevance to the User Query:**\n\nThe page directly discusses several concepts related to the user's query:\n\n*   **Reasoning LLMs & Self-Reflection:** Reflection tuning is fundamentally a self-reflection mechanism designed to improve the quality of responses, which is crucial for reasoning-heavy tasks.\n*   **Hallucination Reduction and Detection:** A primary goal of reflection tuning is to systematically catch and correct false claims, thereby reducing hallucinations and improving factual accuracy.\n*   **Grounding & Factuality:** The critique step specifically targets factual mistakes, aiming to ground the model's output more reliably.\n*   **Inference-time Compute:** The text heavily discusses the trade-off, noting that reflection tuning doubles or triples computational costs due to multiple forward passes during inference. It suggests strategies like confidence scoring to limit the process to risky inputs to manage this compute cost.\n\n**Concepts Not Explicitly Covered (or only tangentially):**\n\n*   **Chain-of-Thought (CoT):** While reflection tuning improves reasoning, the text focuses on the *critique/rewrite* loop rather than the specific CoT prompting structure itself.\n*   **Planning with LLMs / MCTS (Monte Carlo Tree Search) for Language Models:** These are advanced search/planning algorithms; reflection tuning is a data-centric self-correction method and is not described as using MCTS.\n*   **Test-time Scaling:** While inference-time compute is discussed, the specific concept of \"test-time scaling\" (often related to dynamic resource allocation based on input complexity) is addressed indirectly through suggestions like confidence scoring and variable depth processing.\n\n**Summary:**\n\nReflection tuning is a self-improving technique for LLMs that enhances **reasoning**, **factuality**, and **hallucination reduction** by forcing the model to critique and rewrite its initial output. This process significantly increases **inference-time compute** due to multiple forward passes, making it best suited for knowledge-intensive tasks where accuracy outweighs latency concerns.\n\nReflection tuning is a technique that teaches AI models to critique and rewrite their own responses, creating a feedback loop for self-improvement. This process involves generating an answer, having the model (or an oracle model) critique it for errors (factual, logical, stylistic), and then generating an improved answer, which is then incorporated back into the training data.\n\n**Relevance to the User Query:**\n\nThe page directly discusses concepts related to **reasoning and planning** and **hallucination reduction and detection**:\n\n*   **Reasoning LLMs / Chain-of-Thought:** Reflection tuning is explicitly stated to work best for \"knowledge-intensive, **reasoning-heavy tasks**\" where faulty logic can damage trust. The critique step inherently involves a form of internal reasoning about the initial output.\n*   **Self-reflection:** This is the core mechanism of reflection tuning, where the model acts as its own critic.\n*   **Hallucination Reduction and Detection / Factuality:** The critique step is designed to identify and correct \"factual mistakes\" and \"logical errors,\" directly addressing hallucination reduction and improving factuality.\n*   **Inference-time compute:** The text notes that reflection tuning **doubles computational costs** due to multiple forward passes, which is a key consideration for inference-time compute.\n\n**Concepts Not Explicitly Covered:**\n\nThe page **does not** detail:\n\n*   **MCTS (Monte Carlo Tree Search) for language models:** This specific planning algorithm is not mentioned.\n*   **Planning with LLMs (beyond self-critique):** While self-critique is a form of iterative refinement, the text does not discuss broader planning frameworks like sequential task decomposition or goal-directed planning using MCTS or similar methods.\n*   **Test-time scaling:** This specific term is not used, although the text discusses managing computational costs at runtime (inference-time compute).\n*   **Grounding:** This term is not explicitly defined or discussed in the context of reflection tuning.\n\n**Summary:**\n\nReflection tuning is a self-improving technique that enhances LLM performance, particularly in **reasoning-heavy tasks**, by incorporating a **self-reflection** and critique step into the generation process. This method is effective for **reducing hallucinations and improving factuality** but significantly increases **inference-time compute** costs. It differs from standard tuning by integrating critique directly into the data pipeline. The document focuses",
    "published_date": "2025-07-18T00:00:00.000Z",
    "score": null
  },
  "https://www.semanticscholar.org/paper/Chain-of-Thought-Prompting-Elicits-Reasoning-in-Wei-Wang/1b6e810ce0afd0dd093f789d2b2742d047e316d5": {
    "content_path": "../data/contents/52321e4222b252b5.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:39.161836",
    "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
    "summary": "The provided webpage focuses heavily on **Chain of Thought (CoT) Prompting** as a method to elicit **reasoning** in Large Language Models (LLMs).\n\nKey concepts mentioned in relation to reasoning and LLMs include:\n\n*   **Chain of Thought Prompting:** Shown to improve performance on arithmetic, commonsense, and symbolic reasoning tasks.\n*   **Self-Consistency:** A decoding strategy proposed to improve CoT reasoning by sampling diverse reasoning paths and selecting the most consistent answer.\n*   **Teaching Small Language Models to Reason:** Finetuning smaller models on CoT outputs generated by larger teacher models.\n*   **Analogical Reasoners:** Using LLMs as analogical reasoners, outperforming 0-shot CoT.\n*   **Chain-of-Knowledge Prompting:** Eliciting explicit knowledge evidence during reasoning and using verification methods to estimate reliability (factuality).\n*   **Systematic Reasoning:** Training models to reliably perform systematic reasoning combining implicit knowledge and explicit statements.\n\nWhile the page extensively covers **Reasoning LLMs** and **Chain-of-Thought**, it does not explicitly detail **inference-time compute**, **self-reflection**, **planning with LLMs**, **MCTS for language models**, **test-time scaling**, **hallucination reduction and detection**, or **grounding** in the context of the user's query, beyond the general concept of improving reasoning reliability (factuality).",
    "published_date": "2022-01-28T02:48:29.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2212.10001": {
    "content_path": "data/contents/d69dc59580d90c9c.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.936903",
    "title": "Computer Science > Computation and Language",
    "summary": "The provided webpage is an abstract for an arXiv paper titled \"Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters.\"\n\nThis paper focuses on **Chain-of-Thought (CoT) prompting**, which is a technique used to improve the multi-step **reasoning** abilities of Large Language Models (LLMs) by encouraging them to generate intermediate rationales.\n\nThe study investigates what aspects of the demonstrated reasoning steps in CoT prompting are most important for performance. Key findings include:\n*   CoT reasoning is possible even with **invalid demonstrations** (prompting with invalid reasoning steps still achieves significant performance).\n*   Other aspects, such as the rationales being **relevant to the query** and having the **correct ordering** of reasoning steps, are much more important for effective CoT reasoning.\n\nWhile the paper directly addresses **Chain-of-Thought Prompting** and **Reasoning LLMs**, it does not explicitly discuss: inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, or grounding/factuality.\n\n**Summary relevant to the query:** The page discusses **Chain-of-Thought Prompting** as a method for improving **Reasoning LLMs**. It empirically studies what aspects of the demonstrated reasoning steps matter for performance, finding that relevance and ordering are more critical than the validity of the steps themselves.",
    "published_date": "2022-12-20T00:00:00.000Z",
    "score": null
  },
  "https://notes.aimodels.fyi/self-rag-improving-the-factual-accuracy-of-large-language-models-through-self-reflection/": {
    "content_path": "../data/contents/80a9f4c2c699c52f.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:39.463081",
    "title": "SELF-RAG: Improving the Factual Accuracy of Large Language Models through Self-Reflection",
    "summary": "The user query asks for a summary related to several advanced LLM concepts, including **reasoning and planning**, **chain-of-thought**, **self-reflection**, **MCTS**, **hallucination reduction and detection**, and **grounding/factuality**.\n\nThe provided webpage focuses on **SELF-RAG (Self-Reflective Retrieval-Augmented Generation)**, a technique designed to improve the **factual accuracy** and **verifiability** of Large Language Models (LLMs).\n\nHere is a summary of the page relevant to the query:\n\nThe SELF-RAG framework addresses the persistent problem of LLM **hallucination** and factual inaccuracy by training models to enhance their own output quality through **selective retrieval** of external knowledge and **self-critiquing** of generations.\n\nKey aspects related to the query:\n\n*   **Self-Reflection/Self-Critiquing:** SELF-RAG trains the LLM to generate novel \"reflection tokens\" that act as fine-grained feedback on the relevance of retrieved passages and whether the model's outputs are properly supported by evidence. This is a form of **self-reflection** used for **hallucination reduction and detection**.\n*   **Grounding/Factuality:** The primary goal of SELF-RAG is to improve **factual accuracy** and **grounding** by selectively retrieving information and using self-critiques to ensure outputs are evidence-based.\n*   **Reasoning and Planning (Implicitly):** While the paper doesn't explicitly detail standard Chain-of-Thought or MCTS, the evaluation section notes that SELF-RAG was tested on **reasoning** tasks, where it significantly outperformed baselines, suggesting its self-correction mechanism aids complex inference. The model learns to decide *when* retrieval is needed, which is a form of dynamic control over its generation process.\n\nIn summary, the page details SELF-RAG, a method that uses **self-reflection** and selective knowledge retrieval to significantly improve LLM **factuality** and **grounding**, showing strong performance on **reasoning** tasks.",
    "published_date": "2023-10-19T15:24:53.000Z",
    "score": null
  },
  "https://arxiv.org/html/2505.12260v1": {
    "content_path": "../data/contents/cd7e0891888e24d1.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.025168",
    "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000× Faster Query Inference",
    "summary": "The webpage describes **LightRetriever**, a novel **LLM-based hybrid retrieval architecture** designed for **extremely lightweight query inference**.\n\nHere's a summary of how it relates to your query topics:\n\n*   **Vector databases:** The architecture is designed to work with retrieval systems that use vector similarities, implying compatibility with vector databases for storing document embeddings.\n*   **Embeddings (new efficient models):** LightRetriever focuses on efficiency by retaining a full-sized LLM for **document encoding** (creating embeddings) but drastically reducing the query encoding workload. For **dense retrieval**, it trains cacheable token vectors, reducing query inference cost to a simple embedding lookup (averaging token vectors).\n*   **Rerankers:** The paper focuses on the initial retrieval step, not explicitly on reranking, though hybrid search often precedes reranking.\n*   **RAG architectures:** The context mentions that text retrieval is broadly applied in Retrieval Augmented Generation (RAG), and LightRetriever aims to make the retrieval component of RAG systems much faster.\n*   **RAG alternatives:** The paper presents an optimized retrieval architecture, which serves as a faster component within RAG or as a general retrieval improvement.\n*   **Hybrid search:** LightRetriever is explicitly an LLM-based **hybrid retriever**, combining dense retrieval (using averaged token vectors) and sparse retrieval (using a modified SPLADE approach where the query vector is derived directly from token frequencies). The final scores are linearly interpolated.\n*   **Chunking strategies:** The text discusses tokenization and sequence length but does not detail specific document chunking strategies.\n\n**Key takeaway:** LightRetriever achieves over a **1000× speedup** in query inference compared to serving a full-sized LLM encoder, while retaining about 95% of the full-sized performance by making the query encoder extremely lightweight (an embedding lookup for dense retrieval and direct frequency mapping for sparse retrieval).\n\nThe user query asks for a summary of information related to: **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, and chunking strategies.**\n\nThe provided webpage text focuses on **LightRetriever**, a novel LLM-based **hybrid retrieval architecture** designed for **extremely lightweight query encoding**, achieving a significant speedup in query inference.\n\nHere is how the page content relates to the query topics:\n\n*   **RAG Architectures / Hybrid Search:** The page explicitly introduces **LightRetriever** as an LLM-based **hybrid retrieval architecture**. It mentions combining retrieval scores/ranks from dense and sparse retrieval systems, and specifically uses **linear interpolation of dense and sparse scores**.\n*   **Embeddings (new efficient models):** The core contribution of LightRetriever is achieving **extremely lightweight query encoding** using LLMs, which addresses the inefficiency of using large LLMs as query encoders. The text also references other embedding models and techniques in its related works section (e.g., BGE, E5, LLM-based retrievers).\n*   **Rerankers / Vector Databases / Chunking Strategies / RAG Alternatives:** The provided text **does not explicitly discuss** vector databases, specific reranker models, chunking strategies, or RAG alternatives. While it mentions dense retrieval (which uses vector search) and sparse retrieval, it focuses on the *architecture* and *efficiency* of the hybrid system rather than these specific components in detail.\n\n**Summary based on the query:**\n\nThe page describes **LightRetriever**, a novel LLM-based **hybrid retrieval architecture** that significantly speeds up query inference by employing **extremely lightweight query encoding**. This architecture combines retrieval scores from dense and sparse methods, often using linear interpolation. The work addresses the efficiency challenge of using large language models (LLMs) as query encoders, a common component in modern retrieval systems.\n\n**Regarding the specific topics in the query:**\n\n*   **Hybrid Search/RAG Architectures:** Covered by the description of LightRetriever.\n*   **Embeddings (new efficient models):** Covered by the focus on lightweight query encoding using LLMs.\n*   **Vector databases, rerankers, RAG alternatives, chunking strategies:** No direct answer found.",
    "published_date": "2025-05-11T00:00:00.000Z",
    "score": null
  },
  "https://amirmalik.net/2025/08/26/rag-embeddings-chunking-retrieval-basics": {
    "content_path": "../data/contents/38a12158e10455b0.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:39.567228",
    "title": "RAG Embeddings, Chunking, and Retrieval Basics",
    "summary": "The webpage provides a breakdown of the fundamentals of Retrieval-Augmented Generation (RAG), covering **embeddings**, **chunking**, and various **search strategies**.\n\nHere is a summary related to your query:\n\n*   **Vector Databases & Conceptual Retrieval:** The page discusses **conceptual retrieval**, which involves converting data into vectors using an embedding model and storing them for search, often in a **vector database**. Vector search uses Approximate Nearest Neighbor (ANN) algorithms (like HNSW) and distance metrics (like cosine similarity).\n*   **Embeddings:** An embedding is a vector (an array of floating-point numbers) generated by an ML model that represents input data (like text). Semantically related words or concepts are clustered close together in the vector space. The page notes that modern embedding models (like BERT) often have high dimensionality, and their individual dimensions are a \"black box.\"\n*   **Chunking Strategies:** Since embedding models perform best on smaller phrases, **chunking** (splitting documents) is necessary. The page mentions several strategies: chunking by bytes, words, sentences, or paragraphs, and more elaborate methods like **contextual chunks**, **overlapping chunks**, and **hierarchical chunks**. Vector search is performed on these chunks, not the original documents.\n*   **RAG Architectures:** RAG is described as **LLM-Assisted Search**. It leverages the LLM to refine the user's intent (query rewriting) into subqueries, which are then embedded and used to search the vector database. The retrieved data is then used by the LLM.\n*   **Rerankers & Hybrid Search:** While the page doesn't explicitly detail \"hybrid search,\" it discusses different retrieval spectra: **Exact retrieval**, **Lexical retrieval** (Full-Text Search), and **Conceptual retrieval** (vector-based). The \"Future\" section suggests that dedicated smaller models for **results reranking** might be used to improve performance and reduce latency in future RAG systems.\n*   **RAG Alternatives:** The page contrasts RAG with prompting an LLM to generate **exact (SQL)** or **lexical (keyword-style)** queries against a database, noting that RAG leverages the LLM's semantic understanding.\n\nThe page focuses heavily on the basics of embeddings, chunking, and the RAG pattern itself, touching upon the components you listed.",
    "published_date": "2025-08-26T00:00:00.000Z",
    "score": null
  },
  "https://www.researchgate.net/publication/390326215_Hybrid_Retrieval-Augmented_Generation_RAG_Systems_with_Embedding_Vector_Databases": {
    "content_path": "data/contents/6aa0af18c489782d.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.562073",
    "title": "(PDF) Hybrid Retrieval-Augmented Generation (RAG) Systems with Embedding Vector Databases",
    "summary": "The webpage discusses **Hybrid Retrieval-Augmented Generation (RAG) Systems with Embedding Vector Databases**.\n\nHere is a summary addressing the components mentioned in your query:\n\n*   **Vector databases & Embeddings:** The article explores the integration of **embedding vector databases** to store and efficiently search document embeddings, which are generated using transformer-based models (like BERT) to capture semantic meaning. Vector databases use specialized indexing structures like ANN algorithms (e.g., HNSW) for fast similarity search.\n*   **RAG Architectures:** The core focus is on the **RAG architecture**, which supplements LLMs with external knowledge retrieval to reduce hallucinations and access domain-specific information. The proposed system uses a modular framework with a retriever component (using dense passage retrieval) and a generator component.\n*   **Hybrid Search & Rerankers:** The paper strongly advocates for **hybrid retrieval strategies** that combine **dense vector search** (semantic) with **traditional keyword-based methods** (lexical, like BM25). This combination is shown to significantly improve recall and precision over single-method approaches. The methodology mentions a \"novel rank fusion methodology\" and the implementation section notes that many systems implement **re-ranking** after initial retrieval to further refine results.\n*   **New efficient models:** While the text mentions using BERT-based encoders and Sentence Transformers to generate embeddings, it does not specifically detail *new* efficient embedding models, focusing more on the architecture and hybrid retrieval.\n*   **Chunking strategies & Context window management:** The experimental results mention that **content-aware segmentation methods** (a form of chunking strategy) that preserve semantic units outperform naive fixed-length chunking. The implementation section notes that **context window management** is crucial because LLMs have restricted input lengths, requiring careful selection and prioritization of retrieved information.\n*   **RAG alternatives:** The paper focuses on *improving* RAG via hybrid methods, not explicitly detailing alternatives to the RAG paradigm itself, though it compares hybrid RAG performance against standalone LLMs.",
    "published_date": "2025-08-10T00:00:00.000Z",
    "score": null
  },
  "https://weaviate.io/blog/vector-embeddings-explained": {
    "content_path": "../data/contents/409f1eb0241f330b.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:39.837610",
    "title": "Vector Embeddings Explained",
    "summary": "The webpage explains **Vector Embeddings**, which are the core of **vector databases** and enable semantic search.\n\nHere's a summary of the concepts covered in relation to your query:\n\n*   **Vector Databases:** Their core function is to provide high-quality search results by finding what the user *means* (semantic search) rather than just matching keywords.\n*   **Embeddings (Models):** Vector embeddings are numeric representations (arrays of numbers) of data (text, images, audio, etc.) generated by machine learning models. The quality of the search depends crucially on the quality of the model used for vectorization.\n    *   **Word-level models** like `word2vec` and `GloVe` are mentioned.\n    *   **Transformer models** (like BERT) are described as the current state-of-the-art, creating contextual embeddings that better handle polysemy. Models like CLIP can vectorize multiple data types (e.g., images and text) into one space.\n*   **Retrieval Architectures (RAG context):** While the page focuses on the fundamental concept of vector embeddings and vector databases, it doesn't explicitly detail RAG architectures, alternatives, rerankers, hybrid search, or chunking strategies.\n\n**Conclusion:** The page covers **Vector Databases** and **Embeddings (new efficient models)**, explaining how they work and the evolution of the models used to create them. It does **not** cover RAG architectures, rerankers, hybrid search, or chunking strategies.",
    "published_date": "2023-01-16T00:00:00.000Z",
    "score": null
  },
  "https://www.pinecone.io/learn/vector-database/": {
    "content_path": "../data/contents/87cc10593318c402.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.580436",
    "title": "What is a Vector Database & How Does it Work? Use Cases + Examples",
    "summary": "The webpage explains what a **Vector Database** is, how it works, and its use cases, focusing on its role in handling **vector embeddings** for AI applications like large language models and semantic search.\n\nHere is a summary of the topics mentioned in your query that are covered on the page:\n\n*   **Vector databases:** The page extensively defines vector databases, contrasting them with traditional databases and standalone vector indexes (like FAISS). They are specialized to store and query vector embeddings efficiently.\n*   **Embeddings:** The text explains that embeddings are vector data representations generated by AI models that carry semantic information, which vector databases are designed to manage.\n*   **RAG architectures (implied):** The description of how vector databases add \"knowledge to our AIs, like semantic information retrieval, long-term memory,\" directly relates to the core function of Retrieval-Augmented Generation (RAG) systems.\n*   **Hybrid search (mentioned):** The API/SDK section notes that developers use vector databases for use cases including \"hybrid search.\"\n*   **Chunking strategies (mentioned):** The \"Recommended for you\" section lists an article titled \"Chunking Strategies for LLM Applications,\" indicating this is a related topic, though the main body of the text does not detail chunking strategies.\n*   **Rerankers (implied):** The \"Post Processing\" step in the vector database pipeline mentions that it \"can include re-ranking the nearest neighbors using a different similarity measure.\"\n\nThe page **does not explicitly detail** the following terms from your query:\n\n*   New efficient models (for generating embeddings)\n*   RAG alternatives",
    "published_date": "2023-05-03T00:00:00.000Z",
    "score": null
  },
  "https://heungno.net/wp-content/uploads/2025/03/paper-5.pdf": {
    "content_path": "../data/contents/97b7599b84b4175c.md",
    "source": "exa",
    "saved_at": "2025-12-28T19:39:40.197896",
    "title": "",
    "summary": "The webpage text provides a comprehensive survey of Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs), detailing its progression through Naive RAG, Advanced RAG, and Modular RAG paradigms.\n\nThe summary relevant to your query is as follows:\n\n**Vector Databases and Embeddings:**\n*   **Vector Databases:** Mentioned as the storage for vector representations of document chunks during the **Indexing** phase of Naive RAG.\n*   **Embeddings (New Efficient Models):** The text notes that retrieval relies on calculating similarity between embeddings of the query and document chunks. It highlights recent prominent embedding models like **AngIE, Voyage, and BGE**, which benefit from multi-task instruction tuning. It also discusses the use of **sparse encoders (BM25)** alongside dense retrievers, and the importance of **fine-tuning embedding models** for domain-specific jargon.\n\n**RAG Architectures and Alternatives:**\n*   **RAG Architectures:** The paper details three main paradigms: **Naive RAG** (simple Indexing, Retrieval, Generation), **Advanced RAG** (incorporating pre-retrieval and post-retrieval optimizations), and **Modular RAG** (highly flexible architecture allowing module substitution and new components like Search, Memory, and Routing modules).\n*   **RAG Alternatives:** The text compares RAG with **Fine-tuning (FT)** and **Prompt Engineering**, noting that RAG excels in dynamic environments requiring real-time knowledge updates, while FT is better for deep customization of style/format.\n\n**Rerankers:**\n*   **Rerankers:** These are explicitly mentioned under **Post-Retrieval Process** in Advanced RAG and **Context Curation** in the Generation section. Reranking reorders retrieved chunks to highlight the most pertinent results first. Methods include rule-based approaches and **model-based approaches** using BERT series models or specialized reranking models like **Cohere rerank or bge-reranker-large**.\n\n**Hybrid Search:**\n*   **Hybrid Search:** Mentioned in the context of Modular RAG's **New Patterns**, where hybrid retrieval strategies integrate **keyword, semantic, and vector searches** to cater to diverse queries.\n\n**Chunking Strategies:**\n*   **Chunking Strategies:** Discussed under **Indexing Optimization**. The most common method is splitting documents into fixed-size chunks. The\n\nThe user query asks for a summary covering several specific topics related to Retrieval-Augmented Generation (RAG): **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, and chunking strategies.**\n\nThis webpage text discusses various advanced retrieval techniques and RAG evaluation, but it does not explicitly detail all the requested components:\n\n*   **Vector databases:** Not explicitly mentioned.\n*   **Embeddings (new efficient models):** Not explicitly mentioned, though retrieval relies on them.\n*   **Rerankers:** Mentioned indirectly through the discussion of improving retrieval quality (e.g., Self-RAG's beam search across paragraphs, and the general evaluation of retrieval quality).\n*   **RAG architectures:** Discusses advanced architectures like ITER RETGEN, Recursive Retrieval, and Adaptive Retrieval (Flare, Self-RAG).\n*   **RAG alternatives:** Not explicitly discussed as alternatives, but the comparison with Long Context LLMs suggests a shift in reliance.\n*   **Hybrid search:** Not explicitly mentioned.\n*   **Chunking strategies:** Mentioned in the context of recursive retrieval (summarizing sections of a document) and in future prospects regarding operational efficiency, but specific strategies are not detailed.\n\nSince the text focuses heavily on advanced retrieval methods (Recursive, Adaptive) and RAG evaluation, but lacks specific sections on vector databases, efficient embeddings, hybrid search, or detailed chunking strategies, a complete summary matching the query is not possible.\n\n**No answer found**",
    "published_date": "2024-03-28T00:00:00.000Z",
    "score": null
  },
  "https://www.reddit.com/r/Rag/comments/1oh10m3/besides_langchain_are_there_any_other_alternative/": {
    "content_path": "../data/contents/8b1395cf0ffe2ff7.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.765411",
    "title": "Besides langchain, are there any other alternative frameworks? : r/Rag",
    "summary": "The webpage mentions **Haystack** as a solid alternative framework to LangChain for building RAG systems.",
    "published_date": "2025-10-27T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2507.17186": {
    "content_path": "../data/contents/2507.17186.md",
    "pdf_path": "../data/contents/2507.17186.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:52:54.089111",
    "title": "FinGAIA: A Chinese Benchmark for AI Agents in Real-World Financial Domain"
  },
  "https://arxiv.org/html/2508.17565v1": {
    "content_path": "../data/contents/2508.17565v1.md",
    "pdf_path": "../data/contents/2508.17565v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:52:54.413700",
    "title": "TradingGroup: Multi-Agent Trading System with Self-Reflection and Data-Synthesis"
  },
  "https://arxiv.org/abs/2509.09995": {
    "content_path": "../data/contents/2509.09995.md",
    "pdf_path": "../data/contents/2509.09995.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:52:58.155695",
    "title": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading"
  },
  "https://arxiv.org/abs/2506.09026": {
    "content_path": "../data/contents/2506.09026.md",
    "pdf_path": "../data/contents/2506.09026.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:52:59.142968",
    "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs"
  },
  "https://arxiv.org/abs/2512.02230": {
    "content_path": "../data/contents/2512.02230.md",
    "pdf_path": "../data/contents/2512.02230.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:00.258649",
    "title": "Benchmarking LLM Agents for Wealth-Management Workflows"
  },
  "https://arxiv.org/abs/2501.00309": {
    "content_path": "../data/contents/2501.00309.md",
    "pdf_path": "../data/contents/2501.00309.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:02.072884",
    "title": "Retrieval-Augmented Generation with Graphs (GraphRAG) Survey"
  },
  "https://arxiv.org/abs/2505.19197": {
    "content_path": "../data/contents/2505.19197.md",
    "pdf_path": "../data/contents/2505.19197.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:02.936838",
    "title": "Structuring the Unstructured: A Multi-Agent System for Extracting and Querying Financial KPIs and Guidance"
  },
  "https://arxiv.org/abs/2508.00828": {
    "content_path": "../data/contents/2508.00828.md",
    "pdf_path": "../data/contents/2508.00828.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:03.750659",
    "title": "Finance Agent Benchmark: Benchmarking LLMs on Real-world Financial Research Tasks"
  },
  "https://arxiv.org/html/2506.21182v1": {
    "content_path": "../data/contents/2506.21182v1.md",
    "pdf_path": "../data/contents/2506.21182v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:03.932550",
    "title": "Maintaining MTEB: Towards Long Term Usability of Embedding Benchmarks"
  },
  "https://arxiv.org/abs/2506.00054": {
    "content_path": "../data/contents/2506.00054.md",
    "pdf_path": "../data/contents/2506.00054.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:04.357129",
    "title": "RAG Survey: Architectures, Enhancements, and Robustness Frontiers"
  },
  "https://arxiv.org/abs/2508.14052": {
    "content_path": "../data/contents/2508.14052.md",
    "pdf_path": "../data/contents/2508.14052.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:05.527696",
    "title": "FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering"
  },
  "https://arxiv.org/abs/2512.10971": {
    "content_path": "../data/contents/2512.10971.md",
    "pdf_path": "../data/contents/2512.10971.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:06.435378",
    "title": "AI-Trader: Benchmarking Autonomous Agents in Real-Time Financial Markets"
  },
  "https://arxiv.org/abs/2510.12635": {
    "content_path": "../data/contents/2510.12635.md",
    "pdf_path": "../data/contents/2510.12635.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:06.731833",
    "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks"
  },
  "https://arxiv.org/abs/2311.05772": {
    "content_path": "../data/contents/2311.05772.md",
    "pdf_path": "../data/contents/2311.05772.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:07.308201",
    "title": "ADaPT: As-Needed Decomposition and Planning with Language Models"
  },
  "https://arxiv.org/abs/2512.21083": {
    "content_path": "../data/contents/2512.21083.md",
    "pdf_path": "../data/contents/2512.21083.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:08.393440",
    "title": "Hierarchical Modeling Approach to Fast and Accurate Table Recognition"
  },
  "https://arxiv.org/abs/2510.12839": {
    "content_path": "../data/contents/2510.12839.md",
    "pdf_path": "../data/contents/2510.12839.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:10.224084",
    "title": "FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs"
  },
  "https://arxiv.org/abs/2512.10619": {
    "content_path": "../data/contents/2512.10619.md",
    "pdf_path": "../data/contents/2512.10619.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:12.915617",
    "title": "DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM"
  },
  "https://arxiv.org/abs/2511.21631": {
    "content_path": "../data/contents/2511.21631.md",
    "pdf_path": "../data/contents/2511.21631.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:13.531681",
    "title": "Qwen3-VL Technical Report"
  },
  "https://arxiv.org/abs/2509.01455": {
    "content_path": "../data/contents/2509.01455.md",
    "pdf_path": "../data/contents/2509.01455.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:14.515580",
    "title": "Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal"
  },
  "https://arxiv.org/abs/2511.10037": {
    "content_path": "../data/contents/2511.10037.md",
    "pdf_path": "../data/contents/2511.10037.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:14.908171",
    "title": "Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning"
  },
  "https://arxiv.org/abs/2510.21538": {
    "content_path": "../data/contents/2510.21538.md",
    "pdf_path": "../data/contents/2510.21538.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:16.767008",
    "title": "InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation"
  },
  "https://arxiv.org/abs/2409.02977": {
    "content_path": "../data/contents/2409.02977.md",
    "pdf_path": "../data/contents/2409.02977.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:20.697842",
    "title": "Large Language Model-Based Agents for Software Engineering: A Survey"
  },
  "https://arxiv.org/abs/2502.10325": {
    "content_path": "../data/contents/2502.10325.md",
    "pdf_path": "../data/contents/2502.10325.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:23.062346",
    "title": "Process Reward Models for LLM Agents: Practical Framework and Directions"
  },
  "https://arxiv.org/abs/2508.18265": {
    "content_path": "../data/contents/2508.18265.md",
    "pdf_path": "../data/contents/2508.18265.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:23.480286",
    "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
  },
  "https://arxiv.org/abs/2402.01680": {
    "content_path": "../data/contents/2402.01680.md",
    "pdf_path": "../data/contents/2402.01680.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:24.076211",
    "title": "Large Language Model based Multi-Agents: A Survey of Progress and Challenges"
  },
  "https://arxiv.org/abs/2510.07233": {
    "content_path": "../data/contents/2510.07233.md",
    "pdf_path": "../data/contents/2510.07233.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:33.627279",
    "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding"
  },
  "https://arxiv.org/abs/2510.27261": {
    "content_path": "../data/contents/2510.27261.md",
    "pdf_path": "../data/contents/2510.27261.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:36.467537",
    "title": "RegionRAG: Region-level Retrieval-Augumented Generation for Visually-Rich Documents"
  },
  "https://arxiv.org/abs/2511.12309": {
    "content_path": "../data/contents/2511.12309.md",
    "pdf_path": "../data/contents/2511.12309.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:44.224854",
    "title": "Optimal Self-Consistency for Efficient Reasoning with Large Language Models"
  },
  "https://arxiv.org/abs/2512.06432": {
    "content_path": "../data/contents/2512.06432.md",
    "pdf_path": "../data/contents/2512.06432.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:44.631553",
    "title": "HiveMind: Contribution-Guided Online Prompt Optimization of LLM Multi-Agent Systems"
  },
  "https://arxiv.org/abs/2511.20227": {
    "content_path": "../data/contents/2511.20227.md",
    "pdf_path": "../data/contents/2511.20227.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:47.662569",
    "title": "HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents"
  },
  "https://arxiv.org/abs/2512.10787": {
    "content_path": "../data/contents/2512.10787.md",
    "pdf_path": "../data/contents/2512.10787.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:48.299799",
    "title": "Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly"
  },
  "https://arxiv.org/abs/2511.18808": {
    "content_path": "../data/contents/2511.18808.md",
    "pdf_path": "../data/contents/2511.18808.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:49.904397",
    "title": "HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations"
  },
  "https://arxiv.org/abs/2511.02770": {
    "content_path": "../data/contents/2511.02770.md",
    "pdf_path": "../data/contents/2511.02770.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:50.714902",
    "title": "Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval"
  },
  "https://arxiv.org/abs/2510.20356": {
    "content_path": "../data/contents/2510.20356.md",
    "pdf_path": "../data/contents/2510.20356.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:52.319159",
    "title": "FreeChunker: A Cross-Granularity Chunking Framework"
  },
  "https://arxiv.org/abs/2510.24402": {
    "content_path": "../data/contents/2510.24402.md",
    "pdf_path": "../data/contents/2510.24402.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:53.023952",
    "title": "Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering"
  },
  "https://arxiv.org/abs/2510.22264": {
    "content_path": "../data/contents/2510.22264.md",
    "pdf_path": "../data/contents/2510.22264.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:53.684679",
    "title": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding"
  },
  "https://arxiv.org/abs/2501.07301": {
    "content_path": "../data/contents/2501.07301.md",
    "pdf_path": "../data/contents/2501.07301.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:54.022470",
    "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning"
  },
  "https://arxiv.org/abs/2510.19340": {
    "content_path": "../data/contents/2510.19340.md",
    "pdf_path": "../data/contents/2510.19340.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:55.260324",
    "title": "CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale"
  },
  "https://arxiv.org/abs/2510.12474": {
    "content_path": "../data/contents/2510.12474.md",
    "pdf_path": "../data/contents/2510.12474.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:58.794260",
    "title": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression"
  },
  "https://arxiv.org/abs/2510.04626": {
    "content_path": "../data/contents/2510.04626.md",
    "pdf_path": "../data/contents/2510.04626.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:53:59.623922",
    "title": "Compressed Concatenation of Small Embedding Models"
  },
  "https://arxiv.org/html/2508.00554v3": {
    "content_path": "../data/contents/2508.00554v3.md",
    "pdf_path": "../data/contents/2508.00554v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:54:00.171650",
    "title": "ContestTrade: Multi-Agent Trading System with Internal Contest Mechanism"
  },
  "https://arxiv.org/abs/2509.18095": {
    "content_path": "../data/contents/2509.18095.md",
    "pdf_path": "../data/contents/2509.18095.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:54:01.088484",
    "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction"
  },
  "https://arxiv.org/html/2502.00989v1": {
    "content_path": "../data/contents/2502.00989v1.md",
    "pdf_path": "../data/contents/2502.00989v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:54:01.583982",
    "title": "ChartCitor: Answer Citations for ChartQA via Multi-Agent LLM Retrieval"
  },
  "https://arxiv.org/abs/2512.13168": {
    "content_path": "../data/contents/2512.13168.md",
    "pdf_path": "../data/contents/2512.13168.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-28T19:54:03.952812",
    "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows"
  },
  "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct": {
    "content_path": "../data/contents/557edb1d63e8fcfd.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:04.871141",
    "title": "Qwen/Qwen2.5-VL-7B-Instruct · Hugging Face"
  },
  "https://www.letta.com/blog/agent-memory": {
    "content_path": "../data/contents/fbd37f28303e4e69.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.553069",
    "title": "Agent Memory: How to Build Agents that Learn and Remember",
    "summary": "The provided webpage focuses on **Agent Memory**, detailing how to build agents that learn and remember over time by managing their context window.\n\nKey concepts discussed include:\n*   **Agent Memory as Context Management:** Designing memory is essentially context engineering—determining what information enters the agent's context window.\n*   **Types of Agent Memory:**\n    *   **Message Buffer:** Stores recent messages for immediate conversational context.\n    *   **Core Memory:** In-context, editable memory blocks focused on specific topics (user, persona, task).\n    *   **Recall Memory:** Stores and allows searching of the complete conversational history.\n    *   **Archival Memory:** Explicitly formulated, indexed knowledge stored in external databases (like vector or graph DBs).\n*   **Techniques for Memory Management:** Message eviction, recursive summarization, managing structured memory blocks, and using external storage/retrieval (RAG).\n*   **Engineering Systems:** Mentions **MemGPT** (treating context as a constrained resource with memory tiers) and **Sleep-Time Compute** (using asynchronous agents for proactive memory refinement).\n\n**Regarding your specific query:**\n\nThe query lists several technical components related to the agent stack: 'agent\\_infrastructure: MCP servers, tool use, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration'.\n\nThe webpage directly addresses **agent memory** and **agentic memory**, **tool use** (via tool calling for retrieval), and mentions **agent frameworks** implicitly through concepts like MemGPT and the discussion of context engineering. It also touches upon **structured outputs** via memory blocks.\n\nHowever, the page **does not mention or discuss**:\n*   MCP servers\n*   LangChain\n*   LlamaIndex\n*   OpenAI Agents SDK\n*   Anthropic Agents SDK\n*   Google SDK\n*   Agent orchestration\n\nSince the page does not cover the majority of the specific infrastructure and SDK components listed in your query, a complete summary addressing all points is not possible.\n\n**No answer found** for the full scope of the query.",
    "published_date": "2025-07-07T00:00:00.000Z",
    "score": null
  },
  "https://developers.openai.com/blog/responses-api/": {
    "content_path": "../data/contents/37a96ea0ee57798e.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:05.125835",
    "title": "Why we built the Responses API"
  },
  "https://press.spglobal.com/2025-12-01-S-P-Global-Collaborates-with-AWS-to-Bring-Trusted-Data-Directly-to-Customer-AI-Workflows": {
    "content_path": "../data/contents/3d0cca7a4d5831df.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:05.163949",
    "title": "S&P Global Collaborates with AWS to Bring Trusted Data Directly to Customer AI Workflows"
  },
  "https://blog.voyageai.com/2025/10/22/the-case-against-llms-as-rerankers/": {
    "content_path": "../data/contents/b5cc9d6a5230b213.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:05.250373",
    "title": "The Case Against LLMs as Rerankers"
  },
  "https://gofastmcp.com/updates": {
    "content_path": "../data/contents/cccd5a040f40cdc0.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:05.368599",
    "title": "FastMCP Updates - FastMCP"
  },
  "https://mistral.ai/news/mistral-ocr": {
    "content_path": "../data/contents/78ded670eeed449a.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:05.407083",
    "title": "Mistral OCR"
  },
  "https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/": {
    "content_path": "../data/contents/12be02af93e0ec05.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:05.435771",
    "title": "The next chapter of the Gemini era for developers"
  },
  "https://research.google/blog/google-research-2024-breakthroughs-for-impact-at-every-scale/": {
    "content_path": "../data/contents/5aae838590a05f8d.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:05.499256",
    "title": "Google Research 2024: Breakthroughs for impact at every scale"
  },
  "https://github.com/Teddy-XiongGZ/RAGLens": {
    "content_path": "../data/contents/302bd568f1c2cd54.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:05.567964",
    "title": "GitHub - Teddy-XiongGZ/RAGLens"
  },
  "https://aclanthology.org/2025.emnlp-main.1062/": {
    "content_path": "../data/contents/ae7718b06538218f.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:05.941740",
    "title": "MultiDocFusion : Hierarchical and Multimodal Chunking Pipeline for Enhanced RAG on Long Industrial Documents"
  },
  "https://developers.openai.com/apps-sdk/quickstart/": {
    "content_path": "../data/contents/0ed974581714ccfd.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:06.021772",
    "title": "Quickstart"
  },
  "https://simonwillison.net/2024/Dec/2/pydanticai/": {
    "content_path": "../data/contents/93745e061d6f3170.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:06.293131",
    "title": "PydanticAI"
  },
  "https://proceedings.iclr.cc/paper_files/paper/2025/hash/1b623663fd9b874366f3ce019fdfdd44-Abstract-Conference.html": {
    "content_path": "../data/contents/8edc3f61ebb11d12.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:06.353800",
    "title": "Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning"
  },
  "https://docling-project.github.io/docling/v2/": {
    "content_path": "../data/contents/75f2ef3cc5889e1b.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:06.491172",
    "title": "V2 - Docling"
  },
  "https://internvl.github.io/blog/2025-08-26-InternVL-3.5/": {
    "content_path": "../data/contents/84c0108ffab861a0.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:06.635652",
    "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency"
  },
  "https://aclanthology.org/2025.acl-long.1528/": {
    "content_path": "../data/contents/dbaa6ed93d8adee6.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:06.840495",
    "title": "REAL-MM-RAG: A Real-World Multi-Modal Retrieval Benchmark"
  },
  "https://huggingface.co/docs/transformers/en/model_doc/qwen3_vl": {
    "content_path": "../data/contents/37b4712e804ad69d.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:06.973141",
    "title": "Qwen3-VL"
  },
  "https://github.com/openai/skills": {
    "content_path": "../data/contents/2cc5eab13f08dcd3.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:07.267685",
    "title": "GitHub - openai/skills: Skills Catalog for Codex"
  },
  "https://www.fintechweekly.com/magazine/articles/agentic-ai-finance-funding-boom": {
    "content_path": "../data/contents/308507d3b274ab3b.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:07.631522",
    "title": "The Rise of Agentic AI: Who is Funding the Next Generation of Autonomous Financial Assistants? - FinTech Weekly"
  },
  "https://github.com/modelcontextprotocol/inspector": {
    "content_path": "../data/contents/f7daa8dde74bba8a.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:07.806932",
    "title": "GitHub - modelcontextprotocol/inspector: Visual testing tool for MCP servers"
  },
  "https://blog.langchain.com/memory-for-agents/": {
    "content_path": "../data/contents/7bbdb5b7a4f5bdb1.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:07.913546",
    "title": "Memory for agents"
  },
  "https://www.paradedb.com/blog/hybrid-search-in-postgresql-the-missing-manual": {
    "content_path": "../data/contents/c2d6e59fa4d7642a.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:07.947721",
    "title": "Hybrid Search in PostgreSQL: The Missing Manual"
  },
  "https://www.alvarezandmarsal.com/thought-leadership/leveraging-portfolio-monitoring-data-with-agentic-ai": {
    "content_path": "../data/contents/0ac3f31fb465e61b.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:08.157256",
    "title": "Access Denied"
  },
  "https://openai.com/index/developers-can-now-submit-apps-to-chatgpt": {
    "content_path": "../data/contents/97debc210b6974cc.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:08.339443",
    "title": "Developers can now submit apps to ChatGPT"
  },
  "https://www.businesswire.com/news/home/20240702978459/en/Qdrant-Launches-Groundbreaking-Pure-Vector-Based-Hybrid-Search-Setting-Higher-Standards-for-RAG-and-AI-Applications": {
    "content_path": "../data/contents/d8c1c570ee62ba93.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:08.450912",
    "title": "Qdrant Launches Groundbreaking Pure Vector-Based Hybrid Search, Setting Higher Standards for RAG and AI Applications"
  },
  "https://github.com/Aeryn666/RegionRAG": {
    "content_path": "../data/contents/ecf6885d6c1ae636.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:08.532420",
    "title": "GitHub - Aeryn666/RegionRAG: [AAAI2026] Source code for RegionRAG"
  },
  "https://www.globenewswire.com/news-release/2025/06/12/3098386/0/en/Milvus-2-6-Built-for-Scale-Designed-to-Reduce-Costs.html": {
    "content_path": "../data/contents/a0604172fe0f91b7.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:08.563341",
    "title": "Milvus 2.6: Built for Scale, Designed to Reduce Costs"
  },
  "https://aijourn.com/building-ai-agents-for-venture-capital-workflows-lessons-from-our-experience/": {
    "content_path": "../data/contents/64153c3cb7a8a8f0.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:08.828588",
    "title": "Building AI Agents for Venture Capital Workflows: Lessons from our Experience"
  },
  "https://openai.com/index/introducing-structured-outputs-in-the-api/": {
    "content_path": "../data/contents/265a674d3f00c632.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:08.877338",
    "title": "Introducing Structured Outputs in the API"
  },
  "https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025": {
    "content_path": "../data/contents/8df5827c2fed7985.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:08.949442",
    "title": "Best Chunking Strategies for RAG in 2025"
  },
  "https://www.anthropic.com/engineering/code-execution-with-mcp/": {
    "content_path": "../data/contents/3f8b7c4c773fc3e4.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:09.073171",
    "title": "Code execution with MCP: building more efficient AI agents"
  },
  "https://github.com/oneal2000/UniFact/": {
    "content_path": "../data/contents/9265dfcd97f5c2ae.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:09.188407",
    "title": "GitHub - oneal2000/UniFact: Unified Automated Evaluation for Hallucination Detection and Fact Verification"
  },
  "https://github.com/mosherino/SEAL-RAG": {
    "content_path": "../data/contents/04cf31138a54093b.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:09.221116",
    "title": "GitHub - mosherino/SEAL-RAG: A multi-hop retrieval-augmented generation system with entity-based reasoning and repair loops"
  },
  "https://github.com/swiss-ai/mmore": {
    "content_path": "../data/contents/3219fbc0f8eeb2eb.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:09.255591",
    "title": "GitHub - swiss-ai/mmore: Massive Multimodal Open RAG & Extraction  A scalable multimodal pipeline for processing, indexing, and querying multimodal documents  Ever needed to take 8000 PDFs, 2000 videos, and 500 spreadsheets and feed them to an LLM as a knowledge base? Well, MMORE is here to help you!"
  },
  "https://github.com/modelcontextprotocol/ext-apps": {
    "content_path": "../data/contents/9f31d2f13c4c9ad8.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:09.312268",
    "title": "GitHub - modelcontextprotocol/ext-apps: Official repo for SDK of upcoming Apps / UI extension"
  },
  "https://github.com/VikParuchuri/marker": {
    "content_path": "../data/contents/beead684105d8121.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:09.362805",
    "title": "GitHub - datalab-to/marker: Convert PDF to markdown + JSON quickly with high accuracy"
  },
  "https://github.com/rongzizi/Awesome-LLM-for-Chart": {
    "content_path": "../data/contents/15501b8faf0fa44d.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:09.390702",
    "title": "GitHub - rongzizi/Awesome-LLM-for-Chart: A curated list of papers and resources on using Large Language Models (LLMs) for chart understanding, including chart data extraction, visual reasoning, chart-based QA, and chart-to-knowledge generation."
  },
  "https://www.zeroentropy.dev/articles/ultimate-guide-to-choosing-the-best-reranking-model-in-2025": {
    "content_path": "../data/contents/9948e845c9cb94f7.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:09.418442",
    "title": "Ultimate Guide to Choosing the Best Reranking Model in 2025 - ZeroEntropy"
  },
  "https://www.lseg.com/en/insights/from-interoperability-to-agents-powering-financial-workflows-with-ai": {
    "content_path": "../data/contents/b09281a0037c1aeb.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:09.649916",
    "title": "From interoperability to agents: Powering financial workflows with AI"
  },
  "https://huggingface.co/learn/cookbook/search_and_learn": {
    "content_path": "../data/contents/7bd260397c080618.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:09.919780",
    "title": "Scaling Test-Time Compute for Longer Thinking in LLMs - Hugging Face Open-Source AI Cookbook"
  },
  "https://huggingface.co/blog/rteb": {
    "content_path": "../data/contents/6409df0f92018357.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:10.385142",
    "title": "Introducing RTEB: A New Standard for Retrieval Evaluation"
  },
  "https://openai.com/index/endex/": {
    "content_path": "../data/contents/bdfd1ae92bf3a029.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:10.442914",
    "title": "Building an autonomous financial analyst with o1 and o3-mini"
  },
  "https://www.analyticsvidhya.com/blog/2025/02/financial-market-analysis-ai-agent/": {
    "content_path": "../data/contents/29ed6d3480795992.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:10.475503",
    "title": "Building a Multi-Agent AI System for Financial Market Analysis"
  },
  "https://huggingface.co/google/embeddinggemma-300m": {
    "content_path": "../data/contents/c38ed95a24ce8321.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:10.859530",
    "title": "google/embeddinggemma-300m · Hugging Face"
  },
  "https://github.com/djsamseng/blpapi-mcp": {
    "content_path": "../data/contents/25c5554f3e789e6b.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:11.232203",
    "title": "GitHub - djsamseng/blpapi-mcp: A MCP server providing financial data from Bloomberg blpapi"
  },
  "https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms": {
    "content_path": "../data/contents/1abbac902877f35f.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:11.452879",
    "title": "A Visual Guide to Reasoning LLMs"
  },
  "https://modelcontextprotocol.io/specification/2025-11-25/basic/index": {
    "content_path": "../data/contents/e4368924c019e142.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:11.580074",
    "title": "Overview - Model Context Protocol"
  },
  "https://huggingface.co/blog/smolvlm": {
    "content_path": "../data/contents/9b2e84fe16a02534.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:12.141768",
    "title": "SmolVLM - small yet mighty Vision Language Model"
  },
  "https://huggingface.co/blog/smolervlm": {
    "content_path": "../data/contents/92942597e1111eb9.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:12.259142",
    "title": "SmolVLM Grows Smaller – Introducing the 256M & 500M Models!"
  },
  "https://github.com/jlowin/fastmcp": {
    "content_path": "../data/contents/8f8e6f416ded65c4.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:12.370469",
    "title": "GitHub - jlowin/fastmcp: 🚀 The fast, Pythonic way to build MCP servers and clients"
  },
  "https://cameronrwolfe.substack.com/p/olmo-3": {
    "content_path": "../data/contents/56f33443fd1dfae9.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:12.682645",
    "title": "Olmo 3 and the Open LLM Renaissance"
  },
  "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00713": {
    "content_path": "../data/contents/042be5554677725d.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:12.757788",
    "title": "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs"
  },
  "https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-bedrock-rerank-api-accuracy-rag-applications/": {
    "content_path": "../data/contents/18194f1c3df02fa5.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:13.160108",
    "title": "Amazon Bedrock now supports Rerank API to improve accuracy of RAG applications"
  },
  "https://milvus.io/docs/zh-hant/v2.5.x/release_notes.md": {
    "content_path": "../data/contents/22383400c1cc826d.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:13.216204",
    "title": "Release Notes Milvus v2.5.x documentation"
  },
  "https://openai.com/index/introducing-chatgpt-agent/": {
    "content_path": "../data/contents/d83da36ac7616de8.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:13.304211",
    "title": "Introducing ChatGPT agent: bridging research and action"
  },
  "https://platform.openai.com/docs/guides/agent-builder": {
    "content_path": "../data/contents/9e3209f2db08ce48.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:13.550303",
    "title": "Agent Builder | OpenAI API"
  },
  "https://planr.com/the-complete-guide-to-ai-powered-portfolio-monitoring-for-private-equity-2025/": {
    "content_path": "../data/contents/8c1d41d4f7118e82.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:14.441966",
    "title": "Planr Omnia - The AI Operating System for Private Equity"
  },
  "https://www.microsoft.com/en-us/research/articles/autogen-v0-4-reimagining-the-foundation-of-agentic-ai-for-scale-extensibility-and-robustness/": {
    "content_path": "../data/contents/c66b54f5a0efe8ea.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:15.055244",
    "title": "AutoGen v0.4: Reimagining the foundation of agentic AI for scale, extensibility, and robustness - Microsoft Research"
  },
  "https://www.reuters.com/business/citadel-debuts-new-ai-tool-equities-investors-cto-subramanian-says-2025-12-03/": {
    "content_path": "../data/contents/227fd76b587a9af0.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:28.674779",
    "title": "Citadel debuts new AI tool for equities investors, CTO Subramanian says"
  },
  "https://finance.yahoo.com/news/milvus-surpasses-40-000-github-010000562.html": {
    "content_path": "../data/contents/0a9c5274dd27b055.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:30.892167",
    "title": "Milvus Surpasses 40,000 GitHub Stars, Reinforcing Leadership in Open-Source Vector Databases"
  },
  "https://agentskills.io/": {
    "content_path": "../data/contents/4a5a5c2b4be3b18c.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:31.895178",
    "title": "Overview - Agent Skills"
  },
  "https://docs.pinecone.io/models/bge-reranker-v2-m3": {
    "content_path": "../data/contents/21fe9fdf79ac2f0e.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:34.910663",
    "title": "bge-reranker-v2-m3 - Pinecone Docs"
  },
  "https://www.reuters.com/business/lseg-partners-with-openai-integrate-chatgpt-with-financial-data-2025-12-03/": {
    "content_path": "../data/contents/13ee606aeb3f364e.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:35.237429",
    "title": "LSEG to integrate financial data into ChatGPT in AI push"
  },
  "https://blogs.nvidia.com/blog/financial-services-agentic-ai/": {
    "content_path": "../data/contents/2fa953faab4ad46f.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:35.320740",
    "title": "AI On: How Financial Services Companies Use Agentic AI to Enhance Productivity, Efficiency and Security"
  },
  "https://www.anthropic.com/research/reasoning-models-dont-say-think": {
    "content_path": "../data/contents/763973bbed381f66.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:36.579904",
    "title": "Reasoning models don't always say what they think"
  },
  "https://developer.nvidia.com/blog/nvidia-nemo-retriever-delivers-accurate-multimodal-pdf-data-extraction-15x-faster/": {
    "content_path": "../data/contents/ecebd4d98cd1a9c8.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:54:37.005101",
    "title": "NVIDIA NeMo Retriever Delivers Accurate Multimodal PDF Data Extraction 15x Faster"
  },
  "https://github.com/openai/openai-apps-sdk-examples": {
    "content_path": "../data/contents/cf76eaa07e914d9e.md",
    "source": "jina",
    "saved_at": "2025-12-28T19:59:38.248112",
    "title": "GitHub - openai/openai-apps-sdk-examples: Example apps for the Apps SDK"
  },
  "https://medium.com/@servifyspheresolutions/rag-without-embeddings-5-powerful-alternatives-you-can-use-today-9beeb570c9dc": {
    "content_path": "../data/contents/15db60cbc6e74fae.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:12.189366",
    "title": "RAG Without Embeddings: 5 Powerful Alternatives You Can Use Today",
    "summary": "The webpage discusses **RAG (Retrieval-Augmented Generation)**, focusing on the common misconception that it *requires* embeddings. It states that RAG combines **retrieval** and **generation** to enhance LLMs, and the standard flow involves generating embeddings, storing them in a vector database, performing a vector search, and then feeding the retrieved chunks to the LLM.\n\nCrucially, the article's title and premise suggest that **there are powerful alternatives to RAG that do not rely on embeddings** for the retrieval step, although the provided text snippet does not detail those five alternatives.\n\nThe page touches upon:\n*   **RAG Architectures** (the standard flow is described).\n*   **Retrieval** (the standard method involves embeddings and vector databases).\n\nIt **does not** explicitly detail:\n*   Vector databases (only mentions examples like Pinecone, Weaviate).\n*   New efficient embedding models.\n*   Rerankers.\n*   RAG alternatives (only promises to discuss them).\n*   Hybrid search.\n*   Chunking strategies.",
    "published_date": null,
    "score": null
  },
  "https://medium.com/@mehulpratapsingh/2025s-ultimate-guide-to-rag-retrieval-how-to-pick-the-right-method-and-why-your-ai-s-success-2cedcda99f8a": {
    "content_path": "../data/contents/9505f5133abcf535.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.633312",
    "title": "2025’s Ultimate Guide to RAG Retrieval: How to Pick the Right Method (And Why Your AI’s Success Depends On It)",
    "summary": "The webpage provides a guide to Retrieval-Augmented Generation (RAG) retrieval methods, focusing on how the choice of retrieval impacts AI system quality.\n\nThe guide covers the following core retrieval types:\n*   **Vector Search (Dense Retrieval):** Uses dense vector embeddings for semantic similarity matching.\n*   **Keyword Search (Sparse Retrieval):** Uses traditional keyword matching (like BM25) for exact term retrieval.\n*   **Hybrid Search:** Combines vector and keyword search, often using Reciprocal Rank Fusion (RRF).\n*   **Graph-Based Retrieval (GraphRAG):** Uses knowledge graphs to preserve entity relationships for multi-hop reasoning.\n*   **Long-Context Retrieval (Long RAG):** Processes entire documents or large sections to preserve narrative flow.\n\nIt also details **2025’s RAG Breakthroughs**, including:\n*   Adaptive Retrieval (dynamically adjusting strategies).\n*   Multimodal RAG (integrating text, images, and video).\n*   Self-Reflective RAG (SELF-RAG) for critiquing retrievals.\n*   Real-Time Knowledge Integration (triggering web searches).\n*   Security-Focused Retrieval.\n*   Agentic RAG Systems for multi-step planning.\n\nThe article emphasizes that the best method depends on the use case and suggests starting with **hybrid search** for most enterprise applications.\n\n**Regarding the specific terms in your query:**\n\n*   **Vector databases:** Mentioned implicitly under Vector Search (\"Converts text into dense vector embeddings... in a shared vector space\").\n*   **Embeddings (new efficient models):** Mentioned under Vector Search (\"using models like BERT or Sentence-BERT\"). The text discusses 2025 advancements but doesn't detail *new efficient models* specifically, only the general concept of using embeddings.\n*   **Rerankers:** Mentioned under Hybrid Search (\"Reduces noise by reranking results (e.g., Cross-Encoder models)\").\n*   **RAG architectures:** The entire article discusses various RAG architectures and retrieval strategies.\n*   **RAG alternatives:** Not explicitly discussed as alternatives to RAG itself, but different retrieval *methods* within RAG are detailed.\n*   **Hybrid search:** Covered extensively.\n*   **Chunking strategies:** Not explicitly detailed, though Long RAG is presented as an alternative to",
    "published_date": "2025-02-16T00:00:00.000Z",
    "score": null
  },
  "https://www.emergentmind.com/papers/2407.06567": {
    "content_path": "data/contents/da186e2f8e233880.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:11.830557",
    "title": "FINCON: LLM Multi-Agent System for Finance",
    "summary": "The webpage describes **FINCON**, an LLM-based multi-agent framework designed for enhanced financial decision-making.\n\nIt addresses the user query regarding **agents and finance** by detailing a system that utilizes a **Manager-Analyst hierarchical structure** inspired by real-world investment firms. This structure facilitates synchronized collaboration for tasks like **single stock trading and portfolio management**.\n\nKey features relevant to the query include:\n*   **Multi-agent system** for complex financial tasks.\n*   **Risk Control Component** using Conceptual Verbal Reinforcement (CVRF) and Conditional Value at Risk (CVaR) for enhanced decision quality and belief updating.\n*   Demonstrated strong generalization capabilities in financial tasks.\n\nThe text **does not explicitly mention** due diligence automation, deal sourcing, portfolio monitoring, financial report generation, or specific integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**.",
    "published_date": "2024-07-09T00:00:00.000Z",
    "score": null
  },
  "https://www.accenture.com/content/dam/accenture/final/accenture-com/document-2/Accenture-Its-Time-to-Rethink-Private-Equity-Due-Diligence-Report.pdf": {
    "content_path": "../data/contents/c4165e6ed0b9152d.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.421763",
    "title": "It's Time to Rethink Private Equity Due Diligence Report | Accenture",
    "summary": "The webpage discusses rethinking **Private Equity (PE) due diligence** to move beyond just risk assessment towards **value creation**.\n\nKey points relevant to your query include:\n\n*   **Technology and Generative AI in Due Diligence:** Leveraging analytics and generative AI can help deal teams scrutinize targets faster and better. Generative AI can potentially **automate up to 30% of due diligence tasks and augment another 20%** in areas like data gathering, analysis, and report generation.\n*   **Increased Complexity:** 75% of PE leaders agree that investments have grown more complex, leading to a greater need for sophisticated analysis.\n*   **Focus on Value Creation Planning:** There is a rising focus on pre-deal activities, including deal flow, sourcing, screening, and due diligence, with the goal of using due diligence to kickstart the value creation plan.\n*   **Leadership Assessment:** Leadership assessment remains critical, with nearly half of leaders citing a lack of talent to execute value creation plans as a top obstacle. Bringing in specialized leadership, like a Chief Transformation Officer (CTO) from Day One, is suggested.\n\nWhile the document heavily focuses on **due diligence automation** and **value creation** within Private Equity using technology (including AI), it **does not specifically mention or detail the use of multi-agent systems** for finance, nor does it mention specific integrations with **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg** beyond citing Preqin and PitchBook data sources.",
    "published_date": "2024-05-30T00:00:00.000Z",
    "score": null
  },
  "https://www.toltiq.com/insights/the-permanent-document-problem-why-ai-can-transform-private-equity-due-diligence": {
    "content_path": "../data/contents/95aef65ea275e743.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.445489",
    "title": "The Permanent Document Problem: Why AI Can Transform Private Equity Due Diligence",
    "summary": "The webpage discusses how Artificial Intelligence (AI), particularly Generative AI (GenAI) with capabilities like vision models and Retrieval-Augmented Generation (RAG), can transform private equity (PE) due diligence by solving the \"Permanent Document Problem.\"\n\nThe core issue in PE due diligence is managing the vast, chaotic, and fragmented information found in Virtual Data Rooms (VDRs), which include documents in various formats (scans, PDFs, Excel, PowerPoint) with inconsistent terminology and legacy data burdens. Traditional approaches focusing on structured data fail because private market data is inherently messy.\n\nThe GenAI breakthrough allows for holistic processing of entire data rooms, enabling:\n*   Comparing management claims against extracted financials from scanned documents.\n*   Analyzing contract terms across hundreds of agreements.\n*   Extracting and reconciling financial metrics from different formats.\n*   Identifying risk patterns missed by manual review.\n*   Generating comprehensive Q&A responses by synthesizing information across thousands of pages.\n\nThis shift is economically viable due to decreasing model pricing and expanding context windows. Firms implementing these integrated intelligence systems are seeing significant results, such as **56% average time savings** across deal workflows and **10-minute report generation** (down from 3-4 hours). The key takeaway is that success in due diligence will belong to firms building systems that thrive on document complexity rather than waiting for perfect structured data.",
    "published_date": "2025-10-15T00:00:00.000Z",
    "score": null
  },
  "https://blog.privateequitylist.com/how-ai-is-transforming-due-diligence-in-private-markets/": {
    "content_path": "../data/contents/ed236318b9b060d5.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.530095",
    "title": "How AI Is Transforming Due Diligence in Private Markets",
    "summary": "AI is transforming due diligence in private markets (private equity, private credit, and venture capital) by automating manual processes, enhancing financial analysis, and improving risk prediction.\n\n**Key applications and benefits mentioned include:**\n\n*   **Automation:** Accelerating document review (contracts, financials, KPIs) from weeks to hours using AI/NLP.\n*   **Financial Analysis:** Enhancing Quality of Earnings (QoE) analysis, detecting irregularities, and supporting valuation.\n*   **Risk Detection:** Using Machine Learning (ML) for red flag detection, risk scoring, and identifying anomalies that traditional methods miss.\n*   **Market Intelligence:** Utilizing Natural Language Processing (NLP) to analyze reports, news, and sentiment for commercial diligence.\n*   **Speed and Confidence:** Enabling faster deal execution, reducing human bias, and increasing deal team conviction.\n*   **Tools:** AI is used in deal sourcing/screening platforms, Virtual Data Rooms (VDRs) with built-in AI review, financial modeling tools, and portfolio intelligence platforms.\n\nThe page also notes that AI augments, rather than replaces, human analysts, and highlights challenges such as data quality, model transparency, and security concerns.",
    "published_date": "2025-12-12T17:00:22.000Z",
    "score": null
  },
  "https://ionanalytics.com/insights/mergermarket/generative-ai-in-due-diligence-crunching-new-numbers": {
    "content_path": "../data/contents/1a344d5b6f4a1499.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.555405",
    "title": "Generative AI in due diligence: Crunching new numbers",
    "summary": "Generative AI is being rediscovered by private equity (PE) firms as a due diligence tool, leveraging its ability to process unstructured data via large language models (LLMs) to analyze complex information sources like social media and private records.\n\n**Key applications and observations in due diligence:**\n\n*   **Data Analysis:** Generative AI helps analyze informal, unfiltered data sources (estimated to be 80% of investigation data) to uncover patterns that answer complex questions, such as discerning management performance from pandemic impact.\n*   **Internal Use Cases:** Initial experiments focus on internal data, including streamlining data sharing, advanced data management, automating busywork, generating reports from internal data, and drafting correspondence (including LP compliance documentation).\n*   **Value Enhancement:** PE firms see AI as a co-pilot to enhance insights in areas like predictive modeling, portfolio monitoring, forecasting industry disruption, and optimizing exit timing, though significant implementation in these areas is still nascent.\n*   **Data Security and Governance Concerns:** A major hurdle is data security, as using external models like ChatGPT requires sending private data via API, raising concerns about data usage for model training. While private deployment of LLMs is possible, results may be inferior to public models. Governance issues also arise regarding data custody and liability for breaches or copyright infringement when using external data.\n*   **Accuracy and Bias:** A risk is that the user-friendly nature of generative AI can lead to overlooking result viability, as models can confirm existing biases if not fed a complete and unbiased dataset. Furthermore, the models can sometimes produce incorrect information, risking reputational and legal issues if misinformation is distributed in reports.\n*   **Talent Focus:** Experts suggest that the advantage in adopting AI will lie more in **talent**—specifically, having individuals with the technical expertise to properly feed models, evaluate inputs, and know when human intervention is needed—rather than just accessing the technology itself.\n*   **Productivity Gains:** For some firms, the immediate benefit is improved productivity, freeing up time for in-person due diligence with founders.\n\n**Venture Capital (VC) Example (Rocketship):**\n\n*   VC firm Rocketship has used a predictive AI deal flow engine based on Natural Language Processing (NLP) since 2014. Generative AI extends this by automating recommendation report creation and building categorization engines.\n*   Rocketship relies exclusively on its algorithms for deal sourcing and investment selection, using over 20",
    "published_date": "2023-08-30T05:21:04.000Z",
    "score": null
  },
  "https://discovery.researcher.life/article/optimizing-due-diligence-with-ai-a-comparative-analysis-of-investment-outcomes-in-technology-enabled-private-equity/b8b20a76a0be3a438a05b79afb9e307c": {
    "content_path": "../data/contents/596b50b39177ee16.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.636980",
    "title": "Optimizing Due Diligence with AI: A Comparative Analysis of Investment Outcomes in Technology-Enabled Private Equity",
    "summary": "The provided webpage text focuses on the use of **Artificial Intelligence (AI) in optimizing due diligence processes within private equity (PE) firms** and its impact on investment outcomes.\n\nIt discusses:\n*   How AI supplements or replaces time-consuming manual due diligence with tools that enhance efficiency and accuracy in processing large datasets.\n*   The application of AI in financial analysis, legal review, and data integration to detect anomalies, predict revenue, and mitigate risks for more informed decisions.\n*   The strategic and operational implications of AI adoption, including shifts in required skillsets and workflows, while emphasizing the need for human oversight and governance.\n\n**However, the text does not mention or discuss:**\n*   Multi-agent systems specifically for finance.\n*   Due diligence automation using multi-agent systems.\n*   Agents for deal sourcing, portfolio monitoring, or financial report generation.\n*   Integrations with specific databases like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.\n\nTherefore, regarding your specific query: **No answer found**.",
    "published_date": "2024-04-20T00:00:00.000Z",
    "score": null
  },
  "https://rsmus.com/insights/industries/private-equity/ai-due-diligence-assessment-private-equity.html": {
    "content_path": "../data/contents/2fd7cf543f7900ba.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.681417",
    "title": "Artificial intelligence-ready or not?",
    "summary": "The provided webpage focuses on **evaluating Artificial Intelligence (AI) readiness during due diligence for private equity deals**. It outlines six prerequisites to look for in a target company to assess its AI maturity and potential for value creation: Data, Technology infrastructure, Skilled talent, Governance and security guardrails, Cultural openness, and Aligned use cases.\n\n**The user query specifically asks about:** 'agents\\_and\\_finance: Multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations'.\n\n**Summary relative to the query:**\n\nThe webpage discusses **due diligence automation** and **private equity** in the context of AI readiness. It emphasizes using due diligence to identify practical AI use cases that can drive early EBITDA gains.\n\nHowever, the page **does not explicitly mention or detail** the use of **multi-agent systems** for finance, venture capital, deal sourcing, portfolio monitoring, or specific integrations with tools like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg. While it touches upon data analysis and financial report generation indirectly through general AI capabilities, the focus is on assessing a company's foundational readiness for *any* AI implementation, not the specifics of multi-agent finance applications.\n\n**No answer found** for the specific details regarding multi-agent systems and the listed financial data platforms.",
    "published_date": "2025-10-16T00:00:00.000Z",
    "score": null
  },
  "https://www.bain.com/insights/harnessing-generative-ai-global-private-equity-report-2024/": {
    "content_path": "../data/contents/4ade912cacf38856.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.716956",
    "title": "Harnessing Generative AI in Private Equity",
    "summary": "The webpage discusses how private equity firms are harnessing Generative AI across three main areas: **within the portfolio** (to transform companies and link initiatives to strategy), **in due diligence** (using scorecards to assess AI threats/opportunities and speed up underwriting), and **at the firm level** (streamlining back-office functions and expanding the scope of information for investment decisions).\n\nGenerative AI is shown to be valuable for:\n*   **Due Diligence:** Speeding up analysis by scanning vast amounts of data (e.g., ingesting 10,000 customer reviews or summarizing interviews) and building live prototypes to test disruption theses.\n*   **Investment Professionals/Deal Sourcing:** Making professionals smarter and faster by significantly reducing the time spent on screening deals (from a day to an hour per company) and leveraging institutional knowledge via chatbots.\n\nThe query specifically asks about **multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, and integrations with PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg.**\n\nWhile the text extensively covers **due diligence automation, investment opportunity analysis, deal sourcing, and portfolio monitoring** using Generative AI, it **does not explicitly mention \"multi-agent systems\"** or specific integrations with external databases like **PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg**. It discusses using AI tools for data analysis and report generation implicitly through summarizing findings, but not in the context of named \"agents\" or specific financial data platform integrations.\n\n**No answer found** for the specific mention of \"multi-agent systems\" and the named external data platforms.",
    "published_date": "2024-03-11T20:55:30.000Z",
    "score": null
  },
  "https://medium.com/@roberto.g.infante/the-state-of-ai-agent-frameworks-comparing-langgraph-openai-agent-sdk-google-adk-and-aws-d3e52a497720": {
    "content_path": "../data/contents/7772f7f2131bac4e.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.774775",
    "title": "The State of AI Agent Frameworks: Comparing LangGraph, OpenAI Agent SDK, Google ADK, and AWS Bedrock Agents",
    "summary": "The user query is a list of keywords related to AI agent infrastructure and frameworks: 'agent\\_infrastructure: MCP servers, tool use, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration'.\n\nThe provided webpage text discusses four major AI agent frameworks: **LangGraph (built on LangChain), OpenAI Agents SDK, Google Agent Development Kit (ADK), and AWS Bedrock Agents**.\n\nHere is a summary of how the page addresses the concepts in the query:\n\n*   **Agent Frameworks:** The entire article compares and contrasts **LangChain/LangGraph, OpenAI Agents SDK, Google ADK, and AWS Bedrock Agents**.\n*   **Tool Use / Function Calling:** This is a core component discussed. All frameworks support this, allowing agents to call external tools/APIs. The text notes the shift from text parsing to structured **function calling** interfaces.\n*   **Agent Memory / Agentic Memory:** Memory is covered extensively. Frameworks support short-term (conversation history) and long-term memory. **LangChain/LangGraph** offers various memory modules; **Google ADK** emphasizes structured session state; **AWS Bedrock Agents** handles session memory automatically; the **OpenAI Agents SDK** relies on feeding conversation history back via the Runner.\n*   **Structured Outputs / Function Calling:** Directly addressed under \"Tool/Function Calling.\" All frameworks leverage structured inputs/outputs for reliable tool invocation. Google ADK also mentions encouraging structured outputs for agent final answers.\n*   **Agent Orchestration:** This is a key theme. **LangGraph** uses an explicit graph model for orchestration. **Google ADK** provides explicit workflow agents (Sequential, Parallel, Loop) alongside dynamic LLM agents. The **OpenAI SDK** relies on code-based handoffs. **AWS Bedrock Agents** abstracts orchestration entirely to the managed service.\n*   **LangChain / LangGraph:** Detailed section covering its open-source nature, graph-based workflow model, and large ecosystem of tools.\n*   **OpenAI Agents SDK:** Detailed section covering its minimalist, code-first approach, reliance on function calling, and support for agent handoffs.\n*   **Google SDK (ADK):** Detailed section covering its structured, code-first approach, explicit workflow agents (Sequence, Parallel), rich built\n\nThe provided webpage discusses the state and comparison of major AI agent frameworks: **LangChain/LangGraph, OpenAI Agents SDK, Google ADK, and AWS Bedrock Agents**.\n\nThe summary relevant to your query, which lists various agent infrastructure components, is as follows:\n\n*   **Agent Frameworks:** The page compares **LangChain/LangGraph**, **OpenAI Agents SDK**, **Google ADK**, and **AWS Bedrock Agents**.\n*   **Tool Use/Function Calling:** All frameworks support tool use, allowing agents to interact with external systems. The OpenAI SDK heavily utilizes **function calling**.\n*   **Agent Memory:** The conclusion mentions that frameworks have mechanisms to maintain **memory and state** so the agent isn't short-sighted.\n*   **Agent Orchestration:** **LangGraph** is highlighted for explicitly modeling workflows, and **Google ADK** supports multi-agent scenarios and structured workflows.\n*   **Structured Outputs:** While not explicitly detailed as a dedicated section, the frameworks support structured outputs through features like OpenAI's function calling and the general ability to enforce response formats via guardrails or callbacks.\n*   **Guardrails and Safety:** All frameworks feature robust safety mechanisms: LangChain uses **evaluators as guardrails**, OpenAI has built-in **Guardrail objects** (input/output), Google ADK uses **callbacks and safety filters**, and AWS Bedrock offers **configurable Guardrails** (e.g., profanity blocking, PII masking).\n*   **Evaluation and Observability:** All frameworks emphasize **Evaluation-Driven Development (EDD)**. LangChain uses **LangSmith** for tracing and evaluation. OpenAI offers **tracing** and the **OpenAI Evals** library. Google ADK has a built-in **evaluation module** and comprehensive telemetry via Vertex AI. AWS Bedrock provides a **trace viewer** in the console.\n*   **MCP Servers/Deployment:** The text discusses deployment flexibility. OpenAI SDK and Google ADK can be run on your own infrastructure (self-managed). AWS Bedrock Agents are inherently tied to AWS-managed infrastructure, trading portability for ease-of-use.\n\n**Note on Missing Terms:** The page does not explicitly mention \"MCP servers,\" \"agentic memory,\" or specific SDKs for Anthropic or general \"Google SDK\" (it focuses on the ADK).",
    "published_date": "2025-07-10T00:00:00.000Z",
    "score": null
  },
  "https://www.prompthub.us/blog/openais-agents-sdk-and-anthropics-model-context-protocol-mcp": {
    "content_path": "../data/contents/9d31666d801692f7.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.804923",
    "title": "OpenAI's Agents SDK and Anthropic's Model Context Protocol (MCP)",
    "summary": "The webpage discusses two significant recent developments in AI agent building: **OpenAI's Agents SDK** and **Anthropic's Model Context Protocol (MCP)**.\n\n**OpenAI's Agents SDK** is a lightweight Python framework for building AI agents that can plan, use tools, and execute multi-step tasks. Key features include:\n*   **Agent Orchestration:** Managing multiple agents, creating handoffs between them, and tracing interactions.\n*   **Built-in Tools:** Includes Web Search, File Search, and Computer Use tools.\n*   **Responses API:** A new API designed to handle multi-turn conversations, tool calls, and various input modalities, signaling a shift away from the sunsetting Assistants API.\n\n**Anthropic's Model Context Protocol (MCP)** is an open, model-agnostic standard for connecting AI models to external data sources and tools securely.\n*   **MCP Servers:** Connect to specific data sources (like databases or Slack) and expose them via the protocol.\n*   **Standardized Interface:** Acts like a \"USB-C port for AI,\" allowing any model (Claude, GPT, open-source) to query these servers dynamically without custom integrations.\n\n**Comparison and Synergy:**\nThe two are complementary, not substitutes. The **Agents SDK** focuses on the *framework* for orchestrating agents (planning, execution, handoffs), while **MCP** focuses on the *protocol* for securely accessing external data and tools. They can be used together, for instance, using the Agents SDK to orchestrate an agent that uses MCP to fetch secure data from an internal database.",
    "published_date": null,
    "score": null
  },
  "https://blog.stackademic.com/agent-vii-navigating-the-agentic-ai-landscape-a-deep-dive-into-frameworks-powering-next-gen-ai-c3b18102408e": {
    "content_path": "../data/contents/3c67915686297827.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:13.947528",
    "title": "Agent VII —  Navigating the Agentic AI Landscape: A Deep Dive into Frameworks Powering Next-Gen AI Agents",
    "summary": "The webpage provides a deep dive into various AI agent frameworks, covering their architecture, use cases, pros, cons, and suitable contexts.\n\nHere is a summary of the information relevant to your query:\n\n*   **Agent Frameworks:** The page details **CrewAI, LangGraph (LangChain), LlamaIndex, Smolagents, OpenAI Agents SDK, PydanticAI, AutoGen, Semantic Kernel, Atomic Agents, Google ADK, and Azure AI Agents**.\n*   **Tool Use/Function Calling/Structured Outputs:**\n    *   **CrewAI:** Agents can be given **tools** (APIs, retrieval, etc.) and use LLMs as reasoning engines to select actions via user-defined tool functions.\n    *   **LangGraph:** Tools are integrated via LangChain's ecosystem (e.g., search, calculators, databases).\n    *   **LlamaIndex:** Agents can call **tools** and use memory.\n    *   **Smolagents:** Supports using **LangChain tools** or **MCP tools** (though MCP tools are only mentioned in passing as a supported type).\n    *   **OpenAI Agents SDK:** Agents call **tools** via the function-calling API. It auto-generates JSON schemas for tool calls using Pydantic.\n    *   **PydanticAI:** Uses `@` decorators to define Python functions as LLM-callable **tools**, with automatic JSON schema generation from type hints, ensuring **structured outputs**.\n    *   **AutoGen:** Agents can call external **tools** (API clients, search, etc.).\n    *   **Semantic Kernel:** Agents use **plugins** (tools) to call APIs/functions.\n    *   **Atomic Agents:** Emphasizes clear input/output **schemas** (via Pydantic) for tools and chaining components.\n    *   **Google ADK:** Provides built-in **tools** (Web Search, Code Exec) and allows plugging in any API as a tool.\n*   **Agent Memory/Agentic Memory:**\n    *   **LangGraph:** Has a built-in **state/memory** feature that persistently logs data across nodes, enabling short-term and long-term memory.\n    *   **LlamaIndex:** Includes a **Memory Module** where previous interactions can be stored in a vector DB and recalled (RAG memory).\n    *   **Semantic Kernel",
    "published_date": "2025-05-28T16:10:26.000Z",
    "score": null
  },
  "https://langfuse.com/blog/2025-03-19-ai-agent-comparison": {
    "content_path": "../data/contents/5d4b0639802906e9.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.079007",
    "title": "Open-Source AI Agent Frameworks: Which One Is Right for You?",
    "summary": "The webpage discusses and compares several leading **open-source AI agent frameworks**, including **LangGraph**, the **OpenAI Agents SDK**, **Smolagents**, **CrewAI**, **AutoGen**, **Semantic Kernel**, **LlamaIndex agents**, **Strands Agents**, and **Pydantic AI agents**.\n\nThe comparison highlights their core paradigms, primary strengths, and best use cases. For example:\n*   **LangGraph** uses a graph-based workflow for complex, multi-step tasks requiring explicit control.\n*   **OpenAI Agents SDK** is best for those deeply integrated into the OpenAI ecosystem.\n*   **CrewAI** focuses on multi-agent collaboration using role-based crews.\n*   **LlamaIndex agents** excel in data-heavy tasks involving Retrieval-Augmented Generation (RAG).\n*   **Strands Agents** is model-agnostic and emphasizes production readiness with OpenTelemetry tracing.\n\nThe article also touches on decision factors like task complexity, multi-agent needs, and integrations, and emphasizes the importance of observability tools like Langfuse for debugging agent behavior.\n\n**Regarding your specific query terms:**\n\n*   **agent frameworks:** The entire page is a comparison of these.\n*   **LangChain:** Mentioned as the base library extended by LangGraph.\n*   **LlamaIndex:** Mentioned via \"LlamaIndex agents.\"\n*   **OpenAI Agents SDK:** Explicitly detailed.\n*   **Anthropic Agents SDK:** Not explicitly detailed, though **Strands Agents** supports Anthropic via LiteLLM.\n*   **Google SDK:** Not mentioned.\n*   **function calling/structured outputs:** Implied within the capabilities of frameworks like the OpenAI Agents SDK and Pydantic AI (which focuses on type safety/validation).\n*   **agent orchestration:** A core theme, addressed by frameworks like LangGraph and Semantic Kernel.\n*   **agent memory/agentic memory:** Mentioned as being attachable to CrewAI, but not detailed as a primary feature across all frameworks.\n*   **agent_infrastructure: MCP servers, tool use:** Not mentioned.\n\n**Summary based on query:** The page details several agent frameworks, including LangChain (via LangGraph), LlamaIndex agents, and the OpenAI Agents SDK, focusing on orchestration, collaboration, and data retrieval capabilities. It does not specifically detail MCP servers or provide a dedicated section on agent memory",
    "published_date": "2025-03-19T00:00:00.000Z",
    "score": null
  },
  "https://agentkit.inngest.com/concepts/memory": {
    "content_path": "../data/contents/89606b99d7dbcbc8.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.280755",
    "title": "Memory - AgentKit by Inngest",
    "summary": "The provided web page from AgentKit by Inngest details how to give agents **long-term, reflective memory** using **Mem0**.\n\nThe core mechanism involves abstracting memory operations (create, read, update, delete) into **tools** that an agent can call. These tools leverage **Inngest** for durable, asynchronous background processing of memory writes, ensuring faster agent responses.\n\nThe page outlines two primary patterns for integrating memory tools:\n\n1.  **Granular, Single-Purpose Tools:** Creating separate tools for each operation (`create_memories`, `recall_memories`, `update_memories`, `delete_memories`). This offers fine-grained control.\n2.  **Consolidated Tools:** Using a single tool (`manage_memories`) to handle all modifications (create, update, delete) atomically, which is often preferred for multi-agent networks.\n\nIt also describes two architectural patterns for agent memory implementation:\n\n1.  **Autonomous Agent w/ Tools (Non-Deterministic):** A single agent uses its reasoning to decide when and how to use memory tools based on a detailed system prompt (e.g., a \"recall-reflect-respond\" process).\n2.  **Multi-Agent or Lifecycle-based (Deterministic):** A network of specialized agents (e.g., retrieval agent, assistant agent, updater agent) orchestrated by a code-based router ensures a predictable, step-by-step workflow.\n\nAdvanced patterns include **State-Based Memory Retrieval/Routing** and **Lifecycle Integration** (`onStart`/`onFinish` hooks) to manage memory operations outside of explicit tool calls.\n\n**Regarding your specific query terms:**\n\n*   **agent_infrastructure:** The page discusses agent concepts, tools, networks, and deployment within the AgentKit framework.\n*   **tool use:** Central to the memory implementation, detailing how tools are created and used for memory operations.\n*   **agent memory, agentic memory:** The entire page is dedicated to providing agents with long-term, reflective memory using Mem0.\n*   **agent frameworks:** The context is **AgentKit by Inngest**.\n*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** **Not mentioned.**\n*   **function calling, structured outputs:** Related concepts are implied through tool definition parameters, but these specific terms are **not explicitly detailed",
    "published_date": "2024-11-27T00:00:00.000Z",
    "score": null
  },
  "https://www.linkedin.com/posts/kwkramer_memory-for-voice-ai-agents-and-composite-activity-7292569739002884101-NtRZ": {
    "content_path": "../data/contents/53b8036f6c11f5d5.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.311275",
    "title": "Memory for voice AI agents (and composite function calling) ...\n\nThere are… | Kwindla Hultman Kramer | 10 comments",
    "summary": "The webpage discusses methods for storing and retrieving conversation state for voice AI agents, focusing on using local filesystems with function calling hooks (specifically mentioning Pipecat and Gemini 2.0 Flash). A key feature highlighted is **composite function calling**, where an LLM can chain multiple function calls together to perform complex operations, such as loading a conversation from a natural language description like \"a few minutes ago\" by first listing available files and then selecting the correct one to load.\n\nThe user query asks about several specific technical terms related to agent infrastructure, including: `agent_infrastructure: MCP servers, tool use, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration`.\n\nThe page directly addresses **agent memory** (storing/retrieving conversation state), **tool use** (via function calling), and **function calling**. It also mentions **Google SDK** implicitly through the use of Gemini.\n\nHowever, the page **does not mention or discuss**: MCP servers, agent frameworks (LangChain, LlamaIndex), OpenAI Agents SDK, Anthropic Agents SDK, structured outputs, or agent orchestration (beyond the specific example of composite function calling).\n\n**Summary based on the query:**\n\nThe page focuses on **memory for voice AI agents** and demonstrates **composite function calling** using Gemini 2.0 Flash to flexibly load past conversations based on natural language input. This involves **tool use** where the LLM organically determines the necessary sequence of function calls. While it touches upon memory and function calling, it does not provide information on MCP servers, LangChain, LlamaIndex, the specific SDKs mentioned (OpenAI, Anthropic, Google SDK in detail), structured outputs, or general agent orchestration.",
    "published_date": "2025-02-04T00:00:00.000Z",
    "score": null
  },
  "https://blog.dataengineerthings.org/when-function-calling-isnt-enough-the-intuition-behind-ai-agents-8873ab5131ad": {
    "content_path": "../data/contents/d04f2ec714519c81.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.341981",
    "title": "When Function Calling Isn’t Enough — The Intuition Behind AI Agents",
    "summary": "The provided web page explains the concept of AI Agents as an evolution beyond simple Function Calling, focusing on the key components that enable autonomous problem-solving.\n\nHere is a summary of the agent infrastructure components mentioned in the text, in relation to your query:\n\n*   **Agent Core (Brain):** Uses a Large Language Model (LLM) to interpret instructions, reason, generate language, and orchestrate other components.\n*   **Memory Module:** Allows the agent to retain context across multiple interactions by automatically storing conversation history and tool outputs. The text specifically discusses implementing this using **LangChain's** `ConversationBufferMemory`.\n*   **Planning Module:** Enables the agent to think ahead, break down complex tasks into sequential steps (using techniques like chain-of-thought reasoning), and decide the correct sequence of tool invocations autonomously.\n*   **Tools:** External functions or APIs the agent can use to interact with the real world (e.g., fetching sensor data or dates). The agent autonomously calls these tools and feeds the results back into the loop.\n\nThe article demonstrates building an agent using **LangChain** and a **Databricks LLM** (`ChatDatabricks`), utilizing the `initialize_agent` function with `AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`.\n\n**Regarding other terms in your query:**\n\n*   **agent_infrastructure, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration:** The text heavily focuses on **LangChain** as the framework and **function calling** as the precursor concept. It describes the autonomous orchestration of tools, which is the core of agent behavior. **Structured outputs** are implied by the agent type used (`STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`).\n*   **LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** These specific SDKs are **not** discussed or mentioned in the provided text.\n*   **agentic memory:** This term is used synonymously with the **Memory Module** described in the text.\n*   **MCP servers (Model Context Protocol):** The conclusion mentions that **Model Context Protocol (MCP)** will be covered in **upcoming blogs**, but it is **not** explained in the current article.\n\n**In summary:** The page details the core components of an agent (",
    "published_date": "2025-10-29T17:00:12.000Z",
    "score": null
  },
  "https://fireworks.ai/blog/function-calling": {
    "content_path": "../data/contents/2209df094b3e1741.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.390568",
    "title": "Understanding Function Calling: The Bridge to Agentic AI",
    "summary": "The user query asks for a summary covering several specific technical terms related to AI agents, including **agent infrastructure, MCP servers, tool use, agent memory, agentic memory, agent frameworks (LangChain, LlamaIndex, SDKs for OpenAI, Anthropic, Google), function calling, structured outputs, and agent orchestration.**\n\nHere is a summary of the page content relevant to the query:\n\n**Function Calling and Agentic AI:**\n*   **Function Calling (Tool Use/Tool Calling):** This is the process where a Large Language Model (LLM) detects a need for external data or action and produces a **structured output** (typically JSON) specifying which function to call and its arguments. This transforms LLMs from static generators into dynamic, interactive agents capable of real-world tasks.\n*   **Technical Underpinnings:** Involves **Tool Specification and Prompting** (defining functions with JSON schemas), **Detecting and Generating Function Calls** (outputting structured JSON), and **Function Execution and Feedback Loop** (an external system executes the function and feeds the result back to the LLM).\n*   **Path to Agentic AI:** Function calling is a stepping stone to **agentic AI**, which involves **Multi-Step Planning** (orchestrating workflows), **Adaptive Decision Making**, and **Memory and Context** (maintaining state across interactions).\n\n**Infrastructure and Standards:**\n*   **Model Context Protocol (MCP):** An open standard enabling secure, two-way connections between data sources and AI-powered tools. It acts as a standardized interface, solving the \"N×M problem\" by providing **universal compatibility** between any MCP-compliant AI and any **MCP server**.\n\n**Agent Orchestration and Frameworks:**\n*   The text discusses the evolution toward **agentic systems** that orchestrate entire workflows (a form of **agent orchestration**).\n*   While the text mentions the need for building effective AI agents and the future involving collaborative networks, it **does not explicitly mention or detail** the specific agent frameworks listed in the query: **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, or Google SDK.**\n*   The text also **does not explicitly detail** concepts like **agent memory** or **agentic memory** beyond mentioning the general need for \"Memory and Context\" in agentic systems.\n\n**Conclusion:**\nThe page extensively covers **function calling**,",
    "published_date": "2025-07-11T00:00:00.000Z",
    "score": null
  },
  "https://microsoft.github.io/ai-agents-for-beginners/13-agent-memory/": {
    "content_path": "../data/contents/5ede8b40211150ce.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.414557",
    "title": "ai-agents-for-beginners",
    "summary": "The user query lists several technical terms related to AI agents, including infrastructure components, frameworks, SDKs, and agent capabilities like memory and tool use.\n\nThe provided webpage focuses exclusively on **Memory for AI Agents**, detailing:\n*   **Types of Memory:** Working, Short-Term, Long-Term, Persona, Workflow/Episodic, Entity, and Structured RAG.\n*   **Implementation/Storage Tools:** Mem0 and Cognee, and using Azure AI Search for Structured RAG.\n*   **Self-Improvement:** How memory enables agents to learn via a \"knowledge agent\" pattern.\n\nThe webpage **does not** discuss the following specific terms from the user query:\n*   agent\\_infrastructure: MCP servers\n*   tool use (only mentioned generally as a benefit of agents)\n*   agent frameworks (only LangChain and LlamaIndex are mentioned in the context of tutorials, but the main focus is on memory implementation)\n*   LangChain (mentioned in the context of a tutorial, but not detailed)\n*   LlamaIndex (mentioned in the context of a tutorial, but not detailed)\n*   OpenAI Agents SDK\n*   Anthropic Agents SDK\n*   Google SDK\n*   function calling\n*   structured outputs\n*   agent orchestration\n\nSince the page only covers a small subset of the requested topics (primarily memory), and omits most of the infrastructure, SDKs, and specific capabilities listed, the summary must reflect this limitation.\n\n**Summary:**\n\nThe webpage focuses on **Memory for AI Agents**, explaining its importance and detailing various types, including Working, Short-Term, Long-Term, Persona, Episodic, and Entity memory. It covers implementation using specialized tools like **Mem0** and **Cognee**, and leveraging **Azure AI Search** for Structured RAG. It also touches upon how memory enables self-improving agents.\n\nHowever, the page **does not** provide information on **MCP servers, tool use, agent frameworks (beyond mentioning tutorials for Semantic Kernel, Mem0, and Cognee), OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, or agent orchestration.**",
    "published_date": null,
    "score": null
  },
  "https://learn.microsoft.com/en-us/agent-framework/tutorials/agents/memory": {
    "content_path": "../data/contents/a55c86bc796ccab8.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.440799",
    "title": "Adding Memory to an Agent",
    "summary": "This page provides a tutorial on **Adding Memory to an Agent** using Microsoft's agent framework, focusing on implementing a custom context provider (`AIContextProvider` in C# or `ContextProvider` in Python) and attaching it to a `ChatClientAgent` or `ChatAgent`, respectively.\n\nKey aspects covered include:\n\n*   **Implementing Memory:** Creating a custom class that inherits from `AIContextProvider` or `ContextProvider` to manage state (like user information).\n*   **Pre and Post Invocation Events:** Overriding methods (`InvokingAsync`/`Invoking` and `InvokedAsync`/`Invoked`) to inject context (instructions, tools) before the agent calls the inference service and to inspect results afterward.\n*   **Serialization:** Implementing serialization methods to persist the memory component's state when the agent thread is saved and resumed.\n*   **Attaching to the Agent:** Using an `AIContextProviderFactory` (C#) or passing the instantiated provider directly (Python) when creating the agent to ensure the memory is available for each thread.\n*   **Example:** A sample implementation (`UserInfoMemory`) demonstrates extracting and storing a user's name and age, using the memory to guide the agent's behavior, and accessing the stored memory via the thread.\n\nThe tutorial specifically uses C# with `ChatClientAgent` and Python with `ChatAgent`.\n\n**Regarding your query keywords:**\n\nThe page directly addresses **agent memory** and **agent frameworks** (specifically the Microsoft agent framework). It shows how to implement custom logic that acts like a form of **agent orchestration** by modifying the context before and after inference. It also touches upon providing **instructions** to the agent.\n\nHowever, the page **does not** mention or discuss: **MCP servers, tool use, agentic memory, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, or structured outputs** (though the Python example uses Pydantic for structured output extraction from the chat client, it's not a general discussion of structured outputs in the agent framework context).",
    "published_date": "2025-10-02T00:00:00.000Z",
    "score": null
  },
  "https://github.com/microsoft/agent-framework": {
    "content_path": "../data/contents/b6a803a2c827c895.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.465475",
    "title": "Search code, repositories, users, issues, pull requests...",
    "summary": "The webpage describes the **Microsoft Agent Framework**, a framework for building, orchestrating, and deploying AI agents and multi-agent workflows with support for **Python** and **.NET**.\n\nKey features highlighted include:\n*   **Graph-based Workflows** with streaming, checkpointing, human-in-the-loop, and time-travel capabilities.\n*   Support for **Python** and **C#/.NET** implementations with consistent APIs.\n*   **Observability** via OpenTelemetry integration.\n*   Support for **Multiple Agent Providers**.\n*   A flexible **Middleware** system.\n*   An interactive **DevUI** for development, testing, and debugging.\n\nWhile the query lists many specific concepts related to agent infrastructure (like MCP servers, LangChain, LlamaIndex, OpenAI Agents SDK, function calling, structured outputs, etc.), the webpage focuses on the features of the **Microsoft Agent Framework** itself. It mentions **tool usage** in the Python samples section (\"Getting Started with Agents: basic agent creation and **tool usage**\") and **orchestration** in its description and topics. It does not explicitly mention or detail MCP servers, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, agent memory, agentic memory, or structured outputs by name.\n\n**Summary relevant to the query:** The page introduces the **Microsoft Agent Framework** for building and **orchestrating** AI agents in Python and .NET, which supports **tool usage** and multi-agent workflows.\n\n**Concepts from the query that are NOT explicitly detailed or mentioned:** MCP servers, agent memory, agentic memory, agent frameworks (other than this one), LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs.",
    "published_date": "2025-04-28T19:40:42.000Z",
    "score": null
  },
  "https://blog.promptlayer.com/llm-agents-vs-function-calling/": {
    "content_path": "../data/contents/7cd3633afe77d34d.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.518318",
    "title": "LLM Agents vs. Function Calling: An Analysis of Techniques",
    "summary": "The user query lists several technical terms related to LLM agents and infrastructure: 'agent\\_infrastructure: MCP servers, tool use, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration'.\n\nHere is a summary of the relevant information found in the text:\n\n*   **Tool Use:** LLM Agents utilize **Tools**, which are external functions or APIs that the agent can access to interact with the real world. Function Calling is also a technique that empowers LLMs to interact with external tools/APIs.\n*   **Agent Memory:** LLM Agents include a **Memory Module** that allows them to retain context from previous interactions, categorized as short-term (scratchpad) or long-term (diary).\n*   **Function Calling & Structured Outputs:** Function Calling is a technique where an LLM produces **structured data** (usually JSON) specifying which predefined function to execute.\n*   **Agent Orchestration:** The text mentions that LLM Agents can involve **Multi-Agent Collaboration** within a framework. PromptLayer Agents allow users to create complex AI workflows combining multiple LLMs and business logic, suggesting a form of orchestration.\n\n**Terms not explicitly detailed or found in the text:**\n\n*   MCP servers\n*   Agentic memory (though \"Memory Module\" is discussed)\n*   Agent frameworks (LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK)\n*   Agent orchestration (beyond the mention of multi-agent collaboration and workflow building)\n\n**Summary:**\n\nThe document primarily compares LLM Agents and Function Calling. LLM Agents are described as modular systems featuring an **Agent Core (Brain), Memory Module, Planning Module, and Tools** for complex, multi-step reasoning. **Function Calling** is a simpler method enabling LLMs to generate **structured outputs** (like JSON) to invoke predefined external functions or APIs. Agents use **tool use** autonomously within a plan and possess internal **memory**, while Function Calling focuses on structured interaction with external systems. The text also mentions the possibility of **multi-agent collaboration** and workflow building tools that facilitate orchestration.\n\n**No answer found** for specific mentions of \"MCP servers,\" \"agentic memory,\" or specific agent frameworks like \"LangChain,\" \"LlamaIndex,\" \"OpenAI Agents SDK,\" \"Anthropic",
    "published_date": "2025-03-13T00:00:00.000Z",
    "score": null
  },
  "https://www.xugj520.cn/en/archives/ai-agent-memory-systems.html": {
    "content_path": "../data/contents/2c820cd4377255b5.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:14.585885",
    "title": "Why Your AI Agent Keeps Forgetting—and How to Give It a Human-Like Memory",
    "summary": "The user query is a list of technical terms related to AI agent infrastructure and development. The webpage discusses the \"Amnesia Problem\" in AI agents, focusing heavily on **agent memory** (Memory, Memories, **Agentic Memory**), memory architectures (Human-Centric vs. Code-Centric stacks), data journeys, short-term tricks, long-term housekeeping (ADD, UPDATE, DELETE), storage mediums (Vector DB, Graph DB, Relational DB), and provides a **Framework Cheat-Sheet**.\n\nThe framework cheat-sheet explicitly mentions:\n*   **LangGraph** (under Framework Cheat-Sheet)\n*   **Google ADK** (Google Agents Development Kit, which relates to Google SDK)\n\nThe concepts of **tool use**, **function calling** (implied by tool use), **structured outputs** (implied by memory storage formats like triplets/tables), and **agent orchestration** (implied by memory flows and management) are covered in the context of building memory systems.\n\nHowever, the page **does not explicitly mention or detail**:\n*   **MCP servers**\n*   **OpenAI Agents SDK**\n*   **Anthropic Agents SDK**\n*   **LlamaIndex** (though it discusses vector DBs and memory layers which LlamaIndex facilitates)\n\n**Summary based on the query:**\n\nThe webpage focuses on **agent memory** systems, contrasting short-term context windows with long-term storage solutions (like Vector DBs). It details memory architectures (e.g., Working, Semantic, Episodic, Procedural), the necessity of **ADD, UPDATE, and DELETE** operations for housekeeping, and discusses metrics like hit rate and latency. It mentions **LangGraph** and the **Google ADK** in its framework list. Concepts like **tool use** and **agent orchestration** are central to how agentic memory is managed (deciding when to write/read memory).\n\n**No answer found** for specific mentions of: **MCP servers**, **OpenAI Agents SDK**, **Anthropic Agents SDK**, or **LlamaIndex**.",
    "published_date": "2025-11-21T00:00:00.000Z",
    "score": null
  },
  "https://github.com/aman-panjwani/multimodal-rag-llm": {
    "content_path": "data/contents/22d175cc49eb437e.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.505856",
    "title": "Search code, repositories, users, issues, pull requests...",
    "summary": "This webpage describes a GitHub repository named `multimodal-rag-llm` which implements a **Multimodal Retrieval-Augmented Generation (RAG)** pipeline.\n\nThe pipeline's purpose is to enable **chatting with any PDF** by processing documents containing **text, tables, and images**.\n\nKey features and technologies mentioned that align with your query include:\n\n*   **Multimodal RAG:** The core functionality.\n*   **PDF Parsing/Document Understanding:** It uses `Unstructured` to parse PDFs into text chunks, tables (as HTML strings), and images (as base64 JPEGs).\n*   **Summarization/Generation:** It uses LLMs (`Groq`'s `llama-3.1-8b-instant` for text/tables and `OpenAI`'s `gpt-4o-mini` for image descriptions) to summarize extracted components.\n*   **LLMs used:** **Groq** and **OpenAI** (specifically mentioning `gpt-4o-mini`).\n*   **Report Generation/Intelligent Question Answering:** The final step uses `gpt-4o-mini` to generate coherent answers grounded in the retrieved multimodal context.\n*   **Structured Document Output:** While not explicitly detailing structured output formats like JSON, the process involves parsing tables into structured HTML-like strings before summarization and retrieval.\n\nThe project is powered by **LangChain** and uses **ChromaDB** for vector storage.",
    "published_date": "2025-04-10T03:36:34.000Z",
    "score": null
  },
  "https://github.com/Clay-111/Multimodal-RAG-PDFs": {
    "content_path": "data/contents/7956d89a0f5b1210.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.605494",
    "title": "GitHub - Clay-111/Multimodal-RAG-PDFs: A multimodal RAG pipeline for PDFs that integrates OCR-based text extraction, VLMs for image understanding, and an LLM-powered Q&A chain, supported by PineconeDB and Streamlit UI.",
    "summary": "This webpage describes a **Multimodal RAG Pipeline for PDFs** built using Streamlit, LangChain, Pinecone, and Google Gemini (Text + Vision models).\n\nThe system performs the following:\n*   **Multimodal Input:** Uploads and processes both text and images from PDFs (Bangla/English supported).\n*   **Extraction & OCR:** Uses OCR (via `pytesseractOCR`) for text extraction and extracts images from the PDF.\n*   **Vision-Language Models (VLMs):** Generates descriptions for the extracted images using the **Gemini Vision model**.\n*   **RAG Pipeline:** Chunks the extracted text and image descriptions, embeds them using HuggingFace embeddings, and stores them in **Pinecone Vector DB**.\n*   **Q&A:** Powers an LLM-based Question & Answer chain using the **Gemini LLM** to retrieve context from Pinecone.\n*   **UI:** Provides a **Streamlit UI** for uploading, viewing preprocessed data (text chunks and image descriptions), asking questions, and viewing the source chunks used for the answer.\n\nIt specifically integrates **VLMs for image understanding** and uses **Gemini** for both vision and text generation within a **Retrieval-Augmented Generation (RAG)** framework applied to documents.",
    "published_date": "2025-08-18T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/@marouane.lasmak/building-a-multimodal-rag-system-for-pdf-files-with-langchain4j-gemini-and-hugging-face-399be16250cf": {
    "content_path": "data/contents/0934d51175dea448.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.671102",
    "title": "Building a Multimodal RAG System for PDF Files with Langchain4j, Gemini, and Hugging Face",
    "summary": "The webpage describes how to build a **Multimodal Retrieval-Augmented Generation (RAG) system for PDF files** using **Langchain4j**, **Gemini** (for vision/description generation), and **Hugging Face** (for embeddings).\n\nThe system performs the following key functions:\n1.  **PDF Processing:** Uses Apache PDFBox to extract both text and images from PDFs.\n2.  **Multimodal Interaction:** For each extracted image, it uses the **Gemini** language model to generate a textual description.\n3.  **Document Understanding/Indexing:** It combines the extracted text and image descriptions, generates vector embeddings using a **Hugging Face** model (`sentence-transformers/all-MiniLM-L6-v2`), and stores them in a **PgVector** embedding store.\n4.  **Report Generation/Chat:** It uses **Langchain4j** to tie these components together in a Spring Boot application, enabling a chat interface powered by the retrieved context (RAG) and responses generated by the **Gemini** LLM.\n5.  **Context Management:** It includes configuration for **Chat Memory** to maintain context across interactions.\n\nIn summary, the article details the implementation of a multimodal RAG pipeline covering PDF parsing, chart/table extraction (implied through general document understanding, though not explicitly detailed as a separate step), image description generation using Gemini, and structured document output via the LLM-powered chat service.",
    "published_date": "2025-02-10T00:00:00.000Z",
    "score": null
  },
  "https://pub.towardsai.net/docling-an-opensource-python-library-for-pdf-parsing-ocr-support-rag-ibm-research-fe6177235329?gi=c08fb943d9ed": {
    "content_path": "data/contents/f23f0bf882de4c49.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.838564",
    "title": "Docling — An OpenSource Python library for PDF Parsing | OCR Support | RAG | IBM Research",
    "summary": "The webpage describes **Docling**, an open-source Python library from IBM Research for **PDF parsing** and handling other formats (DOCX, PPTX, Images, HTML, AsciiDoc). It converts these formats locally into **Markdown or JSON**, which is useful for **RAG (Retrieval-Augmented Generation)** use cases where LLMs require structured input. Docling can parse text, **tables**, and maintain structure like page layout and reading order.",
    "published_date": "2024-11-22T00:00:00.000Z",
    "score": null
  },
  "https://www.instill-ai.com/blog/the-best-way-to-parse-complex-pdfs-for-rag-hybrid-multimodal-parsing": {
    "content_path": "data/contents/1c4bfa4a5e5b4216.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.869946",
    "title": "The Best Way to Parse Complex PDFs for RAG: Hybrid Multimodal Parsing",
    "summary": "The webpage discusses the challenges of parsing complex PDF documents for use in Retrieval-Augmented Generation (RAG) systems and introduces **Instill AI's Hybrid Multimodal Approach** as the best solution.\n\nThis approach combines:\n1.  **Heuristic methods** (like their Document Operator) to generate an initial structured Markdown draft.\n2.  **Multimodal Visual Language Models (VLMs)** to refine the draft by correcting inaccuracies, formatting complex elements like tables, and adding details for visual elements, using high-resolution images of the document pages.\n\nThis hybrid method aims to mitigate the inaccuracies (hallucinations or omissions) sometimes seen in purely AI-based parsing while overcoming the structural limitations of purely heuristic methods, resulting in high-quality, Markdown-formatted text ready for AI ingestion. The article also mentions that this parsing is the critical first step in automating knowledge base creation using tools like **Instill Core Artifact**.",
    "published_date": "2024-11-28T00:00:00.000Z",
    "score": null
  },
  "https://kx.com/blog/advanced-pdf-parsing-for-rag/": {
    "content_path": "data/contents/e60360886d36dd23.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.968695",
    "title": "From documents to insights: Advanced PDF parsing for RAG",
    "summary": "The user query asks for a summary related to **multimodal and generation** topics, specifically mentioning: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.\n\nThe provided webpage text focuses on **Advanced PDF parsing for RAG** using **KDB.AI** and **LlamaParse**.\n\nHere is a summary of how the page relates to the query:\n\nThe webpage details building a **Retrieval Augmented Generation (RAG)** pipeline for complex files like **PDFs** by using **LlamaParse** to extract data and transform it into ingestible formats like markdown.\n\n*   **Document Understanding/PDF Parsing/Chart/Table Extraction:** LlamaParse is used for this, supporting extraction of text, tables, images, and graphs. It has a **JSON mode** for extracting tables in JSON format, which aids in structured output.\n*   **Report Generation with LLMs (RAG):** The core purpose is to build a RAG pipeline where the parsed document content is ingested into **KDB.AI** (a vector database) and then queried using an LLM (**GPT-4o** is used in the example).\n*   **Multimodal RAG:** While the focus is heavily on document text and tables, LlamaParse explicitly supports image extraction, which aligns with the broader concept of multimodal RAG, although the primary demonstration is text/table-based retrieval.\n\n**Models Mentioned:**\n*   **LLMs/Generation:** GPT-4o is used for the final generation step.\n*   **Embedding Models:** OpenAI's `text-embedding-3-small` is used.\n\n**Missing/Not Explicitly Covered:**\nThe page does not explicitly discuss **Vision-language models** (like GPT-4V, Claude vision, or Gemini) in the context of processing visual inputs directly, nor does it detail **structured document output** beyond the JSON mode for tables or the use of markdown structure for chunking. The focus is on text/table extraction from PDFs for RAG, not general multimodal input processing.\n\n**Summary:**\nThe webpage describes building an advanced RAG pipeline for **PDF parsing** and **document understanding** by leveraging **LlamaParse** to extract text and tables (including JSON output for tables) and ingesting this",
    "published_date": "2025-04-01T12:28:12.000Z",
    "score": null
  },
  "https://pathway.com/developers/templates/rag/multimodal-rag": {
    "content_path": "data/contents/d55d3178ad5fc43c.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.019047",
    "title": "Multimodal RAG for PDFs with Text, Images, and Charts",
    "summary": "The webpage describes how to implement a **Multimodal Retrieval-Augmented Generation (MM-RAG)** system using **Pathway** and **GPT-4o** to enhance Language Models by integrating diverse data types, specifically focusing on **PDF parsing, chart/table extraction**, and generating reports from documents.\n\nKey takeaways relevant to your query:\n\n*   **Multimodal RAG (MM-RAG):** Enhances LLMs by incorporating text, images, audio, and video, which is particularly useful for handling complex visual elements like charts and tables stored as images within documents (like PDFs).\n*   **Document Understanding/PDF Parsing:** The guide focuses on a finance use case involving understanding data within PDFs, using **GPT-4o** for parsing. Tables are extracted as images, and GPT-4o explains their content, which is then indexed for searchability.\n*   **Vision Models:** The implementation leverages **GPT-4o** for its multimodal capabilities (understanding both text and images) to improve data extraction accuracy over standard RAG toolkits.\n*   **Report Generation:** The goal is to generate accurate responses based on comprehensive multimodal context retrieved from documents.\n*   **Comparison:** It briefly distinguishes MM-RAG from traditional RAG (text-only) and Multimodal Search (retrieval vs. generation).\n\nWhile the page discusses the concepts of multimodal RAG, PDF parsing, and using models like GPT-4o for visual data extraction, it does not explicitly detail the use of **Claude vision** or **Gemini** in this specific implementation, nor does it focus on general **report generation with LLMs** beyond the context of answering queries based on extracted document data. It also doesn't cover **structured document output** as a primary feature, though extracting structured data (like tables) is part of the process.",
    "published_date": "2024-06-19T00:00:00.000Z",
    "score": null
  },
  "https://docs.nomic.ai/platform/embeddings-and-retrieval/guides/pdf-rag-with-nomic-embed-multimodal": {
    "content_path": "data/contents/939f20c1c4152874.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.049766",
    "title": "RAG Over PDFs with Nomic Embed Multimodal",
    "summary": "This webpage describes how to build a **multimodal Retrieval-Augmented Generation (RAG)** system specifically for **PDFs** using **Nomic Embed Multimodal**.\n\nKey aspects covered include:\n\n*   **Multimodal RAG:** Building a system that can answer questions from PDFs containing both text and visual elements.\n*   **PDF Parsing/Document Understanding:** Using Nomic Embed Multimodal, which can directly process the visual content in PDFs without requiring separate OCR or image captioning steps.\n*   **Embedding:** Generating embeddings for each page of the PDFs using the `nomic-ai/nomic-embed-multimodal-3b` model.\n*   **Retrieval:** Setting up a retrieval function based on semantic similarity using the generated embeddings.\n*   **Generation (Vision-Language Models):** Using the **Qwen 2.5 VL 7B Instruct** model to query the retrieved PDF pages (images and text) and generate in-depth answers.\n\nThe guide provides runnable code examples for installation, embedding, retrieval, and querying the VLM for both single-document and multi-document scenarios.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://docs.cohere.com/page/agentic-rag-mixed-data": {
    "content_path": "data/contents/28fe7e2c853d916c.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.084256",
    "title": "Agentic RAG for PDFs with mixed data",
    "summary": "The webpage describes how to set up an **Agentic Retrieval-Augmented Generation (RAG) pipeline for PDFs containing mixed data (text and tables)** using Cohere models and LangChain components.\n\nKey aspects covered include:\n\n*   **Parsing:** Using the `unstructured` library with specific settings (`infer_table_structure=True`, `chunking_strategy=\"by_title\"`) to effectively separate and structure tables and text from PDFs.\n*   **Vector Store Setup:** Implementing **Multi-vector retrieval** with ChromaDB, where summaries of document chunks are embedded and stored, allowing retrieval of the full context document based on the summary embedding.\n*   **RAG Pipeline:** A process involving query augmentation (using Cohere's model to generate search queries), retrieval, **reranking** of retrieved documents, and final generation.\n*   **Agent Integration:** The RAG pipeline is wrapped as a tool for a **Cohere ReAct Agent**. This agent also has access to a **Python interpreter** tool, enabling it to perform post-processing, such as mathematical calculations (like finding the mean of retrieved values), on the RAG output to improve accuracy.\n\nIn summary, the page details a best-practice approach for building an accurate RAG system for complex documents by combining advanced parsing, multi-vector retrieval, reranking, and agentic tool use for post-processing.",
    "published_date": "2025-09-03T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2510.14528v3": {
    "content_path": "../data/contents/d30b44fafdfd5870.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.130944",
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing \n via a 0.9B Ultra-Compact Vision-Language Model",
    "summary": "PaddleOCR-VL is a state-of-the-art and resource-efficient model for **document parsing**, built around a compact **Vision-Language Model (VLM)** called PaddleOCR-VL-0.9B.\n\nKey aspects relevant to your query:\n\n*   **Vision-Language Models (VLM):** The core is PaddleOCR-VL-0.9B, which integrates a NaViT-style dynamic resolution visual encoder with the lightweight ERNIE-4.5-0.3B language model.\n*   **Document Understanding & PDF Parsing:** The system is designed for deep structural and semantic understanding of documents, supporting 109 languages. It decomposes the task into layout analysis (using PP-DocLayoutV2) followed by element recognition.\n*   **Chart/Table Extraction & Report Generation:** The model excels at recognizing complex elements, including **tables, formulas, and charts**. The final output is formatted into structured **Markdown and JSON**, which supports report generation capabilities when combined with LLMs.\n*   **Comparison to other VLMs:** The paper benchmarks PaddleOCR-VL against top-tier VLMs like GPT-4o, Gemini-2.5 Pro, and Qwen2.5-VL-72B, showing superior performance on several document parsing benchmarks (OmniDocBench v1.5, OmniDocBench v1.0, and olmOCR-Bench).\n\nThe document focuses on **document parsing** using a compact VLM architecture, rather than providing direct comparisons or details on general-purpose models like GPT-4V, Claude vision, or Gemini (though Gemini-2.5 Pro is used as a benchmark).\n\nPaddleOCR-VL is an advanced and efficient vision-language model for document parsing that excels at recognizing elements like text, tables, formulas, and charts across over 100 languages. It is built using a NaViT-style visual encoder and the ERNIE-4.5-0.3B language model.\n\nThe model demonstrates superior performance in:\n*   **Table Recognition:** It leads benchmarks like OmniDocBench-Table-block (Overall TEDS of 0.9195) and achieves the highest scores on an in-house diverse table set.\n*   **Formula Recognition:** It achieves state-of-the-art CDM scores on both OmniDocBench-Formula-block (0.9453) and an in-house set (0.9882).\n*   **Chart Recognition:** It outperforms other models on an in-house chart benchmark (RMS-F1 of 0.8440).\n*   **Inference Performance:** It shows clear advantages in processing speed, achieving 53.1% higher page throughput and 50.9% higher token throughput than the leading baseline, MinerU2.5, when deployed with FastDeploy.\n\nThe model's training data construction involved large-scale, high-quality synthesis and automatic annotation for text (20M pairs), tables (55M pairs), formulas, and charts (0.8M pairs), supporting 109 languages. The overall goal is to advance multimodal document processing and enhance the performance and stability of **RAG systems** by providing more reliable data extraction from complex documents.",
    "published_date": null,
    "score": null
  },
  "https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_Docopilot_Improving_Multimodal_Models_for_Document-Level_Understanding_CVPR_2025_paper.pdf": {
    "content_path": "../data/contents/902d9639bcb8645c.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.160876",
    "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding",
    "summary": "The webpage describes **Docopilot**, a native multimodal model designed for **document-level understanding** that does not rely on Retrieval-Augmented Generation (RAG).\n\nKey aspects related to your query include:\n\n*   **Vision-Language Models (MLLMs) & Document Understanding:** The paper addresses the inadequacy of current MLLMs for complex, multi-page document comprehension due to a lack of high-quality, document-level datasets.\n*   **Multimodal RAG:** The paper contrasts Docopilot with RAG methods, noting that RAG suffers from fragmented retrieval contexts, multi-stage error accumulation, and extra time costs. Docopilot aims to overcome these issues by natively handling long-range and cross-page dependencies.\n*   **Document Understanding, PDF Parsing, Chart/Table Extraction, Report Generation:** Docopilot is built upon a new, large-scale dataset called **Doc-750K**, which supports in-depth understanding of multimodal documents, including diverse structures and cross-page dependencies. The dataset construction involves extracting content (in interleaved text-image and multi-image formats) and generating high-quality question-answer pairs, including proxy tasks like **Abstract Writing**, **Paper Titling**, and **Caption Writing** (for figures/tables).\n*   **Model Performance:** Docopilot achieves superior coherence, accuracy, and efficiency compared to RAG methods and other open-source MLLMs on various document benchmarks (e.g., MM-NIAH, MMLongBench-Doc). For instance, Docopilot-8B outperforms InternVL2-8B by 19.9 points on MM-NIAH and surpasses InternVL2-26B with significantly lower latency.\n*   **GPT-4V, Claude vision, Gemini:** The paper compares Docopilot's performance against proprietary models like **Gemini-1.5-Pro** and **GPT-4o**, positioning Docopilot-8B as the closest open-source model to GPT-4o on several benchmarks.\n\nIn summary, Docopilot is presented as an improved, retrieval-free MLLM specifically engineered for complex, long-context document understanding, supported by the creation of the Doc-750K dataset.",
    "published_date": "",
    "score": null
  },
  "https://towardsdatascience.com/using-vision-language-models-to-process-millions-of-documents/": {
    "content_path": "../data/contents/0b5c148306ab8438.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.185173",
    "title": "Using Vision Language Models to Process Millions of Documents",
    "summary": "The webpage discusses the application of **Vision Language Models (VLMs)** for document understanding. It highlights that VLMs are necessary when the meaning of text depends on its visual position, which pure LLMs struggle with after OCR.\n\nKey application areas covered include:\n*   **Agentic use cases:** Such as controlling a computer interface or debugging code.\n*   **Question answering:** Answering questions based on an image (document) input.\n*   **Classification:** Categorizing documents based on visual and textual content.\n*   **Information extraction:** Extracting specific data points (e.g., date, location) from documents, often requesting structured output like JSON.\n\nThe article also mentions limitations, specifically the **cost of running VLMs** (due to processing many image tokens) and their difficulty in processing **very long documents** due to context window limitations.\n\nThe user query mentions several specific topics: 'multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, structured document output'.\n\nThe webpage directly addresses **document understanding**, **information extraction** (which relates to structured output and potentially chart/table extraction, though not explicitly detailed), and the general concept of **Vision Language Models** (which encompasses models like GPT-4V, Claude vision, and Gemini). It does **not** specifically mention multimodal RAG, PDF parsing, report generation with LLMs, or explicitly name the commercial models listed in the query.\n\n**Summary relevant to the query:**\nThe page explains how Vision Language Models (VLMs) are used for document understanding tasks where visual context is crucial. Applications include question answering, classification, and information extraction, often resulting in structured output. While it covers the general capabilities of VLMs relevant to models like GPT-4V, Claude vision, and Gemini, it does not specifically detail multimodal RAG, PDF parsing, chart/table extraction, or report generation with LLMs.",
    "published_date": "2025-09-26T12:30:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2512.17221v2": {
    "content_path": "../data/contents/9f851c1f386d5fa5.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.209758",
    "title": "DAVE : A VLM Vision Encoder for Document Understanding and Web Agents",
    "summary": "The user query asks for a summary related to **multimodal and generation** topics, specifically mentioning: Vision-language models (VLMs), multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.\n\nThe provided webpage introduces **DAVE (Document and webAgents Vision Encoder)**, a vision encoder purpose-built for VLMs tailored for **document understanding and web agents**.\n\nHere is a summary of how the page relates to the query:\n\n*   **Vision-language models (VLMs):** DAVE is explicitly designed as a vision encoder for VLMs to improve performance on document and web tasks.\n*   **Document understanding, PDF parsing, chart/table extraction:** DAVE's training focuses on learning robust structural and spatial information essential for these tasks. The paper shows DAVE significantly outperforms baselines on classic document tasks like DocBank and DocLayNet (related to document recognition and segmentation).\n*   **Report generation with LLMs:** While the paper focuses on the *encoder* component, DAVE is integrated into a VLM architecture (using LLMs like Llama-3.2 and Qwen-2.5 as decoders) for vision-language tasks, which underpins generation capabilities like VQA and agent actions.\n*   **GPT-4V, Claude vision, Gemini:** The paper mentions that DAVE is designed to integrate seamlessly with diverse decoders and VLM stacks, and its weight-merging scheme ensures it is largely decoder-agnostic, implying compatibility with various LLMs that power models like GPT-4V or Gemini.\n*   **Multimodal RAG & Structured document output:** DAVE's strength in structural and spatial understanding (derived from tasks like layout extraction and grounding) is highly relevant for structured document output and multimodal RAG systems that rely on accurately parsing document layouts.\n\n**In summary:** The webpage details **DAVE**, a specialized vision encoder that significantly enhances **Vision-Language Models (VLMs)** for **document understanding** tasks (including parsing and extraction) by combining self-supervised learning with supervised fine-tuning and fusing domain-specific features with general visual knowledge. This specialized encoder is designed to be compatible with various LLM backbones used in multimodal generation systems.",
    "published_date": "2025-12-24T22:50:09.204Z",
    "score": null
  },
  "https://huggingface.co/papers/2412.08746": {
    "content_path": "../data/contents/5c7e20e4e12e99d3.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.235398",
    "title": "DocVLM: Make Your VLM an Efficient Reader",
    "summary": "DocVLM is a method designed to enhance Vision-Language Models (VLMs) for document understanding by integrating OCR-encoded text and layout information. This integration reduces the reliance on high-resolution images, which typically cause significant computational overhead in reading-intensive tasks, while preserving the VLM's original weights.\n\nKey benefits and findings include:\n*   **Improved Efficiency:** DocVLM compresses OCR text and layout into a compact set of learned queries incorporated into the VLM.\n*   **Performance Gains:** In limited-token regimes (448x448), DocVLM significantly boosts DocVQA results (e.g., from 56.0% to 86.6% with InternVL2).\n*   **Token Reduction:** When integrated with LLaVA-OneVision, DocVLM achieves improved results while using 80% fewer image tokens, enabling the effective processing of multiple pages.\n*   **State-of-the-Art Results:** DocVLM shows impressive zero-shot results on DUDE and state-of-the-art performance on MP-DocVQA.\n\nThe paper focuses on making VLMs more efficient readers for document processing tasks.",
    "published_date": "2025-07-28T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/@kram254/unlocking-the-power-of-vision-language-models-for-long-documents-4f0eb7393a84": {
    "content_path": "../data/contents/1437f277cd0bef6a.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.260107",
    "title": "Unlocking the Power of Vision Language Models for Long Documents",
    "summary": "Vision Language Models (VLMs) are transforming document processing, especially for long and complex documents, by integrating visual interpretation with language understanding. Unlike traditional OCR, which often misses crucial context from images, symbols, and layout, VLMs process document images directly to capture text positioning, interpret non-textual elements, and manage dense information like charts and tables.\n\nKey aspects mentioned include:\n*   **Enhancing OCR:** VLM-based OCR models can perform tasks like Markdown extraction and provide visual explanations, with Deepseek cited as an example.\n*   **Implementation Considerations:** Implementing VLMs requires significant processing power (GPUs), careful management of image resolution and page limits, hierarchical processing strategies, and cost management due to increased token usage.\n*   **Model Choices:** Options include high-performing closed-source models like **Gemini 2.5 Pro** and **GPT-5**, and customizable, cost-effective open-source alternatives like **SenseNova-V6–5-Pro**.\n*   **Applications:** VLMs are useful in legal firms, construction/engineering (for blueprints), and academic research.\n\nThe article focuses on how VLMs improve the accuracy and depth of document analysis by respecting visual context, which is often lost in traditional text-only extraction methods.",
    "published_date": "2025-11-05T00:00:00.000Z",
    "score": null
  },
  "https://youssefh.substack.com/p/how-to-apply-vision-language-models": {
    "content_path": "../data/contents/f159cc9087718d3e.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.288573",
    "title": "To Data & Beyond",
    "summary": "Vision Language Models (VLMs) are powerful tools for long context document understanding, offering advantages over traditional Optical Character Recognition (OCR) followed by a Large Language Model (LLM) because they can interpret visual information alongside text.\n\n**Key applications and benefits of VLMs for documents discussed:**\n\n*   **Advanced OCR:** VLMs can perform more sophisticated text extraction than traditional OCR, including extracting **Markdown** (which preserves formatting like headers and tables, improving LLM performance), explaining purely **visual information** (like drawings), and **adding missing information** (e.g., explicitly marking empty fields).\n*   **Direct Visual Interpretation:** VLMs allow direct processing of document images, capturing spatial relationships and non-textual elements that are lost when only text is extracted via traditional OCR.\n*   **Handling Long Documents:** While challenging due to token limits, recent advancements in VLM technology are improving their ability to compress visual information, making them usable for very long or dense documents.\n\nThe article focuses on applying VLMs to document understanding tasks, discussing the necessity of VLMs over OCR+LLM, and detailing advanced OCR capabilities using VLMs.",
    "published_date": "2025-12-19T07:51:22.000Z",
    "score": null
  },
  "https://huggingface.co/papers/2503.11576": {
    "content_path": "../data/contents/a1fd93107d31845e.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.314510",
    "title": "SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion",
    "summary": "SmolDocling is an ultra-compact vision-language model (256M parameters) designed for **end-to-end document conversion**. It uses a new markup format called **DocTags** to capture all page elements, including their content, structure, and spatial location.\n\nThe model performs well across diverse document types (business documents, academic papers, technical reports, patents, forms) and can accurately reproduce features like **code listings, tables, equations, and charts**. It competes with much larger Vision Language Models while requiring substantially fewer computational resources. The authors also contribute novel, publicly sourced datasets for charts, tables, equations, and code recognition.\n\nWhile the summary doesn't explicitly mention \"multimodal RAG\" or \"PDF parsing\" by those exact terms, its function of end-to-end document conversion, capturing structure and spatial location, and handling various document elements (like tables and charts) directly relates to **document understanding** and **chart/table extraction**. The model itself is a **vision-language model**. There is no specific mention of **report generation with LLMs**, **GPT-4V, Claude vision, Gemini**, or **structured document output** beyond the DocTags format.",
    "published_date": "2025-03-17T00:00:00.000Z",
    "score": null
  },
  "https://www.ibm.com/think/topics/vision-language-models": {
    "content_path": "../data/contents/9e5ba15c48c8f00b.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.343952",
    "title": "What are vision language models (VLMs)?",
    "summary": "Vision-Language Models (VLMs) are AI models that combine **computer vision** and **natural language processing (NLP)** capabilities. They learn the relationship between text and visual data (images/videos) to generate text from visual inputs or understand natural language prompts in a visual context. VLMs are **multimodal AI systems** that take text and images/videos as input and produce text output, such as descriptions or answers to questions about the visual input.\n\nKey elements of VLMs include:\n*   **Language Encoder:** Uses architectures like the **transformer model** (e.g., BERT, GPT) to convert text into embeddings.\n*   **Vision Encoder:** Extracts visual properties and converts them into vector embeddings, often using a **Vision Transformer (ViT)** in modern models.\n\nVLMs are trained using strategies like **contrastive learning** (e.g., CLIP), **masking** (e.g., FLAVA), and **generative model training** (e.g., text-to-image generation). They can also be built from **pretrained models** (e.g., LLaVA).\n\n**Use cases** for VLMs include:\n*   Captioning and summarization (including summarizing visual information in documents).\n*   Image generation.\n*   Image search and retrieval.\n*   Image segmentation and object detection.\n*   **Visual Question Answering (VQA)**, which can be extended to tasks like producing maintenance reports from inspection videos.\n\nThe text mentions several examples of VLMs, including **GPT-4o**, **Gemini 2.0 Flash**, and LLaVA.\n\nThe user query also mentioned several related concepts: **multimodal RAG**, **document understanding**, **PDF parsing**, **chart/table extraction**, **report generation with LLMs**, **Claude vision**, and **structured document output**. While the text discusses VLM use cases like summarizing visual information in documents and generating reports from video analysis (VQA), it **does not explicitly detail** multimodal RAG, specific PDF parsing techniques, chart/table extraction, or structured document output as distinct features of VLMs, nor does it specifically mention **Claude vision** as an example VLM.",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/abs/2403.05525": {
    "content_path": "../data/contents/3e7bf2e78cee8a89.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.370397",
    "title": "Computer Science > Artificial Intelligence",
    "summary": "The webpage describes **DeepSeek-VL**, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding.\n\nThe model's approach focuses on:\n*   **Diverse Data:** Utilizing data covering web screenshots, **PDFs**, **OCR**, **charts**, and knowledge-based content to represent practical contexts.\n*   **Instruction Tuning:** Creating a dataset based on real user scenarios to improve practical application user experience.\n*   **Efficient Vision Encoder:** Incorporating a hybrid vision encoder that efficiently processes high-resolution images (1024 x 1024).\n*   **Language Preservation:** Investigating a VL pretraining strategy that integrates LLM training from the beginning to maintain strong language abilities.\n\nDeepSeek-VL (1.3B and 7B models) is presented as a superior vision-language chatbot with state-of-the-art or competitive performance on various benchmarks while maintaining robust language performance.\n\nWhile the paper discusses **PDFs**, **charts**, and general vision-language understanding, it does not explicitly detail **multimodal RAG**, **document understanding**, **report generation with LLMs**, or specific comparisons to models like **GPT-4V, Claude vision, or Gemini**, or focus on **structured document output**.",
    "published_date": "2024-03-08T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2507.06261v4": {
    "content_path": "../data/contents/0c3dba4d6f6345d2.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.400533",
    "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities.",
    "summary": "Gemini 2.5 Pro is highlighted as a highly capable model that excels at **multimodal understanding** and can process up to 3 hours of video content. The model family, including Gemini 2.5 Flash, spans the Pareto frontier of model capability vs cost. While the text emphasizes advanced reasoning, long context, and new agentic workflows, it specifically mentions Gemini 2.5 Pro's strong **multimodal understanding** capabilities.\n\nHowever, the summary does not explicitly mention specific terms like \"multimodal RAG,\" \"document understanding,\" \"PDF parsing,\" \"chart/table extraction,\" \"report generation with LLMs,\" \"GPT-4V,\" \"Claude vision,\" or \"structured document output.\"\n\nThe provided text is a list of contributors to the development of Gemini 2.5 and an acknowledgment section. It does not contain information about **Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, or Gemini structured document output**.\n\nNo answer found.",
    "published_date": "2024-01-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2503.20757v1": {
    "content_path": "../data/contents/4a29bc45c1fd0bad.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.430239",
    "title": "MCTS-RAG: Enhance Retrieval-Augmented Generation with \n Monte Carlo Tree Search",
    "summary": "The webpage introduces **MCTS-RAG**, a novel framework that enhances the reasoning capabilities of small language models (SLMs) on knowledge-intensive tasks by integrating **Monte Carlo Tree Search (MCTS)** with **Retrieval-Augmented Generation (RAG)**.\n\nKey aspects related to your query:\n\n*   **Reasoning and Planning with LLMs / MCTS for Language Models:** MCTS-RAG uses MCTS to dynamically explore multiple reasoning paths, integrating retrieval actions at key decision points. This structured search mechanism refines the reasoning process, similar to planning. The framework builds upon the rStar framework, adding RAG-related actions to its action space (A1-A6).\n*   **Inference-time Compute:** The method enhances reasoning by scaling inference-time compute through the iterative search process of MCTS, allowing smaller models to achieve performance comparable to frontier LLMs.\n*   **Grounding and Factuality / Hallucination Reduction and Detection:** MCTS-RAG improves factual accuracy and consistency by dynamically integrating retrieved knowledge to evaluate intermediate states. The structured reasoning and reflection on retrieved information (R3: Knowledge Reflection) help reduce hallucinations. The paper explicitly notes that MCTS-RAG minimizes hallucinations compared to baselines.\n*   **Chain-of-Thought:** While MCTS-RAG is distinct, it incorporates reasoning steps (like A2: Quick Reasoning and A3: Decompose Question) that are analogous to structured, step-by-step thinking, improving upon methods like standard Chain-of-Thought (CoT) by integrating dynamic retrieval.\n\nIn summary, MCTS-RAG uses MCTS to structure and refine the reasoning and planning process while adaptively using RAG to ground the model in external knowledge, leading to better factuality and reduced hallucinations, especially for smaller models.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2307.13702": {
    "content_path": "data/contents/edbf487f3cf39e76.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.546337",
    "title": "Computer Science > Artificial Intelligence",
    "summary": "The webpage is an abstract for an arXiv paper titled \"**Measuring Faithfulness in Chain-of-Thought Reasoning**.\"\n\nThe paper investigates whether the step-by-step reasoning (Chain-of-Thought or CoT) produced by Large Language Models (LLMs) is a faithful explanation of their actual answering process. The authors examine how model predictions change when the CoT is intervened upon (e.g., by adding mistakes). They find that models vary significantly in how much they rely on the CoT, and that the performance boost from CoT doesn't seem solely due to the added **test-time compute** or the phrasing. Interestingly, as models get larger and more capable, they tend to produce *less* faithful reasoning on most tasks studied. The conclusion is that CoT can be faithful, but this depends on carefully chosen circumstances like model size and task.\n\nThis summary relates to the user query topics of **Reasoning LLMs**, **chain-of-thought**, **test-time compute**, and **faithfulness** (related to hallucination reduction/detection).",
    "published_date": "2023-07-17T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2501.01306v2": {
    "content_path": "../data/contents/636ab72fb7ebebfd.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:15.800424",
    "title": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking",
    "summary": "The user query asks for a summary related to several topics in **reasoning and planning with LLMs**, including: Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.\n\nThe provided webpage, titled \"Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking,\" focuses primarily on **hallucination reduction** during inference by introducing the **HaluSearch** framework.\n\nHere is a summary of how the page addresses the concepts in the user query:\n\n*   **Reasoning LLMs & Planning with LLMs:** The core of HaluSearch is modeling text generation as a **step-by-step reasoning process** using **Monte Carlo Tree Search (MCTS)**, which explicitly implements a \"slow thinking\" process for deliberate planning and exploration of generation paths.\n*   **Hallucination Reduction and Detection / Factuality:** This is the primary goal. HaluSearch uses a **self-evaluation reward model (RM)** to score generation steps, guiding the search towards reliable pathways to mitigate hallucinations.\n*   **Inference-time Compute / Test-time Scaling:** The paper addresses the high cost of slow thinking (MCTS) by introducing a **Dynamic System Switch Mechanism (MCTSwitch)**. This mechanism adaptively alternates between \"fast thinking\" (direct generation) and \"slow thinking\" (MCTS) at both the instance and step levels to balance efficiency and quality.\n*   **Chain-of-Thought (CoT):** CoT is mentioned as a baseline and as an example of existing internal knowledge-based methods that operate at the response level, which HaluSearch aims to improve upon by using step-level reasoning via MCTS.\n*   **Self-Reflection:** The framework relies on a self-evaluation reward model trained to assess the degree of hallucinations, which serves as a form of self-feedback/reflection guiding the search.\n\n**Concepts not explicitly detailed or central to the summary:**\n\n*   **Grounding:** Not explicitly discussed as a primary mechanism, though improving factuality inherently relates to grounding.\n*   **Self-Reflection (beyond the RM):** While the RM provides feedback, the paper doesn't detail other explicit self-reflection techniques beyond the MCTS",
    "published_date": "2022-12-09T10:00:49.000Z",
    "score": null
  },
  "https://www.seangoedecke.com/mcts-and-llms/": {
    "content_path": "../data/contents/50c9d3f25e31a14c.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:16.086831",
    "title": "MCTS and LLMs: what's the big deal?",
    "summary": "The user query asks for information related to several advanced topics in LLM reasoning and planning, including **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**\n\nThe provided webpage focuses specifically on **Monte Carlo Tree Search (MCTS) and its application to Large Language Models (LLMs)**.\n\nHere is a summary of what the page covers relevant to the query:\n\n*   **MCTS for LLMs:** MCTS is discussed as a method to potentially allow LLMs to \"look ahead\" and break through the performance ceiling imposed by human-generated training data, similar to how it improved performance in games like Go and Chess.\n*   **Mechanism:** MCTS involves tracing possible subtrees of moves/outputs and using random sampling (the Monte Carlo method) to estimate the \"ideal\" rating of a move. This rating can then be used as a loss function to update model weights, allowing the model to \"play against itself\" and improve beyond human-level performance in specific domains.\n*   **Limitations for General LLMs:** The author is skeptical about MCTS producing a generally superhuman planner for open-ended tasks (like career planning) because these tasks lack a limited set of rules and a clear success/failure condition necessary for effective tree search and simulation (rollout).\n*   **Potential Successful Areas:** MCTS seems potentially successful in LLM applications where the world model is limited and predictable, such as **solving mathematical problems** and possibly **AI video generation** or **code generation**, where success/failure is easier to identify.\n\n**Regarding the other specific terms in the query:**\n\n*   **Chain-of-thought, inference-time compute, self-reflection, test-time scaling, hallucination reduction and detection, grounding, and factuality:** These topics are **not** discussed in the provided text.\n\n**Conclusion:**\n\nThe page provides detailed information on **MCTS for language models** and its potential role in **planning with LLMs** (specifically in limited domains), but it does not address the other components of the user's query.\n\n**Summary:** The webpage explains Monte Carlo Tree Search (MCTS) and its potential application to LLMs to improve reasoning and planning by allowing models to look ahead and",
    "published_date": "2024-06-17T00:00:00.000Z",
    "score": null
  },
  "https://aclanthology.org/2025.naacl-long.515.pdf": {
    "content_path": "../data/contents/ecd8d90d3965af10.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:16.113597",
    "title": "",
    "summary": "The webpage describes **LE-MCTS (Language model Ensemble with Monte Carlo Tree Search)**, a novel framework for **process-level ensembling of language models** designed to improve performance on **complex reasoning tasks**, particularly where open-source models struggle.\n\nKey aspects related to your query:\n\n*   **Reasoning LLMs & Planning with LLMs:** LE-MCTS formulates step-by-step reasoning with an ensemble of LLMs as a **Markov Decision Process (MDP)**. The process involves searching over reasoning steps generated by different LLMs to find the most accurate chain.\n*   **MCTS (Monte Carlo Tree Search) for language models:** The core of the framework uses an MCTS algorithm (inspired by AlphaZero) to perform a tree search over the reasoning steps generated by the ensemble of LLMs, guided by a **process-based reward model (PRM)**.\n*   **Inference-time compute/Scaling:** The method performs a lookahead search during inference to select the best sequence of reasoning steps, outperforming single-model decoding and existing token/output-level ensemble methods.\n*   **Performance:** LE-MCTS showed significant improvements on mathematical reasoning benchmarks, specifically boosting performance by **3.6% on MATH** and **4.3% on MQA** compared to the second-best models.\n\nThe paper focuses on improving reasoning accuracy through process-level ensembling guided by MCTS, rather than explicitly detailing self-reflection, hallucination reduction/detection, grounding, or factuality mechanisms outside of the reward model guiding the search.",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/abs/2412.15797": {
    "content_path": "../data/contents/425e7f7f80365764.md",
    "source": "exa",
    "saved_at": "2025-12-29T23:50:16.139614",
    "title": "Computer Science > Computation and Language",
    "summary": "The webpage describes a novel framework called **Language model Ensemble with Monte Carlo Tree Search (LE-MCTS)** designed to improve the complex reasoning capabilities of Large Language Models (LLMs), particularly open-source ones.\n\nKey aspects related to your query:\n\n*   **Reasoning LLMs & Complex Reasoning:** The framework specifically targets better performance on complex reasoning tasks.\n*   **MCTS (Monte Carlo Tree Search) for language models:** LE-MCTS formulates step-by-step reasoning as a Markov decision process where MCTS is used to search over reasoning steps generated by an ensemble of LLMs.\n*   **Ensembling:** It uses a process-level ensembling technique, where actions involve selecting the next reasoning step from a pool of different language models.\n*   **Inference-time compute/Process Reward:** The search is guided by a **process-based reward model** to identify the most accurate reasoning chain.\n\nThe paper demonstrates that LE-MCTS outperforms single LLM decoding algorithms and other ensemble methods on mathematical reasoning benchmarks (MATH and MQA).",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/abs/2512.21884v1": {
    "content_path": "../data/contents/2512.21884v1.md",
    "pdf_path": "../data/contents/2512.21884v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:24.284200",
    "title": "Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models"
  },
  "https://arxiv.org/abs/2512.21435v1": {
    "content_path": "../data/contents/2512.21435v1.md",
    "pdf_path": "../data/contents/2512.21435v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:29.138751",
    "title": "Dynamic Attention (DynAttn): Interpretable High-Dimensional Spatio-Temporal Forecasting (with Application to Conflict Fatalities)"
  },
  "https://arxiv.org/abs/2512.21651v1": {
    "content_path": "../data/contents/2512.21651v1.md",
    "pdf_path": "../data/contents/2512.21651v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:29.415179",
    "title": "Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models"
  },
  "https://arxiv.org/abs/2512.21867v1": {
    "content_path": "../data/contents/2512.21867v1.md",
    "pdf_path": "../data/contents/2512.21867v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:32.992149",
    "title": "DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation"
  },
  "https://arxiv.org/abs/2512.21878v1": {
    "content_path": "../data/contents/2512.21878v1.md",
    "pdf_path": "../data/contents/2512.21878v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:33.418268",
    "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting"
  },
  "https://arxiv.org/abs/2512.21431v1": {
    "content_path": "../data/contents/2512.21431v1.md",
    "pdf_path": "../data/contents/2512.21431v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:33.873543",
    "title": "Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors"
  },
  "https://arxiv.org/abs/2512.21648v1": {
    "content_path": "../data/contents/2512.21648v1.md",
    "pdf_path": "../data/contents/2512.21648v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:34.114050",
    "title": "Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search"
  },
  "https://arxiv.org/abs/2512.21860v1": {
    "content_path": "../data/contents/2512.21860v1.md",
    "pdf_path": "../data/contents/2512.21860v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:35.650609",
    "title": "Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models"
  },
  "https://arxiv.org/abs/2509.11536": {
    "content_path": "../data/contents/2509.11536.md",
    "pdf_path": "../data/contents/2509.11536.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:36.727310",
    "title": "HARP: Hallucination Detection via Reasoning Subspace Projection"
  },
  "https://arxiv.org/abs/2512.21877v1": {
    "content_path": "../data/contents/2512.21877v1.md",
    "pdf_path": "../data/contents/2512.21877v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:36.952802",
    "title": "CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics"
  },
  "https://arxiv.org/abs/2512.21422v1": {
    "content_path": "../data/contents/2512.21422v1.md",
    "pdf_path": "../data/contents/2512.21422v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:37.567206",
    "title": "Teaching People LLM's Errors and Getting it Right"
  },
  "https://arxiv.org/abs/2512.21635v1": {
    "content_path": "../data/contents/2512.21635v1.md",
    "pdf_path": "../data/contents/2512.21635v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:38.023907",
    "title": "Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations"
  },
  "https://arxiv.org/abs/2512.21778v1": {
    "content_path": "../data/contents/2512.21778v1.md",
    "pdf_path": "../data/contents/2512.21778v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:38.935786",
    "title": "Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models"
  },
  "https://arxiv.org/abs/2512.21871v1": {
    "content_path": "../data/contents/2512.21871v1.md",
    "pdf_path": "../data/contents/2512.21871v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:40.197142",
    "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?"
  },
  "https://arxiv.org/abs/2512.21414v1": {
    "content_path": "../data/contents/2512.21414v1.md",
    "pdf_path": "../data/contents/2512.21414v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:41.924137",
    "title": "A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding"
  },
  "https://arxiv.org/abs/2512.21964v1": {
    "content_path": "../data/contents/2512.21964v1.md",
    "pdf_path": "../data/contents/2512.21964v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:43.296079",
    "title": "Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models"
  },
  "https://arxiv.org/abs/2512.21626v1": {
    "content_path": "../data/contents/2512.21626v1.md",
    "pdf_path": "../data/contents/2512.21626v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:43.576918",
    "title": "Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing"
  },
  "https://arxiv.org/abs/2512.21698v1": {
    "content_path": "../data/contents/2512.21698v1.md",
    "pdf_path": "../data/contents/2512.21698v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:45.491618",
    "title": "Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding"
  },
  "https://arxiv.org/abs/2509.01092": {
    "content_path": "../data/contents/2509.01092.md",
    "pdf_path": "../data/contents/2509.01092.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:46.090161",
    "title": "REFRAG: Rethinking RAG based Decoding"
  },
  "https://arxiv.org/abs/2512.21866v1": {
    "content_path": "../data/contents/2512.21866v1.md",
    "pdf_path": "../data/contents/2512.21866v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:46.906812",
    "title": "Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation"
  },
  "https://arxiv.org/abs/2512.21411v1": {
    "content_path": "../data/contents/2512.21411v1.md",
    "pdf_path": "../data/contents/2512.21411v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:48.818207",
    "title": "Thermodynamic Characterizations of Singular Bayesian Models: Specific Heat, Susceptibility, and Entropy Flow in Posterior Geometry"
  },
  "https://arxiv.org/abs/2512.21625v1": {
    "content_path": "../data/contents/2512.21625v1.md",
    "pdf_path": "../data/contents/2512.21625v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:50.664148",
    "title": "Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards"
  },
  "https://arxiv.org/abs/2512.21684v1": {
    "content_path": "../data/contents/2512.21684v1.md",
    "pdf_path": "../data/contents/2512.21684v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:51.539822",
    "title": "SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration"
  },
  "https://arxiv.org/abs/2512.21863v1": {
    "content_path": "../data/contents/2512.21863v1.md",
    "pdf_path": "../data/contents/2512.21863v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:52.762280",
    "title": "Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion"
  },
  "https://arxiv.org/abs/2512.21404v1": {
    "content_path": "../data/contents/2512.21404v1.md",
    "pdf_path": "../data/contents/2512.21404v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:53.380761",
    "title": "LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors"
  },
  "https://arxiv.org/abs/2512.21623v1": {
    "content_path": "../data/contents/2512.21623v1.md",
    "pdf_path": "../data/contents/2512.21623v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:53.963698",
    "title": "Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design"
  },
  "https://arxiv.org/abs/2512.21681v1": {
    "content_path": "../data/contents/2512.21681v1.md",
    "pdf_path": "../data/contents/2512.21681v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:55.052250",
    "title": "Exploring the Security Threats of Retriever Backdoors in Retrieval-Augmented Code Generation"
  },
  "https://arxiv.org/abs/2512.21859v1": {
    "content_path": "../data/contents/2512.21859v1.md",
    "pdf_path": "../data/contents/2512.21859v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:55.849355",
    "title": "TimeBill: Time-Budgeted Inference for Large Language Models"
  },
  "https://arxiv.org/abs/2512.21395v1": {
    "content_path": "../data/contents/2512.21395v1.md",
    "pdf_path": "../data/contents/2512.21395v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:56.992174",
    "title": "A Reinforcement Learning Approach to Synthetic Data Generation"
  },
  "https://arxiv.org/abs/2512.21613v1": {
    "content_path": "../data/contents/2512.21613v1.md",
    "pdf_path": "../data/contents/2512.21613v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:57.627408",
    "title": "AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design"
  },
  "https://arxiv.org/abs/2512.21627v1": {
    "content_path": "../data/contents/2512.21627v1.md",
    "pdf_path": "../data/contents/2512.21627v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:58.633783",
    "title": "AstraNav-Memory: Contexts Compression for Long Memory"
  },
  "https://arxiv.org/html/2504.14891v1": {
    "content_path": "../data/contents/2504.14891v1.md",
    "pdf_path": "../data/contents/2504.14891v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:58.997062",
    "title": "Retrieval Augmented Generation Evaluation in the Era of LLMs: A Comprehensive Survey"
  },
  "https://arxiv.org/abs/2512.21852v1": {
    "content_path": "../data/contents/2512.21852v1.md",
    "pdf_path": "../data/contents/2512.21852v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:04:59.650634",
    "title": "A Comedy of Estimators: On KL Regularization in RL Training of LLMs"
  },
  "https://arxiv.org/abs/2512.21309v2": {
    "content_path": "../data/contents/2512.21309v2.md",
    "pdf_path": "../data/contents/2512.21309v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:00.473751",
    "title": "A Plan Reuse Mechanism for LLM-Driven Agent"
  },
  "https://arxiv.org/abs/2512.21596v1": {
    "content_path": "../data/contents/2512.21596v1.md",
    "pdf_path": "../data/contents/2512.21596v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:00.764941",
    "title": "Quantitative Verification of Omega-regular Properties in Probabilistic Programming"
  },
  "https://arxiv.org/abs/2512.21616v1": {
    "content_path": "../data/contents/2512.21616v1.md",
    "pdf_path": "../data/contents/2512.21616v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:02.152578",
    "title": "TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant"
  },
  "https://arxiv.org/abs/2512.20174": {
    "content_path": "../data/contents/2512.20174.md",
    "pdf_path": "../data/contents/2512.20174.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:03.515343",
    "title": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark"
  },
  "https://arxiv.org/abs/2512.21842v1": {
    "content_path": "../data/contents/2512.21842v1.md",
    "pdf_path": "../data/contents/2512.21842v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:03.713698",
    "title": "AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts"
  },
  "https://arxiv.org/abs/2512.21220v2": {
    "content_path": "../data/contents/2512.21220v2.md",
    "pdf_path": "../data/contents/2512.21220v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:04.179453",
    "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic"
  },
  "https://arxiv.org/abs/2512.21595v1": {
    "content_path": "../data/contents/2512.21595v1.md",
    "pdf_path": "../data/contents/2512.21595v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:04.581589",
    "title": "LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model"
  },
  "https://arxiv.org/abs/2512.21598v1": {
    "content_path": "../data/contents/2512.21598v1.md",
    "pdf_path": "../data/contents/2512.21598v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:05.288781",
    "title": "From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement"
  },
  "https://arxiv.org/abs/2511.20478": {
    "content_path": "../data/contents/2511.20478.md",
    "pdf_path": "../data/contents/2511.20478.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:06.015004",
    "title": "NVIDIA Nemotron Parse 1.1"
  },
  "https://arxiv.org/abs/2512.21837v1": {
    "content_path": "../data/contents/2512.21837v1.md",
    "pdf_path": "../data/contents/2512.21837v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:06.200888",
    "title": "Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco"
  },
  "https://arxiv.org/abs/2512.21373v1": {
    "content_path": "../data/contents/2512.21373v1.md",
    "pdf_path": "../data/contents/2512.21373v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:06.589063",
    "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories"
  },
  "https://arxiv.org/abs/2512.21586v1": {
    "content_path": "../data/contents/2512.21586v1.md",
    "pdf_path": "../data/contents/2512.21586v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:07.869272",
    "title": "Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations"
  },
  "https://arxiv.org/abs/2512.21508v1": {
    "content_path": "../data/contents/2512.21508v1.md",
    "pdf_path": "../data/contents/2512.21508v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:08.440832",
    "title": "Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification"
  },
  "https://arxiv.org/abs/2512.21835v1": {
    "content_path": "../data/contents/2512.21835v1.md",
    "pdf_path": "../data/contents/2512.21835v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:10.043201",
    "title": "LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices"
  },
  "https://arxiv.org/abs/2512.20957v2": {
    "content_path": "../data/contents/2512.20957v2.md",
    "pdf_path": "../data/contents/2512.20957v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:11.177019",
    "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents"
  },
  "https://arxiv.org/abs/2512.21583v1": {
    "content_path": "../data/contents/2512.21583v1.md",
    "pdf_path": "../data/contents/2512.21583v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:12.286710",
    "title": "A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning"
  },
  "https://arxiv.org/abs/2512.21430v1": {
    "content_path": "../data/contents/2512.21430v1.md",
    "pdf_path": "../data/contents/2512.21430v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:14.754616",
    "title": "EVE: A Generator-Verifier System for Generative Policies"
  },
  "https://arxiv.org/abs/2510.19817": {
    "content_path": "../data/contents/2510.19817.md",
    "pdf_path": "../data/contents/2510.19817.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:15.597424",
    "title": "olmOCR 2: Unit Test Rewards for Document OCR"
  },
  "https://arxiv.org/abs/2512.21829v1": {
    "content_path": "../data/contents/2512.21829v1.md",
    "pdf_path": "../data/contents/2512.21829v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:19.472787",
    "title": "Tilt Matching for Scalable Sampling and Fine-Tuning"
  },
  "https://arxiv.org/abs/2512.20905v2": {
    "content_path": "../data/contents/2512.20905v2.md",
    "pdf_path": "../data/contents/2512.20905v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:19.698443",
    "title": "DiEC: Diffusion Embedded Clustering"
  },
  "https://arxiv.org/abs/2512.21580v1": {
    "content_path": "../data/contents/2512.21580v1.md",
    "pdf_path": "../data/contents/2512.21580v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:20.312655",
    "title": "Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM"
  },
  "https://arxiv.org/abs/2512.03643": {
    "content_path": "../data/contents/2512.03643.md",
    "pdf_path": "../data/contents/2512.03643.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:20.662776",
    "title": "Optical Context Compression Is Just (Bad) Autoencoding"
  },
  "https://arxiv.org/abs/2512.21823v1": {
    "content_path": "../data/contents/2512.21823v1.md",
    "pdf_path": "../data/contents/2512.21823v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:21.049593",
    "title": "Investigating Conditional Restricted Boltzmann Machines in Regime Detection"
  },
  "https://arxiv.org/abs/2512.20491v3": {
    "content_path": "../data/contents/2512.20491v3.md",
    "pdf_path": "../data/contents/2512.20491v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:21.863075",
    "title": "Step-DeepResearch Technical Report"
  },
  "https://arxiv.org/abs/2512.22113v1": {
    "content_path": "../data/contents/2512.22113v1.md",
    "pdf_path": "../data/contents/2512.22113v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:22.198083",
    "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications"
  },
  "https://arxiv.org/abs/2512.21578v1": {
    "content_path": "../data/contents/2512.21578v1.md",
    "pdf_path": "../data/contents/2512.21578v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:22.573379",
    "title": "NEMO-4-PAYPAL: Leveraging NVIDIA's Nemo Framework for empowering PayPal's Commerce Agent"
  },
  "https://arxiv.org/abs/2512.06556": {
    "content_path": "../data/contents/2512.06556.md",
    "pdf_path": "../data/contents/2512.06556.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:27.406753",
    "title": "Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks"
  },
  "https://arxiv.org/abs/2512.04554": {
    "content_path": "../data/contents/2512.04554.md",
    "pdf_path": "../data/contents/2512.04554.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:28.777454",
    "title": "Counterfeit Answers: Adversarial Forgery against OCR-Free Document Visual Question Answering"
  },
  "https://arxiv.org/abs/2512.21818v1": {
    "content_path": "../data/contents/2512.21818v1.md",
    "pdf_path": "../data/contents/2512.21818v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:28.969444",
    "title": "Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development"
  },
  "https://arxiv.org/abs/2512.20458v2": {
    "content_path": "../data/contents/2512.20458v2.md",
    "pdf_path": "../data/contents/2512.20458v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:29.931489",
    "title": "Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register"
  },
  "https://arxiv.org/abs/2512.22106v1": {
    "content_path": "../data/contents/2512.22106v1.md",
    "pdf_path": "../data/contents/2512.22106v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:30.139215",
    "title": "Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks"
  },
  "https://arxiv.org/abs/2512.21577v1": {
    "content_path": "../data/contents/2512.21577v1.md",
    "pdf_path": "../data/contents/2512.21577v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:30.724404",
    "title": "A Unified Definition of Hallucination, Or: It's the World Model, Stupid"
  },
  "https://arxiv.org/abs/2511.18192": {
    "content_path": "../data/contents/2511.18192.md",
    "pdf_path": "../data/contents/2511.18192.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:32.356563",
    "title": "ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization"
  },
  "https://arxiv.org/abs/2512.21817v1": {
    "content_path": "../data/contents/2512.21817v1.md",
    "pdf_path": "../data/contents/2512.21817v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:32.583431",
    "title": "Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments"
  },
  "https://arxiv.org/abs/2512.20387v2": {
    "content_path": "../data/contents/2512.20387v2.md",
    "pdf_path": "../data/contents/2512.20387v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:33.022743",
    "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems"
  },
  "https://arxiv.org/abs/2510.22222": {
    "content_path": "../data/contents/2510.22222.md",
    "pdf_path": "../data/contents/2510.22222.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:33.972702",
    "title": "CreditXAI: A Multi-Agent System for Explainable Corporate Credit Rating"
  },
  "https://arxiv.org/abs/2512.22102v1": {
    "content_path": "../data/contents/2512.22102v1.md",
    "pdf_path": "../data/contents/2512.22102v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:34.516271",
    "title": "Explainable Multimodal Regression via Information Decomposition"
  },
  "https://arxiv.org/abs/2512.21576v1": {
    "content_path": "../data/contents/2512.21576v1.md",
    "pdf_path": "../data/contents/2512.21576v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:34.795161",
    "title": "Towards Long-window Anchoring in Vision-Language Model Distillation"
  },
  "https://arxiv.org/abs/2511.16654": {
    "content_path": "../data/contents/2511.16654.md",
    "pdf_path": "../data/contents/2511.16654.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:35.103191",
    "title": "Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems"
  },
  "https://arxiv.org/abs/2512.21815v1": {
    "content_path": "../data/contents/2512.21815v1.md",
    "pdf_path": "../data/contents/2512.21815v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:35.854424",
    "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models"
  },
  "https://arxiv.org/abs/2512.20312v2": {
    "content_path": "../data/contents/2512.20312v2.md",
    "pdf_path": "../data/contents/2512.20312v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:36.176968",
    "title": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning"
  },
  "https://arxiv.org/abs/2507.22758": {
    "content_path": "../data/contents/2507.22758.md",
    "pdf_path": "../data/contents/2507.22758.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:36.571367",
    "title": "MASCA: LLM based-Multi Agents System for Credit Assessment"
  },
  "https://arxiv.org/abs/2512.22101v1": {
    "content_path": "../data/contents/2512.22101v1.md",
    "pdf_path": "../data/contents/2512.22101v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:36.801637",
    "title": "A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting"
  },
  "https://arxiv.org/abs/2512.21572v1": {
    "content_path": "../data/contents/2512.21572v1.md",
    "pdf_path": "../data/contents/2512.21572v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:37.204944",
    "title": "RefineBridge: Generative Bridge Models Improve Financial Forecasting by Foundation Models"
  },
  "https://arxiv.org/abs/2512.01017": {
    "content_path": "../data/contents/2512.01017.md",
    "pdf_path": "../data/contents/2512.01017.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:41.023513",
    "title": "ChartAnchor: Chart Grounding with Structural-Semantic Fidelity"
  },
  "https://arxiv.org/abs/2512.21799v1": {
    "content_path": "../data/contents/2512.21799v1.md",
    "pdf_path": "../data/contents/2512.21799v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:41.367160",
    "title": "KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering"
  },
  "https://arxiv.org/abs/2512.20308v2": {
    "content_path": "../data/contents/2512.20308v2.md",
    "pdf_path": "../data/contents/2512.20308v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:42.447930",
    "title": "SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision"
  },
  "https://arxiv.org/abs/2508.17565": {
    "content_path": "../data/contents/2508.17565.md",
    "pdf_path": "../data/contents/2508.17565.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:42.891736",
    "title": "TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis"
  },
  "https://arxiv.org/abs/2512.22100v1": {
    "content_path": "../data/contents/2512.22100v1.md",
    "pdf_path": "../data/contents/2512.22100v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:43.304253",
    "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis"
  },
  "https://arxiv.org/abs/2512.21571v1": {
    "content_path": "../data/contents/2512.21571v1.md",
    "pdf_path": "../data/contents/2512.21571v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:45.708278",
    "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures"
  },
  "https://arxiv.org/abs/2512.00947": {
    "content_path": "../data/contents/2512.00947.md",
    "pdf_path": "../data/contents/2512.00947.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:46.195888",
    "title": "Table as a Modality for Large Language Models"
  },
  "https://arxiv.org/abs/2512.21798v1": {
    "content_path": "../data/contents/2512.21798v1.md",
    "pdf_path": "../data/contents/2512.21798v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:46.420776",
    "title": "Applications of synthetic financial data in portfolio and risk modeling"
  },
  "https://arxiv.org/abs/2512.21360v1": {
    "content_path": "../data/contents/2512.21360v1.md",
    "pdf_path": "../data/contents/2512.21360v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:46.650284",
    "title": "From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration"
  },
  "https://arxiv.org/abs/2510.23032": {
    "content_path": "../data/contents/2510.23032.md",
    "pdf_path": "../data/contents/2510.23032.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:47.327527",
    "title": "P1GPT: a multi-agent LLM workflow module for multi-modal financial information analysis"
  },
  "https://arxiv.org/abs/2512.22087v1": {
    "content_path": "../data/contents/2512.22087v1.md",
    "pdf_path": "../data/contents/2512.22087v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:47.883083",
    "title": "Context as a Tool: Context Management for Long-Horizon SWE-Agents"
  },
  "https://arxiv.org/abs/2512.21567v1": {
    "content_path": "../data/contents/2512.21567v1.md",
    "pdf_path": "../data/contents/2512.21567v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:48.105140",
    "title": "Beyond Heuristics: A Decision-Theoretic Framework for Agent Memory Management"
  },
  "https://arxiv.org/abs/2512.21791v1": {
    "content_path": "../data/contents/2512.21791v1.md",
    "pdf_path": "../data/contents/2512.21791v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:48.799213",
    "title": "Synthetic Financial Data Generation for Enhanced Financial Modelling"
  },
  "https://arxiv.org/abs/2512.19920v2": {
    "content_path": "../data/contents/2512.19920v2.md",
    "pdf_path": "../data/contents/2512.19920v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:49.119992",
    "title": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning"
  },
  "https://arxiv.org/abs/2512.21878": {
    "content_path": "../data/contents/2512.21878.md",
    "pdf_path": "../data/contents/2512.21878.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:50.009831",
    "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting"
  },
  "https://arxiv.org/abs/2512.22066v1": {
    "content_path": "../data/contents/2512.22066v1.md",
    "pdf_path": "../data/contents/2512.22066v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:51.523105",
    "title": "Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling"
  },
  "https://arxiv.org/abs/2512.21562v1": {
    "content_path": "../data/contents/2512.21562v1.md",
    "pdf_path": "../data/contents/2512.21562v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:51.727893",
    "title": "Exploration of Reproducible Generated Image Detection"
  },
  "https://arxiv.org/abs/2512.21789v1": {
    "content_path": "../data/contents/2512.21789v1.md",
    "pdf_path": "../data/contents/2512.21789v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:51.987327",
    "title": "Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning"
  },
  "https://arxiv.org/abs/2512.19864v2": {
    "content_path": "../data/contents/2512.19864v2.md",
    "pdf_path": "../data/contents/2512.19864v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:52.309136",
    "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data"
  },
  "https://arxiv.org/abs/2512.22060v1": {
    "content_path": "../data/contents/2512.22060v1.md",
    "pdf_path": "../data/contents/2512.22060v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:52.516740",
    "title": "Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management"
  },
  "https://arxiv.org/abs/2512.21543v1": {
    "content_path": "../data/contents/2512.21543v1.md",
    "pdf_path": "../data/contents/2512.21543v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:05:52.967038",
    "title": "CEMG: Collaborative-Enhanced Multimodal Generative Recommendation"
  },
  "https://arxiv.org/abs/2512.21788v1": {
    "content_path": "../data/contents/2512.21788v1.md",
    "pdf_path": "../data/contents/2512.21788v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:00.373414",
    "title": "InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation"
  },
  "https://arxiv.org/abs/2512.19491v2": {
    "content_path": "../data/contents/2512.19491v2.md",
    "pdf_path": "../data/contents/2512.19491v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:02.912558",
    "title": "Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico"
  },
  "https://arxiv.org/abs/2512.22036v1": {
    "content_path": "../data/contents/2512.22036v1.md",
    "pdf_path": "../data/contents/2512.22036v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:03.306678",
    "title": "FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion"
  },
  "https://arxiv.org/abs/2512.21540v1": {
    "content_path": "../data/contents/2512.21540v1.md",
    "pdf_path": "../data/contents/2512.21540v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:03.535740",
    "title": "Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model"
  },
  "https://arxiv.org/abs/2512.21280": {
    "content_path": "../data/contents/2512.21280.md",
    "pdf_path": "../data/contents/2512.21280.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:03.756855",
    "title": "SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance"
  },
  "https://arxiv.org/abs/2512.21787v1": {
    "content_path": "../data/contents/2512.21787v1.md",
    "pdf_path": "../data/contents/2512.21787v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:04.002975",
    "title": "Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation"
  },
  "https://arxiv.org/abs/2512.19432v2": {
    "content_path": "../data/contents/2512.19432v2.md",
    "pdf_path": "../data/contents/2512.19432v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:06.813825",
    "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive and MCP-Augmented Environments"
  },
  "https://arxiv.org/abs/2512.22035v1": {
    "content_path": "../data/contents/2512.22035v1.md",
    "pdf_path": "../data/contents/2512.22035v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:07.575044",
    "title": "Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View"
  },
  "https://arxiv.org/abs/2512.21529v1": {
    "content_path": "../data/contents/2512.21529v1.md",
    "pdf_path": "../data/contents/2512.21529v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:07.889059",
    "title": "Hierarchy-Aware Fine-Tuning of Vision-Language Models"
  },
  "https://arxiv.org/abs/2511.07555": {
    "content_path": "../data/contents/2511.07555.md",
    "pdf_path": "../data/contents/2511.07555.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:08.101651",
    "title": "LLM Optimization Unlocks Real-Time Pairwise Reranking"
  },
  "https://arxiv.org/abs/2512.07584": {
    "content_path": "../data/contents/2512.07584.md",
    "pdf_path": "../data/contents/2512.07584.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:23.535929",
    "title": "LongCat-Image Technical Report"
  },
  "https://arxiv.org/abs/2512.21782v1": {
    "content_path": "../data/contents/2512.21782v1.md",
    "pdf_path": "../data/contents/2512.21782v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:28.324494",
    "title": "Accelerating Scientific Discovery with Autonomous Goal-evolving Agents"
  },
  "https://arxiv.org/abs/2512.19171v2": {
    "content_path": "../data/contents/2512.19171v2.md",
    "pdf_path": "../data/contents/2512.19171v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:29.096181",
    "title": "JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation"
  },
  "https://arxiv.org/abs/2512.22029v1": {
    "content_path": "../data/contents/2512.22029v1.md",
    "pdf_path": "../data/contents/2512.22029v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:29.491261",
    "title": "LibContinual: A Comprehensive Library towards Realistic Continual Learning"
  },
  "https://arxiv.org/abs/2512.21527v1": {
    "content_path": "../data/contents/2512.21527v1.md",
    "pdf_path": "../data/contents/2512.21527v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:30.669405",
    "title": "Generative Actor Critic"
  },
  "https://arxiv.org/abs/2511.05385": {
    "content_path": "../data/contents/2511.05385.md",
    "pdf_path": "../data/contents/2511.05385.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:31.422983",
    "title": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework"
  },
  "https://arxiv.org/abs/2512.02660": {
    "content_path": "../data/contents/2512.02660.md",
    "pdf_path": "../data/contents/2512.02660.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:33.378368",
    "title": "Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation"
  },
  "https://arxiv.org/abs/2512.21775v1": {
    "content_path": "../data/contents/2512.21775v1.md",
    "pdf_path": "../data/contents/2512.21775v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:33.765380",
    "title": "Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets"
  },
  "https://arxiv.org/abs/2512.21354v1": {
    "content_path": "../data/contents/2512.21354v1.md",
    "pdf_path": "../data/contents/2512.21354v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:34.101637",
    "title": "Reflection-Driven Control for Trustworthy Code Agents"
  },
  "https://arxiv.org/html/2402.01817v3": {
    "content_path": "../data/contents/2402.01817v3.md",
    "pdf_path": "../data/contents/2402.01817v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:34.430031",
    "title": "Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks"
  },
  "https://arxiv.org/abs/2512.21999v1": {
    "content_path": "../data/contents/2512.21999v1.md",
    "pdf_path": "../data/contents/2512.21999v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:36.208953",
    "title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs"
  },
  "https://arxiv.org/abs/2512.21526v1": {
    "content_path": "../data/contents/2512.21526v1.md",
    "pdf_path": "../data/contents/2512.21526v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:36.547371",
    "title": "Selective LLM-Guided Regularization for Enhancing Recommendation Models"
  },
  "https://arxiv.org/abs/2512.21766v1": {
    "content_path": "../data/contents/2512.21766v1.md",
    "pdf_path": "../data/contents/2512.21766v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:40.345602",
    "title": "UniLabOS: An AI-Native Operating System for Autonomous Laboratories"
  },
  "https://arxiv.org/abs/2512.18564v2": {
    "content_path": "../data/contents/2512.18564v2.md",
    "pdf_path": "../data/contents/2512.18564v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:40.715188",
    "title": "Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V"
  },
  "https://arxiv.org/abs/2512.21985v1": {
    "content_path": "../data/contents/2512.21985v1.md",
    "pdf_path": "../data/contents/2512.21985v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:41.457875",
    "title": "LVLM-Aided Alignment of Task-Specific Vision Models"
  },
  "https://arxiv.org/abs/2512.21521v1": {
    "content_path": "../data/contents/2512.21521v1.md",
    "pdf_path": "../data/contents/2512.21521v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:41.771380",
    "title": "First Provable Guarantees for Practical Private FL: Beyond Restrictive Assumptions"
  },
  "https://arxiv.org/abs/2512.21757v1": {
    "content_path": "../data/contents/2512.21757v1.md",
    "pdf_path": "../data/contents/2512.21757v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:42.512810",
    "title": "How Do Agents Perform Code Optimization? An Empirical Study"
  },
  "https://arxiv.org/abs/2512.21352v1": {
    "content_path": "../data/contents/2512.21352v1.md",
    "pdf_path": "../data/contents/2512.21352v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:42.769180",
    "title": "Multi-Agent LLM Committees for Autonomous Software Beta Testing"
  },
  "https://arxiv.org/abs/2512.21960v1": {
    "content_path": "../data/contents/2512.21960v1.md",
    "pdf_path": "../data/contents/2512.21960v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:43.372454",
    "title": "Modeling high dimensional point clouds with the spherical cluster model"
  },
  "https://arxiv.org/abs/2512.21516v1": {
    "content_path": "../data/contents/2512.21516v1.md",
    "pdf_path": "../data/contents/2512.21516v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:43.650422",
    "title": "Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data"
  },
  "https://arxiv.org/abs/2512.21746v1": {
    "content_path": "../data/contents/2512.21746v1.md",
    "pdf_path": "../data/contents/2512.21746v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:44.081626",
    "title": "A Model of Causal Explanation on Neural Networks for Tabular Data"
  },
  "https://arxiv.org/abs/2512.21351v1": {
    "content_path": "../data/contents/2512.21351v1.md",
    "pdf_path": "../data/contents/2512.21351v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:44.307624",
    "title": "CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation"
  },
  "https://arxiv.org/abs/2512.21956v1": {
    "content_path": "../data/contents/2512.21956v1.md",
    "pdf_path": "../data/contents/2512.21956v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:45.076263",
    "title": "Self-attention vector output similarities reveal how machines pay attention"
  },
  "https://arxiv.org/abs/2512.21515v1": {
    "content_path": "../data/contents/2512.21515v1.md",
    "pdf_path": "../data/contents/2512.21515v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:46.585044",
    "title": "Perplexity-Aware Data Scaling Law: Perplexity Landscapes Predict Performance for Continual Pre-training"
  },
  "https://arxiv.org/html/2510.09586v1": {
    "content_path": "../data/contents/2510.09586v1.md",
    "pdf_path": "../data/contents/2510.09586v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:46.864658",
    "title": "Vision Language Models: A Survey of 26K Papers (CVPR, ICLR, NeurIPS 2023-2025)"
  },
  "https://arxiv.org/abs/2512.21743v1": {
    "content_path": "../data/contents/2512.21743v1.md",
    "pdf_path": "../data/contents/2512.21743v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:47.162528",
    "title": "Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning"
  },
  "https://arxiv.org/abs/2512.18190v3": {
    "content_path": "../data/contents/2512.18190v3.md",
    "pdf_path": "../data/contents/2512.18190v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:47.529871",
    "title": "External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning"
  },
  "https://arxiv.org/abs/2512.08188": {
    "content_path": "../data/contents/2512.08188.md",
    "pdf_path": "../data/contents/2512.08188.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:56.489312",
    "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model"
  },
  "https://arxiv.org/abs/2512.21933v1": {
    "content_path": "../data/contents/2512.21933v1.md",
    "pdf_path": "../data/contents/2512.21933v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:56.716061",
    "title": "Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs"
  },
  "https://arxiv.org/abs/2512.21506v1": {
    "content_path": "../data/contents/2512.21506v1.md",
    "pdf_path": "../data/contents/2512.21506v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:57.055978",
    "title": "MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding"
  },
  "https://arxiv.org/html/2506.04788v1": {
    "content_path": "../data/contents/2506.04788v1.md",
    "pdf_path": "../data/contents/2506.04788v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:57.355672",
    "title": "Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies"
  },
  "https://arxiv.org/abs/2512.21730v1": {
    "content_path": "../data/contents/2512.21730v1.md",
    "pdf_path": "../data/contents/2512.21730v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:58.813998",
    "title": "Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference"
  },
  "https://arxiv.org/abs/2512.21345v1": {
    "content_path": "../data/contents/2512.21345v1.md",
    "pdf_path": "../data/contents/2512.21345v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:59.057614",
    "title": "Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks"
  },
  "https://arxiv.org/abs/2510.16916": {
    "content_path": "../data/contents/2510.16916.md",
    "pdf_path": "../data/contents/2510.16916.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:06:59.818665",
    "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search"
  },
  "https://arxiv.org/abs/2512.21925v1": {
    "content_path": "../data/contents/2512.21925v1.md",
    "pdf_path": "../data/contents/2512.21925v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:01.244661",
    "title": "Hybrid Combinatorial Multi-armed Bandits with Probabilistically Triggered Arms"
  },
  "https://arxiv.org/abs/2512.21499v1": {
    "content_path": "../data/contents/2512.21499v1.md",
    "pdf_path": "../data/contents/2512.21499v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:02.072396",
    "title": "Weighted Fourier Factorizations: Optimal Gaussian Noise for Differentially Private Marginal and Product Queries"
  },
  "https://arxiv.org/abs/2512.21727v1": {
    "content_path": "../data/contents/2512.21727v1.md",
    "pdf_path": "../data/contents/2512.21727v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:02.419154",
    "title": "PERELMAN: Pipeline for scientific literature meta-analysis. Technical report"
  },
  "https://arxiv.org/abs/2512.17367v2": {
    "content_path": "../data/contents/2512.17367v2.md",
    "pdf_path": "../data/contents/2512.17367v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:03.464023",
    "title": "Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach"
  },
  "https://arxiv.org/abs/2512.21921v1": {
    "content_path": "../data/contents/2512.21921v1.md",
    "pdf_path": "../data/contents/2512.21921v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:12.858103",
    "title": "AutoPP: Towards Automated Product Poster Generation and Optimization"
  },
  "https://arxiv.org/abs/2512.21487v1": {
    "content_path": "../data/contents/2512.21487v1.md",
    "pdf_path": "../data/contents/2512.21487v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:13.243497",
    "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism"
  },
  "https://arxiv.org/abs/2512.21723v1": {
    "content_path": "../data/contents/2512.21723v1.md",
    "pdf_path": "../data/contents/2512.21723v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:15.023518",
    "title": "HELP: Hierarchical Embodied Language Planner for Household Tasks"
  },
  "https://arxiv.org/abs/2512.16917v2": {
    "content_path": "../data/contents/2512.16917v2.md",
    "pdf_path": "../data/contents/2512.16917v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:15.285743",
    "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning"
  },
  "https://arxiv.org/abs/2512.21919v1": {
    "content_path": "../data/contents/2512.21919v1.md",
    "pdf_path": "../data/contents/2512.21919v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:15.586112",
    "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents"
  },
  "https://arxiv.org/abs/2512.21482v1": {
    "content_path": "../data/contents/2512.21482v1.md",
    "pdf_path": "../data/contents/2512.21482v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:16.334554",
    "title": "LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis"
  },
  "https://arxiv.org/abs/2512.21720v1": {
    "content_path": "../data/contents/2512.21720v1.md",
    "pdf_path": "../data/contents/2512.21720v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:18.818305",
    "title": "An Information Theoretic Perspective on Agentic System Design"
  },
  "https://arxiv.org/abs/2512.16038v2": {
    "content_path": "../data/contents/2512.16038v2.md",
    "pdf_path": "../data/contents/2512.16038v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:19.233881",
    "title": "LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines"
  },
  "https://arxiv.org/abs/2512.21917v1": {
    "content_path": "../data/contents/2512.21917v1.md",
    "pdf_path": "../data/contents/2512.21917v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:19.599921",
    "title": "Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model"
  },
  "https://arxiv.org/abs/2512.21481v1": {
    "content_path": "../data/contents/2512.21481v1.md",
    "pdf_path": "../data/contents/2512.21481v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:20.265661",
    "title": "The AI Committee: A Multi-Agent Framework for Automated Validation and Remediation of Web-Sourced Data"
  },
  "https://arxiv.org/abs/2512.21715v1": {
    "content_path": "../data/contents/2512.21715v1.md",
    "pdf_path": "../data/contents/2512.21715v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:20.581831",
    "title": "CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation"
  },
  "https://arxiv.org/abs/2512.14693v3": {
    "content_path": "../data/contents/2512.14693v3.md",
    "pdf_path": "../data/contents/2512.14693v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:20.810862",
    "title": "Universal Reasoning Model"
  },
  "https://arxiv.org/abs/2512.21915v1": {
    "content_path": "../data/contents/2512.21915v1.md",
    "pdf_path": "../data/contents/2512.21915v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:24.554476",
    "title": "Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs"
  },
  "https://arxiv.org/abs/2512.21473v1": {
    "content_path": "../data/contents/2512.21473v1.md",
    "pdf_path": "../data/contents/2512.21473v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:25.202482",
    "title": "Demystifying ARM SME to Optimize General Matrix Multiplications"
  },
  "https://arxiv.org/abs/2512.21711v1": {
    "content_path": "../data/contents/2512.21711v1.md",
    "pdf_path": "../data/contents/2512.21711v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:25.719873",
    "title": "Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought"
  },
  "https://arxiv.org/abs/2512.13478v5": {
    "content_path": "../data/contents/2512.13478v5.md",
    "pdf_path": "../data/contents/2512.13478v5.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:25.939565",
    "title": "Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation"
  },
  "https://arxiv.org/abs/2512.21913v1": {
    "content_path": "../data/contents/2512.21913v1.md",
    "pdf_path": "../data/contents/2512.21913v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:26.179866",
    "title": "GQ-VAE: A gated quantized VAE for learning variable length tokens"
  },
  "https://arxiv.org/abs/2512.21456v1": {
    "content_path": "../data/contents/2512.21456v1.md",
    "pdf_path": "../data/contents/2512.21456v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:30.504500",
    "title": "Statistical vs. Deep Learning Models for Estimating Substance Overdose Excess Mortality in the US"
  },
  "https://arxiv.org/abs/2512.21708v1": {
    "content_path": "../data/contents/2512.21708v1.md",
    "pdf_path": "../data/contents/2512.21708v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:31.015585",
    "title": "MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles"
  },
  "https://arxiv.org/abs/2512.13102v3": {
    "content_path": "../data/contents/2512.13102v3.md",
    "pdf_path": "../data/contents/2512.13102v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:31.591829",
    "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions"
  },
  "https://arxiv.org/abs/2512.21911v1": {
    "content_path": "../data/contents/2512.21911v1.md",
    "pdf_path": "../data/contents/2512.21911v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:31.842088",
    "title": "Accelerate Speculative Decoding with Sparse Computation in Verification"
  },
  "https://arxiv.org/abs/2512.21450v1": {
    "content_path": "../data/contents/2512.21450v1.md",
    "pdf_path": "../data/contents/2512.21450v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:32.068357",
    "title": "RLLaVA: An RL-central Framework for Language and Vision Assistants"
  },
  "https://arxiv.org/abs/2512.21706v1": {
    "content_path": "../data/contents/2512.21706v1.md",
    "pdf_path": "../data/contents/2512.21706v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:32.408612",
    "title": "Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech"
  },
  "https://arxiv.org/abs/2512.21653v1": {
    "content_path": "../data/contents/2512.21653v1.md",
    "pdf_path": "../data/contents/2512.21653v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:32.606861",
    "title": "Semantic Codebooks as Effective Priors for Neural Speech Compression"
  },
  "https://arxiv.org/abs/2512.12324v2": {
    "content_path": "../data/contents/2512.12324v2.md",
    "pdf_path": "../data/contents/2512.12324v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:35.240229",
    "title": "UniMark: Artificial Intelligence Generated Content Identification Toolkit"
  },
  "https://arxiv.org/abs/2512.21907v1": {
    "content_path": "../data/contents/2512.21907v1.md",
    "pdf_path": "../data/contents/2512.21907v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:36.203689",
    "title": "SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?"
  },
  "https://arxiv.org/abs/2512.21446v1": {
    "content_path": "../data/contents/2512.21446v1.md",
    "pdf_path": "../data/contents/2512.21446v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:37.661784",
    "title": "dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning"
  },
  "https://arxiv.org/abs/2402.18485": {
    "content_path": "../data/contents/2402.18485.md",
    "pdf_path": "../data/contents/2402.18485.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:42.974409",
    "title": "FinAgent: A Multimodal Foundation Agent for Financial Trading"
  },
  "https://arxiv.org/abs/2512.21699v1": {
    "content_path": "../data/contents/2512.21699v1.md",
    "pdf_path": "../data/contents/2512.21699v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:43.855617",
    "title": "Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning"
  },
  "https://arxiv.org/abs/2512.22120v1": {
    "content_path": "../data/contents/2512.22120v1.md",
    "pdf_path": "../data/contents/2512.22120v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:44.892921",
    "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning"
  },
  "https://arxiv.org/abs/2512.21902v1": {
    "content_path": "../data/contents/2512.21902v1.md",
    "pdf_path": "../data/contents/2512.21902v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:45.281224",
    "title": "Explainable Statute Prediction via Attention-based Model and LLM Prompting"
  },
  "https://arxiv.org/abs/2512.21440v1": {
    "content_path": "../data/contents/2512.21440v1.md",
    "pdf_path": "../data/contents/2512.21440v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:45.798948",
    "title": "Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing"
  },
  "https://arxiv.org/abs/2512.21670v1": {
    "content_path": "../data/contents/2512.21670v1.md",
    "pdf_path": "../data/contents/2512.21670v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:46.278277",
    "title": "The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds"
  },
  "https://arxiv.org/abs/2512.22046v1": {
    "content_path": "../data/contents/2512.22046v1.md",
    "pdf_path": "../data/contents/2512.22046v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:47.300187",
    "title": "Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models"
  },
  "https://arxiv.org/abs/2512.21354": {
    "content_path": "../data/contents/2512.21354.md",
    "pdf_path": "../data/contents/2512.21354.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:48.394912",
    "title": "Reflection-Driven Control for Trustworthy Code Agents"
  },
  "https://arxiv.org/abs/2512.21897v1": {
    "content_path": "../data/contents/2512.21897v1.md",
    "pdf_path": "../data/contents/2512.21897v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:48.656543",
    "title": "MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction"
  },
  "https://arxiv.org/abs/2512.21439v1": {
    "content_path": "../data/contents/2512.21439v1.md",
    "pdf_path": "../data/contents/2512.21439v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:49.081083",
    "title": "Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models"
  },
  "https://arxiv.org/abs/2512.21657v1": {
    "content_path": "../data/contents/2512.21657v1.md",
    "pdf_path": "../data/contents/2512.21657v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:07:49.281658",
    "title": "Near-Optimal Coalition Structures in Polynomial Time"
  },
  "https://arxiv.org/abs/2512.22009v1": {
    "content_path": "../data/contents/2512.22009v1.md",
    "pdf_path": "../data/contents/2512.22009v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T00:08:03.041473",
    "title": "iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception"
  },
  "https://www.langchain.com/state-of-agent-engineering": {
    "content_path": "../data/contents/2253bdfec0d930ce.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:03.738439",
    "title": "State of AI Agents"
  },
  "https://www.anthropic.com/research/building-effective-agents": {
    "content_path": "../data/contents/38c43325a50c50c6.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:03.791347",
    "title": "Building Effective AI Agents"
  },
  "https://aclanthology.org/2024.findings-acl.11/": {
    "content_path": "../data/contents/6cdc9b961698da04.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:03.858583",
    "title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs"
  },
  "https://ai.google.dev/gemini-api/docs/changelog": {
    "content_path": "../data/contents/a24fd97eb3d00730.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:04.025761",
    "title": "Release notes"
  },
  "https://microsoft.ai/news/introducing-mai-image-1-debuting-in-the-top-10-on-lmarena/": {
    "content_path": "../data/contents/86da896e63f7fbc7.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:04.074631",
    "title": "Introducing MAI-Image-1, debuting in the top 10 on LMArena | Microsoft AI"
  },
  "https://docs.vllm.ai/en/v0.8.2/features/structured_outputs.html": {
    "content_path": "../data/contents/ead39054523ef88f.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:04.107882",
    "title": "Structured Outputs — vLLM"
  },
  "https://venturebeat.com/technology/anthropic-launches-enterprise-agent-skills-and-opens-the-standard": {
    "content_path": "../data/contents/38965495ac3c030b.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:04.136974",
    "title": "Anthropic launches enterprise ‘Agent Skills’ and opens the standard, challenging OpenAI in workplace AI"
  },
  "https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking": {
    "content_path": "../data/contents/4b95225a7f6d480b.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:04.272357",
    "title": "Not Found - Claude Docs"
  },
  "https://aclanthology.org/2025.findings-emnlp.554/": {
    "content_path": "../data/contents/edf6d8b5f9aea042.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:04.808691",
    "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning"
  },
  "https://aclanthology.org/2025.coling-main.698/": {
    "content_path": "../data/contents/860e9ed604aea94b.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:04.953016",
    "title": "KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions"
  },
  "https://www.anthropic.com/news/agent-capabilities-api": {
    "content_path": "../data/contents/f9ef0476825734e0.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:05.018734",
    "title": "New capabilities for building agents on the Anthropic API"
  },
  "https://scalr-workshop.github.io/": {
    "content_path": "../data/contents/511cc59ca166219b.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:05.052041",
    "title": "The First Workshop on Test-time Scaling and Reasoning Models"
  },
  "https://aclanthology.org/2024.tacl-1.78/": {
    "content_path": "../data/contents/32592259fba8a486.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:05.081160",
    "title": "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs"
  },
  "https://github.com/ShawnKS/rekgmcts": {
    "content_path": "../data/contents/163407da3fce56b6.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:05.355786",
    "title": "GitHub - ShawnKS/rekgmcts"
  },
  "https://www.cbinsights.com/research/ai-agent-startups-top-20-revenue": {
    "content_path": "../data/contents/2e7162385265d1f7.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:05.394578",
    "title": "AI agent startups are becoming revenue machines — here are the top 20 ranked"
  },
  "https://aclanthology.org/2025.findings-acl.484/": {
    "content_path": "../data/contents/2da4b0821d273db1.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:05.763196",
    "title": "ReKG-MCTS: Reinforcing LLM Reasoning on Knowledge Graphs via Training-Free Monte Carlo Tree Search"
  },
  "https://openai.com/index/introducing-4o-image-generation/": {
    "content_path": "../data/contents/877e7da88408c032.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:05.828267",
    "title": "Introducing 4o Image Generation"
  },
  "https://aclanthology.org/2025.findings-emnlp.873/": {
    "content_path": "../data/contents/9274d67d7107d17e.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:06.041823",
    "title": "Hallucination Detection in Structured Query Generation via LLM Self-Debating"
  },
  "https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00": {
    "content_path": "../data/contents/76425ad83a3c9df9.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:06.125052",
    "title": "LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG"
  },
  "https://www.llamaindex.ai/blog/beyond-ocr-how-llms-are-revolutionizing-pdf-parsing": {
    "content_path": "../data/contents/5b4ee3761555be8b.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.093668",
    "title": "Beyond OCR: How LLMs Are Revolutionizing PDF Parsing for Enterprise Document Processing",
    "summary": "The webpage discusses how Large Language Models (LLMs) are revolutionizing PDF parsing and enterprise document processing, moving beyond the limitations of traditional OCR.\n\nKey points related to your query include:\n\n*   **Vision-language models:** The platform, **LlamaParse**, leverages state-of-the-art vision-language models to understand both text and visual elements in PDFs, maintaining document structure.\n*   **Document understanding/PDF parsing:** The core focus is on intelligent PDF parsing and extraction to transform unstructured documents into structured, searchable data.\n*   **Chart/table extraction:** LlamaParse features **Intelligent Table Processing** that preserves table structure, handles complex formatting, and maintains data integrity.\n*   **Structured document output:** The platform supports **Structured Output Generation with LlamaExtract**, providing output in formats like Markdown, JSON, or XML based on configurable extraction schemas.\n*   **Multimodal RAG:** Phase 4 mentions implementing **Document RAG** (Retrieval-Augmented Generation) to build searchable knowledge bases from processed documents.\n\nThe page does not explicitly mention **report generation with LLMs**, **GPT-4V**, **Claude vision**, or **Gemini**.",
    "published_date": "2025-07-22T00:00:00.000Z",
    "score": null
  },
  "https://blog.langchain.com/interrupt-2025-recap/": {
    "content_path": "../data/contents/0293116f05d485c0.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:06.761819",
    "title": "Recap of Interrupt 2025: The AI Agent Conference by LangChain"
  },
  "https://aclanthology.org/2025.emnlp-main.125/": {
    "content_path": "../data/contents/8411e2fd605233fb.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:06.807557",
    "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning"
  },
  "https://www.skylarkai.com/": {
    "content_path": "../data/contents/be62bb96cb1d1ed7.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.049581",
    "title": "Skylark AI - Leading Investment Analyst & Financial AI Analyst"
  },
  "https://cyndx.com/": {
    "content_path": "../data/contents/0509abaabad6d205.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.111519",
    "title": "Cyndx | Purpose-Built AI for Investment Professionals"
  },
  "https://press.spglobal.com/2025-07-15-S-P-Global-and-Anthropic-Announce-Integration-of-S-P-Globals-Trusted-Financial-Data-into-Claude": {
    "content_path": "../data/contents/bd891f73db75bb88.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.304038",
    "title": "S&P Global and Anthropic Announce Integration of S&P Global's Trusted Financial Data into Claude"
  },
  "https://aws.amazon.com/about-aws/whats-new/2025/10/model-context-protocol-proxy-available/": {
    "content_path": "../data/contents/604a4c654474f018.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.357389",
    "title": "The Model Context Protocol (MCP) Proxy for AWS is now generally available"
  },
  "https://github.com/dforwardfeed/memo_generator": {
    "content_path": "../data/contents/c3afc8de3536676c.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.390600",
    "title": "GitHub - dforwardfeed/memo_generator: Tool that generates AI startup investment memorandums"
  },
  "https://github.com/run-llama/workflows-py": {
    "content_path": "../data/contents/eed734b14de9f44a.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.420693",
    "title": "GitHub - run-llama/workflows-py: Workflows are an event-driven, async-first, step-based way to control the execution flow of AI applications like agents."
  },
  "https://docs.cohere.com/changelog/rerank-v3.5": {
    "content_path": "../data/contents/91a7029fc0682ae6.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.451803",
    "title": "Announcing Rerank-v3.5 | Cohere"
  },
  "https://github.com/run-llama/mcp-server-llamacloud": {
    "content_path": "../data/contents/04e9f5e6b9a29e71.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.532389",
    "title": "GitHub - run-llama/mcp-server-llamacloud: A MCP server connecting to managed indexes on LlamaCloud"
  },
  "https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf": {
    "content_path": "../data/contents/fafd703f15bda2e9.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.622331",
    "title": "gemini_v2_5_report.pdf"
  },
  "https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/": {
    "content_path": "../data/contents/81c618acb8085ad6.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.663612",
    "title": "Introducing the MCP Registry"
  },
  "https://www.lseg.com/en/media-centre/press-releases/2025/lseg-announces-new-collaboration-with-openai": {
    "content_path": "../data/contents/499bf8ec0022c1cc.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.763653",
    "title": "LSEG announces new collaboration with OpenAI"
  },
  "https://github.com/huygiatrng/AlpacaTradingAgent": {
    "content_path": "../data/contents/708ccd964b3daf72.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.793713",
    "title": "GitHub - huygiatrng/AlpacaTradingAgent: AlpacaTradingAgent: Multi-Agents LLM Financial Trading Framework and perform trades on alpaca"
  },
  "https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking": {
    "content_path": "../data/contents/0ce367eb09fc4bbd.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:07.961496",
    "title": "Hybrid search scoring (RRF) - Azure AI Search"
  },
  "https://www.prnewswire.com/news-releases/daloopa-expands-financial-data-model-context-protocol-mcp-through-a-new-connector-with-openai-302636805.html": {
    "content_path": "../data/contents/f1473050db48a039.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:08.093293",
    "title": "Daloopa expands financial data Model Context Protocol (MCP) through a new connector with OpenAI"
  },
  "https://www.analyticsvidhya.com/blog/2025/01/multimodal-financial-report-generation-using-llamaindex/": {
    "content_path": "../data/contents/97601bec10f5dee6.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:08.266317",
    "title": "Multimodal Financial Report Generation (from a Slide Deck) using Llamaindex"
  },
  "https://huggingface.co/nvidia/NV-Embed-v2": {
    "content_path": "../data/contents/221e863e18d18e5a.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:08.356580",
    "title": "nvidia/NV-Embed-v2 · Hugging Face"
  },
  "https://onlinelibrary.wiley.com/doi/10.1111/cgf.70146": {
    "content_path": "../data/contents/1806abfb97bbf22b.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:08.427424",
    "title": "Just a moment..."
  },
  "https://huggingface.co/jinaai/jina-reranker-v3": {
    "content_path": "../data/contents/e8bdc4d6f2c15382.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:08.500981",
    "title": "jinaai/jina-reranker-v3 · Hugging Face"
  },
  "https://github.com/yale-nlp/MCTS-RAG": {
    "content_path": "../data/contents/f75177b9a1c82fe9.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:08.559387",
    "title": "GitHub - yale-nlp/MCTS-RAG: Data and Code for EMNLP 2025 Findings Paper \"MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search\""
  },
  "https://github.com/aws/mcp-proxy-for-aws": {
    "content_path": "../data/contents/7d083376ff631821.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:08.596409",
    "title": "GitHub - aws/mcp-proxy-for-aws: AWS MCP Proxy Server"
  },
  "https://github.com/TradingGoose/Trading-Goose.github.io": {
    "content_path": "../data/contents/780bc2d4177982fa.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:08.843131",
    "title": "GitHub - TradingGoose/TradingGoose.github.io: A Multi-Agents LLM Financial Trading Framework for both individual stock analysis & portfolio management"
  },
  "https://huggingface.co/nvidia/NVIDIA-Nemotron-Parse-v1.1": {
    "content_path": "../data/contents/d52a69ccd8043205.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:08.909470",
    "title": "nvidia/NVIDIA-Nemotron-Parse-v1.1 · Hugging Face"
  },
  "https://opensearch.org/blog/introducing-reciprocal-rank-fusion-hybrid-search/": {
    "content_path": "../data/contents/ba2b39e0ede363d4.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:09.004270",
    "title": "Introducing reciprocal rank fusion for hybrid search"
  },
  "https://docs.cloud.google.com/vertex-ai/docs/vector-search/about-hybrid-search": {
    "content_path": "../data/contents/82203a698fb11703.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:09.279079",
    "title": "About hybrid search"
  },
  "https://openai.com/index/introducing-gpt-5/": {
    "content_path": "../data/contents/5cb27984e4018400.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:09.729038",
    "title": "Introducing GPT-5"
  },
  "https://pitchbook.com/news/articles/y-combinator-is-going-all-in-on-ai-agents-making-up-nearly-50-of-latest-batch": {
    "content_path": "../data/contents/eaa88e0397c35f6e.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:09.786583",
    "title": "Y Combinator is going all-in on AI agents, making up nearly 50% of latest batch"
  },
  "https://www.mongodb.com/company/blog/innovation/ai-agents-hybrid-search-indexing-with-langchain-mongodb": {
    "content_path": "../data/contents/8618f21ba53b0d58.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:09.852282",
    "title": "AI Agents, Hybrid Search, and Indexing with LangChain and MongoDB"
  },
  "https://github.com/hananedupouy/LLMs-in-Finance": {
    "content_path": "../data/contents/569d588e29608c59.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:09.884545",
    "title": "GitHub - hananedupouy/LLMs-in-Finance: LLMs in Finance - Generative AI - AI Agents"
  },
  "https://www.microsoft.com/en-us/research/blog/graphrag-auto-tuning-provides-rapid-adaptation-to-new-domains/": {
    "content_path": "../data/contents/15d94306d7d151f3.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:09.945691",
    "title": "GraphRAG Auto-Tuning Provides Rapid Adaptation To New Domains"
  },
  "https://aws.amazon.com/blogs/database/build-persistent-memory-for-agentic-ai-applications-with-mem0-open-source-amazon-elasticache-for-valkey-and-amazon-neptune-analytics/": {
    "content_path": "../data/contents/a2da37515d6f71c2.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:09.994752",
    "title": "Build persistent memory for agentic AI applications with Mem0 Open Source, Amazon ElastiCache for Valkey, and Amazon Neptune Analytics | Amazon Web Services"
  },
  "https://docs.crewai.com/en/introduction": {
    "content_path": "../data/contents/9a53266d81ca32ab.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:10.234411",
    "title": "Introduction - CrewAI"
  },
  "https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api": {
    "content_path": "../data/contents/d3d14ccae08f128a.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:10.484558",
    "title": "Function Calling in the OpenAI API | OpenAI Help Center"
  },
  "https://github.com/agentsmd/agents.md": {
    "content_path": "../data/contents/35be15729752ee58.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:10.520627",
    "title": "GitHub - agentsmd/agents.md: AGENTS.md — a simple, open format for guiding coding agents"
  },
  "https://github.com/yannqi/RCTS-RAG": {
    "content_path": "../data/contents/7495845680c673dc.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:10.548990",
    "title": "GitHub - yannqi/RCTS-RAG: [ICML 2025 Spotlight] Official implementation of the paper: Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger"
  },
  "https://developers.redhat.com/articles/2025/06/03/structured-outputs-vllm-guiding-ai-responses": {
    "content_path": "../data/contents/621ba54ab8917d10.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:10.591583",
    "title": "Structured outputs in vLLM: Guiding AI responses | Red Hat Developer"
  },
  "https://www.analyticsvidhya.com/blog/2024/12/contextual-rag-systems-with-hybrid-search-and-reranking/": {
    "content_path": "../data/contents/fcfe6b07b172fdee.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:10.799331",
    "title": "Building Contextual RAG Systems with Hybrid Search and Reranking"
  },
  "https://docs.weaviate.io/weaviate/search/rerank": {
    "content_path": "../data/contents/dcb12831a539412f.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:11.730577",
    "title": "Reranking | Weaviate Documentation"
  },
  "https://www.mongodb.com/docs/atlas/atlas-vector-search/changelog/": {
    "content_path": "../data/contents/69dd37cac1c9dd6e.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:13.440443",
    "title": "MongoDB Vector Search Changelog"
  },
  "https://medium.com/@georgekar91/testing-langchain-deep-agents-in-financial-services-three-use-cases-and-what-i-learned-5213b20ba15b": {
    "content_path": "../data/contents/c5f18bd5b92dceb5.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:13.645265",
    "title": "Testing LangChain Deep Agents in Financial Services: Three Use Cases and What I Learned"
  },
  "https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai": {
    "content_path": "../data/contents/c1e31a3255ae290d.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:16.472630",
    "title": "The state of AI in 2025: Agents, innovation, and transformation"
  },
  "https://blogs.oracle.com/database/post/oracle-announces-oracle-ai-database-26ai": {
    "content_path": "../data/contents/91234a823e55d4d8.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:16.839755",
    "title": "Oracle Announces Oracle AI Database 26ai"
  },
  "https://www.allvuesystems.com/solutions/agentic-ai-platform/": {
    "content_path": "../data/contents/504b4ab6b02e05d1.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:33.979134",
    "title": "Allvue Agentic AI Platform | Allvue Systems"
  },
  "https://qdrant.tech/articles/sparse-vectors/": {
    "content_path": "../data/contents/6af4ae5f13803ee8.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:34.067128",
    "title": "What is a Sparse Vector? How to Achieve Vector-based Hybrid Search"
  },
  "https://huggingface.co/spaces/mteb/leaderboard": {
    "content_path": "../data/contents/9630f18b8920bde4.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:34.867141",
    "title": "MTEB Leaderboard - a Hugging Face Space by mteb"
  },
  "https://blog.google/technology/developers/build-with-gemini-3-flash": {
    "content_path": "../data/contents/de9a22425a9d8b6f.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:36.224290",
    "title": "Build with Gemini 3 Flash, frontier intelligence that scales with you"
  },
  "https://www.mongodb.com/company/blog/product-release-announcements/boost-search-relevance-mongodb-atlas-native-hybrid-search": {
    "content_path": "../data/contents/bed5408fcf2864ec.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:37.323730",
    "title": "Boost Search Relevance with MongoDB Atlas’ Native Hybrid Search"
  },
  "https://learn.microsoft.com/en-us/azure/databricks/release-notes/product/2025/december": {
    "content_path": "../data/contents/47b2b19b84655768.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:08:39.053904",
    "title": "December 2025 - Azure Databricks"
  },
  "https://github.com/Yuan-Li-FNLP/R3-RAG": {
    "content_path": "../data/contents/2a6e21bd83234102.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:09:35.438540",
    "title": "GitHub - Yuan-Li-FNLP/R3-RAG"
  },
  "https://github.com/microsoft/graphrag": {
    "content_path": "../data/contents/118480639e4f896f.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:09:36.551803",
    "title": "GitHub - microsoft/graphrag: A modular graph-based Retrieval-Augmented Generation (RAG) system"
  },
  "https://github.com/run-llama/llama_deploy": {
    "content_path": "../data/contents/a9eb8f0d1fe126cf.md",
    "source": "jina",
    "saved_at": "2025-12-30T00:09:39.383877",
    "title": "GitHub - run-llama/llama_deploy: Deploy your agentic worfklows to production"
  },
  "https://www.multimodal.dev/agentic-workflows/ai-due-diligence-private-equity": {
    "content_path": "../data/contents/7f6ec32f0ad7b6b9.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:30.204630",
    "title": "Agentic AI for Due Diligence in Private Equity - Multimodal",
    "summary": "The webpage describes **Agentic AI for Due Diligence in Private Equity**, focusing on workflows that use AI agents to **speed up analysis** and **reduce manual work** in the due diligence process, while ensuring **humans remain in control**.\n\nThis directly relates to the user query's interest in **agents and finance**, specifically **private equity**, **venture capital**, **due diligence automation**, and **investment opportunity analysis**.",
    "published_date": null,
    "score": null
  },
  "https://medium.com/@adnanmasood/optimizing-chunking-embedding-and-vectorization-for-retrieval-augmented-generation-ea3b083b68f7": {
    "content_path": "data/contents/dac3ddf380cf6b29.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.700050",
    "title": "Optimizing Chunking, Embedding, and Vectorization for Retrieval-Augmented Generation",
    "summary": "The webpage provides a technical overview of optimizing Retrieval-Augmented Generation (RAG), focusing on three key areas: **smart chunking**, **domain-tuned embeddings**, and **high-recall vector indexes**.\n\nSpecifically, it covers:\n\n*   **Chunking Strategies:** Recommends recursive or semantic chunkers with 10–20% overlap, emphasizing alignment with context windows.\n*   **Embeddings:** Suggests using task-specific sentence transformers (like MiniLM, SBERT, Instruct, E5 for text, or CLIP for multimodal) and normalizing the embeddings.\n*   **Vector Databases/Indexes:** Recommends HNSW or IVF-PQ for scaling, and mentions monitoring recall@k, latency, and index memory.\n*   **Rerankers:** Mentions using cross-encoders (like Cohere ReRank or ColBERT) when precision is critical.\n*   **Hybrid Search:** Notes that combining BM25 + dense search boosts tail recall.\n\nThe article frames these elements as the core levers for successful RAG implementation.",
    "published_date": "2025-05-13T00:00:00.000Z",
    "score": null
  },
  "https://shav.dev/blog/enhancing-rag": {
    "content_path": "data/contents/858cb881b621daeb.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.765359",
    "title": "Building Better Retrieval Systems",
    "summary": "The webpage provides a detailed overview of building better retrieval systems, focusing heavily on **Retrieval-Augmented Generation (RAG)** systems.\n\nKey areas covered that align with your query include:\n\n*   **Embeddings and Retrieval Techniques:** Discusses the rapid evolution of embedding models and suggests advanced techniques to enhance retrieval quality when standard models fall short, such as:\n    *   Retrieving more and re-ranking.\n    *   Using different length windows for embedding.\n    *   LLM-assisted extraction.\n    *   **RAG-Fusion** and the **HyDE Approach**.\n    *   **Hybrid Search** (incorporating lexical algorithms like `bm25`).\n    *   Lemmatization strategies for embedding vs. context.\n*   **Choosing the Right Embedding Model:** Recommends using **Encoder-only Models** (like BERT/RoBERTa) or **Encoder-Decoder Models** (like T5) over Decoder-only models for their superior bidirectional embedding capabilities. Specific recommended models include Instructor-XL, BGE-XXL, and BGE-Large-v1.5.\n*   **Rerankers:** Explains the critical role of rerankers (cross-encoders) in a **two-stage retrieval system** to maximize LLM recall by refining the initial set of retrieved documents. It notes that rerankers are slower but more accurate than initial retrievers (bi-encoders).\n*   **Vector Databases:** Provides an overview of numerous vector database options (e.g., Elasticsearch, Faiss, Milvus, Qdrant, Chroma, pgvector, Pinecone, Redis) and outlines common features like vector similarity search, compression, and flexible nearest neighbor search. It emphasizes that vector databases should not be used as primary persistent storage.\n*   **RAG Architectures:** The entire article focuses on enhancing RAG systems, particularly through the implementation of two-stage retrieval involving vector databases (for the first stage) and rerankers (for the second stage).\n\nThe page touches upon **vector databases**, **embeddings (new efficient models)**, **rerankers**, **RAG architectures**, and **hybrid search**. It does not explicitly detail **chunking strategies** or **context window management** beyond mentioning the \"Lost in the Middle\" phenomenon related to context stuffing.",
    "published_date": "2024-03-28T16:45:08.000Z",
    "score": null
  },
  "https://tensorblue.com/blog/vector-database-comparison-pinecone-weaviate-qdrant-milvus-2025": {
    "content_path": "data/contents/972962ac3b138f3f.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.932764",
    "title": "Vector Databases Comparison 2025: Pinecone vs Weaviate vs Qdrant vs Milvus",
    "summary": "The webpage provides a comparison of popular **Vector Databases** (Pinecone, Weaviate, Qdrant, Milvus) and the **FAISS** library, detailing their strengths, weaknesses, pricing, and performance benchmarks.\n\nThis information is relevant to the user query's topics of **Vector databases** and **RAG architectures** (as vector databases are essential for RAG).\n\n**Summary relevant to the query:**\n\n*   **Vector Databases:** The page compares five options, highlighting that they are specialized databases for storing and searching high-dimensional vectors (embeddings), crucial for RAG systems.\n*   **Comparison Highlights:**\n    *   **Pinecone:** Easiest to use, managed cloud service, good for fast prototyping.\n    *   **Weaviate:** Open-source, built-in vectorization, strong filtering, hybrid search.\n    *   **Qdrant:** Fastest performance (Rust-based), rich filtering, good for cost-conscious teams.\n    *   **Milvus:** Enterprise-grade, designed for massive scale (billions of vectors), complex deployment.\n    *   **FAISS:** A library (not a database), fastest in-memory search, requires building wrapper services.\n*   **Hybrid Search:** Pinecone, Weaviate, Qdrant, and Milvus all support some form of hybrid search (vector + keyword).\n\nThe page **does not** discuss: new efficient embedding models, rerankers, RAG alternatives, chunking strategies, or context window management.",
    "published_date": "2025-10-20T00:00:00.000Z",
    "score": null
  },
  "https://docs.datastax.com/en/ragstack/intro-to-rag/index.html": {
    "content_path": "data/contents/d261bc165299bde4.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.898358",
    "title": "Introduction to RAG",
    "summary": "The provided text introduces Retrieval-Augmented Generation (RAG) as a technique to give Large Language Models (LLMs) knowledge of your specific data. The RAG process involves three main steps:\n\n1.  **Index Data for Retrieval:** This involves **chunking** the input data and converting each chunk into a vector (a series of numbers representing its characteristics) using **embeddings**. These vectors are then indexed in a **vector database** for searching.\n2.  **Query Data:** When a question is asked, the index is queried for semantically similar vectors, and these retrieved results are passed along with the question to the LLM to provide context.\n3.  **Evaluate RAG Performance:** Each step, from the **chunking strategy** to the final answer quality, can be tested and optimized.\n\nWhile the text covers vector databases, embeddings, chunking strategies, and the general RAG architecture, it does not explicitly mention **rerankers**, **RAG alternatives**, **hybrid search**, or **context window management**.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://webkul.com/blog/vector-database-comparison/": {
    "content_path": "data/contents/215cd7cf4deff510.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.962164",
    "title": "Vector Database Comparison : Which One Is Best for You?",
    "summary": "The webpage compares four leading vector databases: **ChromaDB**, **Pinecone**, **FAISS**, and **AWS S3 Vectors**, focusing on features, performance, scalability, ease of use, and cost to help users choose the right one for their AI applications (like semantic search and RAG).\n\nHere is a summary of how the page addresses the components of your query:\n\n*   **Vector databases:** The entire page is a comparison of four leading vector databases.\n*   **Embeddings:** The text mentions that vector databases store and query high-dimensional vector embeddings.\n*   **RAG architectures:** ChromaDB is noted as being easy to integrate with frameworks like LangChain or LlamaIndex for **RAG**.\n*   **Hybrid search:** ChromaDB specifically supports **hybrid search** combining vectors and attributes.\n*   **Chunking strategies, context window management, new efficient models, rerankers, RAG alternatives:** The webpage **does not** discuss chunking strategies, context window management, new efficient embedding models, rerankers, or RAG alternatives.\n\n**In summary:** The page provides a detailed comparison of vector databases, mentions their use in RAG, and confirms support for hybrid search in ChromaDB. It does not cover the other specific topics in your query.",
    "published_date": "2025-07-28T00:00:00.000Z",
    "score": null
  },
  "https://liquidmetal.ai/casesAndBlogs/vector-comparison/": {
    "content_path": "data/contents/5a12d1a1a3f687e6.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.997473",
    "title": "Vector Database Comparison: Pinecone vs Weaviate vs Qdrant vs FAISS vs Milvus vs Chroma (2025)",
    "summary": "The webpage provides a detailed comparison of leading vector databases (Pinecone, Weaviate, Qdrant, FAISS, Milvus, and Chroma) across various evaluation criteria relevant to AI applications, such as performance, scalability, ease of implementation, and cost.\n\nThe comparison is highly relevant to the user query's focus on **Retrieval Augmented Generation (RAG) architectures** and **Vector Databases**, as the text explicitly discusses:\n\n*   **Vector Databases:** It analyzes six major options.\n*   **RAG Architectures:** It frequently mentions RAG in the context of why vector databases are essential infrastructure and highlights which databases are best suited for RAG systems (e.g., Chroma for rapid RAG prototyping, Pinecone for enterprise RAG).\n*   **Hybrid Search:** Pinecone is noted for offering \"Hybrid search combining semantic search with keyword-based approaches.\"\n*   **Context Window Management/Chunking Strategies/Embeddings/Rerankers/RAG Alternatives:** While the page focuses heavily on the database layer, it touches upon related concepts:\n    *   **Embeddings:** It mentions that the quality of embeddings (determined by the model) is critical, though it doesn't compare different embedding models or chunking strategies directly.\n    *   **Rerankers/Alternatives:** It does not explicitly discuss rerankers or RAG alternatives beyond comparing the vector databases themselves.\n\n**Summary relevant to the query:**\n\nThe page compares six major **Vector Databases** (Pinecone, Weaviate, Qdrant, FAISS, Milvus, Chroma) essential for **RAG architectures**. It evaluates them based on features critical for RAG, including performance, scalability, and integration. Specifically, it notes that **Pinecone** supports **hybrid search**. The article emphasizes that the choice of database impacts the performance and scalability of RAG systems, though it does not detail specific **chunking strategies**, **context window management**, or compare **new efficient embedding models** or **rerankers**.",
    "published_date": "2025-04-09T00:00:00.000Z",
    "score": null
  },
  "https://www.zair.top/en/post/vector-database-compare/": {
    "content_path": "data/contents/e54ffbaa2a6da679.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.030996",
    "title": "Vector Database Comparison: Weaviate, Milvus, and Qdrant",
    "summary": "The webpage provides a comparison of various vector databases (Weaviate, Milvus, Qdrant, Pinecone, etc.), vector libraries (FAISS, HNSWLib, ANNOY), and databases with vector support (pgvector, Elasticsearch) primarily in the context of **RAG (Retrieval-Augmented Generation) systems**.\n\nHere is a summary of the information relevant to your query:\n\n*   **Vector Databases:** They are the core of RAG systems, designed to store and manage high-dimensional vector data efficiently for similarity searches. Specialized vector databases like **Weaviate, Milvus, and Qdrant** are highlighted as strong choices for RAG due to their performance, scalability, and features.\n*   **Embeddings:** The text mentions that vector databases store vectors derived from text, images, audio, etc., implying the use of embedding models, and specifically notes that **Weaviate** has built-in text embedding support (FastEmbed).\n*   **RAG Architectures:** The entire article focuses on the foundational component of RAG—the vector database—and how its performance dictates the success of the RAG system.\n*   **Hybrid Search:** **Weaviate** explicitly supports **hybrid search** by combining BM25 keyword search and vector similarity search (using RRF\\*+RSF\\*). **Qdrant** supports applying full-text filters to vector searches, which is a form of hybrid retrieval. **Milvus** supports multi-vector search which can be used for complex retrieval scenarios.\n*   **Rerankers:** **Weaviate** supports **Re-ranking** of retrieved search results using a re-ranking module.\n*   **Chunking Strategies & Context Window Management:** These specific topics are **not** discussed in detail in the provided text. The text focuses on database selection, indexing, and search methods.\n\n**In summary:** The page extensively covers **Vector Databases** and touches upon **Hybrid Search** and **Reranking** capabilities within specific databases (Weaviate, Qdrant, Milvus). It confirms the necessity of **embeddings** for vector storage but does not detail new efficient models, chunking strategies, or context window management.\n\n**Conclusion:** Most of your query topics are addressed, but some key areas are missing.",
    "published_date": "2025-02-26T00:00:00.000Z",
    "score": null
  },
  "https://huggingface.co/papers/2506.04344": {
    "content_path": "../data/contents/f2ae9183a4c70e2f.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.074033",
    "title": "GEM: Empowering LLM for both Embedding Generation and Language\n  Understanding",
    "summary": "The provided web page describes a self-supervised approach called **Generative Embedding LLM (GEM)** that enables large decoder-only LLMs to generate high-quality text embeddings without compromising their existing text generation and reasoning capabilities.\n\nThis is relevant to your query on **retrieval and embeddings** because:\n\n*   It addresses the issue of relying on separate embedding models for tasks like **Retrieval Augmented Generation (RAG)**, which can cause understanding discrepancies.\n*   The method involves inserting special tokens and manipulating the attention mask to generate a summarization embedding.\n*   The results show that GEM significantly improves the original LLMs on **text embedding benchmarks (MTEB)** while minimally impacting their performance on NLP benchmarks (MMLU).\n\nThe page **does not** explicitly discuss: Vector databases, new efficient embedding models (beyond the GEM approach itself), rerankers, specific RAG architectures or alternatives, hybrid search, chunking strategies, or context window management.",
    "published_date": null,
    "score": null
  },
  "https://neptune.ai/blog/what-are-llm-embeddings": {
    "content_path": "../data/contents/6d8bec5390a78456.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.103754",
    "title": "What are LLM Embeddings: All you Need to Know",
    "summary": "The webpage provides information on **LLM Embeddings**, which are numerical, vector representations of text used by Large Language Models (LLMs).\n\nHere is a summary of the topics mentioned in your query that are covered in the text:\n\n*   **Embeddings:** The core topic, explaining what they are, how they work (tokenization, embedding layer, positional encoding like RoPE), and their history (from sparse to static to contextualized embeddings).\n*   **Vector databases:** Mentioned as the storage mechanism for document embeddings in **Semantic Search** and **RAG** architectures.\n*   **New efficient models:** The text discusses model selection criteria like parameter count and dimensionality, and mentions specific models like Qwen3-Embedding-4B and Gemma 3N (using Per-Layer Embeddings for optimization).\n*   **RAG architectures:** **Retrieval-Augmented Generation (RAG)** is detailed as an application where embeddings are used to perform semantic search against an external knowledge base to ground LLM responses.\n*   **Chunking strategies & context window management:** The text discusses **context length** as a key feature, noting how positional encoding methods like RoPE help models handle longer sequences better than older methods (like APE). It also mentions that RAG relies on retrieving relevant text **chunks** (documents/passages).\n\nThe following topics from your query are **not explicitly detailed** in the provided text:\n\n*   **Rerankers:** Not mentioned.\n*   **RAG alternatives:** Not mentioned, though RAG itself is described.\n*   **Hybrid search:** Not mentioned.",
    "published_date": "2025-11-06T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2412.12591": {
    "content_path": "../data/contents/e2657bb9cff262b4.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.133167",
    "title": "Computer Science > Computation and Language",
    "summary": "The provided webpage is an abstract for an arXiv paper titled \"LLMs are Also Effective Embedding Models: An In-depth Overview.\"\n\nThis paper surveys the transition to using Large Language Models (LLMs) as embedding models, moving beyond traditional encoder-only models. It covers:\n*   **Deriving embeddings from LLMs** through two main strategies: **Direct prompting** and **Data-centric tuning** (covering architecture, objectives, and data construction).\n*   **Advanced methods** for producing embeddings from longer texts, multilingual data, code, cross-modal data, and for reasoning-aware scenarios.\n*   **Factors affecting choices** of embedding models, such as performance/efficiency comparisons, dense vs sparse embeddings, pooling strategies, and scaling laws.\n*   **Limitations and challenges**, including cross-task quality, efficiency/accuracy trade-offs, low-resource scenarios, long-context issues, data bias, and robustness.\n\nWhile the paper discusses **embeddings** and factors related to **context** (long-context issues), it **does not explicitly mention or detail** the following specific topics from your query: **Vector databases, rerankers, RAG architectures, RAG alternatives, hybrid search, or chunking strategies.**",
    "published_date": "2024-12-17T00:00:00.000Z",
    "score": null
  },
  "https://llm.datasette.io/en/stable/embeddings/": {
    "content_path": "../data/contents/ea75d3a451ee821d.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.189619",
    "title": "Embeddings - LLM",
    "summary": "The webpage explains what **Embeddings** are: they convert text (or other content like images) into an array of floating-point numbers (an \"embedding vector\") that numerically represents the semantic meaning of the content in a multi-dimensional space. By calculating the distance between these vectors, you can find semantically similar content, which is useful for features like related article lookups or **semantic search**.\n\nThe page also mentions that the LLM tool supports multiple embedding models via plugins, allowing users to calculate and store embeddings, and perform similarity searches using the command line or Python API. It lists various commands and functionalities related to working with embeddings, such as embedding data from different sources (CLI, CSV, SQLite) and managing collections.\n\n**However, the page does not explicitly discuss or provide details on the following specific topics mentioned in your query:**\n\n*   Vector databases\n*   Rerankers\n*   RAG architectures or alternatives\n*   Hybrid search\n*   Chunking strategies\n*   Context window management\n\nTherefore, while it covers the fundamental concept of **embeddings** and mentions tools for working with them, it **does not answer** the majority of your detailed query regarding advanced RAG components and vector database specifics.",
    "published_date": "2023-09-10T00:00:00.000Z",
    "score": null
  },
  "https://vectordb.lqhl.me/": {
    "content_path": "data/contents/240122beda7f7982.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.369751",
    "title": "Comparison of Vector Databases",
    "summary": "The provided webpage is a **Comparison of Vector Databases**, detailing features, performance, and capabilities of various options like MyScale, Pinecone, Qdrant, Weaviate, Milvus, Chroma, Postgres (pgvector), and OpenSearch.\n\nIt covers aspects such as:\n*   **Purpose-built for vector search**\n*   **Managed cloud availability**\n*   **Free pods/tiers**\n*   **Self-hosting and Open-source status**\n*   **SQL support**\n*   **Supported index types** (HNSW, IVF, etc.)\n*   **Disk index support**\n*   **Metadata filtering** (including time-based and geospatial data support)\n*   **Aggregations**\n*   **Hybrid search** capabilities (combining vector and text search)\n*   **Cost benchmarks** for QPS and filtered vector search.\n\n**However, the page does not contain information about:**\n*   Vector embeddings (new efficient models)\n*   Rerankers\n*   RAG architectures or alternatives\n*   Chunking strategies\n*   Context window management\n\nTherefore, regarding the full scope of your query: **No answer found** for embeddings, rerankers, RAG architectures/alternatives, chunking strategies, or context window management. The page only addresses the **Vector Databases** component of your query, specifically mentioning **hybrid search** support within those databases.",
    "published_date": "2023-05-01T00:00:00.000Z",
    "score": null
  },
  "https://zilliz.com/comparison": {
    "content_path": "data/contents/75f5ec330f419dee.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.403452",
    "title": "Vector Database Comparison",
    "summary": "The webpage is a **Vector Database Comparison** tool that allows users to compare various vector databases (such as Milvus, Weaviate, Qdrant, Pinecone, Zilliz Cloud, etc.) based on architecture, scalability, performance, use cases, and costs. It also highlights Zilliz Cloud as a fully-managed vector database service and provides resources like a \"Definitive Guide to Choosing a Vector Database\" and a \"GenAI Resource Hub.\"\n\nWhile the page mentions **Retrieval Augmented Generation (RAG)** as a use case and features Zilliz Cloud (which is related to vector databases essential for RAG), it **does not** provide specific details or comparisons regarding:\n\n*   Embeddings (new efficient models)\n*   Rerankers\n*   RAG architectures or alternatives\n*   Hybrid search\n*   Chunking strategies\n*   Context window management\n\n**Summary:** The page focuses on comparing different vector databases. It mentions RAG as a use case but does not detail the other specific components of the retrieval and embeddings pipeline mentioned in your query.\n\n**No answer found** for the specific details on embeddings, rerankers, RAG architectures/alternatives, hybrid search, chunking strategies, or context window management.",
    "published_date": "2024-11-21T00:00:00.000Z",
    "score": null
  },
  "https://zackproser.com/blog/vector-databases-compared": {
    "content_path": "data/contents/df74cf127384f7e2.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.434234",
    "title": "Vector Database Comparison",
    "summary": "The webpage is a **Vector Database Comparison** tool that is currently under construction. It is designed to allow users to compare leading vector databases based on company metrics, features, performance, security, algorithms, and capabilities.\n\nWhile the page mentions the general topic of vector databases, it **does not provide specific details** on the other elements mentioned in your query, such as:\n\n*   Embeddings (new efficient models)\n*   Rerankers\n*   RAG architectures or alternatives\n*   Hybrid search\n*   Chunking strategies\n*   Context window management\n\n**No answer found** for the specific details requested beyond the general topic of vector databases.",
    "published_date": "2024-03-09T00:00:00.000Z",
    "score": null
  },
  "https://docs.aws.amazon.com/prescriptive-guidance/latest/choosing-an-aws-vector-database-for-rag-use-cases/vector-db-comparison.html": {
    "content_path": "data/contents/5534fd76901afb31.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.467007",
    "title": "Vector database comparison",
    "summary": "The webpage provides a comparison of various AWS vector database options for Retrieval-Augmented Generation (RAG) use cases, categorized into **Individual vector databases** (Amazon Kendra, OpenSearch Service, RDS for PostgreSQL with pgvector) and **Managed service – Amazon Bedrock Knowledge Bases** (which includes storage options like Aurora PostgreSQL, Neptune Analytics, OpenSearch Serverless, Pinecone, and Redis Enterprise Cloud).\n\nThe comparison details features such as architecture, scaling, data source integrations, performance, and security features for each option.\n\n**However, the page does not contain information about:**\n\n*   Vector embeddings (new efficient models)\n*   Rerankers\n*   RAG architectures (beyond mentioning RAG use cases)\n*   RAG alternatives\n*   Hybrid search\n*   Chunking strategies\n*   Context window management\n\nTherefore, regarding the specific components of your query: **No answer found** for embeddings, rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, and context window management. The page focuses solely on comparing vector database solutions for RAG.",
    "published_date": "2021-12-02T00:00:00.000Z",
    "score": null
  },
  "https://superlinked.com/vector-db-comparison": {
    "content_path": "data/contents/572b230349379039.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.500450",
    "title": "Vector DB Comparison",
    "summary": "The provided webpage text is extremely minimal, only containing a title and a header indicating a comparison of Vector Databases.\n\n**It does not contain any information about:**\n* Vector databases (beyond the title)\n* Embeddings (new efficient models)\n* Rerankers\n* RAG architectures or alternatives\n* Hybrid search\n* Chunking strategies\n* Context window management\n\n**Summary:**\nNo answer found. The webpage title suggests a comparison of Vector Databases, but the provided text does not contain any details regarding vector databases, embeddings, rerankers, RAG architectures, hybrid search, chunking strategies, or context window management.",
    "published_date": null,
    "score": null
  },
  "https://research.google/blog/user-llm-efficient-llm-contextualization-with-user-embeddings/": {
    "content_path": "../data/contents/1147f6c56e92c300.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.395614",
    "title": "USER-LLM: Efficient LLM contextualization with user embeddings",
    "summary": "The provided webpage describes **USER-LLM**, a framework designed for **Efficient LLM Contextualization with User Embeddings**.\n\nThe summary relevant to your query is:\n\nUSER-LLM addresses the challenges of using raw, complex, and lengthy user interaction data to personalize LLMs by distilling this data into **user embeddings**. These embeddings capture the essence of a user's behavioral patterns and preferences. The framework then integrates these embeddings with LLMs using **cross-attention** (or soft-prompting) to enhance personalization, improve pattern identification amidst noise, and mitigate the computational demands of processing long histories. The design involves a Transformer-based encoder to create the embeddings and integrates **Perceiver units** to optimize inference efficiency. Evaluations show USER-LLM outperforms text-prompt-based fine-tuning, especially for longer sequences, due to its ability to work with compact representations.\n\n**The webpage does not mention or discuss the following topics from your query:**\n\n*   Vector databases\n*   New efficient embedding models (it discusses creating user embeddings, but not the general state of efficient embedding models)\n*   Rerankers\n*   RAG architectures or RAG alternatives\n*   Hybrid search\n*   Chunking strategies\n*   Context window management (though it addresses the issue of long input sequences by using embeddings)\n\nTherefore, for most of your query, the answer is **No answer found**.",
    "published_date": "2024-05-28T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2406.04165v1": {
    "content_path": "../data/contents/09b6efeec48ff99e.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.422932",
    "title": "Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe",
    "summary": "The user query asks for a summary of the webpage content related to several topics in **retrieval and embeddings**, including: Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, and context window management.\n\nThe provided webpage text focuses on **\"Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe.\"**\n\nHere is a summary of the relevant points from the webpage:\n\n*   **Embeddings (New Efficient Models):** The paper studies how to contrastively train text embedding models from pre-trained decoder-only language models in a **compute-optimal fashion**. It develops a recipe (an algorithm) to determine the optimal configurations of model sizes, data quantities, and fine-tuning methods for different computational budget levels.\n*   **Fine-tuning Methods:** The study compares four fine-tuning methods: **full fine-tuning**, **block freezing**, **bias-only tuning**, and **Low-Rank Adaptation (LoRA)**.\n    *   **Key Finding:** Full fine-tuning produces optimal models at lower computational budgets, while **LoRA** becomes optimal at higher budgets (specifically, above $9.06 \\times 10^{16}$ FLOP).\n    *   Bias-only tuning resulted in the highest achievable loss compared to other methods.\n*   **Scaling Laws:** The authors establish scaling laws for contrastive fine-tuning to predict the optimal loss achievable with a fixed budget.\n\n**Topics Not Covered:**\n\nThe webpage **does not** discuss:\n*   Vector databases\n*   Rerankers\n*   RAG architectures or RAG alternatives\n*   Hybrid search\n*   Chunking strategies\n*   Context window management\n\n**Summary:**\n\nThe webpage details an empirical study on finding the **compute-optimal recipe** for fine-tuning decoder-only Language Models into high-quality **text embedding models** using contrastive learning. The research analyzes how model size, data quantity, and four different fine-tuning methods (full fine-tuning, block freezing, bias-only tuning, and LoRA) perform under various computational budgets. The main conclusion is that **full fine-tuning is optimal for lower budgets, while LoRA becomes the optimal choice for higher budgets.** The paper develops scaling laws to predict this optimal configuration.\n\n**No answer found** for the specific topics of vector\n\nThe user query asks for a summary related to: 'retrieval\\_and\\_embeddings: Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, context window management'.\n\nThe provided webpage text discusses **\"Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe.\"** It focuses on:\n*   **Finding the compute-optimal recipe** for fine-tuning language models into embedding models based on a given computational budget ($C$).\n*   Comparing different **fine-tuning methods** (Full fine-tuning, LoRA, block freezing, bias-only tuning) and deriving an algorithm to select the optimal method, model size ($N$), and data quantity ($D$).\n*   Analyzing **scaling laws** for embedding model performance based on parameters ($N$), training tokens ($D$), and the fraction of trainable parameters ($S$).\n*   Mentioning the use of **Pythia** models and evaluating performance on a subset of the **MTEB benchmark**, including tasks like **retrieval** (SciFact) and **semantic text similarity** (STS15).\n*   Discussing optimal hyperparameters like LoRA rank (optimum around 128) and the superiority of **mean pooling** over last pooling for embedding readout.\n\nWhile the page is highly relevant to **embeddings (new efficient models)** and touches upon **retrieval** tasks via the MTEB benchmark evaluation, it **does not explicitly discuss**:\n*   Vector databases\n*   Rerankers (beyond mentioning a re-ranking task in the evaluation subset)\n*   RAG architectures or alternatives\n*   Hybrid search\n*   Chunking strategies\n*   Context window management (though context length is mentioned as a fixed hyperparameter in one experiment setup, it is not a focus of the optimization or discussion).\n\nSince the query covers several specific topics (Vector databases, RAG, chunking, etc.) that are absent from the text, the summary must reflect that the page only addresses a subset of the query. However, based on the strict instruction to say \"No answer found\" if the page *does not answer the user's question*, and given the broad nature of the query versus the specific focus of the paper (compute-optimal fine-tuning for embeddings), a direct match for the entire query is not present.\n\n**Conclusion based",
    "published_date": "2024-05-18T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2410.02223": {
    "content_path": "../data/contents/baf05d66b4ee4999.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.458189",
    "title": "Computer Science > Computation and Language",
    "summary": "No answer found",
    "published_date": "2024-10-03T00:00:00.000Z",
    "score": null
  },
  "https://www.bentoml.com/blog/a-guide-to-open-source-embedding-models": {
    "content_path": "../data/contents/a42db4571a18a277.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.485515",
    "title": "A Guide to Open-Source Embedding Models",
    "summary": "The webpage provides a guide to several top **open-source embedding models** (like Qwen3-Embedding-0.6B, EmbeddingGemma-300M, Jina Embeddings v4, BGE-M3, all-mpnet-base-v2, gte-multilingual-base, and Nomic Embed Text V2), detailing their features, multilingual support, performance considerations, and deployment aspects.\n\nRegarding your query:\n\n*   **Vector databases:** Not explicitly discussed, though the context implies their use for storing and querying embeddings.\n*   **Embeddings (new efficient models):** The entire page focuses on reviewing various open-source embedding models, many of which are highlighted for their efficiency (e.g., EmbeddingGemma-300M, gte-multilingual-base).\n*   **Rerankers:** Mentioned in relation to the Qwen3 series, where the embedding model can be integrated with its reranker counterpart for full-stack semantic search systems.\n*   **RAG architectures:** Mentioned as a common use case for embedding models (\"A typical use case is RAG, where retrieved data helps improve real-time content generation\").\n*   **RAG alternatives:** No answer found.\n*   **Hybrid search:** Mentioned in relation to BGE-M3, which supports dense, multi-vector, and **sparse retrieval** (a component of hybrid search).\n*   **Chunking strategies:** No answer found.\n*   **Context window management:** Mentioned in terms of input length limitations for specific models (e.g., all-mpnet-base-v2 truncates at 384 word pieces, BGE-M3 handles up to 8192 tokens).\n\nIn summary, the page covers **embeddings (new efficient models)**, **rerankers**, **RAG architectures** (as a use case), and aspects related to **hybrid search** and **context window management** through input length constraints. It does not detail vector databases, RAG alternatives, or chunking strategies.",
    "published_date": "2025-10-10T00:00:00.000Z",
    "score": null
  },
  "https://skywork.ai/blog/deep-research-mcp-agent-an-overview/": {
    "content_path": "data/contents/ce110104c5fe023c.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.637027",
    "title": "Deep Research MCP Agent: An Overview",
    "summary": "The provided web page discusses **Deep Research MCP Agents**, which combine strategic reasoning with the **Model Context Protocol (MCP)** for complex problem-solving.\n\nHere is a summary of the concepts mentioned in your query that are present in the text:\n\n*   **agent\\_infrastructure: MCP servers, tool use, deep research, agent memory, agent frameworks, agent orchestration, multi-agent systems** (referred to as multi-agent structure, Deep Orchestrator, and the overall architecture).\n*   **deep research** (The core focus of the agent).\n*   **agent memory, agentic memory** (Discussed in the context of external memory systems/knowledge banks to combat context window limits).\n*   **context compression** (Mentioned as part of \"context engineering\").\n*   **LangChain** (Mentioned as one of the advanced agents employing a multi-agent structure).\n*   **function calling, structured outputs** (Implied through the discussion of MCP standardizing tool connection and the goal of producing structured, high-quality reports).\n*   **agent evaluations** (Mentioned in the context of internal evaluations at Anthropic showing multi-agent systems outperforming single agents).\n*   **prompt engineering, context engineering** (Context engineering is explicitly mentioned as a method for managing cost).\n*   **long context** (Addressed indirectly through the need for external memory systems to manage the finite context window).\n\nThe following terms from your query were **not** explicitly mentioned or detailed in the text:\n\n*   LlamaIndex\n*   OpenAI Agents SDK\n*   Anthropic Agents SDK (Though Anthropic is mentioned regarding MCP and multi-agent systems)\n*   Google SDK\n*   LLM reasoning trace evaluation\n*   supervised fine-tuning\n*   reinforcement learning\n\n**Summary:** The page describes Deep Research MCP Agents as autonomous systems that use the Model Context Protocol (MCP) for universal tool connectivity. They employ multi-agent architectures for scalability and use external memory banks combined with context engineering to manage the high costs associated with deep research. The architecture favors simpler, adaptive loops over rigid plans.",
    "published_date": "2025-09-15T00:00:00.000Z",
    "score": null
  },
  "https://blog.langchain.com/open-deep-research/": {
    "content_path": "data/contents/22ca432f20344f6e.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.669857",
    "title": "Open Deep Research",
    "summary": "The provided web page describes **Open Deep Research**, an open-source, configurable agent designed for conducting comprehensive research and producing detailed reports.\n\nKey aspects of the system relevant to your query include:\n\n*   **Agent Framework:** The open deep researcher is built on **LangGraph**.\n*   **Tool Use & Research Strategy:** It uses a multi-step agent process (**Scope, Research, Write**) where a **Research Supervisor** agent delegates tasks to **Research Sub-Agents**. These sub-agents conduct research using configured **search tools** and/or **MCP tools**.\n*   **Context Engineering & Long Context Mitigation:** The system heavily relies on **context engineering** to manage token usage, which is a major concern in deep research. This involves translating verbose chat history into a focused **research brief** and having sub-agents prune raw research findings before returning them to the supervisor. This helps mitigate issues like context bloat and context window limits.\n*   **Multi-Agent Use:** Multi-agent architecture is used specifically for the **Research** phase to allow for parallelization across independent sub-topics, which helps isolate context and avoid context clash failures common in single-agent approaches handling multiple topics. Writing the final report is done in one-shot by a single LLM after research is complete to ensure coordination.\n*   **Flexibility:** The supervisor agent allows for flexibility in research depth by deciding whether to parallelize research or use a single thread, depending on the complexity of the request.\n\nThe page mentions **LangChain** and the use of agents, tool calling, and context management, but it **does not explicitly mention** many of the other specific terms you listed, such as: *agent memory, agentic memory, context compression (beyond the brief generation), LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering (beyond the specific methods described), supervised fine-tuning, or reinforcement learning.*",
    "published_date": "2025-07-16T15:43:23.000Z",
    "score": null
  },
  "https://arxiv.org/pdf/2508.20453": {
    "content_path": "data/contents/14c1ec47a83b4c96.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.701758",
    "title": "",
    "summary": "The user query is a list of terms related to agent infrastructure, memory, frameworks, SDKs, function calling, orchestration, evaluation, and training methods for LLM agents.\n\nThe webpage introduces **MCP-Bench**, a benchmark for evaluating **tool-using LLM agents** on complex, multi-step tasks that require **tool use**, **cross-tool coordination**, and **planning/reasoning**.\n\nHere is how the webpage text relates to the user's query terms:\n\n*   **agent\\_infrastructure: MCP servers, tool use:** The benchmark is built on the **Model Context Protocol (MCP)**, connecting LLMs to **28 representative live MCP servers** spanning **250 tools**. It specifically evaluates **tool-using LLM agents**.\n*   **deep research:** The tools span domains including **scientific computing** and **academic search**, supporting complex research tasks (as shown in the example task in Table 2).\n*   **agent memory, agentic memory, context compression:** The execution process (Algorithm 1) includes a crucial step: `𝑜𝑡 ← 𝜋compress (𝑜𝑡) ⊲ Generate a compressed summary of the observation`. This is explicitly stated as being necessary because \"some tools return very long outputs, and summarizing them prevents excessive **context windows**.\" This directly addresses **context compression** and relates to managing the agent's working memory/context.\n*   **agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** The paper discusses the limitations of existing benchmarks and introduces MCP-Bench as a standardized platform, but it **does not mention or compare** specific agent frameworks like LangChain, LlamaIndex, or the specific SDKs from OpenAI, Anthropic, or Google.\n*   **function calling, structured outputs:** The benchmark tests agents' ability to interact with tools that have structured invocation schemas (JSON structures, enumerated types, etc.) and reason over **structured outputs** from tools. The rule-based evaluation checks for **schema adherence**.\n*   **agent orchestration, agent evaluations:** The benchmark is designed to evaluate **agent orchestration** (especially **cross-server orchestration** and **multi-hop workflows**). The entire paper details the **agent evaluations** framework, including rule-based metrics and **LLM-as-a-Judge** scoring across axes like planning effectiveness and task completion.\n*   **LL\n\nThe webpage describes **MCP-Bench**, a large-scale benchmark for evaluating **LLM agents** in realistic, **tool-use** scenarios.\n\nKey aspects mentioned that relate to your query include:\n\n*   **MCP Servers and Tool Use:** MCP-Bench connects agents to **28 production servers** with **250 tools** (detailed in Table 8, covering areas like Math, BioMedical research, Google Maps, NASA Data, etc.) to enable complex workflows and **agent orchestration**.\n*   **Agent Capabilities Evaluation:** The benchmark reveals that state-of-the-art models struggle with capabilities like dependency chain compliance, tool selection, and long-horizon planning.\n*   **Evaluation Framework:** The evaluation combines rule-based checks and **LLM Judge scoring**. The text also mentions strategies like prompt perturbation improving human-perceived evaluation quality.\n*   **Prompts:** Detailed prompts are provided for **Strategic Planning** (tool planning), **Final Solution Generation** (synthesis), and **Content Summarization** (context compression).\n*   **Task Synthesis:** A prompt is detailed for generating complex tasks with deep **tool dependencies** and cross-server requirements.\n\nWhile the page heavily focuses on **tool use**, **agent orchestration**, and **evaluation** within the MCP framework, it **does not explicitly mention or detail** the following terms from your query: *agent memory, agentic memory, context compression (beyond a summarization prompt), agent frameworks (LangChain, LlamaIndex, SDKs), structured outputs, LLM reasoning trace evaluation, prompt engineering (beyond specific execution prompts), context engineering, long context, supervised fine-tuning, or reinforcement learning.*",
    "published_date": null,
    "score": null
  },
  "https://cookbook.openai.com/examples/deep_research_api/introduction_to_deep_research_api_agents": {
    "content_path": "data/contents/68fd3df4514a38f8.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.859709",
    "title": "Deep Research API with the Agents SDK",
    "summary": "The webpage describes how to build **Agentic research workflows** using the **OpenAI Deep Research API** and the **Agents SDK**.\n\nKey aspects covered include:\n*   **Agent Orchestration:** Building single and multi-agent pipelines, specifically a four-agent pipeline involving a Triage Agent, Clarifier Agent, Instruction Builder Agent, and the **Research Agent**.\n*   **Tool Use:** Integrating **Web Search Tool** for public internet search and **MCP (HostedMCPTool)** for internal file search.\n*   **Research Focus:** The Deep Research Agents are recommended for tasks requiring planning, synthesis, **tool use**, or multi-step reasoning, rather than trivial lookups.\n*   **Prompt Engineering/Context:** Demonstrating how a sub-agent (Clarifying Agent and Instruction Agent) enriches the user query to maximize output quality before it reaches the main research agent.\n*   **Structured Outputs:** Using Pydantic models for structured outputs (e.g., for the Clarifying Agent).\n*   **Transparency:** Streaming research progress and providing methods to parse the **LLM reasoning trace evaluation** (Agent Interaction Flow) and extract citations.\n\nThe document focuses on the **Agents SDK**, **Deep Research**, **tool use** (including **MCP**), **agent orchestration**, and **prompt engineering** within this specific research context. It does not explicitly detail concepts like agent memory, context compression, LangChain, LlamaIndex, Anthropic Agents SDK, Google SDK, function calling, structured outputs (beyond Pydantic for clarification), agent evaluations, long context, supervised fine-tuning, or reinforcement learning.",
    "published_date": "2025-06-25T00:00:00.000Z",
    "score": null
  },
  "https://google.github.io/adk-docs/context/compaction/": {
    "content_path": "data/contents/456c48bd1634839f.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.830046",
    "title": "Compress agent context for performance ¶",
    "summary": "The provided web page explains the **Context Compaction** feature in the **Agent Development Kit (ADK)**, which is designed to **compress agent context for performance**.\n\nAs an agent runs, it collects context (instructions, data, tool responses, etc.). To prevent increasing processing times as this context grows, Context Compaction summarizes older parts of the agent workflow event history using a **sliding window** approach.\n\nKey configuration settings include:\n*   **`compaction_interval`**: The number of completed events that triggers compaction.\n*   **`overlap_size`**: How many previously compacted events are included in the new summary.\n*   **`summarizer`**: An optional object, such as `LlmEventSummarizer`, allowing you to specify a particular AI model (e.g., Gemini) for the summarization task.\n\nThis feature directly addresses the concept of **context compression** mentioned in your query.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://openreview.net/pdf/c1aa382adbf5c01f1598c05bd6d67d170ef1c3cd.pdf": {
    "content_path": "data/contents/901dff958a32aab1.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.925891",
    "title": "",
    "summary": "The webpage introduces **MCP-BENCH**, a new benchmark designed to evaluate Large Language Model (LLM) agents on complex, multi-step tasks that require **tool use**, cross-tool coordination, and planning.\n\nKey aspects of MCP-BENCH relevant to your query include:\n\n*   **Agent Infrastructure:** It connects LLM agents to **28 representative live MCP servers** (Model Context Protocol servers) which expose a total of **250 tools** across domains like finance, traveling, scientific computing, and academic search.\n*   **Tool Use & Function Calling:** The benchmark tests the agent's ability to use these tools, including handling **fuzzy instructions** (without explicit tool names or steps) and orchestrating **cross-domain workflows**.\n*   **Agentic Capabilities:** Evaluation focuses on multi-hop planning, grounding responses in intermediate tool outputs, and overall strategic quality. The evaluation framework includes rule-based checks and **LLM-as-a-Judge** scoring across axes like **Task Completion**, **Tool Usage Quality**, and **Planning Effectiveness** (including dependency awareness and parallelism).\n*   **Context Handling:** The process includes a **compression policy ($\\pi_{compress}$)** to generate concise summaries of tool outputs, which is crucial for managing long outputs and preventing context window overflow.\n\nThe document does **not** explicitly discuss: **agent memory, agentic memory, context compression** (beyond mentioning the compression step in the algorithm), **agent frameworks** (like LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK), **structured outputs** (though tool schemas imply structure), **agent orchestration** (though cross-server workflows imply this), **agent evaluations** (it *is* an evaluation framework, but doesn't detail general evaluation methodologies beyond its own), **LLM reasoning trace evaluation**, **prompt engineering**, **context engineering**, **long context** (beyond the need for compression), **supervised fine-tuning**, or **reinforcement learning**.\n\n**Summary based on query terms found:**\n\nMCP-BENCH is a benchmark built on **MCP servers** that tests **tool use** by connecting agents to 250 tools across 28 servers. It evaluates planning and execution, incorporating a step for **context compression** of tool outputs. The evaluation framework assesses strategic quality, including aspects related to **agent orchestration** (cross-server workflows) and execution fidelity.\n\nThe user query lists many concepts related to **AI Agents, Tool Use, and Evaluation**.\n\nThe provided webpage text is an excerpt from a research paper, likely about an agent evaluation benchmark called **MCP-Bench**.\n\nHere is a summary of how the webpage text relates to the user's query:\n\n*   **Agent Infrastructure/Tool Use:** The text extensively details the **MCP servers** and the **tools** they provide (e.g., Math MCP, BioMCP, Google Maps, Hugging Face, Wikipedia). This directly relates to the user's interest in **tool use** and **function calling**.\n*   **Agent Evaluation:** The core of the text (Section A.1) discusses the **key capabilities for tool-using LLM agents** that **MCP-Bench** reflects, such as:\n    *   Tool Schema Understanding and Compliance (related to **structured outputs**).\n    *   Tool Retrieval and Selection.\n    *   Long-Horizon Planning and **Agent Orchestration**.\n    *   Information Grounding and Evidence-Based Reasoning.\n    *   The text also mentions other evaluation benchmarks like **AgentBench** and **BFCL** (Berkeley Function Calling Leaderboard).\n*   **LLM Reasoning Trace Evaluation:** Section A.1 mentions that the evaluation framework measures structural coherence and dependency awareness, which relates to analyzing agent reasoning.\n*   **Context/Long Context:** Section A.1 mentions that MCP-Bench tasks include complex compositions requiring concurrent interactions, which touches upon the need for handling context.\n*   **Specific Frameworks/SDKs (LangChain, LlamaIndex, etc.):** The text does **not** explicitly mention or detail the use of specific agent frameworks like LangChain, LlamaIndex, or the OpenAI/Anthropic/Google Agent SDKs.\n*   **Memory, Context Compression, Prompt Engineering, SFT/RL:** The text does **not** discuss **agent memory**, **agentic memory**, **context compression**, **prompt engineering**, **context engineering**, **supervised fine-tuning (SFT)**, or **reinforcement learning (RL)**.\n\n**Summary tailored to the query:**\n\nThe webpage describes **MCP-Bench**, an evaluation framework for **tool-using LLM agents**. It details the **agent infrastructure** by listing numerous **MCP servers** and their associated **tools** (covering domains like scientific computing, finance, medicine, and general",
    "published_date": "2025-11-24T00:00:00.000Z",
    "score": null
  },
  "http://platform.openai.com/docs/mcp": {
    "content_path": "data/contents/ae309d59e5310786.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.735433",
    "title": "Building MCP servers for ChatGPT and API integrations",
    "summary": "The webpage describes how to build a **Model Context Protocol (MCP) server** to extend AI models like ChatGPT with external tools and knowledge, specifically focusing on connecting to a private data source (a vector store) for use in chat and **deep research**.\n\nThe user query lists many concepts related to building and optimizing AI agents, including:\n\n*   **Agent Infrastructure/Frameworks:** `agent_infrastructure`, `agent frameworks`, `LangChain`, `LlamaIndex`, `OpenAI Agents SDK`, `Anthropic Agents SDK`, `Google SDK`, `agent orchestration`.\n*   **Agent Capabilities/Features:** `tool use`, `function calling`, `structured outputs`, `agent memory`, `agentic memory`, `context compression`, `long context`.\n*   **Research/Data Handling:** `deep research`, `context engineering`.\n*   **Optimization/Training:** `supervised fine-tuning`, `reinforcement learning`, `context compression`.\n*   **Evaluation:** `agent evaluations`, `LLM reasoning trace evaluation`.\n\n**Summary of what the page covers related to the query:**\n\n*   **MCP Servers:** The core topic is building an MCP server, which acts as a mechanism for tool use and connecting models to external data.\n*   **Deep Research:** The guide explicitly details setting up an MCP server to work with ChatGPT's \"deep research\" feature.\n*   **Tool Use/Function Calling:** The server must implement the `search` and `fetch` tools, which is a form of function calling/tool use.\n*   **Agent Frameworks/SDKs:** While the page focuses on the MCP protocol itself (using Python and `FastMCP`), it discusses connecting to ChatGPT, which is part of the broader agent ecosystem.\n*   **Structured Outputs:** The required tool definitions for `search` and `fetch` involve returning data in specific JSON structures, touching upon structured outputs.\n\n**Summary of what the page *does not* cover related to the query:**\n\nThe page does not discuss the following specific terms from the query: `agent_infrastructure` (in a general sense beyond MCP), `LangChain`, `LlamaIndex`, `OpenAI Agents SDK`, `Anthropic Agents SDK`, `Google SDK`, `agent memory`, `agentic memory`, `context compression`, `agent orchestration`, `agent evaluations`, `LLM reasoning trace evaluation`, `prompt engineering`, `supervised fine-tuning`,",
    "published_date": null,
    "score": null
  },
  "https://github.com/mcp-use/mcp-use": {
    "content_path": "../data/contents/a79661a3e9ab285d.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:31.920243",
    "title": "Search code, repositories, users, issues, pull requests...",
    "summary": "The webpage describes **mcp-use**, a framework for building with **Model Context Protocol (MCP) servers, clients, and AI agents** in both Python and TypeScript.\n\nKey features and components mentioned that relate to your query include:\n\n*   **🤖 MCP Agents:** AI agents that can use tools and reason across steps.\n*   **🔌 MCP Clients:** Connect any LLM to any MCP server.\n*   **🛠️ MCP Servers:** Tools to build your own MCP servers.\n*   **🔍 MCP Inspector:** A web-based debugger for MCP servers.\n*   **AI SDK Integration:** Mentioned under Example Use Cases.\n*   **Observability:** Built-in **Langfuse integration** (which relates to tracing/evaluation).\n\nWhile the page heavily features **agent infrastructure** (MCP Agents, Servers, Clients) and **tool use** (agents use tools), it **does not explicitly mention or detail** many of the specific advanced concepts you listed, such as: *deep research, agent memory, agentic memory, context compression, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context, supervised fine-tuning, or reinforcement learning*.\n\nThe page focuses on the **MCP protocol** as the core infrastructure for agent interaction and tool use.\n\n**Summary relevant to query:** The page describes **mcp-use**, a framework providing **MCP Agents** capable of **tool use** and multi-step reasoning, built on the **Model Context Protocol (MCP)**. It offers observability via **Langfuse integration**.\n\n**Specific terms from the query that are NOT explicitly detailed:** tool use (implied by agents), deep research, agent memory, agentic memory, context compression, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context, supervised fine-tuning, reinforcement learning.",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/html/2510.00615v2": {
    "content_path": "data/contents/e879c0fde255e425.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.060249",
    "title": "Acon : Optimizing Context Compression for\n Long-horizon LLM Agents",
    "summary": "The user query is a list of keywords related to LLM agents, infrastructure, memory, context management, frameworks, and training techniques.\n\nThe provided web page, titled \"Acon : Optimizing Context Compression for Long-horizon LLM Agents,\" introduces **Acon**, a unified framework designed to optimally compress environment observations and interaction histories for LLM agents operating in long-horizon tasks.\n\nHere is a summary of how the page relates to the user's query terms:\n\n*   **Agent Infrastructure/Frameworks:** The paper focuses on **LLM agents** operating in dynamic environments, which aligns with the general theme. It mentions the challenges faced by these agents, such as accumulating long histories.\n*   **Agent Memory/Context Compression/Long Context:** This is the core topic. Acon directly addresses the challenge of **growing context length** by compressing interaction histories and observations to reduce memory usage and inference cost, which is crucial for **long-horizon** tasks.\n*   **Tool Use:** The context mentions that success in agentic tasks requires reasoning and **effective tool use**.\n*   **Context Engineering/Prompt Engineering:** Acon uses a novel **compression guideline optimization** pipeline, which refines compression prompts (guidelines) using natural language feedback derived from contrasting successful and failed trajectories. This is a form of advanced context/prompt engineering tailored for compression.\n*   **Agent Orchestration/Evaluations:** The framework is validated on multi-step agent benchmarks like **AppWorld, OfficeBench, and Multi-objective QA**, demonstrating performance improvements and memory reduction (26-54%).\n*   **Supervised Fine-tuning/Reinforcement Learning:** The paper notes that traditional RL is difficult for optimizing compression due to sparse rewards and high rollout costs. Instead, Acon uses a gradient-free, LLM-based optimization of the compression *guidelines* (prompts), followed by **distillation** of the optimized compressor into smaller models to reduce overhead.\n\n**Keywords not explicitly detailed or central to the paper:**\n\n*   **MCP servers, deep research:** Not mentioned.\n*   **LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK:** These specific agent frameworks/SDKs are not mentioned, although the work is applicable to agents built using them.\n*   **Function calling, structured outputs:** Not the focus, though agents use tools/APIs.\n*   **",
    "published_date": null,
    "score": null
  },
  "https://huggingface.co/papers/2510.00615": {
    "content_path": "data/contents/27efe481381f00da.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.095840",
    "title": "Paper page - ACON: Optimizing Context Compression for Long-horizon LLM Agents",
    "summary": "The paper \"ACON: Optimizing Context Compression for Long-horizon LLM Agents\" introduces **Agent Context Optimization (ACON)**, a unified framework designed to **compress context** (environment observations and interaction histories) for **long-horizon LLM agents**.\n\nKey aspects of ACON relevant to your query include:\n\n*   **Context Compression:** ACON optimally compresses context to address the challenge of growing context length in long-horizon tasks, which typically reduces efficiency and increases costs.\n*   **Optimization Method:** It leverages **compression guideline optimization** in natural language space. Capable LLMs analyze failure cases where compressed context fails (but full context succeeds) to update the compression guideline.\n*   **Efficiency:** The framework proposes **distilling the optimized LLM compressor into smaller models** to reduce the overhead of the additional compression module.\n*   **Performance:** Experiments show ACON reduces peak memory usage by **26-54%** while preserving task performance, and distilled compressors maintain over **95% accuracy**. Distillation also enhanced smaller LMs as long-horizon agents by up to **46%**.\n\nWhile the paper focuses heavily on **context compression** and **agent efficiency** for long-horizon tasks, it implicitly relates to several concepts in your query:\n\n*   It addresses challenges faced by **LLM Agents** that require **tool use** and **reasoning** over long histories.\n*   It directly relates to **context compression** and **long context** management.\n\nThe paper does **not** explicitly discuss: MCP servers, agent memory/agentic memory (beyond context history compression), agent frameworks (LangChain, LlamaIndex, SDKs), function calling, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering (beyond the compression guideline optimization), supervised fine-tuning, or reinforcement learning.",
    "published_date": "2025-10-02T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/ai-artistry/context-compression-for-ai-agents-why-structured-memory-beats-aggressive-truncation-0b03596caa5b": {
    "content_path": "../data/contents/8f248b6e99d2ce22.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.004718",
    "title": "Context Compression for AI Agents: Why Structured Memory Beats Aggressive Truncation",
    "summary": "The webpage discusses **Context Compression for AI Agents**, focusing on why **structured memory** is superior to **aggressive truncation** when managing the vast amounts of conversation history generated during long, complex tasks. The core problem addressed is how AI agents lose track of their goals or previous steps when they generate millions of tokens, and the solution lies in distilling this history into concise, actionable summaries so the agent can continue working without re-reading everything.\n\nThe user query lists many specific technologies and concepts related to AI agents, such as: *agent infrastructure, MCP servers, tool use, deep research, agent memory, agentic memory, context compression, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context, supervised fine-tuning, reinforcement learning*.\n\n**Summary relevant to the query:**\n\nThe article directly addresses **context compression** and **agent memory/agentic memory** by advocating for **structured memory** over aggressive truncation to maintain agent functionality during long tasks. It touches upon the need for agents to retain context effectively.\n\n**Concepts from the query that are NOT explicitly discussed in the provided text:**\n\nMCP servers, tool use, deep research, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context, supervised fine-tuning, and reinforcement learning.",
    "published_date": null,
    "score": null
  },
  "https://sparkco.ai/blog/mastering-context-compression-agents-a-deep-dive": {
    "content_path": "data/contents/a64ef4584c1fb3cf.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.135297",
    "title": "Mastering Context Compression Agents: A Deep Dive",
    "summary": "The webpage focuses on **Mastering Context Compression Agents**, primarily through the framework called **Agent Context Optimization (Acon)**.\n\nHere is a summary of the page content relevant to your query:\n\n*   **Agent Infrastructure & Frameworks:** The page discusses context compression agents and mentions integration with frameworks like **LangChain**, **AutoGen**, and **CrewAI**. It also references **LangGraph** for multi-turn conversation handling.\n*   **Context Compression & Memory:** The core topic is context compression, which is essential for managing **long context** and extensive interaction histories efficiently, addressing token constraints. Acon uses **task-specific guideline optimization** and **gradient-free optimization** for adaptive context management.\n*   **Tool Use & Structured Outputs:** The text mentions utilizing **tool calling patterns** and schemas to enhance agent capabilities and orchestration.\n*   **Agent Orchestration:** **Agent orchestration patterns** are discussed, particularly in handling multi-turn conversations.\n*   **MCP Protocol:** The **MCP protocol** is mentioned as being implemented to ensure seamless communication between components.\n*   **Vector Databases:** Integration with vector databases like **Pinecone**, **Weaviate**, and **Chroma** is highlighted for efficient storage and retrieval of compressed context.\n\n**Missing Information:** The page does not explicitly mention or detail the following terms from your query: *deep research*, *agent memory* (though memory management is covered via LangChain's `ConversationBufferMemory`), *agentic memory*, *context engineering*, *supervised fine-tuning*, *reinforcement learning*, *LLM reasoning trace evaluation*, *prompt engineering* (though Acon avoids handcrafted prompts), *OpenAI Agents SDK*, *Anthropic Agents SDK*, or *Google SDK*. It also does not discuss **OpenAI Agents SDK**, **Anthropic Agents SDK**, or **Google SDK**.",
    "published_date": "2023-01-01T05:03:07.000Z",
    "score": null
  },
  "https://openreview.net/pdf/faafa144f22b40a3c5a78ac0401b933c0dbe8f78.pdf": {
    "content_path": "data/contents/e4df3f198e96538f.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.184215",
    "title": "",
    "summary": "The webpage describes **ACON (Agent Context Optimization)**, a unified framework designed to optimally compress environment observations and interaction histories for **long-horizon LLM agents**.\n\nKey aspects of ACON relevant to your query include:\n\n*   **Context Compression:** ACON addresses the challenge of unbounded context growth in agents, which increases cost and dilutes relevant information. It reduces memory usage by **26-54% (peak tokens)** while largely preserving task performance.\n*   **Guideline Optimization:** Instead of fine-tuning LLM parameters, ACON optimizes **compression guidelines (natural language prompts)** using a failure analysis pipeline. This process is gradient-free and can be used with closed-source models.\n*   **Distillation:** Optimized compressors (large LLMs) can be **distilled into smaller models** to reduce overhead, preserving over 95% of the teacher's accuracy.\n*   **Agent Performance:** Optimized contexts allow smaller LLMs to function more effectively as agents, leading to performance improvements of **20-46%** on benchmarks like AppWorld, OfficeBench, and Multi-objective QA.\n\nWhile the paper discusses the general challenges and solutions for LLM agents (reasoning, tool use, context management), it does not explicitly detail or focus on:\n\n*   **agent\\_infrastructure: MCP servers**\n*   **agent memory** (it focuses on *compressing* memory/history, not the underlying memory architecture itself)\n*   **agentic memory** (similar to above)\n*   **agent frameworks** (LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK)\n*   **function calling** or **structured outputs** (though context compression impacts what is passed to these functions)\n*   **agent orchestration**\n*   **agent evaluations** (it uses benchmarks, but doesn't detail general evaluation frameworks)\n*   **LLM reasoning trace evaluation** (it uses failure analysis to update prompts, which is related but distinct)\n*   **prompt engineering** (it uses prompt optimization for compression guidelines, but not general prompt engineering)\n*   **context engineering** (it focuses on compression, a specific form of context management)\n*   **long context** (it focuses on *managing* long context via compression)\n*   **supervised fine-tuning** or **reinforcement learning** (it explicitly avoids fine\n\nThe user query lists numerous concepts related to **agent infrastructure and development**, such as:\n\n*   **Infrastructure/Components:** MCP servers, agent memory, agentic memory, context compression.\n*   **Tool Use & Research:** Tool use, deep research.\n*   **Agent Frameworks/SDKs:** LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK.\n*   **Capabilities/Techniques:** Function calling, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, long context.\n*   **Training/Learning:** Supervised fine-tuning, reinforcement learning.\n\nThe provided webpage text consists of examples from a research paper (likely about agent systems, given the context of the query and the content of the examples). These examples demonstrate:\n\n1.  **Example E.1 (Spotify Task):** An agent using APIs to find the least recommended artist based on Spotify recommendations.\n2.  **Example E.2 (File Deletion Task - Failure):** An agent attempting and failing to delete files due to authorization issues, illustrating API interaction complexity.\n3.  **Example E.3 (File Deletion Task - Success with Compression):** A successful execution of the file deletion task, highlighting the use of an access token for API calls.\n4.  **Example E.4 (History Compression):** Demonstrations of how agent history is compressed (Prompting baseline vs. ACON utility/compression steps) across different tasks (Splitwise, 8-objective QA, OfficeBench).\n\n**Summary relative to the query:**\n\nThe webpage text provides concrete examples of an **agent framework** in action, demonstrating **tool use** (interacting with Spotify, file system, and supervisor apps), **API interaction** (including authentication/access tokens, which relates to infrastructure), and the impact of **history compression** (a form of **context engineering/compression**).\n\nHowever, the text **does not explicitly discuss or define** most of the specific terms listed in the query, such as: MCP servers, deep research, agent memory, agentic memory, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, supervised fine-tuning, or reinforcement learning.\n\nSince the page only shows *examples* of an agent",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/html/2511.18423v1": {
    "content_path": "data/contents/3dcd3cb9dfdc67c6.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.224477",
    "title": "General Agentic Memory Via Deep Research",
    "summary": "The user query lists many concepts related to AI agents, infrastructure, and memory systems. The provided web page describes a novel memory framework called **General Agentic Memory (GAM)**, which operates on the principle of \"just-in-time (JIT) compilation\" to address the information loss inherent in traditional static (Ahead-of-Time) memory systems.\n\nHere is a summary of how the GAM framework relates to the concepts in the user query:\n\n**Agent Memory and Context Management:**\n*   **General Agentic Memory (GAM):** The core topic, proposed as a JIT compilation approach for memory.\n*   **Agent Memory/Agentic Memory:** GAM is a system designed to provide agents with essential contextual information.\n*   **Context Compression/Long Context:** GAM addresses the challenge of managing complex and rapidly expanding contexts by focusing on creating optimized contexts at runtime rather than relying solely on pre-compressed, static memory.\n*   **Deep Research:** The Researcher module in GAM performs \"deep research\" by iteratively planning, searching, and reflecting to integrate useful information from the complete history for an online request.\n\n**Agent Infrastructure and Components:**\n*   **Agent Frameworks:** GAM is presented as a novel memory framework for general AI agents.\n*   **Tool Use/Function Calling:** The Researcher module uses a search toolkit ($\\mathcal{T}$) which includes an embedding model (for vector search), a BM25 retriever, and an ID-based retriever to execute search actions.\n*   **Agent Orchestration:** GAM uses a dual-agent framework: the **Memorizer** (offline stage) and the **Researcher** (online stage), which work together to serve the client agent.\n\n**Optimization and Learning:**\n*   **Reinforcement Learning (RL):** GAM facilitates end-to-end performance optimization through reinforcement learning, where the Memorizer and Researcher policies are learned via policy gradients based on the reward function ($\\Gamma$) derived from the client's answer quality.\n*   **Supervised Fine-tuning (SFT):** While the paper focuses on RL optimization, the Memorizer and Researcher are LLM-based agents, implying they are built upon pre-trained models (like GPT-4o-mini or Qwen2.5-14B) which are typically fine-tuned.\n\n**Concepts Not Explicitly Covered or Mentioned:**\nThe paper does not explicitly detail the following terms from the",
    "published_date": "2025-09-16T00:00:00.000Z",
    "score": null
  },
  "https://python.langchain.com/docs/how_to/function_calling": {
    "content_path": "data/contents/713ea7c3f69bcf93.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.474602",
    "title": "LangChain overview",
    "summary": "LangChain is an open-source framework for building agents and LLM-powered applications. It provides a pre-built agent architecture and integrations for various models (OpenAI, Anthropic, Google, etc.) and tools.\n\nThe page mentions several concepts related to your query:\n\n*   **Agent Frameworks:** LangChain itself is an agent framework. It also mentions **LangGraph** as a low-level agent orchestration framework.\n*   **Tool Use/Function Calling:** LangChain agents can incorporate **Tools** (as shown in the example with `get_weather`).\n*   **Structured Outputs:** The core components list includes **Structured output**.\n*   **Agent Orchestration:** LangGraph is highlighted for agent orchestration.\n*   **Context Engineering:** This is listed under Advanced Usage.\n*   **Long Context:** This is listed under Advanced Usage.\n*   **Model Context Protocol (MCP):** This is mentioned in the context of connecting documentation to other tools.\n*   **Agent Memory:** The core components list includes **Short-term memory**, and advanced usage lists **Long-term memory**.\n\n**Concepts not explicitly detailed or mentioned:** agent infrastructure (MCP servers), deep research, agentic memory, context compression, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, agent evaluations, LLM reasoning trace evaluation, prompt engineering, supervised fine-tuning, reinforcement learning.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://mcpservers.org/servers/modelcontextprotocol/everything": {
    "content_path": "data/contents/dbdf8deb24b8224b.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.803096",
    "title": "Everything MCP Server",
    "summary": "The provided web page describes the \"Everything MCP Server,\" which is a test server designed to exercise all features of the MCP (Model Context Protocol).\n\nThe user query lists many concepts related to **agent infrastructure, frameworks, memory, context handling, and training methods** (e.g., `agent_infrastructure: MCP servers`, `LangChain`, `LlamaIndex`, `agent memory`, `context compression`, `supervised fine-tuning`, `function calling`, etc.).\n\nThe web page details the **components of the Everything MCP Server**, specifically listing the **Tools** it implements (like `echo`, `add`, `longRunningOperation`, `sampleLLM`, `structuredContent`, etc.), its **Resources**, **Prompts**, and **Roots** protocol capabilities. It also provides instructions on how to use and install this specific MCP server.\n\n**Summary relative to the query:**\n\nThe page confirms the existence of an **MCP server** and details the **tools** it exposes, which is a component of agent infrastructure. However, it **does not mention or discuss** the majority of the specific agent concepts listed in the query, such as:\n\n*   Agent frameworks (`LangChain`, `LlamaIndex`, `OpenAI Agents SDK`, etc.)\n*   Agent memory or agentic memory\n*   Context compression\n*   Agent orchestration or agent evaluations\n*   LLM reasoning trace evaluation\n*   Prompt engineering or context engineering (beyond specific prompt examples)\n*   Training methods (`supervised fine-tuning`, `reinforcement learning`)\n\nTherefore, the page does not answer the user's broad query about agent infrastructure concepts.\n\n**No answer found**",
    "published_date": "2024-11-05T00:00:00.000Z",
    "score": null
  },
  "https://factory.ai/news/evaluating-compression": {
    "content_path": "data/contents/c35936e156d0ff71.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.550824",
    "title": "Evaluating Context Compression for AI Agents | Factory.ai",
    "summary": "The user query lists many concepts related to building and evaluating AI agents, including infrastructure components, agent frameworks, memory techniques, and training methods.\n\nThe provided webpage focuses specifically on **Evaluating Context Compression for AI Agents**.\n\nThe page compares three context compression strategies: **Factory** (structured, iterative summarization), **OpenAI** (`/responses/compact`), and **Anthropic** (built-in compression). The evaluation framework uses probe-based testing across dimensions like Accuracy, Context Awareness, and Artifact Trail to measure functional quality after compression.\n\n**Key Findings Relevant to the Query:**\n*   **Context Compression/Agent Memory:** The core topic is context compression, which directly relates to managing agent memory and context engineering for long-running sessions.\n*   **Agent Frameworks/SDKs:** The evaluation involves methods from **OpenAI** and **Anthropic** SDKs.\n*   **Structured Outputs:** The superior \"Factory\" method relies on maintaining a **structured**, persistent summary, which relates to structured outputs in agent design.\n*   **Agent Evaluations:** The entire article details a novel **agent evaluation** methodology using an LLM judge (GPT-5.2) and probe questions to measure functional quality, which is a form of **LLM reasoning trace evaluation** focused on context retention.\n\n**Concepts from the Query NOT Answered by the Page:**\nThe page does not discuss:\n*   MCP servers, tool use, deep research, agentic memory (beyond compression), agent frameworks (other than mentioning LangChain/LlamaIndex are common contexts for these agents, but not evaluating them), function calling, agent orchestration, prompt engineering, long context (only compression *for* long context), supervised fine-tuning, or reinforcement learning.\n\n**Summary:**\nThe webpage evaluates different **context compression** strategies (Factory, OpenAI, Anthropic) for long-running AI agents, finding that structured, iterative summarization (Factory) retains more useful information than opaque or regenerating summaries. This evaluation uses a **probe-based framework** and an **LLM judge** to measure functional quality across dimensions like **Accuracy** and **Artifact Trail**, which is a form of **agent evaluation** focused on preserving context for **agent memory** and **context engineering**.\n\nNo answer found for the majority of the specific infrastructure, framework names, training methods, and other agentic components listed in the query.",
    "published_date": "2025-12-16T00:00:00.000Z",
    "score": null
  },
  "https://docs.agno.com/basics/context-compression": {
    "content_path": "data/contents/bed933c22979a852.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.916897",
    "title": "Context Compression - Agno",
    "summary": "The provided web page focuses on **Context Compression** within the Agno framework, which is a feature designed to manage agent context by summarizing verbose tool results to keep agents within their context window, avoid rate limits, and maintain response quality.\n\n**Summary relevant to your query:**\n\nThe page explicitly mentions **Context Compression** as a feature under the \"Context Management\" section of Agno.\n\nWhile the page details Context Compression, it does **not** provide information on the following specific terms from your query:\n\n*   agent\\_infrastructure: MCP servers\n*   tool use (only discusses tool *results*)\n*   deep research\n*   agent memory\n*   agentic memory\n*   agent frameworks (only mentions Agno)\n*   LangChain\n*   LlamaIndex\n*   OpenAI Agents SDK\n*   Anthropic Agents SDK\n*   Google SDK\n*   function calling\n*   structured outputs\n*   agent orchestration\n*   agent evaluations (mentions \"Evals\" as a separate feature)\n*   LLM reasoning trace evaluation\n*   prompt engineering\n*   context engineering (mentions \"Context Engineering\" as a separate section)\n*   long context\n*   supervised fine-tuning\n*   reinforcement learning\n\n**Conclusion:**\n\nThe page directly addresses **Context Compression**. It does not provide answers for the majority of the other specific terms listed in your query.\n\n**No answer found** for most of the requested terms.",
    "published_date": "2025-11-26T16:13:47.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2510.00615": {
    "content_path": "data/contents/d3a528b57ac89b05.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.053523",
    "title": "Computer Science > Artificial Intelligence",
    "summary": "The webpage describes a paper titled **\"ACON: Optimizing Context Compression for Long-horizon LLM Agents.\"**\n\nThe paper introduces **Agent Context Optimization (ACON)**, a unified framework designed to optimally compress environment observations and interaction histories for LLM agents operating in dynamic, real-world environments. This addresses the challenge of growing context length in long-horizon tasks, which increases costs and reduces efficiency.\n\nKey aspects of ACON mentioned are:\n*   It leverages **compression guideline optimization** in natural language space, using capable LLMs to analyze failures when compressed context is used instead of full context, and updating the guideline accordingly.\n*   It proposes **distilling the optimized LLM compressor** into smaller models to reduce overhead.\n*   Experiments show ACON **reduces memory usage by 26-54%** (peak tokens) while largely preserving performance on benchmarks like AppWorld, OfficeBench, and Multi-objective QA.\n*   Distilled compressors **preserve over 95% of accuracy**, and ACON enhances smaller LMs as long-horizon agents with up to a **46% performance improvement**.\n\nRegarding your specific query terms:\n*   **Context compression** and **long context** are central topics of the paper.\n*   The paper focuses on **LLM Agents** and their need for effective **tool use** in long-horizon tasks.\n\nThe page does not explicitly mention or detail: **agent\\_infrastructure: MCP servers, agent memory, agentic memory, agent frameworks, LangChain, LlamaIndex, OpenAI Agents SDK, Anthropic Agents SDK, Google SDK, function calling, structured outputs, agent orchestration, agent evaluations, LLM reasoning trace evaluation, prompt engineering, context engineering, supervised fine-tuning, reinforcement learning.**\n\n**Summary relevant to query:** The paper focuses on **Optimizing Context Compression** for **LLM Agents** performing **long-horizon tasks** that require **tool use**, aiming to manage the growing **context length**.\n\n**No answer found** for the majority of the specific infrastructure, framework, SDK, and advanced training/evaluation terms listed in your query.",
    "published_date": "2025-10-01T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/@manaranjanp/revolutionizing-rag-systems-chatting-with-complex-documents-using-vision-language-models-c8896a55bc31": {
    "content_path": "../data/contents/834f0c2197eb50d2.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:32.827963",
    "title": "RAG Systems with Vision Language Models: Chatting with Complex Documents",
    "summary": "The webpage describes how **Vision Language Models (VLMs)** are revolutionizing **Retrieval-Augmented Generation (RAG)** systems by enabling them to handle complex documents containing visual elements like tables, charts, and infographics, which traditional text-based RAG struggles with.\n\nKey points related to your query:\n\n*   **Vision-Language Models (VLMs):** VLMs are central to this solution, as they can interpret both visual and textual data holistically.\n*   **Multimodal RAG:** The system described, exemplified by **ColPali**, uses a multimodal RAG approach. ColPali embeds entire PDF pages (including visuals) into vector representations, bypassing traditional text extraction, OCR, and layout analysis.\n*   **Document Understanding & PDF Parsing:** This VLM-enhanced RAG simplifies handling complex documents like PDFs by embedding the whole page, allowing for simultaneous processing of text alongside charts and tables.\n*   **Chart/Table Extraction:** The examples show the system successfully extracting specific data points from both charts (e.g., renewable energy percentage) and financial tables (e.g., consolidated cost of sales expenses).\n*   **Report Generation with LLMs:** The system uses a generative model (**Gemini** in the implementation example) to produce natural language answers based on the multimodal content retrieved by the RAG component.\n*   **Gemini:** The implementation section explicitly mentions using the **Gemini** generative model (`gemini-1.5-flash`) to generate the final response after multimodal retrieval.\n\nThe page focuses on the architecture and implementation (using FastAPI and React) of a VLM-powered RAG system for complex document interaction, aligning closely with most aspects of your query, particularly multimodal processing, document understanding, and the use of LLMs like Gemini for generation.",
    "published_date": "2024-09-29T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2304.02173": {
    "content_path": "../data/contents/9f8b35b081b5476d.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.026284",
    "title": "ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules",
    "summary": "The webpage describes **ChartReader**, a unified framework for chart derendering and comprehension that eliminates the need for heuristic rules. It uses a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. The framework is evaluated on Chart-to-Table, ChartQA, and Chart-to-Text tasks, showing superiority over existing methods. It also offers opportunities for integration with mainstream LLMs like T5 and TaPas to extend their chart comprehension capabilities.\n\nWhile the user query covers a broad range of topics including **Vision-language models**, **report generation with LLMs**, and **document understanding**, the provided text specifically details a framework for **chart comprehension** that leverages vision-language models and can integrate with LLMs.",
    "published_date": "2023-04-05T00:00:00.000Z",
    "score": null
  },
  "https://blog.xmartlabs.com/blog/pdf-charts-images-processing-with-llm/": {
    "content_path": "../data/contents/aeb8d9df3e864c01.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.062115",
    "title": "PDF and image processing with LLMs: Text extraction, charts and image interpretation.",
    "summary": "The webpage discusses processing PDFs and images using multimodal Large Language Models (LLMs) for tasks like text extraction, chart interpretation, and report generation.\n\nKey points relevant to your query:\n\n*   **Multimodal Processing:** The core solution involves using **multimodal language models** (like GPT-4V, mentioned implicitly via `gpt-4.1` and the `o3` model, which aligns with models like Gemini/Claude vision capabilities) to interpret visual content (charts, images) within documents.\n*   **Document Understanding & PDF Parsing:** The article details a process using Langchain to convert PDF pages into images, encode them in base64, and send them to a multimodal LLM for interpretation.\n*   **Chart/Table Extraction & Report Generation:** A detailed system prompt is provided that instructs the LLM to perform **Text Extraction**, **Table Interpretation** (outputting in Markdown/CSV), **Chart and Graph Analysis**, and generate a **Page Summary** and **Key Insights**, resulting in structured JSON output.\n*   **Structured Document Output:** The process heavily relies on **Structured Outputs** (using Pydantic/Langchain features) to ensure the LLM response adheres to a strict JSON schema, which is then converted back into plain text for embedding generation (RAG).\n\nWhile the page doesn't explicitly list \"multimodal RAG\" or \"Vision-language models\" as separate sections, the entire methodology described—using multimodal models to interpret visual data from documents and then embedding that interpreted text—is the practical application of **multimodal RAG** using **vision-language models**.",
    "published_date": "2025-11-24T00:00:00.000Z",
    "score": null
  },
  "https://www.datavise.ai/blog/extracting-pdf-data-for-llm-processing-tools-techniques-and-intelligent-routing": {
    "content_path": "../data/contents/2125c70a71f6a202.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.124150",
    "title": "Extracting PDF Data for LLM Processing: Techniques, Tools and Intelligent Routing",
    "summary": "The webpage discusses techniques and tools for **extracting data from PDFs** to prepare it for processing by **Large Language Models (LLMs)**.\n\nKey points relevant to your query include:\n\n*   **Document Understanding and Parsing:** It details the challenges of handling diverse enterprise data (scientific papers, financial reports, legal documents) and common parsing failures like column merging and table structure loss.\n*   **Tools for Extraction:** It lists various open-source (PyMuPDF, PDFMiner, Nougat) and commercial solutions (Mathpix, Azure Form Recognizer, Google Document AI) specialized for different needs, such as formula extraction (Mathpix/Nougat) or table extraction (Azure/Google).\n*   **LLM Integration:** The final processed data (structured JSON, CSV, or clean text) is prepared for LLM ingestion through cleansing, chunking, embedding, and prompt engineering for tasks like summarization and NER.\n*   **Intelligent Routing:** The article advocates for a two-stage intelligent routing process—analyzing document structure first, then routing pages to the optimal, cost-effective parser—to balance speed, accuracy, and cost when dealing with large volumes of documents.\n\nWhile the page focuses heavily on **PDF parsing, document understanding, and report generation** using LLMs, it **does not explicitly detail** vision-language models like GPT-4V, Claude vision, or Gemini, nor does it specifically cover **multimodal RAG** or **multimodal generation** beyond the context of structured document output preparation.",
    "published_date": "2025-03-31T00:00:00.000Z",
    "score": null
  },
  "https://github.com/oidlabs-com/Lexoid": {
    "content_path": "../data/contents/1734521b6a89aad3.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.178149",
    "title": "Search code, repositories, users, issues, pull requests...",
    "summary": "The webpage describes **Lexoid**, a public GitHub repository for a **multimodal document parser** designed for high-quality data understanding and extraction.\n\nLexoid supports both **LLM-based** and **non-LLM-based (static)** PDF document parsing. It is motivated by the advancement of LLMs and aims to enable convenience for users.\n\nThe library supports various API providers for LLM-based parsing, including **Google, OpenAI, Hugging Face, Together AI, OpenRouter, and Fireworks**.\n\nA benchmark table shows performance comparisons across different models for document parsing, including models like **Gemini** variants and **Claude** variants (e.g., `claude-3-5-sonnet-20241022`).\n\nWhile the query mentions specific topics like \"Vision-language models,\" \"multimodal RAG,\" \"document understanding,\" \"PDF parsing,\" \"chart/table extraction,\" \"report generation with LLMs,\" \"GPT-4V,\" \"Claude vision,\" \"Gemini,\" and \"structured document output,\" the Lexoid description confirms its focus on **multimodal document parsing** and **PDF parsing** using LLMs, which aligns with several of these concepts. The benchmark explicitly lists models like **Gemini** and **Claude**.",
    "published_date": "2024-09-22T00:00:00.000Z",
    "score": null
  },
  "https://medium.com/data-science-collective/pdf-parsing-processing-tools-you-should-know-ea1563e7308f": {
    "content_path": "../data/contents/616a742c0724b05b.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.204508",
    "title": "PDF Parsing Processing Tools You Should Know",
    "summary": "The webpage discusses five key tools for PDF parsing and document understanding, which aligns with the user query's focus on document understanding, PDF parsing, and related concepts like vision-language models (which are often the underlying technology).\n\nHere is a summary of the tools and concepts mentioned in relation to the query:\n\n*   **Document Understanding and PDF Parsing:** The article focuses entirely on tools designed to extract information from unstructured PDF files.\n*   **Vision-Language Models (Multimodal):** Several tools leverage multimodal capabilities:\n    *   **DeepSeek OCR** is part of the DeepSeek-VL (Vision-Language) family, combining vision encoders and LLMs to interpret documents beyond simple text detection.\n    *   **Mistral OCR** is described as natively multilingual and **multimodal**.\n    *   The final section suggests using a **Vanilla VisionLLM** directly for parsing and answering questions from PDFs, highlighting the use of vision models for document processing.\n*   **Chart/Table Extraction:** **Docling** excels at extracting tables, lists, headings, and captions. **DeepSeek OCR** is also designed to extract layout, tables, and structured information.\n*   **Multimodal RAG:** **Docling** is noted for creating representations compelling for **RAG pipelines**. The general theme of extracting structured data from documents is foundational for RAG systems.\n*   **Structured Document Output:** Tools like **Docling** convert unstructured files into computer-understandable formats like **JSON** or **Markdown**. **LlamaParse** returns lean **Markdown or JSONL**.\n*   **Report Generation with LLMs:** Tools like **DeepSeek OCR** and **Mistral OCR** can go beyond simple extraction to perform tasks like **summarising pages** and answering questions about documents, which are precursors to report generation.\n\nThe article covers Docling, DeepSeek OCR, LlamaParse, Mistral OCR, and Unstructured, all of which are modern approaches to document processing that utilize multimodal techniques and aim for structured output suitable for downstream AI tasks.",
    "published_date": "2025-11-25T00:00:00.000Z",
    "score": null
  },
  "https://huggingface.co/ibm-granite/granite-vision-3.3-2b-embedding": {
    "content_path": "../data/contents/98965844c83fe081.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.256496",
    "title": "ibm-granite/granite-vision-3.3-2b-embedding · Hugging Face",
    "summary": "The webpage describes the **granite-vision-3.3-2b-embedding** model, which is an efficient embedding model based on `granite-vision-3.3-2b`.\n\nIt is specifically designed for **multimodal document retrieval** and is well-suited for **multimodal RAG systems** dealing with complex documents like reports, slides, and manuals containing tables, charts, and infographics. It generates ColBERT-style multi-vector representations, simplifying and accelerating RAG pipelines by removing the need for OCR-based text extraction.\n\nThe model architecture includes a **Vision-Language model** (`granite-vision-3.3-2b`) and a projection layer. It was trained on **DocFM**, a large dataset of 85 million document pages.\n\nWhile the page details a model for multimodal retrieval, it **does not explicitly mention or discuss** the following aspects of your query:\n*   Report generation with LLMs\n*   GPT-4V, Claude vision, or Gemini (it focuses on the Granite model)\n*   PDF parsing or chart/table extraction as direct capabilities (it handles documents *with* these elements for retrieval)\n*   Structured document output\n\nTherefore, regarding the specific topics of **report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output**, the page provides **No answer found**.",
    "published_date": "2025-11-17T14:47:24.000Z",
    "score": null
  },
  "https://airparser.com/pdf-parser/": {
    "content_path": "../data/contents/d7484542a251eb99.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.282796",
    "title": "LLM-Powered PDF Parser",
    "summary": "The user query is about **multimodal and generation** topics, specifically mentioning: Vision-language models, multimodal RAG, document understanding, PDF parsing, chart/table extraction, report generation with LLMs, GPT-4V, Claude vision, Gemini, and structured document output.\n\nThe provided webpage text is about an **LLM-Powered PDF Parser** called Airparser.\n\n**Summary relevant to the query:**\n\nThe webpage describes an **LLM-Powered PDF Parser** that extracts structured data from PDFs using advanced AI and LLMs (mentioning GPT technology). This directly relates to the **PDF parsing**, **document understanding**, and **structured document output** aspects of the query.\n\nKey features mentioned include:\n*   Extracting data from tables, forms, and handwriting.\n*   **Table Extraction** that preserves structure.\n*   Handwriting Recognition.\n*   Processing various document types like invoices, contracts, and reports.\n\n**However, the page does not mention or discuss:** Vision-language models (like GPT-4V, Claude vision, Gemini), multimodal RAG, chart extraction (only tables are explicitly detailed), or general report generation with LLMs beyond the structured output from the PDF parsing process.\n\nSince the page focuses narrowly on PDF parsing and does not cover the broader multimodal/vision model aspects requested, the summary is limited.\n\n**Summary:**\nThe webpage details an LLM-Powered PDF Parser that uses AI to extract structured data from various documents (invoices, contracts, forms) with high accuracy (99%). It excels at understanding context, handling handwritten text, and accurately extracting data from tables while preserving structure. It offers API integration and exports data to formats like Excel and Google Sheets.\n\n**Does it answer the full query?** No, it only covers the PDF parsing/document understanding aspect and omits vision models, multimodal RAG, and general report generation.\n\n**No answer found** (for the full scope of the query).",
    "published_date": null,
    "score": null
  },
  "https://www.youtube.com/watch?v=_Bw5o55SRL8": {
    "content_path": "../data/contents/078a51f2146f0a5c.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.397306",
    "title": "Inference Time Compute",
    "summary": "The webpage, titled \"Inference Time Compute,\" explains key technologies related to this concept, which is described as the core of OpenAI O1. The tutorial covers teaching an LLM **Chain-of-Thought** thinking, step-by-step **reasoning** using a process reward model, **self-reflection**, and **backtracking**. It also briefly touches upon **inference-time scaling laws** and community efforts to reproduce OpenAI O1.\n\n**Note:** The page mentions Chain-of-Thought, reasoning, and self-reflection, but does not explicitly cover: planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, or factuality.",
    "published_date": "2024-12-02T00:00:00.000Z",
    "score": null
  },
  "https://en.wikipedia.org/wiki/Reflection_(artificial_intelligence)": {
    "content_path": "../data/contents/029f2d455dc8be25.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.454363",
    "title": "Feedback neural network",
    "summary": "The webpage discusses **Feedback Neural Networks**, which are neural networks capable of providing bottom-up and top-down design feedback to their inputs or previous layers based on their outputs. This technique is notably used in **Reasoning Language Models (RLMs)** to mimic self-assessment and internal deliberation, aiming to minimize errors like **hallucinations** and increase interpretability.\n\nKey concepts related to your query found in the text include:\n\n*   **Reasoning LLMs (RLMs):** Explicitly mentioned as a primary use case for feedback neural networks.\n*   **Chain-of-Thought (CoT):** Techniques like CoT prompting are mentioned as precursors or related methods that encourage intermediate reasoning steps. Increasing the length of the CoT process via feedback loops increases **inference-time compute** (referred to as \"test-time scaling\").\n*   **Self-reflection:** The process is designed to mimic self-assessment and internal deliberation, analogous to human metacognition (\"thinking about thinking\"). Types of reflection discussed include Post-hoc, Iterative, and Intrinsic reflection.\n*   **Hallucination Reduction and Detection:** The process aims to minimize errors like hallucinations.\n*   **Test-time scaling:** Mentioned in relation to increasing the length of the CoT reasoning process through multiple network passes.\n\nThe page **does not explicitly mention or detail**:\n\n*   **Planning with LLMs** (though reflection supports multi-step reasoning).\n*   **MCTS (Monte Carlo Tree Search) for language models.**\n*   **Grounding.**\n*   **Factuality** (though hallucination reduction relates to this).\n\n**Summary:** The page describes Feedback Neural Networks as a technique used in Reasoning LLMs that employs self-reflection and iterative processes (related to Chain-of-Thought) to improve performance by utilizing additional **inference-time compute** and reducing **hallucinations**. It does not cover MCTS, grounding, or explicit planning mechanisms.",
    "published_date": "2025-02-05T09:54:43.000Z",
    "score": null
  },
  "https://proceedings.neurips.cc/paper_files/paper/2023/file/65a39213d7d0e1eb5d192aa77e77eeb7-Paper-Conference.pdf": {
    "content_path": "../data/contents/e7ca83fe5d6e37e2.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.510865",
    "title": "",
    "summary": "The user query asks for a summary related to several advanced topics in Large Language Models (LLMs) concerning **reasoning and planning**, including chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction, grounding, and factuality.\n\nThe provided webpage focuses on a specific method called **LLM-MCTS** for **large-scale task planning**, which combines the LLM's role as a **commonsense world model (L-Model)** and as a **heuristic policy (L-Policy)** within a **Monte Carlo Tree Search (MCTS)** framework.\n\nHere is a summary of the page content relevant to the query:\n\n*   **Planning with LLMs and MCTS:** The paper introduces **LLM-MCTS**, an algorithm that integrates LLMs into MCTS for complex task planning (like object rearrangement in a household environment).\n    *   The LLM acts as a **commonsense world model** to provide a prior belief for MCTS, improving reasoning.\n    *   The LLM also acts as a **heuristic policy** to guide the MCTS search, vastly improving efficiency.\n*   **Reasoning and Inference-Time Compute:** LLM-MCTS uses the LLM's reasoning capabilities (via the world model and heuristic guidance) during the search process (inference time) to navigate large search spaces, outperforming LLMs used purely as policies (L-Policy) on complex, novel tasks.\n*   **Model vs. Policy Choice (Reasoning Insight):** The paper investigates when using an LLM as a model (L-Model) is better than using it as a policy (L-Policy).\n    *   They suggest the **Minimum Description Length (MDL) principle** as a guiding principle: if the description length of the world model is substantially smaller than that of the policy, model-based planning (L-Model) is likely better.\n    *   This was demonstrated in **multiplication** and **travel planning** tasks, where the model-based approach (LLM-derived small tables + search algorithm) was significantly more scalable and accurate than the direct policy approach.\n*   **Ablation Studies:** The ablation study confirms the importance of both components:\n    *   Removing the heuristic policy (**No Heuristic Policy**) results in 0% success\n\nThe user query asks for a summary related to several advanced topics in Language Model (LLM) reasoning and planning, including: **Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**\n\nThe provided webpage text discusses the application of LLMs in **task planning** within a simulated environment (VirtualHome), specifically comparing an **L-Model** (LLM as a world model) approach with an **L-Policy** approach, often utilizing **MCTS** (Monte Carlo Tree Search) to accelerate planning.\n\nHere is a summary of how the text relates to the query topics:\n\n*   **Planning with LLMs & MCTS for language models:** The core of the paper is using LLMs as a **commonsense world model** and a **heuristic policy** within an **MCTS** framework (LLM-MCTS) for large-scale task planning. This approach leverages the LLM's knowledge to guide the search, improving efficiency over pure MCTS or LLM-as-policy methods.\n*   **Reasoning LLMs & Chain-of-Thought:** The text mentions using LLMs for multi-step reasoning problems and as few-shot planners, acknowledging that recent research suggests inherent limitations for complex reasoning. The use of MCTS with an LLM world model is presented as a way to achieve \"better-reasoned decision-making.\"\n*   **Hallucination Reduction and Detection:** The text explicitly notes that LLMs used as open-loop or closed-loop policies \"may suffer from **hallucinations**.\" The proposed LLM-MCTS method aims to produce \"reasoned results\" by grounding the LLM's knowledge within the search framework.\n*   **Grounding:** The text discusses translating LLM outputs (actions and object names) into admissible actions within the VirtualHome environment, noting that translation errors occur when the LLM output doesn't align with the underlying logic of the environment (e.g., needing to be close to an object to interact with it).\n*   **Inference-time compute & Test-time scaling:** The \"Limitations\" section mentions that the runtime of LLM-MCTS is \"hindered by **multiple LLM calls**,\" suggesting a concern",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/html/2408.16326v3": {
    "content_path": "../data/contents/1ce9ac27866a2c4e.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.591624",
    "title": "Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic",
    "summary": "The user query asks for a summary related to several topics in **reasoning and planning with LLMs**, including: Chain-of-Thought (CoT), inference-time compute, self-reflection, planning, MCTS, test-time scaling, hallucination reduction, grounding, and factuality.\n\nThe provided webpage describes **Critic-CoT**, a novel framework designed to boost the reasoning abilities of Large Language Models (LLMs) through a **Chain-of-Thought Critic**.\n\nHere is a summary of how the page addresses the concepts in the query:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** Critic-CoT explicitly focuses on enhancing reasoning by pushing LLMs toward **System-2-like critic capability** using a **step-wise CoT reasoning paradigm** for self-critique and refinement.\n*   **Self-Reflection:** The core mechanism of Critic-CoT is **self-criticism and iterative refinement** based on the model's own step-wise analysis of its reasoning process.\n*   **Inference-time compute:** The paper discusses inference strategies like **Iterative Refine** and **Critic As Filter (Majority Vote Filtering)**, which utilize the trained critic model during inference.\n*   **Hallucination Reduction and Detection:** While not explicitly using the term \"hallucination,\" the method improves performance by **filtering out invalid solutions** and refining reasoning, which inherently addresses errors and inaccuracies in the generated output.\n*   **Factuality/Grounding:** The experiments focus on mathematical reasoning (GSM8K, MATH), where correctness is verifiable, suggesting an improvement in the factuality of the final answer through rigorous step-wise checking.\n\n**Topics not explicitly detailed or mentioned:**\n\n*   **Planning with LLMs** (beyond step-wise refinement, which is a form of local planning).\n*   **MCTS (Monte Carlo Tree Search) for language models.**\n*   **Test-time scaling.**\n*   **Grounding** (in a broader sense than just mathematical verification).\n\n**Summary:**\n\nThe paper introduces **Critic-CoT**, a framework that enhances LLM reasoning by training the model to perform **step-wise self-critique and refinement** using a **Chain-of-Thought (CoT)** approach. This shifts the model's critique mechanism from simple, intuitive feedback (System-1) to\n\nThe provided web page text is a collection of references and an appendix detailing the methodology, data construction, and evaluation metrics for a paper titled \"Critic-CoT: Boosting the Reasoning Abilities of Large Language Model via Chain-of-Thought Critic.\"\n\nThe user query asks for a summary related to: **'reasoning\\_and\\_planning: Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection, planning with LLMs, MCTS (Monte Carlo Tree Search) for language models, test-time scaling, hallucination reduction and detection, grounding, factuality'**\n\nThe webpage text directly discusses several of these concepts in the context of the Critic-CoT method:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** The paper's title and content heavily focus on improving reasoning using Chain-of-Thought (Critic-CoT). References mention CoT empowerment (Li et al., 2024) and self-consistency improving CoT reasoning (Wang et al., 2023b).\n*   **Self-Reflection:** The paper explicitly compares Critic-CoT to **Reflexion** (Shinn et al., 2023) and discusses how their method teaches intrinsic self-critique, which is a form of self-reflection (Section A.4.2). Other works on self-reflection are also cited (Ji et al., 2023; Yan et al., 2024).\n*   **Hallucination Reduction and Detection:** The concept of critique is used to detect errors, which relates to hallucination reduction. The paper discusses how the critic model's ability to critique can be leveraged to reject imperfect responses (Section A.4.5).\n*   **Inference-time Compute / Test-time Scaling:** The paper analyzes the inference cost of its methods, specifically **Iterative Refinement** and **Critic As Filter**, detailing the average token usage and latency as rounds/samples increase (Tables A3, A4). It also contrasts its controllable test-time scaling with the potential \"overthinking\" of long reasoning models (Section A.4.3).\n*   **Planning with LLMs / MCTS:** While MCTS is mentioned in the user query, the provided text does not explicitly detail the use of MCTS for language models. However, it discusses alternatives to binary",
    "published_date": "2014-02-13T00:00:00.000Z",
    "score": null
  },
  "https://ui.adsabs.harvard.edu/abs/2022arXiv221003493Z/abstract": {
    "content_path": "../data/contents/099a27ddd04831e8.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.733030",
    "title": "Automatic Chain of Thought Prompting in Large Language Models - ADS",
    "summary": "The webpage describes **Auto-CoT**, an automatic Chain-of-Thought (CoT) prompting method for Large Language Models (LLMs).\n\nThe summary relevant to your query is:\n\n*   **Reasoning LLMs and Chain-of-Thought (CoT):** LLMs can perform complex reasoning by generating intermediate reasoning steps (CoT).\n*   **CoT Prompting Paradigms:**\n    1.  Using a simple prompt like \"Let's think step by step.\"\n    2.  Using manual demonstrations (question + reasoning chain).\n*   **Auto-CoT:** This method eliminates the need for manual demonstration hand-crafting by leveraging LLMs with the \"Let's think step by step\" prompt to *generate* reasoning chains for demonstrations one by one.\n*   **Mitigation:** To handle mistakes in these generated chains, Auto-CoT emphasizes the importance of **diversity** by sampling questions with diversity and generating reasoning chains to construct demonstrations.\n*   **Performance:** Auto-CoT matches or exceeds the performance of manual CoT prompting on ten public benchmark reasoning tasks using GPT-3.\n\nThe page focuses specifically on **Chain-of-Thought prompting** and its automation, but does not explicitly detail **inference-time compute, self-reflection, planning with LLMs, MCTS, test-time scaling, hallucination reduction/detection, or grounding/factuality**.",
    "published_date": "2022-10-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/html/2505.22148v1": {
    "content_path": "../data/contents/30cf27ddbf9301f9.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:33.760185",
    "title": "What Makes a Good Reasoning Chain? \n Uncovering Structural Patterns in Long Chain-of-Thought Reasoning",
    "summary": "The provided web page discusses **Long Chain-of-Thought (LCoT) reasoning** in Large Language Models (LLMs) and introduces a framework called **LCoT2Tree** designed to analyze the *structure* of these reasoning chains.\n\nHere is a summary of the key points relevant to your query:\n\n*   **Reasoning LLMs and Chain-of-Thought (CoT):** The page confirms the popularity of LCoT as a strategy for enabling LLMs to perform complex, step-by-step reasoning (System 2 thinking) in tasks like math and code generation.\n*   **Structural Analysis over Length:** The research emphasizes that simple metrics like response length are poor predictors of reasoning correctness, highlighting the \"overthinking\" phenomenon. LCoT2Tree converts sequential LCoTs into hierarchical tree structures to analyze underlying patterns.\n*   **Key Structural Patterns:** The framework identifies structural patterns such as **exploration, backtracking, and verification** as stronger predictors of final performance than length alone.\n*   **Error Detection:** By analyzing these tree structures, the authors identified critical thought patterns that account for failures, such as **over-branching** (excessive exploration/verification within a single node).\n*   **Planning/Decoding Application:** The structural analysis is applied practically to improve **Best-of-N decoding**, where the tree-based classifier is used to select the highest-quality reasoning candidate, outperforming reward model (ORM/PRM) and length-based selection methods.\n\nThe page focuses heavily on the structural analysis of CoT reasoning using the LCoT2Tree framework, which relates to **Reasoning LLMs** and **Chain-of-Thought**. It does not explicitly detail **inference-time compute, self-reflection, MCTS, test-time scaling, grounding, or factuality** beyond mentioning that semantic metrics (which relate to factuality/coherence) struggle to scale compared to structural analysis.\n\nThe provided web page focuses on analyzing and structuring the **Chain-of-Thought (CoT)** reasoning of Large Language Models (LLMs) using a framework called **LCoT2Tree**.\n\nHere is a summary of how the page relates to your query topics:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** The entire document is dedicated to analyzing the structure of CoT reasoning. It introduces the **LCoT2Tree** process, which automatically converts long CoTs into structured trees to better understand the reasoning process.\n*   **Inference-time compute & Test-time scaling:** The page mentions scaling and compute in the context of evaluating different decoding strategies (Table 6), comparing methods like **Length-Best** (which relates to inference length/compute) against the proposed tree-based method. It also references work on test-time scaling (Muennighoff et al., 2025; Snell et al., 2024).\n*   **Self-reflection & Self-Correction:** The page references work related to self-refinement (Madaan et al., 2023) and self-correction (Gou et al., 2024; Miao et al., 2024) in the bibliography, and the analysis of error patterns (Appendix D.1) touches upon identifying issues within the reasoning structure.\n*   **Planning with LLMs & MCTS:** While the core focus is on analyzing *existing* CoT, the bibliography includes references to planning and tree-based search methods like **Tree of Thoughts** (Yao et al., 2023), which is related to planning and MCTS concepts for language models.\n*   **Hallucination Reduction and Detection & Grounding/Factuality:** The page addresses reasoning quality and correctness through classification tasks (Table 3, Table 5) and by analyzing error patterns (Appendix D.1). It references work on verification (Lightman et al., 2023; Ling et al., 2023) and grounded proof generation (Welleck et al., 2022), which relate to factuality and reducing errors.\n\n**In summary:** The page details a method (**LCoT2Tree**) for structurally analyzing the **Chain-of-Thought** reasoning of various **Reasoning LLMs** across different",
    "published_date": "2024-01-01T00:00:00.000Z",
    "score": null
  },
  "https://aclanthology.org/2023.emnlp-main.507.pdf": {
    "content_path": "../data/contents/d71b5a9ebb1e870c.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:34.010745",
    "title": "",
    "summary": "The user query asks for a summary related to several concepts in LLM reasoning: **Reasoning LLMs, chain-of-thought (CoT), inference-time compute, self-reflection, planning with LLMs, MCTS for language models, test-time scaling, hallucination reduction and detection, grounding, and factuality.**\n\nThe provided webpage text describes a framework called **Reasoning via Planning (RAP)**, which addresses limitations in current LLM reasoning, particularly those related to planning and the lack of an internal world model, which are often issues when using methods like Chain-of-Thought (CoT).\n\nHere is a summary of how the text relates to the query concepts:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** The paper acknowledges the reasoning capabilities of LLMs, especially with **Chain-of-Thought (CoT)** prompting, but notes their struggles with tasks requiring deliberate planning. RAP is proposed as an alternative/improvement to CoT.\n*   **Planning with LLMs & MCTS (Monte Carlo Tree Search) for language models:** RAP explicitly incorporates a principled planning algorithm based on **Monte Carlo Tree Search (MCTS)**. RAP repurposes the LLM to act as both a **world model** (to predict future states) and a **reasoning agent**, guided by MCTS to explore the reasoning space efficiently, balancing exploration and exploitation.\n*   **Self-reflection/Self-evaluation:** The reward design in RAP includes **Self-evaluation by the LLM** (asking \"Is this reasoning step correct?\") to guide the planning process.\n*   **Grounding & Factuality:** The introduction of the **world model** in RAP helps LLMs generate a more **grounded and coherent inference** by simulating the state of the world (e.g., block configurations, intermediate variable values), which addresses issues related to maintaining factual consistency across reasoning steps.\n*   **Inference-time compute & Test-time scaling:** While not explicitly using the term \"test-time scaling,\" the framework relies on iterative computation via MCTS, where performance is shown to improve with more **iterations** (e.g., RAP(10) vs RAP(20) in Table 1), indicating a dependence on **inference-time compute**.\n\n**Concepts not explicitly detailed or covered:** Hallucination reduction and detection (beyond the grounding/factuality improvements from the\n\nThe webpage describes the **Reasoning via Planning (RAP)** framework, which enhances the reasoning abilities of Large Language Models (LLMs) by equipping them with an ability to reason akin to human-like strategic planning.\n\nKey aspects related to your query:\n\n*   **Reasoning LLMs & Chain-of-Thought (CoT):** RAP is presented as a novel framework that outperforms the CoT baseline, especially on tasks requiring many steps. It repurposes the LLM to act as both a **world model** (simulating states and anticipating outcomes) and a **reasoning agent**.\n*   **Planning with LLMs & MCTS:** RAP achieves an effective balance between exploration and exploitation via **Monte Carlo Tree Search (MCTS)**. The document details modifications made to classic MCTS to handle the large reasoning space and high computational cost of LLMs, including sampling potential actions and using lightweight local rewards during selection.\n*   **Inference-time compute & Test-time scaling:** The framework operates during inference. The use of MCTS inherently involves iterative computation. The text notes that RAP maintains a high success rate when CoT fails on cases requiring more steps, suggesting better scaling with reasoning depth.\n*   **Self-reflection/Self-evaluation:** The framework utilizes a **self-evaluation reward** ($R_3$), which is shown to be crucial for guiding reasoning and achieving the best performance in math reasoning tasks, suggesting a form of self-reflection on generated steps.\n*   **Grounding/Factuality:** While not explicitly using the terms \"grounding\" or \"hallucination reduction,\" the framework's use of an LLM as a **world model** to simulate states and anticipate outcomes (as seen in the detailed Blocksworld examples) is an attempt to ground the reasoning process in simulated reality. The effectiveness of rewards like action likelihood and task-specific rewards also aims to guide the LLM toward correct, factual steps.\n\nIn summary, the page details the RAP framework, which integrates **MCTS planning** with LLMs acting as **world models** and uses various **rewards** (including self-evaluation) to improve reasoning performance over standard **Chain-of-Thought** methods.",
    "published_date": "2023-12-06T00:00:00.000Z",
    "score": null
  },
  "https://ieeexplore.ieee.org/iel8/10934113/10934151/10934217.pdf": {
    "content_path": "../data/contents/379dc186280c50c3.md",
    "source": "exa",
    "saved_at": "2025-12-30T11:20:34.119433",
    "title": "MVP-MCTS: Language Modelling a Flexible Multi-Agent Planning Framework",
    "summary": "No answer found",
    "published_date": null,
    "score": null
  },
  "https://arxiv.org/abs/2503.17407": {
    "content_path": "../data/contents/2503.17407.md",
    "pdf_path": "../data/contents/2503.17407.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:24.654060",
    "title": "A Comprehensive Survey on Long Context Language Modeling"
  },
  "https://arxiv.org/abs/2512.23171v1": {
    "content_path": "../data/contents/2512.23171v1.md",
    "pdf_path": "../data/contents/2512.23171v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:25.025443",
    "title": "Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning"
  },
  "https://arxiv.org/abs/2512.20460v2": {
    "content_path": "../data/contents/2512.20460v2.md",
    "pdf_path": "../data/contents/2512.20460v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:25.655332",
    "title": "The Aligned Economic Index & The State Switching Model"
  },
  "https://arxiv.org/abs/2512.23167v1": {
    "content_path": "../data/contents/2512.23167v1.md",
    "pdf_path": "../data/contents/2512.23167v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:26.011118",
    "title": "SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search"
  },
  "https://arxiv.org/abs/2512.22238v1": {
    "content_path": "../data/contents/2512.22238v1.md",
    "pdf_path": "../data/contents/2512.22238v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:26.970752",
    "title": "Masking Teacher and Reinforcing Student for Distilling Vision-Language Models"
  },
  "https://arxiv.org/abs/2512.23165v1": {
    "content_path": "../data/contents/2512.23165v1.md",
    "pdf_path": "../data/contents/2512.23165v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:27.296179",
    "title": "Evaluating Parameter Efficient Methods for RLVR"
  },
  "https://arxiv.org/abs/2512.20291v2": {
    "content_path": "../data/contents/2512.20291v2.md",
    "pdf_path": "../data/contents/2512.20291v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:28.669339",
    "title": "Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity"
  },
  "https://arxiv.org/abs/2512.23145v1": {
    "content_path": "../data/contents/2512.23145v1.md",
    "pdf_path": "../data/contents/2512.23145v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:29.083816",
    "title": "Reservoir Computing inspired Matrix Multiplication-free Language Model"
  },
  "https://arxiv.org/abs/2512.22236v1": {
    "content_path": "../data/contents/2512.22236v1.md",
    "pdf_path": "../data/contents/2512.22236v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:32.911530",
    "title": "We are not able to identify AI-generated images"
  },
  "https://arxiv.org/abs/2512.23138v1": {
    "content_path": "../data/contents/2512.23138v1.md",
    "pdf_path": "../data/contents/2512.23138v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:33.267823",
    "title": "Why Machine Learning Models Systematically Underestimate Extreme Values II: How to Fix It with LatentNN"
  },
  "https://arxiv.org/abs/2512.20156v2": {
    "content_path": "../data/contents/2512.20156v2.md",
    "pdf_path": "../data/contents/2512.20156v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:34.081460",
    "title": "Fun-Audio-Chat Technical Report"
  },
  "https://arxiv.org/abs/2512.23133v1": {
    "content_path": "../data/contents/2512.23133v1.md",
    "pdf_path": "../data/contents/2512.23133v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:34.372608",
    "title": "Principled Algorithms for Optimizing Generalized Metrics in Binary Classification"
  },
  "https://arxiv.org/abs/2512.22234v1": {
    "content_path": "../data/contents/2512.22234v1.md",
    "pdf_path": "../data/contents/2512.22234v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:34.650338",
    "title": "DiRL: An Efficient Post-Training Framework for Diffusion Language Models"
  },
  "https://arxiv.org/abs/2512.23132v1": {
    "content_path": "../data/contents/2512.23132v1.md",
    "pdf_path": "../data/contents/2512.23132v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:35.607342",
    "title": "Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems"
  },
  "https://arxiv.org/abs/2512.22226v1": {
    "content_path": "../data/contents/2512.22226v1.md",
    "pdf_path": "../data/contents/2512.22226v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:37.397542",
    "title": "VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs"
  },
  "https://arxiv.org/abs/2512.23128v1": {
    "content_path": "../data/contents/2512.23128v1.md",
    "pdf_path": "../data/contents/2512.23128v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:37.719761",
    "title": "It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents"
  },
  "https://arxiv.org/abs/2512.22225v1": {
    "content_path": "../data/contents/2512.22225v1.md",
    "pdf_path": "../data/contents/2512.22225v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:38.289139",
    "title": "Literature Mining System for Nutraceutical Biosynthesis: From AI Framework to Biological Insight"
  },
  "https://arxiv.org/abs/2512.23126v1": {
    "content_path": "../data/contents/2512.23126v1.md",
    "pdf_path": "../data/contents/2512.23126v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:38.645119",
    "title": "InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization"
  },
  "https://arxiv.org/abs/2512.22223v1": {
    "content_path": "../data/contents/2512.22223v1.md",
    "pdf_path": "../data/contents/2512.22223v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:39.753962",
    "title": "ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis"
  },
  "https://arxiv.org/abs/2512.23109v1": {
    "content_path": "../data/contents/2512.23109v1.md",
    "pdf_path": "../data/contents/2512.23109v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:40.014597",
    "title": "How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure"
  },
  "https://arxiv.org/abs/2512.22221v1": {
    "content_path": "../data/contents/2512.22221v1.md",
    "pdf_path": "../data/contents/2512.22221v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:40.610287",
    "title": "Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning"
  },
  "https://arxiv.org/abs/2512.23097v1": {
    "content_path": "../data/contents/2512.23097v1.md",
    "pdf_path": "../data/contents/2512.23097v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:40.824073",
    "title": "A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms"
  },
  "https://arxiv.org/abs/2512.22219v1": {
    "content_path": "../data/contents/2512.22219v1.md",
    "pdf_path": "../data/contents/2512.22219v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:41.425076",
    "title": "Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs"
  },
  "https://arxiv.org/abs/2512.23096v1": {
    "content_path": "../data/contents/2512.23096v1.md",
    "pdf_path": "../data/contents/2512.23096v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:43.459756",
    "title": "Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation"
  },
  "https://arxiv.org/abs/2512.22213v1": {
    "content_path": "../data/contents/2512.22213v1.md",
    "pdf_path": "../data/contents/2512.22213v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:44.239264",
    "title": "On the Existence and Behaviour of Secondary Attention Sinks"
  },
  "https://arxiv.org/abs/2512.23090v1": {
    "content_path": "../data/contents/2512.23090v1.md",
    "pdf_path": "../data/contents/2512.23090v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:44.956857",
    "title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients"
  },
  "https://arxiv.org/abs/2512.22212v1": {
    "content_path": "../data/contents/2512.22212v1.md",
    "pdf_path": "../data/contents/2512.22212v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:45.566974",
    "title": "Transformer Reconstructed with Dynamic Value Attention"
  },
  "https://arxiv.org/abs/2512.23087v1": {
    "content_path": "../data/contents/2512.23087v1.md",
    "pdf_path": "../data/contents/2512.23087v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:45.805408",
    "title": "Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning"
  },
  "https://arxiv.org/abs/2512.22211v1": {
    "content_path": "../data/contents/2512.22211v1.md",
    "pdf_path": "../data/contents/2512.22211v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:46.187059",
    "title": "With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems"
  },
  "https://arxiv.org/abs/2512.23078v1": {
    "content_path": "../data/contents/2512.23078v1.md",
    "pdf_path": "../data/contents/2512.23078v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:48.400340",
    "title": "Deep Learning for Art Market Valuation"
  },
  "https://arxiv.org/abs/2512.22208v1": {
    "content_path": "../data/contents/2512.22208v1.md",
    "pdf_path": "../data/contents/2512.22208v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:48.954884",
    "title": "Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA"
  },
  "https://arxiv.org/abs/2512.23075v1": {
    "content_path": "../data/contents/2512.23075v1.md",
    "pdf_path": "../data/contents/2512.23075v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:49.536889",
    "title": "Trust Region Masking for Long-Horizon LLM Reinforcement Learning"
  },
  "https://arxiv.org/abs/2512.22207v1": {
    "content_path": "../data/contents/2512.22207v1.md",
    "pdf_path": "../data/contents/2512.22207v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:49.973370",
    "title": "GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks"
  },
  "https://arxiv.org/abs/2512.23073v1": {
    "content_path": "../data/contents/2512.23073v1.md",
    "pdf_path": "../data/contents/2512.23073v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:51.135989",
    "title": "Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models"
  },
  "https://arxiv.org/abs/2512.22206v1": {
    "content_path": "../data/contents/2512.22206v1.md",
    "pdf_path": "../data/contents/2512.22206v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:51.890434",
    "title": "CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks"
  },
  "https://arxiv.org/abs/2512.23070v1": {
    "content_path": "../data/contents/2512.23070v1.md",
    "pdf_path": "../data/contents/2512.23070v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:52.484481",
    "title": "FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment"
  },
  "https://arxiv.org/abs/2512.18841v2": {
    "content_path": "../data/contents/2512.18841v2.md",
    "pdf_path": "../data/contents/2512.18841v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:53.074178",
    "title": "MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models"
  },
  "https://arxiv.org/abs/2512.23068v1": {
    "content_path": "../data/contents/2512.23068v1.md",
    "pdf_path": "../data/contents/2512.23068v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:53.491144",
    "title": "Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution"
  },
  "https://arxiv.org/abs/2512.22201v1": {
    "content_path": "../data/contents/2512.22201v1.md",
    "pdf_path": "../data/contents/2512.22201v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:53.840542",
    "title": "Emergent Persuasion: Will LLMs Persuade Without Being Prompted?"
  },
  "https://arxiv.org/abs/2512.23067v1": {
    "content_path": "../data/contents/2512.23067v1.md",
    "pdf_path": "../data/contents/2512.23067v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:54.138494",
    "title": "The Reward Model Selection Crisis in Personalized Alignment"
  },
  "https://arxiv.org/abs/2512.22200v1": {
    "content_path": "../data/contents/2512.22200v1.md",
    "pdf_path": "../data/contents/2512.22200v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:54.608837",
    "title": "Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents"
  },
  "https://arxiv.org/abs/2512.23065v1": {
    "content_path": "../data/contents/2512.23065v1.md",
    "pdf_path": "../data/contents/2512.23065v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:54.912618",
    "title": "TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish"
  },
  "https://arxiv.org/abs/2512.22199v1": {
    "content_path": "../data/contents/2512.22199v1.md",
    "pdf_path": "../data/contents/2512.22199v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:55.499050",
    "title": "Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation"
  },
  "https://arxiv.org/abs/2512.23049v1": {
    "content_path": "../data/contents/2512.23049v1.md",
    "pdf_path": "../data/contents/2512.23049v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:56.437392",
    "title": "Accelerating Language Model Workflows with Prompt Choreography"
  },
  "https://arxiv.org/abs/2512.22195v1": {
    "content_path": "../data/contents/2512.22195v1.md",
    "pdf_path": "../data/contents/2512.22195v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:56.715662",
    "title": "MatKV: Trading Compute for Flash Storage in LLM Inference"
  },
  "https://arxiv.org/abs/2512.23043v1": {
    "content_path": "../data/contents/2512.23043v1.md",
    "pdf_path": "../data/contents/2512.23043v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:58.892067",
    "title": "Mechanistic Analysis of Circuit Preservation in Federated Learning"
  },
  "https://arxiv.org/abs/2512.18129v2": {
    "content_path": "../data/contents/2512.18129v2.md",
    "pdf_path": "../data/contents/2512.18129v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:59.591791",
    "title": "TraCeR: Transformer-Based Competing Risk Analysis with Longitudinal Covariates"
  },
  "https://arxiv.org/abs/2512.23036v1": {
    "content_path": "../data/contents/2512.23036v1.md",
    "pdf_path": "../data/contents/2512.23036v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:41:59.939651",
    "title": "Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education"
  },
  "https://arxiv.org/abs/2512.22183v1": {
    "content_path": "../data/contents/2512.22183v1.md",
    "pdf_path": "../data/contents/2512.22183v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:03.400393",
    "title": "Unbiased Visual Reasoning with Controlled Visual Inputs"
  },
  "https://arxiv.org/abs/2512.23032v1": {
    "content_path": "../data/contents/2512.23032v1.md",
    "pdf_path": "../data/contents/2512.23032v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:04.174652",
    "title": "Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization"
  },
  "https://arxiv.org/abs/2512.22180v1": {
    "content_path": "../data/contents/2512.22180v1.md",
    "pdf_path": "../data/contents/2512.22180v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:04.979284",
    "title": "iOS as Acceleration"
  },
  "https://arxiv.org/abs/2512.23029v1": {
    "content_path": "../data/contents/2512.23029v1.md",
    "pdf_path": "../data/contents/2512.23029v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:05.790158",
    "title": "Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware"
  },
  "https://arxiv.org/abs/2512.22179v1": {
    "content_path": "../data/contents/2512.22179v1.md",
    "pdf_path": "../data/contents/2512.22179v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:06.216037",
    "title": "Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection"
  },
  "https://arxiv.org/abs/2512.23028v1": {
    "content_path": "../data/contents/2512.23028v1.md",
    "pdf_path": "../data/contents/2512.23028v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:06.605946",
    "title": "An Architecture-Led Hybrid Report on Body Language Detection Project"
  },
  "https://arxiv.org/abs/2512.22178v1": {
    "content_path": "../data/contents/2512.22178v1.md",
    "pdf_path": "../data/contents/2512.22178v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:08.492184",
    "title": "Wireless Traffic Prediction with Large Language Model"
  },
  "https://arxiv.org/abs/2512.23025v1": {
    "content_path": "../data/contents/2512.23025v1.md",
    "pdf_path": "../data/contents/2512.23025v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:09.404577",
    "title": "LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models"
  },
  "https://arxiv.org/abs/2512.22174v1": {
    "content_path": "../data/contents/2512.22174v1.md",
    "pdf_path": "../data/contents/2512.22174v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:10.553568",
    "title": "BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs"
  },
  "https://arxiv.org/abs/2512.23021v1": {
    "content_path": "../data/contents/2512.23021v1.md",
    "pdf_path": "../data/contents/2512.23021v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:11.269130",
    "title": "Squeezed Covariance Matrix Estimation: Analytic Eigenvalue Control"
  },
  "https://arxiv.org/abs/2512.16531v4": {
    "content_path": "../data/contents/2512.16531v4.md",
    "pdf_path": "../data/contents/2512.16531v4.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:11.740901",
    "title": "Scaling Laws for Energy Efficiency of Local LLMs"
  },
  "https://arxiv.org/abs/2512.23017v1": {
    "content_path": "../data/contents/2512.23017v1.md",
    "pdf_path": "../data/contents/2512.23017v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:13.191407",
    "title": "Merge before Forget: A Single LoRA Continual Learning via Continual Merging"
  },
  "https://arxiv.org/abs/2512.22173v1": {
    "content_path": "../data/contents/2512.22173v1.md",
    "pdf_path": "../data/contents/2512.22173v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:14.542898",
    "title": "AiiDAlab: on the route to accelerate science"
  },
  "https://arxiv.org/abs/2512.23014v1": {
    "content_path": "../data/contents/2512.23014v1.md",
    "pdf_path": "../data/contents/2512.23014v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:15.328563",
    "title": "Improving Generalization in LLM Structured Pruning via Function-Aware Neuron Grouping"
  },
  "https://arxiv.org/abs/2512.17956v2": {
    "content_path": "../data/contents/2512.17956v2.md",
    "pdf_path": "../data/contents/2512.17956v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:15.993056",
    "title": "Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration"
  },
  "https://arxiv.org/abs/2501.07391": {
    "content_path": "../data/contents/2501.07391.md",
    "pdf_path": "../data/contents/2501.07391.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:16.616306",
    "title": "Enhancing Retrieval-Augmented Generation: A Study of Best Practices"
  },
  "https://arxiv.org/abs/2512.22999v1": {
    "content_path": "../data/contents/2512.22999v1.md",
    "pdf_path": "../data/contents/2512.22999v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:18.931987",
    "title": "JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference"
  },
  "https://arxiv.org/abs/2512.22170v1": {
    "content_path": "../data/contents/2512.22170v1.md",
    "pdf_path": "../data/contents/2512.22170v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:20.847460",
    "title": "SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models"
  },
  "https://arxiv.org/abs/2512.22991v1": {
    "content_path": "../data/contents/2512.22991v1.md",
    "pdf_path": "../data/contents/2512.22991v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:21.325813",
    "title": "Fusion or Confusion? Multimodal Complexity Is Not All You Need"
  },
  "https://arxiv.org/abs/2512.22168v1": {
    "content_path": "../data/contents/2512.22168v1.md",
    "pdf_path": "../data/contents/2512.22168v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:21.740206",
    "title": "TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures"
  },
  "https://arxiv.org/abs/2512.22966v1": {
    "content_path": "../data/contents/2512.22966v1.md",
    "pdf_path": "../data/contents/2512.22966v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:22.709870",
    "title": "Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks"
  },
  "https://arxiv.org/abs/2512.22167v1": {
    "content_path": "../data/contents/2512.22167v1.md",
    "pdf_path": "../data/contents/2512.22167v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:23.100972",
    "title": "IANEC: Digital Forensic Investigation of Contemporary Writers' Archives"
  },
  "https://arxiv.org/abs/2512.22955v1": {
    "content_path": "../data/contents/2512.22955v1.md",
    "pdf_path": "../data/contents/2512.22955v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:24.047301",
    "title": "Diversity or Precision? A Deep Dive into Next Token Prediction"
  },
  "https://arxiv.org/abs/2512.14244v3": {
    "content_path": "../data/contents/2512.14244v3.md",
    "pdf_path": "../data/contents/2512.14244v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:24.580637",
    "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition"
  },
  "https://arxiv.org/abs/2512.22953v1": {
    "content_path": "../data/contents/2512.22953v1.md",
    "pdf_path": "../data/contents/2512.22953v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:25.048169",
    "title": "APO: Alpha-Divergence Preference Optimization"
  },
  "https://arxiv.org/abs/2512.14044v2": {
    "content_path": "../data/contents/2512.14044v2.md",
    "pdf_path": "../data/contents/2512.14044v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:27.449005",
    "title": "OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving"
  },
  "https://arxiv.org/abs/2512.22944v1": {
    "content_path": "../data/contents/2512.22944v1.md",
    "pdf_path": "../data/contents/2512.22944v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:27.974381",
    "title": "Multiple Token Divergence: Measuring and Steering In-Context Computation Density"
  },
  "https://arxiv.org/abs/2512.22154v1": {
    "content_path": "../data/contents/2512.22154v1.md",
    "pdf_path": "../data/contents/2512.22154v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:28.745015",
    "title": "Practical challenges of control monitoring in frontier AI deployments"
  },
  "https://arxiv.org/abs/2312.10997": {
    "content_path": "../data/contents/2312.10997.md",
    "pdf_path": "../data/contents/2312.10997.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:29.243060",
    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey"
  },
  "https://arxiv.org/abs/2512.22941v1": {
    "content_path": "../data/contents/2512.22941v1.md",
    "pdf_path": "../data/contents/2512.22941v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:35.313754",
    "title": "Heterogeneity in Multi-Agent Reinforcement Learning"
  },
  "https://arxiv.org/abs/2512.22149v1": {
    "content_path": "../data/contents/2512.22149v1.md",
    "pdf_path": "../data/contents/2512.22149v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:35.704619",
    "title": "Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments"
  },
  "https://arxiv.org/abs/2408.08921": {
    "content_path": "../data/contents/2408.08921.md",
    "pdf_path": "../data/contents/2408.08921.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:36.603320",
    "title": "Graph Retrieval-Augmented Generation: A Survey"
  },
  "https://arxiv.org/abs/2512.22933v1": {
    "content_path": "../data/contents/2512.22933v1.md",
    "pdf_path": "../data/contents/2512.22933v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:37.859647",
    "title": "Multimodal Fact-Checking: An Agent-based Approach"
  },
  "https://arxiv.org/abs/2512.22147v1": {
    "content_path": "../data/contents/2512.22147v1.md",
    "pdf_path": "../data/contents/2512.22147v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:38.235976",
    "title": "GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs"
  },
  "https://arxiv.org/abs/2512.22931v1": {
    "content_path": "../data/contents/2512.22931v1.md",
    "pdf_path": "../data/contents/2512.22931v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:38.537007",
    "title": "Geometric Structural Knowledge Graph Foundation Model"
  },
  "https://arxiv.org/abs/2512.23703v1": {
    "content_path": "../data/contents/2512.23703v1.md",
    "pdf_path": "../data/contents/2512.23703v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:39.031102",
    "title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation"
  },
  "https://arxiv.org/abs/2512.22925v1": {
    "content_path": "../data/contents/2512.22925v1.md",
    "pdf_path": "../data/contents/2512.22925v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:39.398558",
    "title": "Argus: Token Aware Distributed LLM Inference Optimization"
  },
  "https://arxiv.org/abs/2512.23646v1": {
    "content_path": "../data/contents/2512.23646v1.md",
    "pdf_path": "../data/contents/2512.23646v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:39.852493",
    "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding"
  },
  "https://arxiv.org/abs/2512.22903v1": {
    "content_path": "../data/contents/2512.22903v1.md",
    "pdf_path": "../data/contents/2512.22903v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:40.258536",
    "title": "Debugging Tabular Log as Dynamic Graphs"
  },
  "https://arxiv.org/abs/2512.23592v1": {
    "content_path": "../data/contents/2512.23592v1.md",
    "pdf_path": "../data/contents/2512.23592v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:40.758900",
    "title": "Same or Not? Enhancing Visual Perception in Vision-Language Models"
  },
  "https://arxiv.org/abs/2512.22899v1": {
    "content_path": "../data/contents/2512.22899v1.md",
    "pdf_path": "../data/contents/2512.22899v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:41.186448",
    "title": "HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery"
  },
  "https://arxiv.org/abs/2512.23573v1": {
    "content_path": "../data/contents/2512.23573v1.md",
    "pdf_path": "../data/contents/2512.23573v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:41.604499",
    "title": "ProGuard: Towards Proactive Multimodal Safeguard"
  },
  "https://arxiv.org/abs/2512.22895v1": {
    "content_path": "../data/contents/2512.22895v1.md",
    "pdf_path": "../data/contents/2512.22895v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:42.724151",
    "title": "SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning"
  },
  "https://arxiv.org/abs/2512.23568v1": {
    "content_path": "../data/contents/2512.23568v1.md",
    "pdf_path": "../data/contents/2512.23568v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:43.188120",
    "title": "ThinkGen: Generalized Thinking for Visual Generation"
  },
  "https://arxiv.org/abs/2512.22894v1": {
    "content_path": "../data/contents/2512.22894v1.md",
    "pdf_path": "../data/contents/2512.22894v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:43.524517",
    "title": "DECEPTICON: How Dark Patterns Manipulate Web Agents"
  },
  "https://arxiv.org/abs/2512.23546v1": {
    "content_path": "../data/contents/2512.23546v1.md",
    "pdf_path": "../data/contents/2512.23546v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:43.937537",
    "title": "PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation"
  },
  "https://arxiv.org/abs/2512.22886v1": {
    "content_path": "../data/contents/2512.22886v1.md",
    "pdf_path": "../data/contents/2512.22886v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:44.846168",
    "title": "Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral"
  },
  "https://arxiv.org/abs/2512.23511v1": {
    "content_path": "../data/contents/2512.23511v1.md",
    "pdf_path": "../data/contents/2512.23511v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:46.454254",
    "title": "Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving"
  },
  "https://arxiv.org/abs/2512.22883v1": {
    "content_path": "../data/contents/2512.22883v1.md",
    "pdf_path": "../data/contents/2512.22883v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:47.141004",
    "title": "Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations"
  },
  "https://arxiv.org/abs/2512.23502v1": {
    "content_path": "../data/contents/2512.23502v1.md",
    "pdf_path": "../data/contents/2512.23502v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:47.597019",
    "title": "Hierarchical Decision Mamba Meets Agentic AI: A Novel Approach for RAN Slicing in 6G"
  },
  "https://arxiv.org/abs/2512.22876v1": {
    "content_path": "../data/contents/2512.22876v1.md",
    "pdf_path": "../data/contents/2512.22876v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:48.092832",
    "title": "Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks"
  },
  "https://arxiv.org/abs/2512.23483v1": {
    "content_path": "../data/contents/2512.23483v1.md",
    "pdf_path": "../data/contents/2512.23483v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:48.627120",
    "title": "TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding"
  },
  "https://arxiv.org/abs/2512.22857v1": {
    "content_path": "../data/contents/2512.22857v1.md",
    "pdf_path": "../data/contents/2512.22857v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:49.014946",
    "title": "AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning"
  },
  "https://arxiv.org/abs/2512.23454v1": {
    "content_path": "../data/contents/2512.23454v1.md",
    "pdf_path": "../data/contents/2512.23454v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:49.302955",
    "title": "Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin"
  },
  "https://arxiv.org/abs/2512.22846v1": {
    "content_path": "../data/contents/2512.22846v1.md",
    "pdf_path": "../data/contents/2512.22846v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:49.721969",
    "title": "Causal-Policy Forest for End-to-End Policy Learning"
  },
  "https://arxiv.org/abs/2512.23365v1": {
    "content_path": "../data/contents/2512.23365v1.md",
    "pdf_path": "../data/contents/2512.23365v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:50.200506",
    "title": "SpatialMosaic: A Multiview VLM Dataset for Partial Visibility"
  },
  "https://arxiv.org/abs/2512.22838v1": {
    "content_path": "../data/contents/2512.22838v1.md",
    "pdf_path": "../data/contents/2512.22838v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:50.625764",
    "title": "OrchANN: A Unified I/O Orchestration Framework for Skewed Out-of-Core Vector Search"
  },
  "https://arxiv.org/abs/2512.23333v1": {
    "content_path": "../data/contents/2512.23333v1.md",
    "pdf_path": "../data/contents/2512.23333v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:50.925479",
    "title": "CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation"
  },
  "https://arxiv.org/abs/2512.01248": {
    "content_path": "../data/contents/2512.01248.md",
    "pdf_path": "../data/contents/2512.01248.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:53.026455",
    "title": "TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition"
  },
  "https://arxiv.org/abs/2512.22832v1": {
    "content_path": "../data/contents/2512.22832v1.md",
    "pdf_path": "../data/contents/2512.22832v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:53.618664",
    "title": "MARPO: A Reflective Policy Optimization for Multi Agent Reinforcement Learning"
  },
  "https://arxiv.org/abs/2512.23327v1": {
    "content_path": "../data/contents/2512.23327v1.md",
    "pdf_path": "../data/contents/2512.23327v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:54.878742",
    "title": "An Empirical Study of Generative AI Adoption in Software Engineering"
  },
  "https://arxiv.org/abs/2512.22827v1": {
    "content_path": "../data/contents/2512.22827v1.md",
    "pdf_path": "../data/contents/2512.22827v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:56.011857",
    "title": "FasterPy: An LLM-based Code Execution Efficiency Optimization Framework"
  },
  "https://arxiv.org/abs/2512.23243v1": {
    "content_path": "../data/contents/2512.23243v1.md",
    "pdf_path": "../data/contents/2512.23243v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:56.320959",
    "title": "Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism"
  },
  "https://arxiv.org/abs/2512.22814v1": {
    "content_path": "../data/contents/2512.22814v1.md",
    "pdf_path": "../data/contents/2512.22814v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:57.737997",
    "title": "Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models"
  },
  "https://arxiv.org/abs/2512.22804v1": {
    "content_path": "../data/contents/2512.22804v1.md",
    "pdf_path": "../data/contents/2512.22804v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:58.708231",
    "title": "MoR: Mixture Of Representations For Mixed-Precision Training"
  },
  "https://arxiv.org/abs/2512.22795v1": {
    "content_path": "../data/contents/2512.22795v1.md",
    "pdf_path": "../data/contents/2512.22795v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:42:59.387645",
    "title": "CNSight: Evaluation of Clinical Note Segmentation Tools"
  },
  "https://arxiv.org/abs/2512.22790v1": {
    "content_path": "../data/contents/2512.22790v1.md",
    "pdf_path": "../data/contents/2512.22790v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:00.498457",
    "title": "ChatGraPhT: A Visual Conversation Interface for Multi-Path Reflection with Agentic LLM Support"
  },
  "https://arxiv.org/abs/2512.22777v1": {
    "content_path": "../data/contents/2512.22777v1.md",
    "pdf_path": "../data/contents/2512.22777v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:00.811020",
    "title": "Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning"
  },
  "https://arxiv.org/abs/2512.22768v1": {
    "content_path": "../data/contents/2512.22768v1.md",
    "pdf_path": "../data/contents/2512.22768v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:03.325532",
    "title": "Understanding the Mechanisms of Fast Hyperparameter Transfer"
  },
  "https://arxiv.org/abs/2512.22749v1": {
    "content_path": "../data/contents/2512.22749v1.md",
    "pdf_path": "../data/contents/2512.22749v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:04.020510",
    "title": "From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms"
  },
  "https://arxiv.org/abs/2512.22744v1": {
    "content_path": "../data/contents/2512.22744v1.md",
    "pdf_path": "../data/contents/2512.22744v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:04.586806",
    "title": "Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL"
  },
  "https://arxiv.org/abs/2512.22742v1": {
    "content_path": "../data/contents/2512.22742v1.md",
    "pdf_path": "../data/contents/2512.22742v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:04.951497",
    "title": "Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning"
  },
  "https://arxiv.org/abs/2512.22738v1": {
    "content_path": "../data/contents/2512.22738v1.md",
    "pdf_path": "../data/contents/2512.22738v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:05.777019",
    "title": "Harnessing Large Language Models for Biomedical Named Entity Recognition"
  },
  "https://arxiv.org/abs/2512.22737v1": {
    "content_path": "../data/contents/2512.22737v1.md",
    "pdf_path": "../data/contents/2512.22737v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:06.177340",
    "title": "WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference"
  },
  "https://arxiv.org/abs/2512.22733v1": {
    "content_path": "../data/contents/2512.22733v1.md",
    "pdf_path": "../data/contents/2512.22733v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:07.081325",
    "title": "FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents"
  },
  "https://arxiv.org/abs/2412.08179": {
    "content_path": "../data/contents/2412.08179.md",
    "pdf_path": "../data/contents/2412.08179.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:07.845416",
    "title": "RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial Analysis"
  },
  "https://arxiv.org/abs/2512.22725v1": {
    "content_path": "../data/contents/2512.22725v1.md",
    "pdf_path": "../data/contents/2512.22725v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:09.721726",
    "title": "Mitigating Social Desirability Bias in Random Silicon Sampling"
  },
  "https://arxiv.org/abs/2512.22716v1": {
    "content_path": "../data/contents/2512.22716v1.md",
    "pdf_path": "../data/contents/2512.22716v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:10.486938",
    "title": "Memento-II: Learning by Stateful Reflective Memory"
  },
  "https://arxiv.org/abs/2512.22712v1": {
    "content_path": "../data/contents/2512.22712v1.md",
    "pdf_path": "../data/contents/2512.22712v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:10.954338",
    "title": "Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages"
  },
  "https://arxiv.org/abs/2512.22702v1": {
    "content_path": "../data/contents/2512.22702v1.md",
    "pdf_path": "../data/contents/2512.22702v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:11.619997",
    "title": "What Matters in Deep Learning for Time Series Forecasting?"
  },
  "https://arxiv.org/abs/2512.22697v1": {
    "content_path": "../data/contents/2512.22697v1.md",
    "pdf_path": "../data/contents/2512.22697v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:12.155074",
    "title": "Canonical correlation regression with noisy data"
  },
  "https://arxiv.org/abs/2512.22695v1": {
    "content_path": "../data/contents/2512.22695v1.md",
    "pdf_path": "../data/contents/2512.22695v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:12.514804",
    "title": "Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference"
  },
  "https://arxiv.org/abs/2512.22682v1": {
    "content_path": "../data/contents/2512.22682v1.md",
    "pdf_path": "../data/contents/2512.22682v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:13.015083",
    "title": "Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency"
  },
  "https://arxiv.org/abs/2512.22673v1": {
    "content_path": "../data/contents/2512.22673v1.md",
    "pdf_path": "../data/contents/2512.22673v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:13.316859",
    "title": "TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning"
  },
  "https://arxiv.org/abs/2512.22671v1": {
    "content_path": "../data/contents/2512.22671v1.md",
    "pdf_path": "../data/contents/2512.22671v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:15.789414",
    "title": "Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2"
  },
  "https://arxiv.org/abs/2512.22664v1": {
    "content_path": "../data/contents/2512.22664v1.md",
    "pdf_path": "../data/contents/2512.22664v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:17.126126",
    "title": "Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains"
  },
  "https://arxiv.org/abs/2512.22650v1": {
    "content_path": "../data/contents/2512.22650v1.md",
    "pdf_path": "../data/contents/2512.22650v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:18.012835",
    "title": "Scaling Unverifiable Rewards: A Case Study on Visual Insights"
  },
  "https://arxiv.org/abs/2512.22631v1": {
    "content_path": "../data/contents/2512.22631v1.md",
    "pdf_path": "../data/contents/2512.22631v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:18.236554",
    "title": "Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs"
  },
  "https://arxiv.org/abs/2512.22630v1": {
    "content_path": "../data/contents/2512.22630v1.md",
    "pdf_path": "../data/contents/2512.22630v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:18.688173",
    "title": "On the Role of Discreteness in Diffusion LLMs"
  },
  "https://arxiv.org/abs/2512.23707v1": {
    "content_path": "../data/contents/2512.23707v1.md",
    "pdf_path": "../data/contents/2512.23707v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:19.804181",
    "title": "Training AI Co-Scientists Using Rubric Rewards"
  },
  "https://arxiv.org/abs/2512.22629v1": {
    "content_path": "../data/contents/2512.22629v1.md",
    "pdf_path": "../data/contents/2512.22629v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:21.094858",
    "title": "DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation"
  },
  "https://arxiv.org/abs/2512.23701v1": {
    "content_path": "../data/contents/2512.23701v1.md",
    "pdf_path": "../data/contents/2512.23701v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:21.465125",
    "title": "Eliciting Behaviors in Multi-Turn Conversations"
  },
  "https://arxiv.org/abs/2512.22628v1": {
    "content_path": "../data/contents/2512.22628v1.md",
    "pdf_path": "../data/contents/2512.22628v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:22.256930",
    "title": "M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation"
  },
  "https://arxiv.org/abs/2512.23694v1": {
    "content_path": "../data/contents/2512.23694v1.md",
    "pdf_path": "../data/contents/2512.23694v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:22.550867",
    "title": "Bellman Calibration for V-Learning in Offline Reinforcement Learning"
  },
  "https://arxiv.org/abs/2512.22627v1": {
    "content_path": "../data/contents/2512.22627v1.md",
    "pdf_path": "../data/contents/2512.22627v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:22.958771",
    "title": "Chain-of-thought Reviewing and Correction for Time Series Question Answering"
  },
  "https://arxiv.org/abs/2512.23693v1": {
    "content_path": "../data/contents/2512.23693v1.md",
    "pdf_path": "../data/contents/2512.23693v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:23.207174",
    "title": "Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans"
  },
  "https://arxiv.org/abs/2512.22625v1": {
    "content_path": "../data/contents/2512.22625v1.md",
    "pdf_path": "../data/contents/2512.22625v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:23.992663",
    "title": "The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?"
  },
  "https://arxiv.org/abs/2512.23686v1": {
    "content_path": "../data/contents/2512.23686v1.md",
    "pdf_path": "../data/contents/2512.23686v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:24.262088",
    "title": "PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech"
  },
  "https://arxiv.org/abs/2512.22623v1": {
    "content_path": "../data/contents/2512.22623v1.md",
    "pdf_path": "../data/contents/2512.22623v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:24.967018",
    "title": "Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback"
  },
  "https://arxiv.org/abs/2512.23684v1": {
    "content_path": "../data/contents/2512.23684v1.md",
    "pdf_path": "../data/contents/2512.23684v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:25.193499",
    "title": "Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing"
  },
  "https://arxiv.org/abs/2512.22615v1": {
    "content_path": "../data/contents/2512.22615v1.md",
    "pdf_path": "../data/contents/2512.22615v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:25.982312",
    "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone"
  },
  "https://arxiv.org/abs/2512.23676v1": {
    "content_path": "../data/contents/2512.23676v1.md",
    "pdf_path": "../data/contents/2512.23676v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:26.773522",
    "title": "Web World Models"
  },
  "https://arxiv.org/abs/2512.22608v1": {
    "content_path": "../data/contents/2512.22608v1.md",
    "pdf_path": "../data/contents/2512.22608v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:27.488773",
    "title": "LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation"
  },
  "https://arxiv.org/abs/2512.23675v1": {
    "content_path": "../data/contents/2512.23675v1.md",
    "pdf_path": "../data/contents/2512.23675v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:27.837917",
    "title": "End-to-End Test-Time Training for Long Context"
  },
  "https://arxiv.org/abs/2512.22587v1": {
    "content_path": "../data/contents/2512.22587v1.md",
    "pdf_path": "../data/contents/2512.22587v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:28.308821",
    "title": "On Admissible Rank-based Input Normalization Operators"
  },
  "https://arxiv.org/abs/2512.23671v1": {
    "content_path": "../data/contents/2512.23671v1.md",
    "pdf_path": "../data/contents/2512.23671v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:29.461816",
    "title": "Calibrated Multi-Level Quantile Forecasting"
  },
  "https://arxiv.org/abs/2512.22568v1": {
    "content_path": "../data/contents/2512.22568v1.md",
    "pdf_path": "../data/contents/2512.22568v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:29.784876",
    "title": "Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI"
  },
  "https://arxiv.org/abs/2510.08483": {
    "content_path": "../data/contents/2510.08483.md",
    "pdf_path": "../data/contents/2510.08483.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:30.223190",
    "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy"
  },
  "https://arxiv.org/abs/2512.23670v1": {
    "content_path": "../data/contents/2512.23670v1.md",
    "pdf_path": "../data/contents/2512.23670v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:30.578660",
    "title": "Random Controlled Differential Equations"
  },
  "https://arxiv.org/abs/2512.22562v1": {
    "content_path": "../data/contents/2512.22562v1.md",
    "pdf_path": "../data/contents/2512.22562v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:31.162198",
    "title": "Learning When Not to Attend Globally"
  },
  "https://arxiv.org/abs/2511.02309": {
    "content_path": "../data/contents/2511.02309.md",
    "pdf_path": "../data/contents/2511.02309.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:31.760848",
    "title": "The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute"
  },
  "https://arxiv.org/abs/2512.23647v1": {
    "content_path": "../data/contents/2512.23647v1.md",
    "pdf_path": "../data/contents/2512.23647v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:32.078439",
    "title": "Nested Browser-Use Learning for Agentic Information Seeking"
  },
  "https://arxiv.org/abs/2512.22560v1": {
    "content_path": "../data/contents/2512.22560v1.md",
    "pdf_path": "../data/contents/2512.22560v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:32.900516",
    "title": "RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure"
  },
  "https://arxiv.org/abs/2511.03408": {
    "content_path": "../data/contents/2511.03408.md",
    "pdf_path": "../data/contents/2511.03408.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:33.299336",
    "title": "Efficient Reasoning via Thought-Training and Thought-Free Inference"
  },
  "https://arxiv.org/abs/2512.23631v1": {
    "content_path": "../data/contents/2512.23631v1.md",
    "pdf_path": "../data/contents/2512.23631v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:33.566380",
    "title": "BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization"
  },
  "https://arxiv.org/abs/2512.22550v1": {
    "content_path": "../data/contents/2512.22550v1.md",
    "pdf_path": "../data/contents/2512.22550v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:34.391057",
    "title": "TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting"
  },
  "https://arxiv.org/abs/2512.23126": {
    "content_path": "../data/contents/2512.23126.md",
    "pdf_path": "../data/contents/2512.23126.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:35.407699",
    "title": "InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization"
  },
  "https://arxiv.org/abs/2512.23617v1": {
    "content_path": "../data/contents/2512.23617v1.md",
    "pdf_path": "../data/contents/2512.23617v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:35.760582",
    "title": "Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning"
  },
  "https://arxiv.org/abs/2512.22545v1": {
    "content_path": "../data/contents/2512.22545v1.md",
    "pdf_path": "../data/contents/2512.22545v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:38.485695",
    "title": "Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains"
  },
  "https://arxiv.org/abs/2512.23611v1": {
    "content_path": "../data/contents/2512.23611v1.md",
    "pdf_path": "../data/contents/2512.23611v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:39.031874",
    "title": "Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing"
  },
  "https://arxiv.org/abs/2512.22536v1": {
    "content_path": "../data/contents/2512.22536v1.md",
    "pdf_path": "../data/contents/2512.22536v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:42.461271",
    "title": "CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation"
  },
  "https://arxiv.org/abs/2512.23602v1": {
    "content_path": "../data/contents/2512.23602v1.md",
    "pdf_path": "../data/contents/2512.23602v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:42.706552",
    "title": "Distribution-Free Process Monitoring with Conformal Prediction"
  },
  "https://arxiv.org/abs/2512.22511v1": {
    "content_path": "../data/contents/2512.22511v1.md",
    "pdf_path": "../data/contents/2512.22511v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:43:50.042708",
    "title": "Decomposing Task Vectors for Refined Model Editing"
  },
  "https://arxiv.org/abs/2512.18311": {
    "content_path": "../data/contents/2512.18311.md",
    "pdf_path": "../data/contents/2512.18311.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:02.437351",
    "title": "Monitoring Monitorability"
  },
  "https://arxiv.org/abs/2512.23601v1": {
    "content_path": "../data/contents/2512.23601v1.md",
    "pdf_path": "../data/contents/2512.23601v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:02.892824",
    "title": "Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation"
  },
  "https://arxiv.org/abs/2512.22508v1": {
    "content_path": "../data/contents/2512.22508v1.md",
    "pdf_path": "../data/contents/2512.22508v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:03.226802",
    "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals"
  },
  "https://arxiv.org/abs/2512.23596v1": {
    "content_path": "../data/contents/2512.23596v1.md",
    "pdf_path": "../data/contents/2512.23596v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:03.817380",
    "title": "The Nonstationarity-Complexity Tradeoff in Return Prediction"
  },
  "https://arxiv.org/abs/2512.22496v1": {
    "content_path": "../data/contents/2512.22496v1.md",
    "pdf_path": "../data/contents/2512.22496v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:04.510092",
    "title": "Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring"
  },
  "https://arxiv.org/abs/2510.11170": {
    "content_path": "../data/contents/2510.11170.md",
    "pdf_path": "../data/contents/2510.11170.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:04.810788",
    "title": "EAGER: Entropy-Aware GEneRation for Adaptive Inference-Time Scaling"
  },
  "https://arxiv.org/abs/2512.23578v1": {
    "content_path": "../data/contents/2512.23578v1.md",
    "pdf_path": "../data/contents/2512.23578v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:05.103930",
    "title": "Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models"
  },
  "https://arxiv.org/abs/2512.22495v1": {
    "content_path": "../data/contents/2512.22495v1.md",
    "pdf_path": "../data/contents/2512.22495v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:05.824717",
    "title": "The Quest for Winning Tickets in Low-Rank Adapters"
  },
  "https://arxiv.org/abs/2509.25420": {
    "content_path": "../data/contents/2509.25420.md",
    "pdf_path": "../data/contents/2509.25420.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:07.270271",
    "title": "Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search"
  },
  "https://arxiv.org/abs/2512.23572v1": {
    "content_path": "../data/contents/2512.23572v1.md",
    "pdf_path": "../data/contents/2512.23572v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:07.574002",
    "title": "Instruction-Following Evaluation of Large Vision-Language Models"
  },
  "https://arxiv.org/abs/2512.22492v1": {
    "content_path": "../data/contents/2512.22492v1.md",
    "pdf_path": "../data/contents/2512.22492v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:07.942656",
    "title": "Role-Based Fault Tolerance System for LLM RL Post-Training"
  },
  "https://arxiv.org/abs/2508.05618": {
    "content_path": "../data/contents/2508.05618.md",
    "pdf_path": "../data/contents/2508.05618.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:08.660796",
    "title": "Learning to Reason for Factuality"
  },
  "https://arxiv.org/abs/2512.23565v1": {
    "content_path": "../data/contents/2512.23565v1.md",
    "pdf_path": "../data/contents/2512.23565v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:08.957546",
    "title": "RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature"
  },
  "https://arxiv.org/abs/2512.22473v1": {
    "content_path": "../data/contents/2512.22473v1.md",
    "pdf_path": "../data/contents/2512.22473v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:09.540026",
    "title": "Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds"
  },
  "https://arxiv.org/abs/2508.05615": {
    "content_path": "../data/contents/2508.05615.md",
    "pdf_path": "../data/contents/2508.05615.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:13.111695",
    "title": "Test-Time Reinforcement Learning for GUI Grounding via Region Consistency"
  },
  "https://arxiv.org/abs/2512.23562v1": {
    "content_path": "../data/contents/2512.23562v1.md",
    "pdf_path": "../data/contents/2512.23562v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:13.506173",
    "title": "VL-RouterBench: A Benchmark for Vision-Language Model Routing"
  },
  "https://arxiv.org/abs/2512.22471v1": {
    "content_path": "../data/contents/2512.22471v1.md",
    "pdf_path": "../data/contents/2512.22471v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:14.872737",
    "title": "The Bayesian Geometry of Transformer Attention"
  },
  "https://arxiv.org/abs/2512.23557v1": {
    "content_path": "../data/contents/2512.23557v1.md",
    "pdf_path": "../data/contents/2512.23557v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:15.104967",
    "title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks"
  },
  "https://arxiv.org/abs/2512.22470v1": {
    "content_path": "../data/contents/2512.22470v1.md",
    "pdf_path": "../data/contents/2512.22470v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:15.327781",
    "title": "DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior"
  },
  "https://arxiv.org/abs/2505.14656": {
    "content_path": "../data/contents/2505.14656.md",
    "pdf_path": "../data/contents/2505.14656.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:15.621399",
    "title": "Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning"
  },
  "https://arxiv.org/abs/2512.23547v1": {
    "content_path": "../data/contents/2512.23547v1.md",
    "pdf_path": "../data/contents/2512.23547v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:15.887546",
    "title": "Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs"
  },
  "https://arxiv.org/abs/2512.22467v1": {
    "content_path": "../data/contents/2512.22467v1.md",
    "pdf_path": "../data/contents/2512.22467v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:16.212967",
    "title": "GLUE: Gradient-free Learning to Unify Experts"
  },
  "https://arxiv.org/abs/2506.06017": {
    "content_path": "../data/contents/2506.06017.md",
    "pdf_path": "../data/contents/2506.06017.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:16.681547",
    "title": "AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search"
  },
  "https://arxiv.org/abs/2512.23545v1": {
    "content_path": "../data/contents/2512.23545v1.md",
    "pdf_path": "../data/contents/2512.23545v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:17.163279",
    "title": "PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis"
  },
  "https://arxiv.org/abs/2512.22457v1": {
    "content_path": "../data/contents/2512.22457v1.md",
    "pdf_path": "../data/contents/2512.22457v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:17.780786",
    "title": "A Real-Time System to Populate FRA Form 57 from News"
  },
  "https://arxiv.org/abs/2512.23524v1": {
    "content_path": "../data/contents/2512.23524v1.md",
    "pdf_path": "../data/contents/2512.23524v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:18.624315",
    "title": "Trustworthy Machine Learning under Distribution Shifts"
  },
  "https://arxiv.org/abs/2512.22455v1": {
    "content_path": "../data/contents/2512.22455v1.md",
    "pdf_path": "../data/contents/2512.22455v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:19.048957",
    "title": "AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing"
  },
  "https://arxiv.org/abs/2512.23518v1": {
    "content_path": "../data/contents/2512.23518v1.md",
    "pdf_path": "../data/contents/2512.23518v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:19.553005",
    "title": "Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias"
  },
  "https://arxiv.org/abs/2512.22443v1": {
    "content_path": "../data/contents/2512.22443v1.md",
    "pdf_path": "../data/contents/2512.22443v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:19.975006",
    "title": "Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models"
  },
  "https://arxiv.org/abs/2512.23515v1": {
    "content_path": "../data/contents/2512.23515v1.md",
    "pdf_path": "../data/contents/2512.23515v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:20.406454",
    "title": "Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning"
  },
  "https://arxiv.org/abs/2512.22442v1": {
    "content_path": "../data/contents/2512.22442v1.md",
    "pdf_path": "../data/contents/2512.22442v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:20.820128",
    "title": "HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG"
  },
  "https://arxiv.org/abs/2512.23512v1": {
    "content_path": "../data/contents/2512.23512v1.md",
    "pdf_path": "../data/contents/2512.23512v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:21.323725",
    "title": "UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?"
  },
  "https://arxiv.org/abs/2512.22435v1": {
    "content_path": "../data/contents/2512.22435v1.md",
    "pdf_path": "../data/contents/2512.22435v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:22.126983",
    "title": "AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience"
  },
  "https://arxiv.org/abs/2512.23504v1": {
    "content_path": "../data/contents/2512.23504v1.md",
    "pdf_path": "../data/contents/2512.23504v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:22.422151",
    "title": "Automatic Detection of Complex Quotation Patterns in Aggadic Literature"
  },
  "https://arxiv.org/abs/2512.22431v1": {
    "content_path": "../data/contents/2512.22431v1.md",
    "pdf_path": "../data/contents/2512.22431v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:22.770665",
    "title": "Monadic Context Engineering"
  },
  "https://arxiv.org/abs/2512.23499v1": {
    "content_path": "../data/contents/2512.23499v1.md",
    "pdf_path": "../data/contents/2512.23499v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:23.606481",
    "title": "AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices"
  },
  "https://arxiv.org/abs/2512.22428v1": {
    "content_path": "../data/contents/2512.22428v1.md",
    "pdf_path": "../data/contents/2512.22428v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:24.202947",
    "title": "Causality-Inspired Safe Residual Correction for Multivariate Time Series"
  },
  "https://arxiv.org/abs/2512.23494v1": {
    "content_path": "../data/contents/2512.23494v1.md",
    "pdf_path": "../data/contents/2512.23494v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:24.752529",
    "title": "Optimal Configuration of API Resources in Cloud Native Computing"
  },
  "https://arxiv.org/abs/2512.22420v1": {
    "content_path": "../data/contents/2512.22420v1.md",
    "pdf_path": "../data/contents/2512.22420v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:25.653786",
    "title": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving"
  },
  "https://arxiv.org/abs/2512.23491v1": {
    "content_path": "../data/contents/2512.23491v1.md",
    "pdf_path": "../data/contents/2512.23491v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:26.686944",
    "title": "SPER: Accelerating Progressive Entity Resolution via Stochastic Bipartite Maximization"
  },
  "https://arxiv.org/abs/2512.22418v1": {
    "content_path": "../data/contents/2512.22418v1.md",
    "pdf_path": "../data/contents/2512.22418v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:28.549113",
    "title": "Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding"
  },
  "https://arxiv.org/abs/2512.23489v1": {
    "content_path": "../data/contents/2512.23489v1.md",
    "pdf_path": "../data/contents/2512.23489v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:28.862574",
    "title": "The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction"
  },
  "https://arxiv.org/abs/2512.22416v1": {
    "content_path": "../data/contents/2512.22416v1.md",
    "pdf_path": "../data/contents/2512.22416v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:29.534130",
    "title": "Hallucination Detection and Evaluation of Large Language Model"
  },
  "https://arxiv.org/abs/2512.23487v1": {
    "content_path": "../data/contents/2512.23487v1.md",
    "pdf_path": "../data/contents/2512.23487v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:29.992209",
    "title": "ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment"
  },
  "https://arxiv.org/abs/2512.22404v1": {
    "content_path": "../data/contents/2512.22404v1.md",
    "pdf_path": "../data/contents/2512.22404v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:30.239998",
    "title": "Mining the Gold: Student-AI Chat Logs as Rich Sources for Automated Knowledge Gap Detection"
  },
  "https://arxiv.org/abs/2512.23485v1": {
    "content_path": "../data/contents/2512.23485v1.md",
    "pdf_path": "../data/contents/2512.23485v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:30.650833",
    "title": "FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence"
  },
  "https://arxiv.org/abs/2512.22403v1": {
    "content_path": "../data/contents/2512.22403v1.md",
    "pdf_path": "../data/contents/2512.22403v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:31.005381",
    "title": "Active Nonparametric Two-Sample Testing by Betting on Heterogeneous Data Sources"
  },
  "https://arxiv.org/abs/2512.23480v1": {
    "content_path": "../data/contents/2512.23480v1.md",
    "pdf_path": "../data/contents/2512.23480v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:31.308632",
    "title": "Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation"
  },
  "https://arxiv.org/abs/2512.22402v1": {
    "content_path": "../data/contents/2512.22402v1.md",
    "pdf_path": "../data/contents/2512.22402v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:31.773983",
    "title": "Efficient Multi-Model Orchestration for Self-Hosted Large Language Models"
  },
  "https://arxiv.org/abs/2512.23471v1": {
    "content_path": "../data/contents/2512.23471v1.md",
    "pdf_path": "../data/contents/2512.23471v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:32.653432",
    "title": "Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings"
  },
  "https://arxiv.org/abs/2512.22398v1": {
    "content_path": "../data/contents/2512.22398v1.md",
    "pdf_path": "../data/contents/2512.22398v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:32.969289",
    "title": "Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings"
  },
  "https://arxiv.org/abs/2512.23461v1": {
    "content_path": "../data/contents/2512.23461v1.md",
    "pdf_path": "../data/contents/2512.23461v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:33.347768",
    "title": "Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance"
  },
  "https://arxiv.org/abs/2512.22396v1": {
    "content_path": "../data/contents/2512.22396v1.md",
    "pdf_path": "../data/contents/2512.22396v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:34.023008",
    "title": "HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification"
  },
  "https://arxiv.org/abs/2512.23457v1": {
    "content_path": "../data/contents/2512.23457v1.md",
    "pdf_path": "../data/contents/2512.23457v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:34.363295",
    "title": "Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following"
  },
  "https://arxiv.org/abs/2512.22387v1": {
    "content_path": "../data/contents/2512.22387v1.md",
    "pdf_path": "../data/contents/2512.22387v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:34.688626",
    "title": "AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents"
  },
  "https://arxiv.org/abs/2512.23453v1": {
    "content_path": "../data/contents/2512.23453v1.md",
    "pdf_path": "../data/contents/2512.23453v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:35.661263",
    "title": "CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models"
  },
  "https://arxiv.org/abs/2512.22386v1": {
    "content_path": "../data/contents/2512.22386v1.md",
    "pdf_path": "../data/contents/2512.22386v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:36.967568",
    "title": "OxygenREC: An Instruction-Following Generative Framework for E-commerce Recommendation"
  },
  "https://arxiv.org/abs/2510.16844": {
    "content_path": "../data/contents/2510.16844.md",
    "pdf_path": "../data/contents/2510.16844.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:39.801229",
    "title": "FinSight: Towards Real-World Financial Deep Research"
  },
  "https://arxiv.org/abs/2512.23448v1": {
    "content_path": "../data/contents/2512.23448v1.md",
    "pdf_path": "../data/contents/2512.23448v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:40.021282",
    "title": "Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion"
  },
  "https://arxiv.org/abs/2512.22385v1": {
    "content_path": "../data/contents/2512.22385v1.md",
    "pdf_path": "../data/contents/2512.22385v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:41.335961",
    "title": "LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition"
  },
  "https://arxiv.org/abs/2510.00156": {
    "content_path": "../data/contents/2510.00156.md",
    "pdf_path": "../data/contents/2510.00156.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:41.863175",
    "title": "AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery"
  },
  "https://arxiv.org/abs/2512.23447v1": {
    "content_path": "../data/contents/2512.23447v1.md",
    "pdf_path": "../data/contents/2512.23447v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:42.232042",
    "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss"
  },
  "https://arxiv.org/abs/2512.22382v1": {
    "content_path": "../data/contents/2512.22382v1.md",
    "pdf_path": "../data/contents/2512.22382v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:43.130577",
    "title": "Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration"
  },
  "https://arxiv.org/abs/2512.23440v1": {
    "content_path": "../data/contents/2512.23440v1.md",
    "pdf_path": "../data/contents/2512.23440v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:43.491532",
    "title": "ClinDEF: A Dynamic Evaluation Framework for Large Language Models in Clinical Reasoning"
  },
  "https://arxiv.org/abs/2512.22378v1": {
    "content_path": "../data/contents/2512.22378v1.md",
    "pdf_path": "../data/contents/2512.22378v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:44.055235",
    "title": "Towards Efficient Post-Training via Fourier-Driven Adapter Architectures"
  },
  "https://arxiv.org/abs/2408.06361": {
    "content_path": "../data/contents/2408.06361.md",
    "pdf_path": "../data/contents/2408.06361.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:44.439565",
    "title": "Large Language Model Agent in Financial Trading: A Survey"
  },
  "https://arxiv.org/abs/2512.23434v1": {
    "content_path": "../data/contents/2512.23434v1.md",
    "pdf_path": "../data/contents/2512.23434v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:44.817619",
    "title": "Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates"
  },
  "https://arxiv.org/abs/2512.22367v1": {
    "content_path": "../data/contents/2512.22367v1.md",
    "pdf_path": "../data/contents/2512.22367v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:45.401978",
    "title": "Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions"
  },
  "https://arxiv.org/abs/2402.02315": {
    "content_path": "../data/contents/2402.02315.md",
    "pdf_path": "../data/contents/2402.02315.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:45.650392",
    "title": "A Survey of Large Language Models in Finance (FinLLMs)"
  },
  "https://arxiv.org/abs/2512.23431v1": {
    "content_path": "../data/contents/2512.23431v1.md",
    "pdf_path": "../data/contents/2512.23431v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:46.041965",
    "title": "Optimal Scalability-Aware Allocation of Swarm Robots: From Linear to Retrograde Performance via Marginal Gains"
  },
  "https://arxiv.org/abs/2512.22364v1": {
    "content_path": "../data/contents/2512.22364v1.md",
    "pdf_path": "../data/contents/2512.22364v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:46.464312",
    "title": "Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries"
  },
  "https://arxiv.org/abs/2512.23430v1": {
    "content_path": "../data/contents/2512.23430v1.md",
    "pdf_path": "../data/contents/2512.23430v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:46.837012",
    "title": "C2PO: Diagnosing and Disentangling Bias Shortcuts in LLMs"
  },
  "https://arxiv.org/abs/2512.22351v1": {
    "content_path": "../data/contents/2512.22351v1.md",
    "pdf_path": "../data/contents/2512.22351v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:51.817398",
    "title": "VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement"
  },
  "https://arxiv.org/abs/2512.23427v1": {
    "content_path": "../data/contents/2512.23427v1.md",
    "pdf_path": "../data/contents/2512.23427v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:52.150284",
    "title": "Towards Integrating Uncertainty for Domain-Agnostic Segmentation"
  },
  "https://arxiv.org/abs/2512.22337v1": {
    "content_path": "../data/contents/2512.22337v1.md",
    "pdf_path": "../data/contents/2512.22337v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:52.515687",
    "title": "The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models"
  },
  "https://arxiv.org/abs/2512.23424v1": {
    "content_path": "../data/contents/2512.23424v1.md",
    "pdf_path": "../data/contents/2512.23424v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:52.779596",
    "title": "AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis"
  },
  "https://arxiv.org/abs/2512.22336v1": {
    "content_path": "../data/contents/2512.22336v1.md",
    "pdf_path": "../data/contents/2512.22336v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:53.151863",
    "title": "Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback"
  },
  "https://arxiv.org/abs/2406.11903": {
    "content_path": "../data/contents/2406.11903.md",
    "pdf_path": "../data/contents/2406.11903.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:55.162195",
    "title": "A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges"
  },
  "https://arxiv.org/abs/2512.23422v1": {
    "content_path": "../data/contents/2512.23422v1.md",
    "pdf_path": "../data/contents/2512.23422v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:55.419435",
    "title": "Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data"
  },
  "https://arxiv.org/abs/2512.22334v1": {
    "content_path": "../data/contents/2512.22334v1.md",
    "pdf_path": "../data/contents/2512.22334v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:58.039975",
    "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence"
  },
  "https://arxiv.org/abs/2512.23415v1": {
    "content_path": "../data/contents/2512.23415v1.md",
    "pdf_path": "../data/contents/2512.23415v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:58.418040",
    "title": "An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes"
  },
  "https://arxiv.org/abs/2512.22326v1": {
    "content_path": "../data/contents/2512.22326v1.md",
    "pdf_path": "../data/contents/2512.22326v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:58.776845",
    "title": "Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers"
  },
  "https://arxiv.org/abs/2512.23412v1": {
    "content_path": "../data/contents/2512.23412v1.md",
    "pdf_path": "../data/contents/2512.23412v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:44:59.277893",
    "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning"
  },
  "https://arxiv.org/abs/2512.22323v1": {
    "content_path": "../data/contents/2512.22323v1.md",
    "pdf_path": "../data/contents/2512.22323v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:04.474869",
    "title": "SpotEdit: Selective Region Editing in Diffusion Transformers"
  },
  "https://arxiv.org/abs/2512.23410v1": {
    "content_path": "../data/contents/2512.23410v1.md",
    "pdf_path": "../data/contents/2512.23410v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:04.693190",
    "title": "Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks"
  },
  "https://arxiv.org/abs/2512.22322v1": {
    "content_path": "../data/contents/2512.22322v1.md",
    "pdf_path": "../data/contents/2512.22322v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:07.500989",
    "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents"
  },
  "https://arxiv.org/abs/2512.23407v1": {
    "content_path": "../data/contents/2512.23407v1.md",
    "pdf_path": "../data/contents/2512.23407v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:07.746618",
    "title": "Theoretical Foundations of Scaling Law in Familial Models"
  },
  "https://arxiv.org/abs/2512.22318v1": {
    "content_path": "../data/contents/2512.22318v1.md",
    "pdf_path": "../data/contents/2512.22318v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:08.080224",
    "title": "Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough"
  },
  "https://arxiv.org/abs/2512.23385v1": {
    "content_path": "../data/contents/2512.23385v1.md",
    "pdf_path": "../data/contents/2512.23385v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:08.378996",
    "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?"
  },
  "https://arxiv.org/abs/2512.22315v1": {
    "content_path": "../data/contents/2512.22315v1.md",
    "pdf_path": "../data/contents/2512.22315v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:10.841611",
    "title": "VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning"
  },
  "https://arxiv.org/abs/2508.16260": {
    "content_path": "../data/contents/2508.16260.md",
    "pdf_path": "../data/contents/2508.16260.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:11.538173",
    "title": "MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use"
  },
  "https://arxiv.org/abs/2512.23372v1": {
    "content_path": "../data/contents/2512.23372v1.md",
    "pdf_path": "../data/contents/2512.23372v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:13.751291",
    "title": "A Design Space for Intelligent Agents in Mixed-Initiative Visual Analytics"
  },
  "https://arxiv.org/abs/2512.22309v1": {
    "content_path": "../data/contents/2512.22309v1.md",
    "pdf_path": "../data/contents/2512.22309v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:14.619174",
    "title": "LLMBoost: Make Large Language Models Stronger with Boosting"
  },
  "https://arxiv.org/abs/2512.23367v1": {
    "content_path": "../data/contents/2512.23367v1.md",
    "pdf_path": "../data/contents/2512.23367v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:14.884951",
    "title": "Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2"
  },
  "https://arxiv.org/abs/2512.22306v1": {
    "content_path": "../data/contents/2512.22306v1.md",
    "pdf_path": "../data/contents/2512.22306v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:15.271379",
    "title": "Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection"
  },
  "https://arxiv.org/abs/2512.23366v1": {
    "content_path": "../data/contents/2512.23366v1.md",
    "pdf_path": "../data/contents/2512.23366v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:15.484575",
    "title": "AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis"
  },
  "https://arxiv.org/abs/2512.22305v1": {
    "content_path": "../data/contents/2512.22305v1.md",
    "pdf_path": "../data/contents/2512.22305v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:16.730732",
    "title": "PDx -- Adaptive Credit Risk Forecasting Model in Digital Lending using Machine Learning Operations"
  },
  "https://arxiv.org/abs/2512.23356v1": {
    "content_path": "../data/contents/2512.23356v1.md",
    "pdf_path": "../data/contents/2512.23356v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:16.946019",
    "title": "A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation"
  },
  "https://arxiv.org/abs/2512.22303v1": {
    "content_path": "../data/contents/2512.22303v1.md",
    "pdf_path": "../data/contents/2512.22303v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:24.851794",
    "title": "Attack-Aware Deepfake Detection under Counter-Forensic Manipulations"
  },
  "https://arxiv.org/abs/2512.23353v1": {
    "content_path": "../data/contents/2512.23353v1.md",
    "pdf_path": "../data/contents/2512.23353v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:25.091106",
    "title": "ISOPO: Proximal policy gradients without pi-old"
  },
  "https://arxiv.org/abs/2512.21798v2": {
    "content_path": "../data/contents/2512.21798v2.md",
    "pdf_path": "../data/contents/2512.21798v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:25.436120",
    "title": "Deep Generative Models for Synthetic Financial Data: Applications to Portfolio and Risk Modeling"
  },
  "https://arxiv.org/abs/2512.23343v1": {
    "content_path": "../data/contents/2512.23343v1.md",
    "pdf_path": "../data/contents/2512.23343v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:26.190331",
    "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents"
  },
  "https://arxiv.org/abs/2512.22293v1": {
    "content_path": "../data/contents/2512.22293v1.md",
    "pdf_path": "../data/contents/2512.22293v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:26.769791",
    "title": "Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against"
  },
  "https://arxiv.org/abs/2512.23340v1": {
    "content_path": "../data/contents/2512.23340v1.md",
    "pdf_path": "../data/contents/2512.23340v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:27.133353",
    "title": "The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models"
  },
  "https://arxiv.org/abs/2512.22291v1": {
    "content_path": "../data/contents/2512.22291v1.md",
    "pdf_path": "../data/contents/2512.22291v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:28.203877",
    "title": "Multi-Head Spectral-Adaptive Graph Anomaly Detection"
  },
  "https://arxiv.org/abs/2512.23330v1": {
    "content_path": "../data/contents/2512.23330v1.md",
    "pdf_path": "../data/contents/2512.23330v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:28.903376",
    "title": "Database Theory in Action: From Inexpressibility to Efficiency in GQL's Order-Constrained Paths"
  },
  "https://arxiv.org/abs/2512.22288v1": {
    "content_path": "../data/contents/2512.22288v1.md",
    "pdf_path": "../data/contents/2512.22288v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:32.749050",
    "title": "Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model"
  },
  "https://arxiv.org/abs/2512.23328v1": {
    "content_path": "../data/contents/2512.23328v1.md",
    "pdf_path": "../data/contents/2512.23328v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:33.462156",
    "title": "CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations"
  },
  "https://arxiv.org/abs/2512.21580v2": {
    "content_path": "../data/contents/2512.21580v2.md",
    "pdf_path": "../data/contents/2512.21580v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:34.515528",
    "title": "Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5B-Parameter LLM"
  },
  "https://arxiv.org/abs/2512.04550": {
    "content_path": "../data/contents/2512.04550.md",
    "pdf_path": "../data/contents/2512.04550.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:35.134182",
    "title": "AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees"
  },
  "https://arxiv.org/abs/2512.23310v1": {
    "content_path": "../data/contents/2512.23310v1.md",
    "pdf_path": "../data/contents/2512.23310v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:35.565520",
    "title": "Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL"
  },
  "https://arxiv.org/abs/2512.22280v1": {
    "content_path": "../data/contents/2512.22280v1.md",
    "pdf_path": "../data/contents/2512.22280v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:36.094379",
    "title": "Valori: A Deterministic Memory Substrate for AI Systems"
  },
  "https://arxiv.org/abs/2512.20092": {
    "content_path": "../data/contents/2512.20092.md",
    "pdf_path": "../data/contents/2512.20092.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:36.834364",
    "title": "Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents"
  },
  "https://arxiv.org/abs/2512.23308v1": {
    "content_path": "../data/contents/2512.23308v1.md",
    "pdf_path": "../data/contents/2512.23308v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:37.520947",
    "title": "Conformal Prediction = Bayes?"
  },
  "https://arxiv.org/abs/2512.22275v1": {
    "content_path": "../data/contents/2512.22275v1.md",
    "pdf_path": "../data/contents/2512.22275v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:38.858352",
    "title": "The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency"
  },
  "https://arxiv.org/abs/2512.23307v1": {
    "content_path": "../data/contents/2512.23307v1.md",
    "pdf_path": "../data/contents/2512.23307v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:39.387274",
    "title": "RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking"
  },
  "https://arxiv.org/abs/2512.22271v1": {
    "content_path": "../data/contents/2512.22271v1.md",
    "pdf_path": "../data/contents/2512.22271v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:41.883564",
    "title": "Choice Modeling and Pricing for Scheduled Services"
  },
  "https://arxiv.org/abs/2512.23300v1": {
    "content_path": "../data/contents/2512.23300v1.md",
    "pdf_path": "../data/contents/2512.23300v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:42.174069",
    "title": "AI4Reading: Chinese Audiobook Interpretation System Based on Multi-Agent Collaboration"
  },
  "https://arxiv.org/abs/2512.22266v1": {
    "content_path": "../data/contents/2512.22266v1.md",
    "pdf_path": "../data/contents/2512.22266v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:48.447440",
    "title": "LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs"
  },
  "https://arxiv.org/abs/2512.23292v1": {
    "content_path": "../data/contents/2512.23292v1.md",
    "pdf_path": "../data/contents/2512.23292v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:48.929960",
    "title": "Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control"
  },
  "https://arxiv.org/abs/2512.21257v2": {
    "content_path": "../data/contents/2512.21257v2.md",
    "pdf_path": "../data/contents/2512.21257v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:49.899295",
    "title": "ReaSeq: Unleashing World Knowledge via Reasoning for Sequential Modeling"
  },
  "https://arxiv.org/abs/2512.23289v1": {
    "content_path": "../data/contents/2512.23289v1.md",
    "pdf_path": "../data/contents/2512.23289v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:50.372430",
    "title": "ChronoConnect: Tracking Pathways Along Highly Dynamic Vertices in Temporal Graphs"
  },
  "https://arxiv.org/abs/2512.21110v2": {
    "content_path": "../data/contents/2512.21110v2.md",
    "pdf_path": "../data/contents/2512.21110v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:50.715236",
    "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent"
  },
  "https://arxiv.org/abs/2512.23280v1": {
    "content_path": "../data/contents/2512.23280v1.md",
    "pdf_path": "../data/contents/2512.23280v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:50.983392",
    "title": "Chinese Morph Resolution in E-commerce Live Streaming Scenarios"
  },
  "https://arxiv.org/abs/2512.22258v1": {
    "content_path": "../data/contents/2512.22258v1.md",
    "pdf_path": "../data/contents/2512.22258v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:51.407183",
    "title": "Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method"
  },
  "https://arxiv.org/abs/2512.23260v1": {
    "content_path": "../data/contents/2512.23260v1.md",
    "pdf_path": "../data/contents/2512.23260v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:51.750845",
    "title": "Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation"
  },
  "https://arxiv.org/abs/2512.22256v1": {
    "content_path": "../data/contents/2512.22256v1.md",
    "pdf_path": "../data/contents/2512.22256v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:52.569680",
    "title": "Agentic Software Issue Resolution with Large Language Models: A Survey"
  },
  "https://arxiv.org/abs/2506.18096": {
    "content_path": "../data/contents/2506.18096.md",
    "pdf_path": "../data/contents/2506.18096.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:53.056232",
    "title": "Deep Research Agents: A Systematic Examination And Roadmap"
  },
  "https://arxiv.org/abs/2512.23244v1": {
    "content_path": "../data/contents/2512.23244v1.md",
    "pdf_path": "../data/contents/2512.23244v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:53.538437",
    "title": "ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing"
  },
  "https://arxiv.org/abs/2512.22255v1": {
    "content_path": "../data/contents/2512.22255v1.md",
    "pdf_path": "../data/contents/2512.22255v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:54.265057",
    "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks"
  },
  "https://arxiv.org/abs/2512.23236v1": {
    "content_path": "../data/contents/2512.23236v1.md",
    "pdf_path": "../data/contents/2512.23236v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:54.981919",
    "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta"
  },
  "https://arxiv.org/abs/2512.22252v1": {
    "content_path": "../data/contents/2512.22252v1.md",
    "pdf_path": "../data/contents/2512.22252v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:56.441319",
    "title": "Graph Attention-based Adaptive Transfer Learning for Link Prediction"
  },
  "https://arxiv.org/abs/2507.21504": {
    "content_path": "../data/contents/2507.21504.md",
    "pdf_path": "../data/contents/2507.21504.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:56.904502",
    "title": "Evaluation and Benchmarking of LLM Agents: A Survey"
  },
  "https://arxiv.org/abs/2512.23214v1": {
    "content_path": "../data/contents/2512.23214v1.md",
    "pdf_path": "../data/contents/2512.23214v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:45:57.127406",
    "title": "Anka: A Domain-Specific Language for Reliable LLM Code Generation"
  },
  "https://arxiv.org/abs/2512.22247v1": {
    "content_path": "../data/contents/2512.22247v1.md",
    "pdf_path": "../data/contents/2512.22247v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:46:00.709505",
    "title": "The Affine Divergence: Aligning Activation Updates Beyond Normalisation"
  },
  "https://arxiv.org/abs/2512.23213v1": {
    "content_path": "../data/contents/2512.23213v1.md",
    "pdf_path": "../data/contents/2512.23213v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:46:01.106767",
    "title": "Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process"
  },
  "https://arxiv.org/abs/2512.22245v1": {
    "content_path": "../data/contents/2512.22245v1.md",
    "pdf_path": "../data/contents/2512.22245v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:46:02.329051",
    "title": "Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation"
  },
  "https://arxiv.org/abs/2512.23206v1": {
    "content_path": "../data/contents/2512.23206v1.md",
    "pdf_path": "../data/contents/2512.23206v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:46:02.657841",
    "title": "Not too long do read: Evaluating LLM-generated extreme scientific summaries"
  },
  "https://arxiv.org/abs/2512.20745v2": {
    "content_path": "../data/contents/2512.20745v2.md",
    "pdf_path": "../data/contents/2512.20745v2.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:46:03.292942",
    "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent"
  },
  "https://arxiv.org/abs/2512.23184v1": {
    "content_path": "../data/contents/2512.23184v1.md",
    "pdf_path": "../data/contents/2512.23184v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:46:03.590002",
    "title": "From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research"
  },
  "https://arxiv.org/abs/2512.22240v1": {
    "content_path": "../data/contents/2512.22240v1.md",
    "pdf_path": "../data/contents/2512.22240v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:46:06.658014",
    "title": "EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs"
  },
  "https://arxiv.org/abs/2512.23173v1": {
    "content_path": "../data/contents/2512.23173v1.md",
    "pdf_path": "../data/contents/2512.23173v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:46:06.922197",
    "title": "EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion"
  },
  "https://arxiv.org/abs/2512.20491v4": {
    "content_path": "../data/contents/2512.20491v4.md",
    "pdf_path": "../data/contents/2512.20491v4.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T11:46:07.743748",
    "title": "Step-DeepResearch Technical Report"
  },
  "https://platform.openai.com/docs/guides/tools-connectors-mcp": {
    "content_path": "../data/contents/26f36023070844d8.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:09.093081",
    "title": "Connectors and MCP servers | OpenAI API"
  },
  "https://newsletter.weaviate.io/p/94-faster-keyword-search-multi-vector-embeddings-nvidia-integrations-and-much-more": {
    "content_path": "../data/contents/707d45d5b4aaf5c9.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:09.314111",
    "title": "94% Faster Keyword Search, Multi-vector Embeddings, NVIDIA integrations, and much more"
  },
  "https://aclanthology.org/2024.findings-acl.91/": {
    "content_path": "../data/contents/5a72c2016a79d40c.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:09.477446",
    "title": "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning"
  },
  "https://aclanthology.org/2025.emnlp-main.211/": {
    "content_path": "../data/contents/a564ba5e44da8538.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:09.513407",
    "title": "FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain"
  },
  "https://blog.modelcontextprotocol.io/posts/2025-11-03-using-server-instructions/": {
    "content_path": "../data/contents/8f9a273a8f222d4e.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:09.605175",
    "title": "Server Instructions: Giving LLMs a user manual for your server"
  },
  "https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents": {
    "content_path": "../data/contents/126e07cf0d684e03.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:09.650455",
    "title": "Effective context engineering for AI agents"
  },
  "https://aclanthology.org/2025.findings-acl.1199/": {
    "content_path": "../data/contents/05403eb4e76412b1.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:09.733947",
    "title": "The Law of Knowledge Overshadowing: Towards Understanding, Predicting and Preventing LLM Hallucination"
  },
  "https://www.businesswaretech.com/blog/benchmarking-ai-on-tables-and-engineering-drawings-results-findings": {
    "content_path": "../data/contents/82c97a0728002904.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:09.769499",
    "title": "We Benchmarked AI on Tables and Engineering Drawings — Here's What We Found"
  },
  "https://zju-real.github.io/gui-rcpo/": {
    "content_path": "../data/contents/c64ea3eae447d400.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:10.271746",
    "title": "Site not found · GitHub Pages"
  },
  "https://deepprune.github.io/": {
    "content_path": "../data/contents/44ab8085ea58d62b.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:10.400264",
    "title": "DeepPrune"
  },
  "https://qwenlm.github.io/blog/qwen3-embedding/": {
    "content_path": "../data/contents/ff72ce52465b0716.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:10.472536",
    "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models"
  },
  "https://learn.microsoft.com/en-us/azure/databricks/vector-search/query-vector-search": {
    "content_path": "../data/contents/5ac6b0af3c0bc690.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:10.970227",
    "title": "Query a vector search index - Azure Databricks"
  },
  "https://docs.kensho.com/llmreadyapi/mcp": {
    "content_path": "../data/contents/86a6abb3459283e4.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.110463",
    "title": "LLM-Ready API MCP Servers | Kensho Docs"
  },
  "https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk/": {
    "content_path": "../data/contents/1486bc645a99a126.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.213651",
    "title": "Building agents with the Claude Agent SDK"
  },
  "https://github.com/modelcontextprotocol/modelcontextprotocol/releases": {
    "content_path": "../data/contents/139640d70140394c.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.406087",
    "title": "Releases · modelcontextprotocol/modelcontextprotocol"
  },
  "https://pmc.ncbi.nlm.nih.gov/articles/PMC11645129/": {
    "content_path": "../data/contents/9c2bee0e0c6272bc.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.506014",
    "title": "A survey on multimodal large language models"
  },
  "https://www.llamaindex.ai/blog/llamaindex-newsletter-2024-12-24": {
    "content_path": "../data/contents/ea95426626b33838.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.542351",
    "title": "LlamaIndex Newsletter 2024-12-24"
  },
  "https://gemini.google/overview/deep-research/": {
    "content_path": "../data/contents/7e932ef884e7ab63.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.572324",
    "title": "Gemini Deep Research — your personal research assistant"
  },
  "https://github.com/SalesforceAIResearch/UniDOC-Bench": {
    "content_path": "../data/contents/858b474b17b84dba.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.732821",
    "title": "GitHub - SalesforceAIResearch/UniDoc-Bench"
  },
  "https://platform.openai.com/docs/mcp": {
    "content_path": "../data/contents/b6430c87e6641d74.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.797357",
    "title": "Building MCP servers for ChatGPT and API integrations"
  },
  "https://github.com/UlrickBL/multimodal_reranker": {
    "content_path": "../data/contents/b4522eaab81c4e8e.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.840321",
    "title": "GitHub - UlrickBL/multimodal_reranker: Mutlimodal reranker training and benchmarks"
  },
  "https://reducto.ai/blog/sota-table-parsing": {
    "content_path": "../data/contents/38c62bbd8d21f3a5.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.917141",
    "title": "Reducto: AI document parsing & extraction software"
  },
  "https://github.com/a2aproject/A2A": {
    "content_path": "../data/contents/79c78ff00570e0db.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:11.970479",
    "title": "GitHub - a2aproject/A2A: An open protocol enabling communication and interoperability between opaque agentic applications."
  },
  "https://github.com/llm-lab-org/Multimodal-RAG-Survey": {
    "content_path": "../data/contents/28ef886901fcc732.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.135186",
    "title": "GitHub - llm-lab-org/Multimodal-RAG-Survey: A Survey on Multimodal Retrieval-Augmented Generation"
  },
  "https://github.com/opendatalab/TRivia": {
    "content_path": "../data/contents/5f46057f1bfcd528.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.228985",
    "title": "GitHub - opendatalab/TRivia: TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition"
  },
  "https://openai.com/index/o1-and-new-tools-for-developers/": {
    "content_path": "../data/contents/fe85a01c7a16a4cf.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.300369",
    "title": "OpenAI o1 and new tools for developers"
  },
  "https://platform.openai.com/docs/changelog/launched-structured-outputs": {
    "content_path": "../data/contents/b270d011a52aa4df.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.371008",
    "title": "Page not found | OpenAI API"
  },
  "https://www.llamaindex.ai/blog/mastering-pdfs-extracting-sections-headings-paragraphs-and-tables-with-cutting-edge-parser-faea18870125": {
    "content_path": "../data/contents/dd74d734e95e625d.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.499932",
    "title": "Mastering PDFs: Extracting Sections, Headings, Paragraphs, and Tables with Cutting-Edge Parser"
  },
  "https://www.anthropic.com/research/contextual-retrieval": {
    "content_path": "../data/contents/b852890527f35653.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.545896",
    "title": "Contextual Retrieval in AI Systems"
  },
  "https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual": {
    "content_path": "../data/contents/fe971512d2262f60.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.576125",
    "title": "jinaai/jina-reranker-v2-base-multilingual · Hugging Face"
  },
  "https://github.com/kensho-technologies/kfinance": {
    "content_path": "../data/contents/d72bfa9a036ae837.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.607527",
    "title": "GitHub - kensho-technologies/kfinance"
  },
  "https://github.com/Finance-LLMs/FinMCP": {
    "content_path": "../data/contents/1ac6b0bc13beb146.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.666722",
    "title": "GitHub - Finance-LLMs/FinMCP: A unified Model Context Protocol (MCP) ecosystem for financial AI applications, providing deep research capabilities, trading integrations, and specialized AI model training for financial analysis."
  },
  "https://huggingface.co/opendatalab/TRivia-3B": {
    "content_path": "../data/contents/92f6facfadb6f1a4.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.840194",
    "title": "opendatalab/TRivia-3B · Hugging Face"
  },
  "https://openai.com/te-IN/index/gpt-5-2-for-science-and-math/": {
    "content_path": "../data/contents/ef348fa45c1d449a.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.872086",
    "title": "Advancing science and math with GPT-5.2"
  },
  "https://www.deeplearning.ai/courses/fine-tuning-and-reinforcement-learning-for-llms-intro-to-post-training/": {
    "content_path": "../data/contents/64bf9457e77ce628.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:12.926172",
    "title": "Fine-tuning and Reinforcement Learning for LLMs: Intro to Post-Training"
  },
  "https://claude.com/connectors/daloopa": {
    "content_path": "../data/contents/9af1333f7419dea2.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:13.008945",
    "title": "Daloopa | Claude"
  },
  "https://www.prnewswire.com/news-releases/alphasense-ai-search-and-market-intelligence-platform-now-available-in-new-aws-ai-marketplace-agents-and-tools-category-302506998.html": {
    "content_path": "../data/contents/de5de3c58337b7e2.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:13.223272",
    "title": "AlphaSense AI Search and Market Intelligence Platform Now Available in New AWS AI Marketplace Agents and Tools Category"
  },
  "https://neo4j.com/blog/genai/what-is-graphrag/": {
    "content_path": "../data/contents/ac56296115aa8941.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:13.521868",
    "title": "What Is GraphRAG?"
  },
  "https://github.com/bloomberg/m3docrag": {
    "content_path": "../data/contents/dae9e71fe6e9cee8.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:13.663584",
    "title": "GitHub - bloomberg/m3docrag"
  },
  "https://huggingface.co/blog/hansolosan/granite-embedding-r2": {
    "content_path": "../data/contents/46f41ae8c429a3d7.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:13.698259",
    "title": "Granite Embedding R2: Setting New Standards for Enterprise Retrieval"
  },
  "https://claude.com/blog/organization-skills-and-directory": {
    "content_path": "../data/contents/510da15fe80cc0cf.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:13.798910",
    "title": "Skills for organizations, partners, the ecosystem"
  },
  "https://github.com/langchain-ai/open_deep_research": {
    "content_path": "../data/contents/ca97ebd52438c31d.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:14.370610",
    "title": "GitHub - langchain-ai/open_deep_research"
  },
  "https://huggingface.co/BAAI/bge-reranker-large": {
    "content_path": "../data/contents/1d71e909633138ce.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:14.487873",
    "title": "BAAI/bge-reranker-large · Hugging Face"
  },
  "https://epoch.ai/frontiermath": {
    "content_path": "../data/contents/f66be288cef448ab.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:14.527809",
    "title": "FrontierMath"
  },
  "https://jina.ai/news/late-chunking-in-long-context-embedding-models/": {
    "content_path": "../data/contents/0dceaf6569a609d0.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:14.600430",
    "title": "Late Chunking in Long-Context Embedding Models"
  },
  "https://www.alpha-sense.com/resources/research-articles/generative-ai-in-private-equity/": {
    "content_path": "../data/contents/cfbedd6e3497c951.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:14.632233",
    "title": "Generative AI in Private Equity: Use Cases and Considerations"
  },
  "https://claude.com/connectors/pitchbook": {
    "content_path": "../data/contents/204cc21924cc58f5.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:14.911165",
    "title": "PitchBook | Claude"
  },
  "https://openai.com/index/chain-of-thought-monitoring/": {
    "content_path": "../data/contents/d4700c15258393ad.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:14.967906",
    "title": "Detecting misbehavior in frontier reasoning models"
  },
  "https://docs.llamaindex.ai/en/stable/examples/cookbooks/contextual_retrieval/": {
    "content_path": "../data/contents/fc1efc2fab1e2b5d.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:15.008345",
    "title": "Contextual Retrieval"
  },
  "https://huggingface.co/blog/UlrickBL/building-and-evaluating-multimodal-rerankers": {
    "content_path": "../data/contents/4a43dc54eb44cc71.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:15.047210",
    "title": "Building and evaluating Multimodal Rerankers"
  },
  "https://simonw.substack.com/p/large-language-models-can-run-tools": {
    "content_path": "../data/contents/a3577935396a010f.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:15.271694",
    "title": "Large Language Models can run tools in your terminal with LLM 0.26"
  },
  "https://standardmetrics.io/library/portfolio-monitoring-tools-managing-a-portfolio-with-ai-and-automation/": {
    "content_path": "../data/contents/0d580f992feb2db3.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:15.490044",
    "title": "Portfolio Monitoring Tools: Managing Data with AI and Automation | Standard Metrics"
  },
  "https://www.deeplearning.ai/short-courses/reinforcement-fine-tuning-llms-grpo/": {
    "content_path": "../data/contents/cfd831675e7dab99.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:15.615932",
    "title": "Reinforcement Fine-Tuning LLMs with GRPO"
  },
  "https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model": {
    "content_path": "../data/contents/42cc2c5eb90f89cb.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:16.312713",
    "title": "Jina Embeddings v3: A Frontier Multilingual Embedding Model"
  },
  "https://medium.com/box-developer-blog/contextual-retrieval-in-retrieval-augmented-generation-rag-49b7bff0c5b7": {
    "content_path": "../data/contents/83dececc9350a569.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:16.366861",
    "title": "Contextual Retrieval in Retrieval-Augmented Generation (RAG)"
  },
  "https://openai.com/index/evaluating-chain-of-thought-monitorability/": {
    "content_path": "../data/contents/74184c707282d206.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:16.615877",
    "title": "Evaluating chain-of-thought monitorability"
  },
  "https://huggingface.co/jinaai/jina-embeddings-v3": {
    "content_path": "../data/contents/3b675b849fe50ec2.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:16.667986",
    "title": "jinaai/jina-embeddings-v3 · Hugging Face"
  },
  "https://www.deeplearning.ai/short-courses/llms-as-operating-systems-agent-memory/": {
    "content_path": "../data/contents/6111f718df3daf3e.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:16.867630",
    "title": "LLMs as Operating Systems: Agent Memory"
  },
  "https://simonw.substack.com/p/how-i-use-llms-to-help-me-write-code": {
    "content_path": "../data/contents/8b7a420bd64b6960.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:17.530480",
    "title": "How I use LLMs to help me write code"
  },
  "https://changelog.langchain.com/announcements/deepagents-0-2-release-for-more-autonomous-agents": {
    "content_path": "../data/contents/bbaabe3bb72e2166.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:18.156155",
    "title": "`deepagents` 0.2 release for more autonomous agents"
  },
  "https://www.mongodb.com/community/forums/t/announcing-hybrid-search-support-via-rankfusion/324476": {
    "content_path": "../data/contents/0c1b04dce7856b75.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:18.770048",
    "title": "Announcing Hybrid Search support via $rankFusion! - MongoDB Atlas / Vector Search - MongoDB Community Hub"
  },
  "https://developer.nvidia.com/blog/nvidia-text-embedding-model-tops-mteb-leaderboard/": {
    "content_path": "../data/contents/e8c4cc919c48f744.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:18.833812",
    "title": "NVIDIA Text Embedding Model Tops MTEB Leaderboard"
  },
  "https://docs.claude.com/en/docs/build-with-claude/context-editing": {
    "content_path": "../data/contents/efc4d9ea5186c27a.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:19.125220",
    "title": "Context editing"
  },
  "https://docs.claude.com/fr/docs/build-with-claude/pdf-support": {
    "content_path": "../data/contents/c108fd49854f94ae.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:19.351923",
    "title": "Support PDF"
  },
  "https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct": {
    "content_path": "../data/contents/b9c90048411606b0.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:19.786410",
    "title": "Qwen/Qwen2-VL-7B-Instruct · Hugging Face"
  },
  "https://unstract.com/blog/extract-table-from-pdf/": {
    "content_path": "../data/contents/e580e60950f35d68.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:19.908000",
    "title": "AI PDF Table Extraction in 2026 | Extract Tables from PDF"
  },
  "https://changelog.langchain.com/announcements/langmem-sdk-for-long-term-agent-memory": {
    "content_path": "../data/contents/40a3e0ec9d07672b.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:19.941569",
    "title": "LangMem SDK for long-term agent memory"
  },
  "https://docs.claude.com/en/docs/agents-and-tools/tool-use/memory-tool": {
    "content_path": "../data/contents/2602e6903ebf80f0.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:20.251511",
    "title": "Memory tool"
  },
  "https://blog.langchain.com/langchain-state-of-ai-2024/": {
    "content_path": "../data/contents/a92ef46961f1fe64.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:20.497716",
    "title": "LangChain State of AI 2024 Report"
  },
  "https://openai.com/index/introducing-deep-research/": {
    "content_path": "../data/contents/8fec3ef3a97d447c.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:22.875933",
    "title": "Introducing deep research"
  },
  "https://azure.microsoft.com/en-us/blog/introducing-deep-research-in-azure-ai-foundry-agent-service/": {
    "content_path": "../data/contents/7138d60869875d01.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:24.020367",
    "title": "Introducing Deep Research in Azure AI Foundry Agent Service"
  },
  "https://cohere.com/blog/embed-4": {
    "content_path": "../data/contents/a7bb32ab4abbd21f.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:27.894564",
    "title": "Introducing Embed 4: Multimodal search for business | Cohere Blog"
  },
  "https://www.getdynamiq.ai/post/llm-guardrails-in-private-equity-how-to-scale-genai-safely-across-portfolio-companies": {
    "content_path": "../data/contents/5df0a59e8cb98755.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:38.431366",
    "title": "LLM Guardrails in Private Equity: How to Scale GenAI Safely Across Portfolio Companies"
  },
  "https://www.anthropic.com/solutions/financial-services": {
    "content_path": "../data/contents/98ca98452ce3c7ab.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:39.134673",
    "title": "Financial services | Claude"
  },
  "https://www.itpro.com/technology/artificial-intelligence/openai-chatgpt-atlas-ai-browser-prompt-injection-attack-risk": {
    "content_path": "../data/contents/b32f5d073b75c666.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:39.168119",
    "title": "OpenAI says prompt injection attacks are a serious threat for AI browsers &ndash; and it&rsquo;s a problem that&rsquo;s&hellip;"
  },
  "https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute": {
    "content_path": "../data/contents/715c1c559e8b6f08.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:39.498563",
    "title": "Scaling test-time compute - a Hugging Face Space by HuggingFaceH4"
  },
  "https://www.techradar.com/ai-platforms-assistants/testing-chatgpt-gemini-and-claude-in-the-multimodal-maze": {
    "content_path": "../data/contents/4872f1b37b50c2ed.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:40.513936",
    "title": "Testing ChatGPT, Gemini, and Claude in the multimodal maze"
  },
  "https://jina.ai/reranker/": {
    "content_path": "../data/contents/714940277d1401f8.md",
    "source": "jina",
    "saved_at": "2025-12-30T11:46:42.497560",
    "title": "Reranker API"
  },
  "https://xapien.com/ai-due-diligence": {
    "content_path": "data/contents/14efec142b187435.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.053542",
    "title": "AI Due Diligence | Xapien",
    "summary": "The provided webpage describes **Xapien**, an AI-powered due diligence tool that significantly reduces the time required for due diligence from days to minutes.\n\n**Key features and capabilities mentioned include:**\n\n*   **Speed and Scope:** Delivers comprehensive reports in minutes by covering 0.5 billion corporate records, 1,500 watchlists, 140 jurisdictions, and 35 trillion web pages.\n*   **Benefits:** Achieves a 65% reduction in client onboarding time and a 58% decrease in annual hours spent analyzing AML risk.\n*   **Due Diligence Details:** Extracts information on business associations, professional history, directorships (including UBO, PSC, PEP, Sanctions), political affiliations, media coverage, family members, charitable activities, assets, and locations.\n*   **Risk Identification:** Flags insolvencies, corporate risk indicators, regulatory, reputational, and ESG risks.\n*   **Interactive Analysis:** Allows users to \"Ask Xapien\" questions about the subject and receive fully-sourced replies.\n*   **Comparison to ChatGPT:** Xapien claims 95% accuracy, is built specifically for due diligence, combines corporate records with real-time data, and is designed to eliminate GDPR risks, unlike ChatGPT.\n\n**The page does not mention:** Multi-agent systems, private equity, venture capital, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, or specific integrations with PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": "2025-02-14T00:00:00.000Z",
    "score": null
  },
  "https://duediligencedirector.com/": {
    "content_path": "data/contents/1f2cd2f4c42cef0d.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.114098",
    "title": "Complete visibility & control of the metrics that matter for enterprise value",
    "summary": "The webpage describes the **Due Diligence Director**, an AI-powered platform designed to transform and accelerate the due diligence process for investment firms.\n\nIt functions as an **AI analyst, risk detective, deal strategist, and execution engine** that compresses weeks of work into hours by automatically reading and extracting data from contracts, emails, financials, and various systems.\n\n**Core capabilities include:**\n*   **Autonomous Data Ingestion** from platforms like Google Workspace, Microsoft 365, Slack, Dropbox, and Salesforce.\n*   **Entity Recognition** to map company hierarchy and ownership.\n*   **Contract Analysis** to auto-extract key terms and identify risky clauses (e.g., change of control, auto-renewals).\n*   **Financial Analysis** to reconstruct revenue streams, identify MRR/ARR, customer concentration, and detect revenue recognition issues.\n*   **Risk Identification** by flagging litigation exposure and customer churn signals.\n*   **Smarter Investment Decisions** by predicting deal risk and benchmarking performance.\n\nWhile the platform clearly addresses aspects of **due diligence automation** and **investment opportunity analysis** for private equity/venture capital, the summary **does not mention** specific multi-agent systems, deal sourcing, portfolio monitoring, financial report generation, or integrations with specific databases like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://v7labs.com/blog/ai-due-diligence": {
    "content_path": "data/contents/9f6a57baf3e3de98.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.143593",
    "title": "AI in Due Diligence: What It Means for M&A and Beyond",
    "summary": "The provided web page discusses the use of **AI and automation in Due Diligence** for M&A and investment firms, which aligns with the user query's focus on **agents and finance**, particularly in areas like **due diligence automation** and **investment opportunity analysis**.\n\nHere is a summary tailored to the user's query:\n\nThe article details how AI and automation are transforming due diligence in investment firms, leading to increased deal velocity, accuracy, and scalability. While overall adoption is low (only 10% of private funds used AI by end of 2023), large firms are leading the implementation of tools for tasks like data extraction.\n\n**Key areas where AI is applied in due diligence (relevant to finance/investment):**\n\n*   **Data Extraction and Analysis:** AI tools (like V7 Go mentioned) use technologies such as OCR and LLMs to automate the extraction of key data from documents, including financial statements (like **10-Q Reports**), **Confidential Information Memorandums (CIMs)**, and **Pitch Decks**.\n*   **Automation of Tasks:** Specific automations mentioned include **Financial Spreading**, **Red Flag Detection**, **Fund Performance Reporting**, and **Portfolio Data Room Analysis**.\n*   **Investment Analysis:** AI agents are described as synthesizing analyst reports, earnings calls, and filings to build investment theses and bull/bear cases (directly related to **investment opportunity analysis**).\n*   **Contract Review:** AI automates legal contract analysis to extract key terms and flag risks.\n*   **Hybrid Approach:** The process still requires human oversight for nuanced interpretation, combining AI efficiency with expert judgment.\n\n**Regarding specific platforms mentioned in the query:**\n\nThe text mentions AI due diligence platforms like **V7 Go** and discusses the general capabilities needed for modern due diligence, but it **does not explicitly mention** integrations with **PitchBook, AlphaSense, Preqin, or CapIQ**, nor does it detail specific multi-agent systems for **private equity** or **venture capital** beyond general investment analysis agents.",
    "published_date": "2025-09-03T00:00:00.000Z",
    "score": null
  },
  "https://papermark.com/due-diligence/ai": {
    "content_path": "data/contents/c196177650643688.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.243025",
    "title": "AI in Due Diligence Hub",
    "summary": "The provided webpage focuses on **AI in Due Diligence**, detailing how artificial intelligence and machine learning automate, enhance, and accelerate traditional due diligence processes across various applications like document analysis, risk assessment, and process automation. It discusses the underlying technologies (NLP, Machine Learning, Computer Vision) and the benefits, such as significant time and cost savings.\n\nHowever, the user query specifically asks about **\"agents_and_finance: Multi-agent systems for finance, private equity, venture capital, due diligence automation, investment opportunity analysis, deal sourcing, portfolio monitoring, agents for data analysis, financial report generation, PitchBook, AlphaSense, Preqin, CapIQ, Bloomberg integrations.\"**\n\nWhile the webpage covers **due diligence automation** and **data analysis** within due diligence, it **does not** mention:\n*   Multi-agent systems.\n*   Specific applications in private equity or venture capital beyond general due diligence.\n*   Deal sourcing or portfolio monitoring.\n*   Integrations with specific platforms like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.\n\nTherefore, the page does not fully answer the user's specific query regarding multi-agent systems and the listed financial platforms.\n\n**No answer found**",
    "published_date": "2025-01-01T00:00:00.000Z",
    "score": null
  },
  "https://www.ansarada.com/article/due-diligence-ai": {
    "content_path": "data/contents/3b794cfbf4525add.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:12.276269",
    "title": "AI Due Diligence",
    "summary": "The provided webpage focuses on **AI Due Diligence** in the context of Mergers & Acquisitions (M&A) and other major transactions. It explains how AI and machine learning tools are used to enhance due diligence by rapidly processing and analyzing vast amounts of data, leading to increased efficiency, accuracy, reduced risk, and predictive insights.\n\nThe page details specific AI tools offered by Ansarada, such as the **Bidder Engagement Score** (predicting bidder motives and outcomes) and **Smart Sort** (automatically classifying and organizing documents).\n\n**However, the page does not mention or discuss:**\n\n*   Multi-agent systems for finance.\n*   Private equity or venture capital specifically (beyond general M&A context).\n*   Due diligence automation specifically for deal sourcing or portfolio monitoring (it focuses on the execution phase of a deal).\n*   Agents for financial report generation.\n*   Integrations with specific platforms like PitchBook, AlphaSense, Preqin, CapIQ, or Bloomberg.\n\n**Conclusion:** The page discusses AI for due diligence, which is related to finance and deal processes, but it does not cover the specific topics of multi-agent systems, private equity/venture capital operations, or the listed third-party data integrations mentioned in your query.\n\nNo answer found",
    "published_date": "2025-05-19T00:00:00.000Z",
    "score": null
  },
  "https://www.reddit.com/r/Rag/comments/1it0kss/best_way_to_multimodal_rag_a_pdf/": {
    "content_path": "data/contents/a8b569f14a6d5a16.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.114393",
    "title": "Best way to Multimodal Rag a PDF - Reddit",
    "summary": "The webpage suggests using a PDF parser to extract content from a PDF, preferably into Markdown format, so it can be displayed to the user in a formatted way. This is in the context of a discussion about the best way to implement Multimodal RAG for a PDF.",
    "published_date": "2025-02-19T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2404.09987": {
    "content_path": "data/contents/3616fbb17fac4308.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.160757",
    "title": "OneChart: Purify the Chart Structural Extraction via One Auxiliary Token",
    "summary": "The provided webpage describes a paper titled \"OneChart: Purify the Chart Structural Extraction via One Auxiliary Token.\" This work focuses on improving the **structural extraction of chart information** using a novel agent called OneChart, which incorporates an autoregressive main body and an auxiliary token with an additional decoder to enhance the reliability of numerical parts.\n\nWhile the user query covers a broad range of topics including **Vision-language models (LVLMs)**, **multimodal RAG**, **document understanding**, **PDF parsing**, **chart/table extraction**, **report generation with LLMs**, and specific models like **GPT-4V, Claude vision, and Gemini**, the webpage specifically addresses **chart structural extraction** using an LVLM-like approach (OneChart).\n\nTherefore, the summary relevant to the query is:\n\nThe paper introduces **OneChart**, a reliable agent specifically designed for the **structural extraction of chart information**. It addresses the challenges faced by even advanced **large vision-language models (LVLMs)** in chart parsing. OneChart uses an autoregressive main body enhanced by an **auxiliary token** and an additional decoder to improve the reliability of numerical output. It also features a self-evaluation mechanism to gauge the confidence of its parsing results. OneChart significantly outperforms current state-of-the-art **chart parsing models** and improves the downstream **ChartQA** benchmark accuracy for LLaVA-1.6.",
    "published_date": "2024-04-15T00:00:00.000Z",
    "score": null
  },
  "https://ui.adsabs.harvard.edu/abs/2023arXiv231116483H/abstract": {
    "content_path": "data/contents/dd604f46b6b9aaa4.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.188951",
    "title": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation",
    "summary": "The webpage describes **ChartLlama**, a multimodal large language model specifically designed for **chart understanding and generation**.\n\nThe summary relevant to your query is:\n\n*   **Vision-language models/Multimodal LLMs:** ChartLlama is a multimodal LLM trained to improve understanding of specific domain data, particularly **chart figures**.\n*   **Report generation with LLMs/Chart understanding:** The model is evaluated on benchmarks like **ChartQA** (chart question answering) and **Chart-to-text** (chart description generation).\n*   **Data Generation:** The authors created a high-quality instruction-tuning dataset using GPT-4 through a multi-step process to generate tabular data, chart figures, and instruction tuning data, which enhances the model's chart comprehension capabilities.\n\nThe page does not explicitly detail general topics like multimodal RAG, general document understanding, PDF parsing, general chart/table extraction (beyond chart-specific extraction), GPT-4V, Claude vision, Gemini, or structured document output, focusing specifically on chart-related tasks.",
    "published_date": "2023-11-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2305.14761": {
    "content_path": "data/contents/454cb0f1727a2ee8.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.217654",
    "title": "UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning",
    "summary": "The webpage describes **UniChart**, a Universal Vision-language Pretrained Model specifically designed for **chart comprehension and reasoning**.\n\nIt addresses the limitation of existing methods that often don't explicitly model chart structure. UniChart works by encoding the relevant text, data, and visual elements of charts and uses a chart-grounded text decoder to generate natural language output. The model is pretrained using chart-specific tasks, including low-level tasks (extracting visual elements and data) and high-level tasks (acquiring understanding and reasoning skills). The paper claims state-of-the-art performance on three downstream tasks after pretraining and fine-tuning.\n\nWhile the user query covers broad topics like Vision-language models, multimodal RAG, document understanding, PDF parsing, and report generation with LLMs (GPT-4V, Claude vision, Gemini), this specific page focuses narrowly on **chart comprehension and reasoning** using a dedicated vision-language model (UniChart).",
    "published_date": "2023-05-24T00:00:00.000Z",
    "score": null
  },
  "https://paperswithcode.com/paper/enhanced-chart-understanding-in-vision-and": {
    "content_path": "data/contents/a45cbfdbc999eab6.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.249158",
    "title": "Papers with Code - Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs",
    "summary": "The webpage describes a research paper titled \"Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs.\"\n\nThe paper introduces **ChartT5**, a Vision and Language (V+L) model designed to interpret table information from chart images. It achieves this through **cross-modal pre-training on plot table pairs** using two novel objectives: **Masked Header Prediction (MHP)** and **Masked Value Prediction (MVP)**. The model shows significant performance gains, outperforming state-of-the-art non-pretraining methods by over 8% on the ChartQA benchmark for chart question answering.\n\nThis relates to your query on **Vision-language models** and **chart/table extraction** within the broader context of multimodal AI.",
    "published_date": "2023-05-29T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2503.18435": {
    "content_path": "data/contents/b60db0d6d486e982.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.284202",
    "title": "On the Perception Bottleneck of VLMs for Chart Understanding",
    "summary": "The webpage describes a research paper titled \"On the Perception Bottleneck of VLMs for Chart Understanding.\" The study investigates the limitations of large vision-language models (LVLMs) in chart understanding, identifying a **perception bottleneck** composed of two parts: the **vision encoder bottleneck** (where visual representations lack correct information) and the **extraction bottleneck** (where the language model struggles to extract information from the representations). The research finds that the vision encoder remains a critical bottleneck even after instruction tuning, and proposes enhancing the visual encoder using a contrastive learning framework to improve chart comprehension.",
    "published_date": "2025-03-24T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2412.20715": {
    "content_path": "data/contents/d305c70479ca4173.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.315404",
    "title": "ChartAdapter: Large Vision-Language Model for Chart Summarization",
    "summary": "The webpage describes a paper titled **\"ChartAdapter: Large Vision-Language Model for Chart Summarization.\"**\n\nThis work proposes **ChartAdapter**, a novel lightweight transformer module designed to bridge the gap between charts and textual summaries for chart summarization. It uses learnable query vectors to extract implicit semantics from chart data and incorporates a cross-modal alignment projector to enhance vision-to-language generative learning. By integrating ChartAdapter with an LLM, the authors enable end-to-end training for efficient chart summarization. The approach is supported by a large-scale dataset (190,618 samples) and experimental results show it outperforms existing methods in generating high-quality chart summaries.\n\nThis relates to your query on **Vision-language models** and **report generation with LLMs** by focusing on a specific application: using a tailored LLM-based approach for **chart summarization** (a form of data/document understanding).",
    "published_date": "2024-12-30T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2508.04650": {
    "content_path": "data/contents/332b2f7f0b23ac6b.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.349857",
    "title": "Computer Science > Computer Vision and Pattern Recognition",
    "summary": "The webpage describes a paper titled **\"EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts.\"**\n\nThe summary relevant to your query is:\n\nThe paper introduces **EncQA**, a novel benchmark designed to systematically cover **visual encodings** and analytic tasks crucial for **chart understanding** by **Vision-Language Models (VLMs)**. It provides 2,076 synthetic question-answer pairs covering six visual encoding channels (position, length, area, color quantitative, color nominal, and shape) and eight tasks. The evaluation of state-of-the-art VLMs on EncQA shows that performance varies significantly across encodings and tasks, suggesting that improving chart understanding requires targeted strategies rather than just scaling model size.\n\nThis directly relates to **Vision-language models** and **document understanding** (specifically charts, which are often part of documents). It does not mention multimodal RAG, PDF parsing, chart/table extraction (beyond the benchmark focus), report generation with LLMs, GPT-4V, Claude vision, Gemini, or structured document output.",
    "published_date": "2025-08-06T00:00:00.000Z",
    "score": null
  },
  "https://aclanthology.org/2024.findings-emnlp.191/": {
    "content_path": "data/contents/ccceec3290a31ff1.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.384739",
    "title": "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning",
    "summary": "The webpage discusses a comprehensive evaluation of **Large Vision Language Models (LVLMs)** for **chart understanding and reasoning tasks**, such as chart question answering, summarization, and fact-checking. This addresses the user's interest in **Vision-language models** and **chart/table extraction**.\n\nThe evaluation assesses both closed and open-sourced LVLMs across five major chart reasoning tasks. The findings indicate that while LVLMs are capable of generating fluent texts covering high-level data insights, they also struggle with issues like hallucinations, factual errors, and data bias. The paper aims to highlight the strengths and limitations of LVLMs in chart comprehension.",
    "published_date": "2024-11-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2407.04172": {
    "content_path": "data/contents/3f6784c71efc8d39.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.419119",
    "title": "ChartGemma: Visual Instruction-tuning for Chart Reasoning in the Wild",
    "summary": "The webpage describes **ChartGemma**, a novel model for chart understanding and reasoning developed over PaliGemma. It addresses drawbacks in existing methods by training directly on instruction-tuning data generated from **chart images**, capturing both high-level trends and low-level visual information, rather than relying solely on underlying data tables. ChartGemma achieves state-of-the-art results across benchmarks for chart summarization, question answering, and fact-checking, generating more realistic and factually correct summaries for real-world charts.\n\nThis relates to your query on **Vision-language models** and **document understanding** (specifically charts within documents) and **report generation with LLMs** (via chart summarization).",
    "published_date": "2024-07-04T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2412.19146": {
    "content_path": "data/contents/4cec2137a9ab44c3.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:13.452141",
    "title": "AskChart: Universal Chart Understanding through Textual Enhancement",
    "summary": "The provided webpage describes **AskChart**, a universal model for chart understanding that explicitly integrates both **textual and visual cues** from charts using a Mixture of Experts (MoE) architecture.\n\nIt addresses chart understanding tasks like ChartQA and Chart-to-Text by moving beyond approaches that primarily focus on visual cues. AskChart aims to learn enhanced visual-textual representations and maintains a smaller model size compared to state-of-the-art models. The authors also curated a large-scale dataset, **ChartBank**, to align textual and visual information.\n\nWhile the paper focuses on **chart understanding** (a multimodal task involving visual data and text), it does not explicitly discuss the broader topics mentioned in your query such as:\n*   General Vision-language models (beyond chart-specific application)\n*   Multimodal RAG\n*   Document understanding (beyond charts)\n*   PDF parsing\n*   Report generation with LLMs\n*   Specific models like GPT-4V, Claude vision, or Gemini (though the work is related to multimodal AI).\n\nTherefore, the page provides information on a specific application of multimodal understanding (chart analysis) but does not cover the full scope of your query.\n\n**No answer found** for the comprehensive list of topics in your query.",
    "published_date": "2024-12-26T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2412.13663": {
    "content_path": "data/contents/a4d437e827290af8.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.064362",
    "title": "Computer Science > Computation and Language",
    "summary": "The webpage describes a paper introducing **ModernBERT**, a modern bidirectional encoder that offers a significant Pareto improvement over older encoders like BERT.\n\nKey points relevant to your query include:\n*   **Retrieval and Embeddings:** ModernBERT exhibits state-of-the-art results on **multi-vector retrieval** across different domains.\n*   **Efficiency:** It is designed to be the most **speed and memory efficient encoder** and is optimized for inference on common GPUs.\n*   **Context Window:** It was trained with a native **8192 sequence length**, suggesting improvements in handling longer contexts compared to older BERT models.\n\nThe paper focuses on improving encoder-only models for tasks like retrieval and classification, but it **does not explicitly detail** vector databases, new efficient embedding models (beyond introducing ModernBERT itself), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, or context window management strategies in detail.",
    "published_date": "2024-12-18T00:00:00.000Z",
    "score": null
  },
  "https://openai.com/index/new-embedding-models-and-api-updates/": {
    "content_path": "data/contents/d956e53172127734.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.096711",
    "title": "New embedding models and API updates",
    "summary": "The webpage announces updates to OpenAI's embedding models and API, which are relevant to several aspects of your query:\n\n*   **Embeddings (new efficient models):** OpenAI introduced two new embedding models: `text-embedding-3-small` (highly efficient, 5X cheaper than the predecessor) and `text-embedding-3-large` (more powerful, up to 3072 dimensions). Both show stronger performance on benchmarks compared to the previous `text-embedding-ada-002`.\n*   **Vector databases & RAG architectures:** The text explicitly states that embeddings power applications like knowledge retrieval in ChatGPT, the Assistants API, and **many retrieval augmented generation (RAG) developer tools**.\n*   **Chunking strategies & Context window management:** The new models support **native support for shortening embeddings** using the `dimensions` API parameter. This allows developers to trade off performance and cost/storage by shortening the vector size without losing significant concept-representing properties, which is relevant for managing vector store size and retrieval efficiency.\n*   **Retrieval:** Embeddings are fundamental for tasks like clustering or **retrieval**.\n\nThe page **does not** explicitly mention:\n\n*   Rerankers\n*   RAG alternatives\n*   Hybrid search\n*   Specific details on chunking strategies (beyond the implication of managing embedding size)\n*   Specific details on context window management (though updated GPT-4 Turbo models are mentioned as having a larger 128k context window).",
    "published_date": "2024-01-25T00:00:00.000Z",
    "score": null
  },
  "https://www.arsturn.com/blog/best-embedding-models-2024": {
    "content_path": "data/contents/38903d75ffa669ad.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.134487",
    "title": "Best Embedding Models in 2024: A Comprehensive Guide",
    "summary": "The webpage provides a guide to the **Top Embedding Models in 2024**, explaining what embedding models are (algorithms that convert data into numerical vectors) and why choosing the right one is important.\n\nIt lists several top embedding models, including:\n*   **OpenAI's Text-Embedding-3** (small and large variants)\n*   **Google's Universal Sentence Encoder (USE)**\n*   **Hugging Face's BERT**\n*   **MigMSum's E5 Models**\n*   **Meta's LLaMA Models**\n*   **Cohere's Embedding Models**\n*   **Open-source Multilingual Models** (e.g., BGE-M3)\n*   **Sentence Transformers**\n\nThe user query asks about **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, and context window management.**\n\nThe page focuses heavily on **embeddings (new efficient models)**, listing several top models for 2024. However, it **does not mention** vector databases, rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, or context window management.\n\n**Summary based on the query:** The page details the top embedding models available in 2024, which aligns with the \"embeddings (new efficient models)\" part of your query. However, it does not provide information on vector databases, rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, or context window management.",
    "published_date": "2024-08-18T00:00:00.000Z",
    "score": null
  },
  "https://proceedings.iclr.cc/paper_files/paper/2025/hash/c4bf73386022473a652a18941e9ea6f8-Abstract-Conference.html": {
    "content_path": "data/contents/d86da1939e9a0e83.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.168829",
    "title": "Improved Techniques for Training LLMs as Generalist Embedding Models",
    "summary": "The webpage describes the **NV-Embed model**, which focuses on **improved techniques for training LLMs to be generalist embedding models** for tasks like dense vector-based retrieval.\n\nThe summary of the content relevant to your query is:\n\n*   **Embeddings (new efficient models):** The paper introduces the **NV-Embed model**, which uses decoder-only LLMs to create high-performing embedding models, outperforming BERT or T5-based models.\n*   **Retrieval:** The model is specifically designed to enhance performance in **dense vector-based retrieval**, achieving the No. 1 position on the Massive Text Embedding Benchmark (MTEB).\n*   **Techniques mentioned:** The improvements come from architectural designs (proposing a **latent attention layer** for pooled embeddings), training procedures (a **two-stage contrastive instruction-tuning method**), and curated datasets (using hard-negative mining and synthetic data).\n\nThe page **does not** discuss: Vector databases, rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, or context window management.",
    "published_date": "2025-05-01T00:00:00.000Z",
    "score": null
  },
  "https://ui.adsabs.harvard.edu/abs/2025arXiv250220204A/abstract": {
    "content_path": "data/contents/6ec971f5e40259ba.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.201806",
    "title": "Granite Embedding Models - ADS",
    "summary": "The webpage describes the **Granite Embedding Models**, a family of encoder-based embedding models designed for **retrieval tasks**, supporting both dense-retrieval and sparse retrieval architectures, with English and Multilingual capabilities. The models come in 12-layer and efficient 6-layer distilled versions. They were trained using techniques like retrieval-oriented pretraining, contrastive finetuning, knowledge distillation, and model merging, and are publicly released under the Apache 2.0 license.\n\nWhile the page discusses **embeddings** and **retrieval** (which aligns with parts of your query), it does not mention: Vector databases, RAG architectures, RAG alternatives, hybrid search, chunking strategies, or context window management.",
    "published_date": "2025-02-01T00:00:00.000Z",
    "score": null
  },
  "https://openreview.net/forum?id=lgsyLSsDRe": {
    "content_path": "data/contents/f345b8724e635592.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.234742",
    "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist...",
    "summary": "The webpage describes the **NV-Embed model**, which focuses on improving **LLMs as generalist embedding models** for tasks like dense vector-based retrieval.\n\nKey techniques mentioned include:\n*   **Model Architecture:** Proposing a **latent attention layer** for obtaining pooled embeddings, which outperforms mean pooling or using the last token embedding.\n*   **Training Procedure:** Introducing a **two-stage contrastive instruction-tuning method**.\n*   **Representation Learning:** Removing the causal attention mask of LLMs during contrastive training.\n*   **Training Data:** Utilizing hard-negative mining, synthetic data generation, and public datasets.\n\nThe NV-Embed model achieved the No.1 position on the Massive Text Embedding Benchmark (MTEB) and performed well on the AIR Benchmark.\n\n**Regarding your specific query topics:**\n\n*   **Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, context window management:** The page directly discusses **embeddings** (specifically improving LLMs as embedding models) and **retrieval**. It does not explicitly detail vector databases, rerankers, RAG architectures/alternatives, hybrid search, chunking strategies, or context window management.",
    "published_date": "2025-01-22T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2405.17428": {
    "content_path": "data/contents/a8a565caff8a3254.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.269723",
    "title": "Computer Science > Computation and Language",
    "summary": "The webpage describes a paper titled **\"NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models\"**.\n\nThe paper introduces **NV-Embed**, which focuses on improving **LLM-based embedding models** for general-purpose text embedding and dense vector-based retrieval. The techniques discussed include:\n\n*   **Model Architecture:** Proposing a **latent attention layer** for obtaining pooled embeddings, which outperforms mean pooling or using the last `<EOS>` token embedding.\n*   **Representation Learning:** Removing the causal attention mask during contrastive training.\n*   **Training Algorithm:** Introducing a **two-stage contrastive instruction-tuning method** that first uses retrieval datasets with hard negatives, and then blends in non-retrieval tasks.\n*   **Training Data:** Utilizing hard-negative mining, synthetic data generation, and public datasets.\n\nThe resulting NV-Embed models achieved the No.1 position on the MTEB leaderboard across 56 tasks. The paper also provides analysis of model compression techniques for these generalist embedding models.\n\nWhile the paper heavily focuses on **embeddings** and improving their performance for **retrieval** (which relates to RAG architectures), it does **not** explicitly detail vector databases, rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, or context window management, although the context of improved embeddings is foundational to RAG.",
    "published_date": "2024-05-27T00:00:00.000Z",
    "score": null
  },
  "https://huggingface.co/papers/2412.13663": {
    "content_path": "data/contents/bef4bbc359e7856f.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.532063",
    "title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\n  Fast, Memory Efficient, and Long Context Finetuning and Inference",
    "summary": "The provided web page discusses **ModernBERT**, an optimized encoder-only transformer model that offers superior performance and efficiency for **retrieval** and classification tasks compared to older models like BERT.\n\nKey points relevant to your query include:\n*   **Retrieval:** ModernBERT exhibits state-of-the-art results on single and multi-vector **retrieval** across different domains.\n*   **Efficiency:** It is designed to be the most **speed** and **memory efficient** encoder for inference on common GPUs.\n*   **Context Window:** It was trained with a native **8192 sequence length**.\n\nHowever, the page **does not** explicitly mention or detail:\n*   Vector databases\n*   Embeddings (new efficient models, beyond the context of the ModernBERT model itself)\n*   Rerankers\n*   RAG architectures or alternatives\n*   Hybrid search\n*   Chunking strategies\n*   Context window management (beyond the native sequence length of the model)\n\nTherefore, for most of the specific topics listed in your query, the answer is **No answer found**.",
    "published_date": "2024-12-18T00:00:00.000Z",
    "score": null
  },
  "https://meetcody.ai/blog/text-embedding-models/": {
    "content_path": "data/contents/43727aed0773e8b8.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:14.565474",
    "title": "Top 8 Text Embedding Models in 2024",
    "summary": "The webpage discusses **text embedding models**, explaining that they transform text into numerical vectors to help computers understand meaning. It covers the basics of text embedding, differentiates it from word embedding, and lists the **Top 8 text embedding models**: Word2Vec, GloVe, FastText, ELMO, BERT, GPT, Doc2Vec, and USE.\n\nThe user query is about **retrieval and embeddings**, specifically mentioning: Vector databases, embeddings (new efficient models), rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, and context window management.\n\n**Summary relative to the query:**\n\nThe page focuses heavily on **embeddings** (defining them and listing 8 models), which aligns with part of the query. However, it **does not mention** vector databases, rerankers, RAG architectures, RAG alternatives, hybrid search, chunking strategies, or context window management.\n\n**Conclusion:** The page provides information on text embeddings but does not cover the broader topics of RAG architectures, vector databases, or related retrieval components mentioned in the query.\n\nNo answer found.",
    "published_date": "2024-01-24T07:58:56.000Z",
    "score": null
  },
  "https://js.langchain.com/v0.2/docs/how_to/contextual_compression/": {
    "content_path": "data/contents/670221b45e6b3aa7.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.297215",
    "title": "LangChain overview",
    "summary": "LangChain is an open-source framework for building agents and LLM-powered applications. It provides a pre-built agent architecture and integrations for various models (OpenAI, Anthropic, Google, etc.) and tools, allowing for quick setup.\n\n**Key features and components mentioned in the text that relate to your query:**\n\n*   **Agent Framework:** LangChain itself is an agent framework. It is built on top of **LangGraph** for advanced needs like orchestration, deterministic/agentic workflows, and customization.\n*   **Tool Use/Function Calling:** LangChain supports connecting tools (like the `get_weather` example) to agents.\n*   **Structured Output:** The framework supports generating structured outputs.\n*   **Model Context Protocol (MCP):** The documentation mentions connecting to services via **MCP** for real-time answers.\n*   **Context Engineering:** This is listed as an advanced usage topic.\n*   **Long Context:** This is listed under Advanced Usage.\n*   **Agent Orchestration:** Handled by **LangGraph**, which LangChain agents are built upon.\n*   **Integrations:** Connects to models from OpenAI, Anthropic, and Google.\n\n**Topics from your query *not* explicitly detailed or mentioned in the provided text:**\n\n*   Deep research\n*   Agent memory / Agentic memory / Short-term memory (though \"Short-term memory\" is listed in the navigation, the text doesn't elaborate on it)\n*   Long-term memory (listed in navigation, but not detailed)\n*   Context compression\n*   OpenAI Agents SDK, Anthropic Agents SDK, Google SDK (LangChain integrates with these providers, but the specific SDKs aren't detailed)\n*   Agent evaluations / LLM reasoning trace evaluation\n*   Prompt engineering\n*   Supervised fine-tuning\n*   Reinforcement learning\n\n**Summary:** LangChain is an easy-to-use framework for building agents, offering standardized model interfaces, tool use, structured outputs, and is built on LangGraph for orchestration. It supports **MCP** and **Context Engineering**. Many advanced topics like specific memory types, evaluations, and fine-tuning are listed in the navigation but not explained in the overview text provided.",
    "published_date": "2025-11-24T00:00:00.000Z",
    "score": null
  },
  "https://docs.langchain.com/oss/python/langchain-mcp": {
    "content_path": "data/contents/1c46472c9a517338.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:15.602725",
    "title": "Page not found",
    "summary": "The user query is a list of technical terms related to building AI agents, including specific frameworks, concepts, and components like \"agent\\_infrastructure,\" \"MCP servers,\" \"LangChain,\" \"agent memory,\" \"function calling,\" and \"prompt engineering.\"\n\nThe provided webpage text is a \"404 Page not found\" error from the LangChain documentation website. While the navigation links on the page mention many of the concepts listed in the query (e.g., \"Agents,\" \"Tools,\" \"Short-term memory,\" \"Context engineering,\" \"Model Context Protocol (MCP),\" \"LangChain\"), the page itself does not contain any substantive content or explanation to summarize regarding these topics.\n\n**Summary:**\n\nNo answer found. The provided webpage is a \"404 Page not found\" error from the LangChain documentation site and does not contain the information necessary to summarize the technical concepts listed in your query.",
    "published_date": "2025-05-12T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/pdf/2510.22751": {
    "content_path": "data/contents/c5580524eec46544.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.286896",
    "title": "",
    "summary": "The user query asks for a summary covering several advanced topics related to Large Language Models (LLMs), including **reasoning and planning, chain-of-thought, inference-time compute, self-reflection, MCTS, test-time scaling, hallucination reduction and detection, grounding, and factuality.**\n\nThis webpage focuses heavily on **hallucination reduction and detection, grounding, and factuality** using a **Multi-Modal Fact-Verification Framework**.\n\nHere is a summary of the page content relevant to the query:\n\nThe document details a **Multi-Modal Fact-Verification Framework** designed to reduce hallucinations in LLMs by cross-checking outputs against multiple knowledge sources in real-time during inference.\n\n**Key aspects covered in the paper related to the query:**\n\n*   **Hallucination Reduction and Detection/Factuality:** This is the core focus. The framework achieves a **67% reduction in hallucinations** by detecting and correcting factual errors as they are generated. It combines structured databases, live web searches, and academic literature for verification.\n*   **Grounding:** The framework emphasizes **Dynamic Knowledge Integration**, combining static knowledge graphs (like Wikidata) with real-time information retrieval (web search) to ensure outputs are grounded in both authoritative and up-to-date sources.\n*   **Inference-Time Compute/Intervention:** The system is designed as a modular, **inference-time intervention** that operates without requiring model retraining.\n*   **Uncertainty Quantification:** The framework uses **Probabilistic Confidence Scoring**, combining model-intrinsic confidence (from attention patterns, dropout) with external evidence strength to provide calibrated reliability estimates.\n*   **Reasoning and Planning (Limited Coverage):** While the primary focus is factuality, the related work section mentions that **Chain-of-Thought (CoT) prompting** has shown effectiveness in logical reasoning tasks, but notes that existing CoT methods primarily address reasoning errors rather than factual inaccuracies—which this new framework aims to solve. The discussion section mentions future work involving integration with **symbolic reasoning systems** for complex logical claims.\n*   **Self-Reflection/Self-Correction (Related):** The related work mentions **Constitutional AI** methods that train models to critique and revise their own outputs, but the proposed framework focuses on external, multi-source verification rather than purely internal self-correction.\n\n**Topics from the query that are NOT substantially covered:**\n\n*",
    "published_date": null,
    "score": null
  },
  "https://openreview.net/pdf?id=zQbff3h0da": {
    "content_path": "data/contents/bf81027aa06597cd.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:16.483723",
    "title": "",
    "summary": "The provided text introduces the **Reasoning via Planning (RAP)** framework, which aims to enhance the reasoning capabilities of Large Language Models (LLMs) by equipping them with an internal **world model** and using **Monte Carlo Tree Search (MCTS)** for planning.\n\nKey aspects related to the user query:\n\n*   **Reasoning LLMs and Chain-of-Thought (CoT):** The paper notes that while LLMs show reasoning capabilities, especially with CoT prompting, they struggle with complex planning, math, or logical reasoning because they lack an internal world model to simulate outcomes.\n*   **Planning with LLMs and MCTS:** RAP repurposes the LLM as both the reasoning agent and the world model. It uses MCTS, a principled planning algorithm, to strategically explore the vast reasoning space, balancing **exploration vs. exploitation** to find high-reward reasoning paths.\n*   **Inference-time compute/Scaling:** MCTS involves multiple iterations (computational budget) to build the reasoning tree, which is how RAP utilizes increased inference-time compute compared to linear CoT.\n*   **Self-reflection (Self-evaluation):** RAP incorporates a reward mechanism that includes **self-evaluation by the LLM** (\"Is this reasoning step correct?\"), using the confidence of the LLM's \"Yes\" response as a reward signal.\n*   **Hallucination Reduction and Grounding:** By incorporating a world model that predicts the next state ($s_{t+1}$) based on the current state ($s_t$) and action ($a_t$), RAP provides a more **grounded and coherent inference** compared to standard CoT, which only generates actions sequentially. The world model helps avoid generating illegal plans (as seen in the Blocksworld example).\n*   **Factuality/Logical Reasoning:** RAP is shown to significantly improve performance on logical reasoning tasks (PrOntoQA) by allowing the framework to backtrack and explore alternatives when a reasoning chain hits a dead end, guided by rewards.\n\nIn summary, RAP addresses the limitations of standard reasoning methods by integrating planning (MCTS) guided by a world model (the LLM itself) and reward signals (including self-evaluation) to achieve better planning, grounding, and performance across various reasoning tasks.\n\nThe provided text discusses **Reasoning LLMs** and **Planning with LLMs**, specifically introducing a framework called **RAP** (Reasoning and Planning) that couples LLMs' reasoning with a world model and **Monte Carlo Tree Search (MCTS)** for planning.\n\nKey points related to your query:\n\n*   **Reasoning LLMs and Chain-of-Thought (CoT):** The RAP framework is shown to outperform contemporary **CoT-based reasoning approaches** and even GPT-4 in certain settings. The text also references work on CoT reasoning (e.g., [59], [47], [37]).\n*   **Planning with LLMs and MCTS:** RAP uses **MCTS** to search for an optimal reasoning path, adapting the algorithm to handle the large reasoning space and computational cost of LLMs. The text details modifications to MCTS for LLMs, including sampling potential actions and using lightweight local rewards (like **self-reflection** rewards) for exploration guidance.\n*   **Inference-time compute and Test-time scaling:** While not explicitly detailed as a section, the use of MCTS inherently involves iterative computation at inference time to explore the search tree.\n*   **Self-reflection:** The ablation studies (Tables 4 and 5) highlight the positive impact of using a **self-evaluation reward** (a form of self-reflection) in guiding the reasoning process, especially in math reasoning tasks.\n*   **Grounding and Hallucination Reduction:** The text mentions using LLMs as **world models** to simulate states and anticipate outcomes, which is a form of grounding the reasoning process in a simulated environment. The use of rewards like action likelihood and self-evaluation aims to improve the quality and factuality of the generated plans/reasoning steps.\n\nThe document focuses heavily on the RAP framework, which integrates reasoning, planning (via MCTS), and world models, showing improvements over standard CoT methods.",
    "published_date": null,
    "score": null
  },
  "https://aclanthology.org/2024.knowledgenlp-1.4/": {
    "content_path": "data/contents/b54b04180bb199c9.md",
    "source": "exa",
    "saved_at": "2025-12-30T14:40:17.649755",
    "title": "Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation",
    "summary": "The provided webpage focuses on **hallucination mitigation in LLMs by leveraging self-awareness** of their internal knowledge state.\n\nIt discusses:\n*   Evaluating LLMs' ability to discern and express their internal knowledge state.\n*   Observing that while LLMs show high accuracy (over 85%) in probing their knowledge state, they often fail to express it faithfully during generation, leading to hallucinations.\n*   The development of **DreamCatcher**, an automated tool for hallucination annotation that uses knowledge probing and consistency checking.\n*   A proposed training framework called **Reinforcement Learning from Knowledge Feedback (RLKF)**, which uses knowledge preference as a reward to enhance the factuality and honesty of LLMs.\n\n**The page does not directly discuss:** Reasoning LLMs, chain-of-thought, inference-time compute, self-reflection (beyond knowledge state awareness), planning with LLMs, MCTS for language models, test-time scaling, detection of hallucinations (only mitigation), grounding, or general factuality beyond the scope of self-awareness and RLKF training.",
    "published_date": "2024-08-01T00:00:00.000Z",
    "score": null
  },
  "https://arxiv.org/abs/2512.08536": {
    "content_path": "data/contents/2512.08536.md",
    "pdf_path": "data/contents/2512.08536.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:11.198164",
    "title": "Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans"
  },
  "https://arxiv.org/abs/2407.16833": {
    "content_path": "data/contents/2407.16833.md",
    "pdf_path": "data/contents/2407.16833.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:11.450191",
    "title": "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach"
  },
  "https://arxiv.org/abs/2512.08228": {
    "content_path": "data/contents/2512.08228.md",
    "pdf_path": "data/contents/2512.08228.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:15.300900",
    "title": "MM-CoT: A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models"
  },
  "https://arxiv.org/abs/2511.05747": {
    "content_path": "data/contents/2511.05747.md",
    "pdf_path": "data/contents/2511.05747.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:16.760357",
    "title": "CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization"
  },
  "https://arxiv.org/abs/2511.04662": {
    "content_path": "data/contents/2511.04662.md",
    "pdf_path": "data/contents/2511.04662.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:18.564651",
    "title": "VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks"
  },
  "https://arxiv.org/abs/2511.09345": {
    "content_path": "data/contents/2511.09345.md",
    "pdf_path": "data/contents/2511.09345.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:19.453185",
    "title": "Seer Self-Consistency: Advance Budget Estimation for Adaptive Test-Time Scaling"
  },
  "https://arxiv.org/abs/2512.23557": {
    "content_path": "data/contents/2512.23557.md",
    "pdf_path": "data/contents/2512.23557.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:19.849736",
    "title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks"
  },
  "https://arxiv.org/abs/2510.18442": {
    "content_path": "data/contents/2510.18442.md",
    "pdf_path": "data/contents/2510.18442.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:23.789103",
    "title": "PlanU: Large Language Model Decision Making through Planning under Uncertainty"
  },
  "https://arxiv.org/abs/2507.05257": {
    "content_path": "data/contents/2507.05257.md",
    "pdf_path": "data/contents/2507.05257.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:24.615655",
    "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions"
  },
  "https://arxiv.org/abs/2510.09312": {
    "content_path": "data/contents/2510.09312.md",
    "pdf_path": "data/contents/2510.09312.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:24.997647",
    "title": "Verifying Chain-of-Thought Reasoning via Its Computational Graph"
  },
  "https://arxiv.org/abs/2512.23547": {
    "content_path": "data/contents/2512.23547.md",
    "pdf_path": "data/contents/2512.23547.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:25.787031",
    "title": "Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs"
  },
  "https://arxiv.org/abs/2508.18669": {
    "content_path": "data/contents/2508.18669.md",
    "pdf_path": "data/contents/2508.18669.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:27.974443",
    "title": "MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use"
  },
  "https://arxiv.org/abs/2510.15674": {
    "content_path": "data/contents/2510.15674.md",
    "pdf_path": "data/contents/2510.15674.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:30.412507",
    "title": "CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning"
  },
  "https://arxiv.org/abs/2505.16122": {
    "content_path": "data/contents/2505.16122.md",
    "pdf_path": "data/contents/2505.16122.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:32.127907",
    "title": "Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning"
  },
  "https://arxiv.org/abs/2510.04514": {
    "content_path": "data/contents/2510.04514.md",
    "pdf_path": "data/contents/2510.04514.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:41.886882",
    "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering"
  },
  "https://arxiv.org/abs/2510.13804": {
    "content_path": "data/contents/2510.13804.md",
    "pdf_path": "data/contents/2510.13804.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:47.967997",
    "title": "Generative Universal Verifier as Multimodal Meta-Reasoner"
  },
  "https://arxiv.org/abs/2508.18984": {
    "content_path": "data/contents/2508.18984.md",
    "pdf_path": "data/contents/2508.18984.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:53.117252",
    "title": "Enhancing Document VQA Models via Retrieval-Augmented Generation"
  },
  "https://arxiv.org/abs/2503.16416": {
    "content_path": "data/contents/2503.16416.md",
    "pdf_path": "data/contents/2503.16416.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:50:54.094648",
    "title": "Survey on Evaluation of LLM-based Agents"
  },
  "https://arxiv.org/abs/2504.09795": {
    "content_path": "data/contents/2504.09795.md",
    "pdf_path": "data/contents/2504.09795.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:00.306457",
    "title": "VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents"
  },
  "https://arxiv.org/abs/2512.18552": {
    "content_path": "data/contents/2512.18552.md",
    "pdf_path": "data/contents/2512.18552.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:00.648811",
    "title": "Self-Play SWE-RL: Training Superintelligent Software Agents"
  },
  "https://arxiv.org/abs/2412.05265": {
    "content_path": "data/contents/2412.05265.md",
    "pdf_path": "data/contents/2412.05265.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:05.353603",
    "title": "Reinforcement Learning: An Overview"
  },
  "https://arxiv.org/html/2503.12326v1": {
    "content_path": "data/contents/2503.12326v1.md",
    "pdf_path": "data/contents/2503.12326v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:05.821178",
    "title": "Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots"
  },
  "https://arxiv.org/abs/2512.08296": {
    "content_path": "data/contents/2512.08296.md",
    "pdf_path": "data/contents/2512.08296.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:06.210515",
    "title": "Towards a Science of Scaling Agent Systems"
  },
  "https://arxiv.org/abs/2512.10942": {
    "content_path": "data/contents/2512.10942.md",
    "pdf_path": "data/contents/2512.10942.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:06.512587",
    "title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language"
  },
  "https://arxiv.org/abs/2512.22431": {
    "content_path": "data/contents/2512.22431.md",
    "pdf_path": "data/contents/2512.22431.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:06.761610",
    "title": "Monadic Context Engineering"
  },
  "https://arxiv.org/abs/2508.00554": {
    "content_path": "data/contents/2508.00554.md",
    "pdf_path": "data/contents/2508.00554.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:08.742607",
    "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism"
  },
  "https://arxiv.org/abs/2510.09586": {
    "content_path": "data/contents/2510.09586.md",
    "pdf_path": "data/contents/2510.09586.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:09.441000",
    "title": "Vision Language Models: A Survey of 26K Papers"
  },
  "https://arxiv.org/abs/2512.06636": {
    "content_path": "data/contents/2512.06636.md",
    "pdf_path": "data/contents/2512.06636.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:11.159178",
    "title": "Distribution-Aware Exploration for Adaptive HNSW Search"
  },
  "https://arxiv.org/abs/2501.02189": {
    "content_path": "data/contents/2501.02189.md",
    "pdf_path": "data/contents/2501.02189.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:12.278231",
    "title": "A Survey of State of the Art Large Vision Language Models"
  },
  "https://arxiv.org/abs/2512.05470": {
    "content_path": "data/contents/2512.05470.md",
    "pdf_path": "data/contents/2512.05470.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:12.858192",
    "title": "Everything is Context: Agentic File System Abstraction for Context Engineering"
  },
  "https://arxiv.org/abs/2511.17908": {
    "content_path": "data/contents/2511.17908.md",
    "pdf_path": "data/contents/2511.17908.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:13.127187",
    "title": "Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction"
  },
  "https://arxiv.org/abs/2509.13160": {
    "content_path": "data/contents/2509.13160.md",
    "pdf_path": "data/contents/2509.13160.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:13.795013",
    "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning"
  },
  "https://arxiv.org/abs/2511.11653": {
    "content_path": "data/contents/2511.11653.md",
    "pdf_path": "data/contents/2511.11653.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:14.353986",
    "title": "GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning"
  },
  "https://arxiv.org/abs/2510.13329": {
    "content_path": "data/contents/2510.13329.md",
    "pdf_path": "data/contents/2510.13329.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:15.413622",
    "title": "Embedding-Based Context-Aware Reranker"
  },
  "https://arxiv.org/abs/2512.23219v1": {
    "content_path": "data/contents/2512.23219v1.md",
    "pdf_path": "data/contents/2512.23219v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:16.614998",
    "title": "MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?"
  },
  "https://arxiv.org/abs/2501.01880": {
    "content_path": "data/contents/2501.01880.md",
    "pdf_path": "data/contents/2501.01880.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:16.935782",
    "title": "Long Context vs. RAG for LLMs: An Evaluation and Revisits"
  },
  "https://arxiv.org/html/2512.21878v1": {
    "content_path": "data/contents/2512.21878v1.md",
    "pdf_path": "data/contents/2512.21878v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:17.206077",
    "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting"
  },
  "https://arxiv.org/abs/2512.23189v1": {
    "content_path": "data/contents/2512.23189v1.md",
    "pdf_path": "data/contents/2512.23189v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:32.329315",
    "title": "The Dawn of Agentic EDA: A Survey of Autonomous Digital Chip Design"
  },
  "https://arxiv.org/abs/2512.23169v1": {
    "content_path": "data/contents/2512.23169v1.md",
    "pdf_path": "data/contents/2512.23169v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:32.767115",
    "title": "REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation"
  },
  "https://arxiv.org/html/2512.16236v1": {
    "content_path": "data/contents/2512.16236v1.md",
    "pdf_path": "data/contents/2512.16236v1.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:32.959176",
    "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models"
  },
  "https://arxiv.org/abs/2512.22250": {
    "content_path": "data/contents/2512.22250.md",
    "pdf_path": "data/contents/2512.22250.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:34.623650",
    "title": "Hallucination Detection for LLM-based Text-to-SQL Generation via Two-Stage Metamorphic Testing"
  },
  "https://arxiv.org/html/2407.06567v3": {
    "content_path": "data/contents/2407.06567v3.md",
    "pdf_path": "data/contents/2407.06567v3.pdf",
    "source": "arxiv",
    "saved_at": "2025-12-30T14:51:35.374093",
    "title": "FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement"
  },
  "https://www.anthropic.com/news/model-context-protocol": {
    "content_path": "data/contents/e283b9c34207eff8.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:01.111932",
    "title": "Introducing the Model Context Protocol"
  },
  "https://www.4degrees.ai/blog/harnessing-ai-a-practical-guide-to-using-large-language-models-in-venture-capital": {
    "content_path": "data/contents/8a55a0859b5da87d.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:01.371045",
    "title": "Harnessing AI: A Practical Guide to Using Large Language Models in Venture Capital"
  },
  "https://huggingface.co/BAAI/bge-m3": {
    "content_path": "data/contents/2eda4773a3cebbc6.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:01.425137",
    "title": "BAAI/bge-m3 · Hugging Face"
  },
  "https://docs.llamaindex.ai/en/stable/use_cases/agents/": {
    "content_path": "data/contents/637a6bbe66ac0bdf.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:01.792381",
    "title": "Agents"
  },
  "https://anthropic.com/news/context-management": {
    "content_path": "data/contents/e6eb1c3cba28cfe8.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:01.924445",
    "title": "Managing context on the Claude Developer Platform"
  },
  "https://openaccess.thecvf.com/content/ICCV2025W/Findings/html/Cho_M3DocVQA_Multi-modal_Multi-page_Multi-document_Understanding_ICCVW_2025_paper.html": {
    "content_path": "data/contents/7a9ef3414c4fa3ae.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:02.007550",
    "title": "ICCV 2025 Open Access Repository"
  },
  "https://aclanthology.org/2025.findings-acl.808/": {
    "content_path": "data/contents/4c6c42dc94204f42.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:02.065463",
    "title": "Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study"
  },
  "https://www.databricks.com/blog/long-context-rag-performance-llms": {
    "content_path": "data/contents/0ebf0703f777406b.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:02.117280",
    "title": "Long Context RAG Performance of LLMs"
  },
  "https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/": {
    "content_path": "data/contents/8498e968739ee2e3.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:02.405528",
    "title": "FACTS Benchmark Suite: a new way to systematically evaluate LLMs factuality"
  },
  "https://www.confident-ai.com/blog/llm-agent-evaluation-complete-guide": {
    "content_path": "data/contents/95b00a6a839de0f7.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:02.906488",
    "title": "LLM Agent Evaluation: Assessing Tool Use, Task Completion, Agentic Reasoning, and More - Confident AI"
  },
  "https://agentdojo.spylab.ai/": {
    "content_path": "data/contents/23cfe4a2cea476a7.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:03.034555",
    "title": "AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents¶"
  },
  "https://venturebeat.com/ai/cohere-launches-embed-4-new-multimodal-search-model-processes-200-page-documents/": {
    "content_path": "data/contents/90f201809b6227fe.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:03.135088",
    "title": "Cohere launches Embed 4: New multimodal search model processes 200-page documents"
  },
  "https://developers.googleblog.com/introducing-gemini-2-5-flash-image/": {
    "content_path": "data/contents/61c7044ad3c4bd4c.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:03.364666",
    "title": "Introducing Gemini 2.5 Flash Image, our state-of-the-art image model"
  },
  "https://proceedings.iclr.cc/paper_files/paper/2025/hash/99e9e141aafc314f76b0ca3dd66898b3-Abstract-Conference.html": {
    "content_path": "data/contents/c674c641f3426c23.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:03.618547",
    "title": "Efficient Document Retrieval with Vision Language Models"
  },
  "https://venturebeat.com/ai/anthropic-says-it-solved-the-long-running-ai-agent-problem-with-a-new-multi": {
    "content_path": "data/contents/e2efe6e8998dc063.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:03.798405",
    "title": "Anthropic says it solved the long-running AI agent problem with a new multi-session Claude SDK"
  },
  "https://ideas.repec.org/p/arx/papers/2402.02315.html": {
    "content_path": "data/contents/50e8c0a41742e073.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:03.849413",
    "title": "A Survey of Large Language Models in Finance (FinLLMs)"
  },
  "https://github.com/milvus-io/milvus/releases/tag/v2.6.7": {
    "content_path": "data/contents/e4eecfa7a9bb1717.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:03.881724",
    "title": "Release milvus-2.6.7 · milvus-io/milvus"
  },
  "https://www.bloomberg.com/company/press/bloomberg-accelerates-financial-analysis-with-gen-ai-document-insights/": {
    "content_path": "data/contents/f723f251976918de.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:03.911589",
    "title": "Bloomberg Accelerates Financial Analysis with Gen AI Document Insights | Press | Bloomberg LP"
  },
  "https://www.globenewswire.com/news-release/2025/03/04/3036461/0/en/TigerGraph-Unveils-Next-Generation-Hybrid-Search-to-its-Graph-Database-to-Power-AI-at-Scale-Also-Introduces-a-Game-Changing-Community-Edition.html": {
    "content_path": "data/contents/6718ffa1a2028155.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:03.941652",
    "title": "TigerGraph Unveils Next Generation Hybrid Search to its Graph Database to Power AI at Scale; Also Introduces a Game-Changing Community Edition"
  },
  "https://github.com/junhongmit/P-and-B": {
    "content_path": "data/contents/4bb45c13ce19b137.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:04.002774",
    "title": "GitHub - junhongmit/P-and-B"
  },
  "https://openai.com/index/healthbench/": {
    "content_path": "data/contents/6033ae17c9c0f472.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:04.063010",
    "title": "Introducing HealthBench"
  },
  "https://github.com/athrael-soju/Snappy": {
    "content_path": "data/contents/bc6dc9fe4809f49f.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:04.215189",
    "title": "GitHub - athrael-soju/Snappy: 🐊 Snappy's unique approach unifies vision-language late interaction with structured OCR for region-level knowledge retrieval. Like the project? Drop a star! ⭐"
  },
  "https://github.com/IDEA-FinAI/Awesome-FinLLMs": {
    "content_path": "data/contents/8b350a2f501fc48a.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:04.321181",
    "title": "GitHub - DataArcTech/Awesome-FinLLMs: 🥇 A curated list of awesome large language models in finance(FinLLMs), including papers,models,datasets and codebases. 金融大模型列表，特别是中英双语大模型。"
  },
  "https://github.com/vals-ai/finance-agent": {
    "content_path": "data/contents/103805be976a5e9b.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:04.377524",
    "title": "GitHub - vals-ai/finance-agent"
  },
  "https://github.com/stefanoamorelli/sec-edgar-agentkit": {
    "content_path": "data/contents/b280a75a63ce7ac8.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:04.484167",
    "title": "GitHub - stefanoamorelli/sec-edgar-agentkit: AI agent toolkit for accessing and analyzing SEC EDGAR filing data. Build intelligent agents with LangChain, MCP-use, Gradio, Dify, and smolagents to analyze financial statements, insider trading, and company filings."
  },
  "https://aws.amazon.com/blogs/aws/amazon-nova-multimodal-embeddings-now-available-in-amazon-bedrock/": {
    "content_path": "data/contents/39f1fff059d249d4.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:04.530461",
    "title": "Amazon Nova Multimodal Embeddings: State-of-the-art embedding model for agentic RAG and semantic search | Amazon Web Services"
  },
  "https://learn.microsoft.com/en-us/dynamics365/release-plan/2025wave1/finance-supply-chain/finance-operations-crossapp-capabilities/build-agents-dynamics-365-finance-operations-model-context-protocol": {
    "content_path": "data/contents/a41fb59c3da50ef3.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:04.563803",
    "title": "Build agents for finance and operations with Model Context Protocol"
  },
  "https://alpaca.markets/blog/introducing-official-mcp-server-enabling-multi-market-trading-with-ai-interfaces/": {
    "content_path": "data/contents/4d97fa2ed1bc566f.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:04.658671",
    "title": "Alpaca Launches Official MCP Server for AI Trading"
  },
  "https://github.com/alpacahq/alpaca-mcp-server": {
    "content_path": "data/contents/6c630cf208a6751a.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:04.748275",
    "title": "GitHub - alpacahq/alpaca-mcp-server: Alpaca’s official MCP Server lets you trade stocks, ETFs, crypto, and options, run data analysis, and build strategies in plain English directly from your favorite LLM tools and IDEs"
  },
  "https://grata.com/ai": {
    "content_path": "data/contents/b77d835de4626b26.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:05.053840",
    "title": "AI Deal Sourcing Platform | Grata AI"
  },
  "https://github.com/huggingface/search-and-learn": {
    "content_path": "data/contents/3ff67b1e14d4b19e.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:05.239373",
    "title": "GitHub - huggingface/search-and-learn: Recipes to scale inference-time compute of open models"
  },
  "https://daloopa.com/blog/press-release/daloopa-expands-financial-data-model-context-protocol-mcp-through-a-new-connector-with-openai": {
    "content_path": "data/contents/d3fe79f64e99e342.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:05.355639",
    "title": "Page Not Found - Daloopa"
  },
  "https://blog.langchain.com/reflection-agents/": {
    "content_path": "data/contents/c4412504face4e87.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:05.874955",
    "title": "Reflection Agents"
  },
  "https://www.litellm.ai/changelog": {
    "content_path": "data/contents/25cf512b820bdd0b.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:06.039644",
    "title": "LiteLLM"
  },
  "https://github.com/openai/codex": {
    "content_path": "data/contents/8c3dc2a0187cafcc.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:06.406197",
    "title": "GitHub - openai/codex: Lightweight coding agent that runs in your terminal"
  },
  "https://www.stack-ai.com/customers/how-a-pe-firm-automated-company-due-diligence-process-with-ai": {
    "content_path": "data/contents/3cfba3bec762b453.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:06.884886",
    "title": "How a Private Equity Firm Cut Diligence Costs by 80%"
  },
  "https://github.com/Pikurrot/RAG-DocVQA": {
    "content_path": "data/contents/11127a3646610580.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:06.943616",
    "title": "GitHub - Pikurrot/RAG-DocVQA: A RAG system applied to Document Visual Question Answering. My project for the internship in CVC."
  },
  "https://github.com/namin/llm-verified-with-monte-carlo-tree-search": {
    "content_path": "data/contents/8647edd4629ef761.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:07.033208",
    "title": "GitHub - namin/llm-verified-with-monte-carlo-tree-search: LLM verified with Monte Carlo Tree Search"
  },
  "https://mistral.ai/news/mistral-3": {
    "content_path": "data/contents/3772caab2a3ff582.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:07.194888",
    "title": "Introducing Mistral 3"
  },
  "https://techcommunity.microsoft.com/blog/adforpostgresql/diskann-on-azure-database-for-postgresql-%E2%80%93-now-generally-available/4414723": {
    "content_path": "data/contents/ae95205952f81d54.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:07.512860",
    "title": "DiskANN on Azure Database for PostgreSQL – Now Generally Available"
  },
  "https://www.linedata.com/transforming-private-equity-portfolio-monitoring-and-valuation-ai-enabled-data-management-0": {
    "content_path": "data/contents/668eb400f935d1e2.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:07.881457",
    "title": "AI-Powered Portfolio Monitoring & Valuations for Private Equity"
  },
  "https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models": {
    "content_path": "data/contents/61d989a2a5ea2f52.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:08.137495",
    "title": "LLM Hallucinations in 2025: How to Understand and Tackle AI’s Most Persistent Quirk | Lakera – Protecting AI teams that disrupt the world."
  },
  "https://github.com/sierra-research/tau2-bench": {
    "content_path": "data/contents/02654a6c71943adf.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:08.843476",
    "title": "GitHub - sierra-research/tau2-bench: τ²-Bench: Evaluating Conversational Agents in a Dual-Control Environment"
  },
  "https://huggingface.co/lumees/lumees-matryoshka-embedding-v1": {
    "content_path": "data/contents/5efa9b9ec64edb4e.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:09.359612",
    "title": "lumees/lumees-matryoshka-embedding-v1 · Hugging Face"
  },
  "https://openai.com/index/gpt-5-2-codex-system-card/": {
    "content_path": "data/contents/778f2e92c8ad9095.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:09.853813",
    "title": "Addendum to GPT-5.2 System Card: GPT-5.2-Codex"
  },
  "https://www.v7labs.com/agents/ai-private-equity-portfolio-reporting-agent": {
    "content_path": "data/contents/6156d6bc53edd07b.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:10.275579",
    "title": "AI PE Portfolio Reporting Agent | Automate PortCo Monitoring | V7 Go"
  },
  "https://www.v7labs.com/automations/ai-investment-memo-generation": {
    "content_path": "data/contents/d6958942fba9a285.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:10.894897",
    "title": "AI Investment Memo Generation | Automate with V7 Go"
  },
  "https://www.pinecone.io/blog/dedicated-read-nodes/": {
    "content_path": "data/contents/6049e8d85faae3f3.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:13.660444",
    "title": "Pinecone Dedicated Read Nodes are now in Public Preview | Pinecone"
  },
  "https://github.com/FinStep-AI/ContestTrade": {
    "content_path": "data/contents/d4aacf1416fe9e6e.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:27.892871",
    "title": "GitHub - FinStep-AI/ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism"
  },
  "https://qdrant.tech/documentation/private-cloud/changelog/": {
    "content_path": "data/contents/d847ee697706a51a.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:31.064463",
    "title": "Changelog"
  },
  "https://cloud.google.com/blog/topics/customers/how-schroders-built-its-multi-agent-financial-analysis-research-assistant": {
    "content_path": "data/contents/b9cbf883ad6e8e7c.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:34.523866",
    "title": "Schroders creates multi-agent AI assistant for complex financial research using Vertex AI Agent Builder"
  },
  "https://cloud.google.com/blog/products/databases/llamaindex-integrates-with-alloydb-and-cloud-sql-for-postgresql": {
    "content_path": "data/contents/21f59dcc2917b91a.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:35.278458",
    "title": "LlamaIndex integrates with AlloyDB and Cloud SQL for PostgreSQL"
  },
  "https://www.promptingguide.ai/guides/context-engineering-guide": {
    "content_path": "data/contents/7a12e5a1b5bbff5b.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:37.931490",
    "title": "Context Engineering Guide | Prompt Engineering Guide"
  },
  "https://huggingface.co/vidore/colqwen2-v1.0": {
    "content_path": "data/contents/9489225576cdadcd.md",
    "source": "jina",
    "saved_at": "2025-12-30T15:02:43.014743",
    "title": "vidore/colqwen2-v1.0 · Hugging Face"
  }
}